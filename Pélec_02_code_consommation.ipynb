{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","pd.options.plotting.backend = 'plotly'\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","from sklearn import metrics\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, \\\n","                             GradientBoostingRegressor\n","\n","from Pélec_04_fonctions import *\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 17] File exists: './Figures/'\n","[Errno 17] File exists: './Tableaux/'\n"]}],"source":["write_data = True\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["BEBNum = pd.read_csv('BEBNum.csv')\n","\n","BEBNumM = BEBNum.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n","SiteEnergyUse = np.array(BEBNum['SiteEnergyUse(kBtu)']).reshape(-1, 1)\n","TotalGHGEmissions = np.array(BEBNum.TotalGHGEmissions).reshape(-1, 1)\n","\n","BEBNumM_train, BEBNumM_test, SiteEnergyUse_train, SiteEnergyUse_test = train_test_split(\n","    BEBNumM, SiteEnergyUse, test_size=.2)\n","\n","score = 'neg_root_mean_squared_error'\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Scaler moins sensible aux outlier d'après la doc\n","scaler = RobustScaler(quantile_range=(10, 90))\n"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Modèle de prédiction sur la consommation énergétique (SiteEnergyUse) avec les données numériques uniquement\n","## 1.1 Consommation énergétique brute"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.8836309836052285\n","rmse : 9861558.367917802\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLR=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4837440,2182976,1302784,1220352,5040576,1971136,6656064,840704,707200,8454208,262976,2851520,3842560,1292736,11015488,3399296,1383168,46912,854090112,3220032,2291136,1159232,989760,921408,4687104,3815360,617728,8018432,2298880,1084224,2721152,1567232,6736768,4332480,790528,5025216,2171392,21732160,1964736,748928,3266880,7385216,1385856,15954688,621184,1353024,1041856,254848,1027840,1131776,3840640,1376768,947264,2878720,4998976,12488768,1223552,6024576,1610880,857792,8055936,1394944,1375360,1357440,1344576,3605184,1964800,1730112,1978176,1438464,3128000,2099648,3606080,749824,4083456,947200,4477696,2180288,3162496,968512,2291328,5578240,1952704,3696448,1026560,1293888,3230912,371712,1670464,1536576,1701120,1210432,11398912,1375232,6260736,17654528,3738624,43890368,4807168,943232,19296960,16350784,2087168,1285376,1770880,3782656,3776064,8386944,818240,4347392,9798976,4566464,732032,20459968,3884416,-12800,11147456,7634432,732352,8366528,43498432,1743296,3179712,2281600,743872,2165184,1701952,9301120,1399744,3096640,880640,788096,2792768,990592,252480,3324992,4700864,1515840,4570816,2880320,286528,1096448,3587264,1112832,2237248,76288,5888512,1693696,1462208,1050112,3006464,846016,4862528,2535040,1216512,1135936,2257408,1361280,4936192,5126976,5784256,1867200,1158976,1768640,5780736,43112320,1049344,1259520,1745408,1144192,6717440,6184320,2202752,3251712,2588288,601664,1131200,3068608,3742720,1765760,20443584,1933504,1557504,5653632,10284096,2144896,3124160,19594944,1258432,4564480,3573056,1143104,2536960,1208960,2277824,4322752,1519488,1247872,2563776,534080,11632640,1126336,3172544,16840384,1100608,1426752,4023680,1257280,1165376,1687808,1445824,14014080,1980800,1622976,3801216,15815744,1382144,1080512,3308672,989696,1340288,3936704,4652736,1329280,1914688,18097216,2272320,12578496,1955968,4321088,164352,5928064,1804928,2307136,4628544,2133632,4563200,1131008,2702144,560000,1372928,4013312,872832,4872256,3492288,863872,26626432,1852352,5343872,1965568,15772352,2350336,1537600,4121664,1669952,2384576,1488512,864960,3816064,2684160,12791552,48986176,44507328,3202240,1181376,796672,6188416,3334144,953920,2472768,211392,5157056,8471424,2992128,1053248,5394944,1054976,1898816,1320768,1007616,1525824,1024064,6476800,1218560,2048320,3471040,4323264,2977600,19445888,17917632,8712960,931840,1078080,1638336,815936,1683072,1083264,1261568,534208,7371456,584896,1569728,1457856,1030784,637888,1031360,556032,271040,1416896,579136,2933760,2260672,770240,1586688,2529856,822720,860544,739456,3668864,3547648,3947776,7266688,1483648,2293632,2074112,3995776,3328192,4264320,251520,1012608,1170240,388352,5057728,1650240,89247104,600448,5529984,5195840,3351104,6085184,1785216,3561408,3940672,8745536,1352384,714880,2423360,362880,-192448,644736,11677760,74077440,927680,843392,1050880,1403136,4820480,4764352,1203968,2316800,2440960,1207680,1607936,3829120,12619776,24937536,1009280,10598976,1965888,1749696,1355584,2143680,2449728,570688,2166656,5314624,588672,3562688,19498240,1872704,1009024,4264384,8146816,12225216,3268480,1921728,1596544,1835264,2086144,2634176,1867456,3278208,1102400,7153088,1949056,2683584,1788352,2828736,8327552,827648,3990912,1619072,329280,1986560,1744192,2970624,12360704,1592448,1242048,6394816,3382144,972800,4422400,1662208,1876608,1402880,629376,3049856,5051136,4113152,1055744,-117184,1939200,5154624,2927040,4161856,6071104,2688640,2512384,1302656,1551616,9244224,4541184,2719552,4561664,4607744,7994304,4615936,1066688,2852160,1038272,9689152,1767552,3918144,4080768,1527424,677888,11702656,4201408,1355136,3041536,866048,1471488,389248,1031232,2649728,1750912,1837056,8481024,2340288,1783168,1542464,2434496,2662464,718720,6251584,7224320,1989184,2505344,4037888,2883904,2378880,27723136,641408,3792128,577536,2790144,2117440,423616,5596480,2530560,1019904,2371200,8040384,917696,2678528,1043200,15234688,2538944,1373952,12003392,1711488,7137792,2923584,5315648,3010880,1985216,2967104,947776,2360192,489600,2807680,5561088,2151680,1058432,1141056,1080448,14330304,4912896,965248,8505152,3799360,16011968,17102400,7011200,218816,64512,979968,1716352,886272,6231936,4502912,985472,3749248,6408832,3077632,3133568,1266688,111296,2790336,2334656,1086208,4677376,10469376,49914560,2532800,8144000,3587648,9622464,1693312,1412864,48535872,10480640,1388672,2072064,1312576,1258048,62592,15222400,1141568,1113088,420608,874496,10183552,1467456,7342016,1248000,12686656,1866432,788032,897728,2194688,989696,2459328,7756224,427968,3486528,784896,1768576,4605760,1106240,11838464,-312576,3693824,2083840,4717184,960320,859072,706368,2962880,28570816,1364992,2949056,14687232,2307904,9410816,1080192,3941888,960064,1989632,2293184,3156544,3131264,920832,1266176,448768,276416,1068032,1515648,2518464,2238912,3803392,7468800,645312,1718528,15103232,1004160,2658816,891392,1078400,22498048,3108480,3755200,1683712,2708032,938112,594240,1113792,996032,1190528,14962880,5633600,5128576,1579968,1452288,4987200,2059648,1650432,8705984,1999488,1108288,8061696,1134208,337664,412224,8380352,1376448,10104704,1071488,1992128,198400,1339648,6459328,4756800,2665408,7204160,1766208,968448,3602560,2700608,74582144,1518464,846016,909888,14362112,1550272,2762624,1327936,1111104,8911616,1030336,6790976,1485824,920512,2148800,4203904,7295488,13901440,1320832,2791360,2606976,7031808,1084672,23350976,8482112,698496,1660032,1933696,1244480,2940160,1624960,2637888,70450688,2554176,1106880,1318336,3597312,6012288,1525504,-41600,501312,1269824,1042560,3059904,3558016,1300096,4243136,638016,56925888,743552,8412288,1896256,3554560,2238656,1497600,-113664,1270912,214144,4012800,8010560,995840,8032512,980736,694592,1211776,31115584,6826432,6132864,59459968,2067328,1218304,1069376,2200448,1987264,38159104,4237568,5078144,1977856,5851584,1428672,3406080,46580736,7416896,2182912,4787392,4112448,749184,2159616,978688,2901824,1326016,1105856,959040,4402432,2778816,1793536,1261440,1452800,5607424,13872704,2001024,1072896,1025408,11826688,3983168,920640,4419008,1244416,21486336,8280512,4123712,1203456,1057728,1238912,424000,4053952,2304256,1104640,2956864,611008,1896320,2126528,2950720,2014208,6263872,2058176,1164864,2347904,2290688,1113920,1986176,4036672,11524288,1317504,3156672,2696256,227776,1954112,835712,2559744,922624,1670784,208768,22143744,1393408,2344576,2123840,2019712,1064576,1284160,865088,3169856,1642176,1914688,751488,1837248,7029504,1086144,1234368,7936960,8726400,4772992,972032,1183680,818240,1989696,1355264,2106624,1024320,14303168,1224128,37223360,51544768,838144,1669952,710272,8624448,1743040,926336,1151104,5478336,1783168,975168,1756800,1709120,1320064,4797056,2317632,1577024,1045184,5494528,18714560,2945088,7127488,908864,1059136,3470080,830016,2643584,12914240,863808,10279872,2042240,1987712,2358336,4947840,1492608,24428032,1093376,672128,989504,7782400,5575616,2529152,1105024,2547008,1383936,4485760,8084224,136765120,3108864,2300288,93632,2366592,3441280,87492160,7228224,1389952,12807168,1953792,1302528,1853696,2031936,3840064,7573504,931904,53715648,464960,1939008,1309184,1281408,58688,19729408,1534208,5825728,11756352,-2048,3121792,15356224,3590464,7760128,7680960,5985920,66627648,21748544,10710784,1244288,3118912,3898176,2159168,3771584,1591744,4611648,14046592,875648,1538496,1291200,2153792,232064,1377728,7353024,2126528,3109120,16821504,810944,8920640,5380352,2485568,1994624,1773504,848768,6968832,12771200,11669376,977984,1177984,4129728,8574656,1543936,1081600,984000,13326912,3582464,1406720,1678592,1786304,4257728,5520960,1133696,585920,1481344,3864064,10379584,465024,-148608,4068480,1954624,2277120,1026112,69051904,144960,1076352,1141376,202368,18962176,1076544,5174272,924672,21045120,4483072,2147520,973696,1210176,2952000,2960832,6050304,2119168,7192896,28184640,-142272,2012800,4201920,1176064,5329152,555264,1323328,9984704,3068672,1721152,1565376,544256,1963904,1187264,1174784,922176,10052096,-157568,605696,2378624,2993152,1079232,4416576,16723264,2085248,8811264,406272,21949376,2003968,446528,2014464,1862656,1701504,1251520,857792,7824576,3266432,2579776,847488,2225728,5503872,1779200,5495104,792832,2450624,4528128,1325120,1372288,1762432,5151232,1201984,1492288,5116864,2806784,1775168,5488832,20735872,4061952,2352576,1079296,13291840,1099264,67980288,1272832,40394944,1011648,1491648,3313216,1099072,4798272,7283264,2806976,4227968,534208,30398528,5186304,1416640,2616768,1839296,1271616,1389504,112576,4064768,17012864,878720,810944,1247168,2053248,3310336,5580736,1255424,867264,14775488,1079552,2259072,36521472,-24576,6049280,5016896,595456,-13824,702592,2362112,8723328,1681728,1069184,1010688,313280,8464768,12256064,4092800,3851712,7371136,3256704,17093376,19181888,125568,140271296,3195392,1447040,917888,3486784,4745024,2668800,1659712,2906496,753600,450240,958528,53862592,2928512,6888192,1250112,1318976,776000,3569856,1130432,18563520,5965824,9381120,2682176,3412352,2962496,5013440,2444032,6607680,2113216,1566912,3886848,349760,954752,18109504,755328,6321984,9999040,4134720,1730752,3321856,-553920,2389824,4428096,14705088,761024,3161536,40911616,5716288,6188480,2381248,3059968,79788608,5895488,2509632,2798848,1990848,7831296,11066368,87447232,2790592,942464,1087040,911232,851584,19173696,5138752,15038656,1670400,3058304,1300480,945984,2461824,24898304,4688448,14187520,2524160,574016,6440960,731776,4076096,894272,2423232,2967168,977792,3871040,9401728,1464576,4118144,4346496,2097024,13710080,2843904,999872,12535744,1779328,1582336,1952960,2305088,1557952,2217344,9304576,395328,11516096,1609344,6424896,569280,1285504,2508416,951296,5621952,1508480,32372480,4374592,2336384,1087296,1109632,1879680,7899264,2772032,852032,5814272,74304,27851200,1157568,5594560,18735040,979584,20958848,4514176,542464,1529856,4011072,5108416,385216,45610112,8028672,1199296,4803200,863360,586048,1690368,2997056,25481472,4795904,1537920,1301184,16242560,-314240,1175616,1296768,7398208,9918336,1908032,4546048,30823360,1842816,250624,1310016,1745920,1134144,1865472,11260096,1339584,1041472,668352,2269824,1091072,711616,778304,11609024,2007040,5225536,3208384,5748480,4627712,9363392,1698304,3949568,7397056,1879104,1909376,1070400,1140800,58432,4540608,12716096,1755072,9567616,1157696,4086656,2716352,3050304,2540096,3059712,6043712,3873280,858944,409088,1479616,2230400,6261184,1815552,3057088,2176960,3336576,3886080,2617856,1120000,1477760,1692672,1342528,6157696],"xaxis":"x","y":[2450488,1188444,1205617,579888.3125,3637973,2228675.5,5662234,783104.125,620104,21069722,3537977,2918553.25,5448975,770333,5907068,6388260,369291.5,1784385.25,873923712,1889990,1275236.125,646505.1875,1058986,533370.625,1384693.5,2196718,1295685.75,5702077,378714.4063,727374.6875,2989919.75,1066901,4155007.75,5253241,533329,1271021,1057256,9341427,2659770,1054995,3874632,13951571,1344512,6508595,1286817.375,398357,729535,5117308.5,1860901,827618,2017423,242805,1212883.75,1079477,4547129,8819864,499887,620483,4735249,478982,7046345.5,1191803,811703.6875,792734.8125,4420650.5,1847401,1073156.375,727703.5,729096.875,858897,4897760,1030150,1784796.5,870633.5,1614322,552015.625,6345009.5,2147013,4235405,2788860,1691963,4946798.5,7482832,4024810,1093653,630040,1582655.875,1835804,722062.875,994194.875,1964681.25,969682.8125,6695413,764906,3291518,13158205,3344833,44984468,12704014,524144.6875,25377650,6962255,872114.5,466672.6875,1381407,2919980.75,3481727.5,2992693,464155.1875,2372126.25,5965171,1811506.375,670779,15552193,5337334,896403,4039667,4784731,749207,8381834.5,25970248,900973,1436773.875,1200381,2225265.5,5348309,966812.125,9791557,777094,5471735,662424,939413.625,17924416,454296,452744,11441,4188660.5,1686262.25,1767480,342726.0938,1987334.75,666385,8611054,410433.0938,1501111.75,1210229,4268054,914670,1509492,1185469,10750010,572654.3125,9290214,4145920.5,1332591.5,549509,1134195.5,1682454.875,2364351,7475577.5,3418139,1035592.813,567608.625,6015128,7380088,102673696,2177075,875789,1972234,618876.1875,6010407,1189402,2916511,3757006,974305.625,505601,1325597.625,4189569.25,3229864.5,1508561.875,15878689,5528223,1605522,1747090.375,19490284,0,14585813,5696695,541950.875,5968153.5,1346952,612158.1875,1076084,731092,2804534,690529.5,2190460,1829122.375,7240274,947884,9162048,2470566,690557.6875,25959642,703847.375,207572,3155391,948975,666687.5,1247361,986353.125,53166156,1419980,1171380,888308,15590743,830945,1074350.375,3805877,2108012,659450,3693616,4696639,2433777,804620.5,5258024.5,2420605.5,13171370,761504.875,4054994.25,858932,3267564,776237.375,12095818,3556291,887403,6797528,2055534,1983256,566183,1989287,1045413,1169932.75,2964206,3782378.25,802312.5,10062436,1034941.313,12086616,1138520,14168729,2173143.75,1441631,1742351.25,1222949.25,2478960,1913558.375,473874.0938,3518459.5,2920918,19645206,36667044,98960776,13010578,2280352.5,1228041,4961405,3157579,809291.8125,2288635,6293359,5454482,7739699,2077039.375,3577120,1574750.875,708967,2458760.5,748335.3125,686949,1465795.75,1244150,2557533,1686405,2186148.5,1194835.5,3744483,4476997,8355988.5,6914839.5,1738724,90558.70313,3087318,965425.1875,913899,2292169,490743,601590,1117658.75,1082531,326778.6875,1821047,230365,809870.8125,387810,678608,1297872,684234.875,1114384.625,742630,2454812,10420096,694241,1966424,3663534.5,515025,592534,600060,2001643.75,1866800,2360288.5,6859198,955641,4358944.5,1034827.313,6596057.5,1784233,1886382,1100575,1632731,835085,738990,4385512,1227772,77164568,549834,6609358,1789710,1375366,5104947,1883864,2137686,2208196,3004271,866096.6875,1679127,498745,3717064,1446455,710825.1875,8259071,16644664,924438.125,542882,711118.375,219483,1041652.813,0,318205,905750.375,1024033,1696448,694479,5457975,5885427.5,13093350,1270139,10213132,1200811,1076644.75,581589,1967637,2912264,967697,4700396,7369591,489557.0938,1529510.25,8320719,1130960,807358,3837228,4988493.5,12719042,2996958,5238803,958725,1529309,1256575.875,11319836,5767765,6036588,683274,16371764,2126288,920068.875,942089,1497496.5,1358022,529807,2912140,1017787.125,685888.1875,5252131,1817550,7338511,17109052,961476.1875,688641.8125,2211671,3864699,647906.1875,1938887,4547379,1750419,648634.875,1739693.25,2962476,3893073.5,2338238,809257,719471.1875,934357,408513,3258819.5,1911591,2890253,468396.9063,1868497.125,1072487.875,2151376.5,8168547,2472350.25,1417778,2337651.25,5917000,5653656,3059758,2266983,2676594.75,1379739,7637986,1128179.125,1472548,3371455,3091559,7003615,4059424.75,1318096,2156407,1765813.625,455798.9063,641713,1027394.313,648879.3125,1947717,896485,2248776,3404992,2158629,1380293.625,7067404,32579658,1489363.875,1359579,3017709.25,10154608,3546459,892227,2439484.75,3040924.25,317581,32381836,1401315.875,1921222,1494727,1703504.875,2563985.5,1762174,4078339,6454983,684155,10403123,3240830,508014.5938,1022957.688,654625.5,20311228,1481698,1093757.75,18661606,293217,922041,8381744,2127534,2694853,362874,9586556,609494,1133028.875,502667.6875,1457118,2265836,962238,815800,1259456,999924.3125,10317852,4022735,877788,6578370,2477499,51168308,11788719,12783255,1677877,989079.5,1431784,928409,1521202,4685346,2148565,538740,1889761.125,3605749.75,1443367.75,4206839.5,578854.125,739662,3740373.75,1385697,608755.375,1634178.25,8696206,137635696,1364344,3590731,3605855.5,5357833,1176342.25,716940.625,47148456,6320060,1045640,1275231.75,783345.6875,1929011,2307938,22055140,759060.8125,924121,2361088,1493131.75,5261096,755618.1875,2735128,688375.1875,8493420,1858224.625,551219,562617,10303305,657385.8125,1645748.25,4760734,1612105.75,3366528,1245923.75,1093743,2468322.5,816510.875,7682562.5,1607972.25,2512443,2069880,1634334,1334125.125,625938,3158151,405062,40062888,287132.5,1192100,4934559,956481,7520554,818317,2003474,827211,1967129,2429021,1006989.688,2616226,454124.3125,872555,268269,1085439,1055624,3191228.25,53401,1012341,2140476,3265995,811900,713326.875,6935723,204499.0938,12525174,2189735,1585441,14361382,1750362.25,1824054.5,1137513.625,1150926,593824,555864,1761137.5,497991,2470642,4321141.5,2274873.5,12731388,1898509.75,1245753,4486477,990361.5,4560492,5707529.5,3643112,1198630,3151107.75,966376,1104074.625,895310.5,6750112,2225134,3775187.5,431390.4063,976537,5415438,306721,2188452,2234448,3928695.25,4983934,2023197,2028125.625,1593610.125,3436032,85357952,591249,505949.0938,833747,25476332,1186682,1307883,965786,876871,1888344.25,758875,8651835,2442220,3863772,3947209.25,2014500.375,4424263,6127771.5,522791.0938,2093011,422133,2646130,680395.875,35383012,6313678,681354.1875,841242,1507168,784584,2025533,7439009,2090543.75,37980712,3239754,730211.1875,599390,3895403,861972.875,162960,580688,930439.375,1453543,642052,1482997,819354.625,4294550,2024367.75,1525624,157606480,869699,5903033,8926736,3289053,1820292,2277316,1844825,94186,869835,2491238.25,4407932,637685,9155568,4912520,3999505.25,6983736,18848780,3303036,6361236.5,25397086,862541,752441,1551204,2056762,802755,136241424,13962749,3312125,2114860.25,3568237,1109924,3542542,46510960,17044842,10960107,2471623,3132453,597761,1107290.625,658297.5,9029489,1015172,2701872,493969.6875,1886007,3226884,1216543,1469556.25,789209.8125,2081072.625,6884914,1623657,393493,645927,5672838.5,2401890,624993,3485865.5,3005926,8024064.5,9373179,1872413,145573,1834570,714898,876237,2015117.25,958242.875,1976856,10077532,462465.5,694189,1056099,1418915.875,2485521,2756494,1271753.25,1176615,1096646.25,1943933.625,534039,849504,2322314,0,2466100,3629065.75,1188994.625,1822834.25,1169948.875,2122581.25,2400507.25,7175646,3996541.5,350264.5938,14709151,2067352.5,415586,1559199.25,832168,1207869.875,1041304,577033.875,1677881,796042.375,813641,1085183,871355.5,5169726,661383,828531,6102883,2227640,2890379,507603.3125,602191.1875,466904,2767381,847419.375,1618695,436848,4425832.5,439065.9063,28450436,72587024,781682.5,5876439,642133,2407904.5,6058494,611536,1926134.875,2556131,1639849.25,696522,1054581.375,872326.375,677536.3125,3751417,818836.125,1334876,1287159.25,2334260,16163194,1172785,2887392,474468.5938,1153690.125,1228963.75,447947,1332912.625,0,443365,8504904,929688,3573328.25,1293336,2075833,1059605.625,1507270,1419453,586827.5,1360340,13659926,1971236,1364214.75,609508,1790148.625,2245568.25,2855545,3944933.5,286685536,4767227,815115.5,1261634.5,1845150.625,1698074.625,61576184,1226435,716071.625,5249190,1128208.125,863703,2301043.75,1080203,2376794,11325513,941564.8125,41655852,387279,1734533,1540235,570982,1689631,8909924,700899,686019.1875,8580279,938908,4167253.75,4222682,6118300.5,7323351,3185005.25,7256842,53079164,9569715,3644982.25,238255,1614026.25,1655612,1570758.25,11594121,1929118,3021669,6814269,841841,1258619.125,803938,938096.1875,961749,2058532.875,2220053.25,1313366,1297633.75,65336980,541542.6875,3608955.75,1983509,3218824,363808.1875,770175.8125,592221,3791402,23364636,49102164,1625308.625,497528,4202335,9986032,709990.8125,1423463,567521,8748273,3373582,846389.3125,1000804.625,2998208.25,2153603.5,2660396,1289073,1247972,4253369.5,2105451,13586070,935580.3125,1082920.125,3321035.75,3288836,3560614.25,512166,51625548,2615973.5,1251906,555153.125,810636.875,48729588,548292.6875,24473814,785052.875,13300859,571309,3570161.75,522972.1875,2473302,2098850,6881405.5,2306723.5,2876857,5089903,10564729,5039523,1730886,1633399.375,1696978.75,1739286.75,395346.3125,349139,6040837,945020.1875,1764767.25,914023,1232822.875,1625695.75,2545405,5807879,1392015,6283226,2662054.25,4133811,1582794.5,1259959,964015,6424029,12448381,873711.125,13911761,1475475.5,10498344,1788148,618366,776991,1331122.125,5903142,634751,520405,11026945,5981430,2203026.5,2166711,1427974.625,2654755,1050982,2581741.75,1639136.25,1402048.5,3165109,729397.1875,937635,868859.8125,4409881.5,705262.125,3542708.5,2729167.75,2628864,813038,2773779,21781324,2161828,1199392,832986.875,602245,1074989,35685224,342493,94178648,1136676,741287.1875,882896.5,1505709,0,1803753.25,3220635.25,8201974,2035595,12265516,3306123,313528.0938,241478.5938,2465266,1270582.125,1982967,824516,533078,19344782,1172065,511180.6875,255971.2031,758008.875,1911611,2503948,636641,677279,7838600,499928,2921368,22591288,964923.8125,2278160,17186624,591622.8125,919746.3125,1717130.875,1815350,3024478.5,5975230,860364.1875,600543,3275779.25,7261732,8921778,1976400,5572503,6159684,2960377.25,13270687,7370743,1323038.75,291614432,1425599,689803,439266.9063,3020001.5,2168701.5,1234095.5,2219744,3045139,638201,1943772,855393,41213584,3674705,2712983.75,1201630.125,648022,556700.6875,816846,196410,9977330,7419765,25453690,1120197,2483320,1358226,1441665.375,33268410,5535626,6471427,1437569,2128963.5,1581425.75,1231546,5037781,1581036.75,5392617.5,4180298,4817062,687953.6875,3743228.5,578789,1519952.125,2355732,7969653,431357,5094197,65047284,10513817,2599171.75,842593,10105379,45074876,2391566.25,1004253,1827424.125,652991.6875,3534690.75,7657568,64715652,1073507,9671619,1540258.5,8163413,520206.6875,6744317,6577130,63668488,695346,2666634,676032.625,571644,1103594.75,16614404,2885740.5,9851937,1743198,572082,40613740,861007.8125,47102428,529033.5,41680640,1398253,987989,4590246.5,7976729,786346,9146727,1688612.5,801264,18628834,1108310.875,500237,17673018,978211,1921890,2926089.5,8839812,1634669.375,1115020,5121586,804002,16246106,3612526.75,10340282,881386.1875,920430.625,1183623,323739.4063,2993032.25,788055,19150024,1929828,2248134.5,669117,495152.6875,941257.3125,21795830,1513700.625,445943.8125,2843033.5,460010,16020642,812013,5838129,18967014,1038351.875,20657852,1808401.375,588492.5,1380283,2345752.5,14833476,1541879,19827916,4396566,899266.125,8354235,466985,1280263.75,1011296,1325972.75,11821841,5481549,537141,1037696,17750994,792015,697397.875,1736775,3616057,8070420,9159185,3730953.5,18576900,1356972.25,656980,1487769.5,706961,1916309,4038090,4027134.75,868163,3767135,517029,1208168.75,864765,783208.625,535649.875,14751117,1896858,1757601.25,1116705,1920352.125,2755578,4828599,873648,13440924,1730431,3441643.25,2178074.5,690750,573395,936616.5,2762846,6821833,897514,18074716,228212,2549892,22530168,19991412,3215953,3245111.5,6057734,3234787.75,1928391.25,416566.1875,741850,1045337,11963428,3035962.25,1855556,5040382,2096835.5,3002337,797170.3125,892455.875,1751483.875,8054334,686055,1381590.125],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données de consommation prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["#modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBNumM_train, SiteEnergyUse_train)\n","\n","SiteEnergyUse_predLR = pipeLR.predict(BEBNumM_test)\n","\n","LRr2 = metrics.r2_score(SiteEnergyUse_test, SiteEnergyUse_predLR)\n","print(\"r2 :\", LRr2)\n","LRrmse = metrics.mean_squared_error(SiteEnergyUse_test,\n","                                    SiteEnergyUse_predLR,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=SiteEnergyUse_predLR.squeeze(),\n","    y=SiteEnergyUse_test.squeeze(),\n","    labels={\n","        'x': f'{SiteEnergyUse_predLR=}'.partition('=')[0],\n","        'y': f'{SiteEnergyUse_test=}'.partition('=')[0]\n","    },\n","    title=\n","    'Visualisation des données de consommation prédites par le modèle de régression linéaire<br>vs les données test'\n",")\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.2 Modèle Ridge"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre    Ridge()\n","0  ridge__alpha  812.66192\n","               R²          RMSE           MAE\n","Ridge()  0.855511  1.098865e+07  3.289552e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRidge=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4301260.835316408,2591039.2730359077,1377569.8957538642,1146913.60331601,5086704.96134838,1928068.8525555418,6164949.19126551,1092381.8790082776,1585191.2952288706,6951783.522917789,706602.6516886584,3686567.235239975,3883380.7178364014,1129777.7472552664,11204680.315313358,5306663.736442735,1462952.3462632939,701158.8763477548,740294750.4079834,3024808.011924502,3044016.287266159,1355922.692010343,1120002.085757383,1072196.1758564762,4187655.111263077,3297941.5464449623,946473.2774886591,7747044.84345718,2038326.385167146,1293892.4740563997,2647833.400854512,1776870.1288152798,6054098.007232322,8362576.931304511,1186395.887812336,3314135.578050833,2164690.462366156,23922234.807672814,2316919.2019018764,1452360.2725511233,3002481.2435801644,7644459.353791114,1533591.2626285385,16917653.766456824,1305190.8922874006,1246501.1990181678,1161830.9004293007,407080.7736404103,1237668.573135262,1410091.1501808092,3563556.429162003,1297855.1643818598,1307917.0014827347,2681396.013696531,4869273.387736522,11963585.88436642,1139262.517131689,5109056.28490057,2400926.1973508587,1096926.5644894778,8521257.465223417,1281125.976045567,1946855.7639168797,2012553.0898948475,1335877.5718159426,3118913.809117371,2041436.8830764573,1828333.617444897,2033138.472655545,1403013.2003851333,3599181.4768692777,1942970.763858712,4196753.355138505,1453166.1882503657,3764716.5710013458,1094064.4560843073,4010128.919136924,1930305.4999884213,3377638.308849572,1043018.0825689808,2412501.9707076573,6205067.778677706,1742328.2600837331,3325919.493250987,1747600.7639116086,1197017.2059385246,2811137.5361662605,750777.8034441806,1778849.6243652538,1737334.1586892335,1667606.427957186,1311205.264243988,9641957.866862211,1338134.1492424412,6911332.953930712,18068465.54581722,5025874.018309668,37325467.62506866,3876885.2398091154,1416733.5592256924,17134604.93044019,15862605.778248427,2036643.4064172274,1200743.216184517,2439650.2768688835,5193817.955266673,2799032.9145265296,7362853.018384788,986825.7741978285,4166136.0792223583,10818133.86220844,3154605.9818679243,1276052.9384608162,19769604.047421224,4491092.427636713,628179.6724212337,10404182.163713535,9116019.929121634,930905.157144458,7811406.023794729,36158405.37814417,1752215.5821719128,2710067.148671563,2274379.255346313,1065036.7113503893,3394229.6281029745,2474573.7771556857,8390194.204270493,1544735.6746677216,3526058.4126114855,1115991.2190470737,1049486.406253686,3562057.1304149455,1206898.7814044354,600527.1056497428,2877089.8888266934,4458554.236627841,1641784.1560948477,3527442.5080429036,3074542.1365543823,759993.7479937999,1380863.352568612,3094573.8767614807,1143969.3946883746,2693569.534881377,778783.0778341978,8004550.485267785,1701108.2980989532,1509830.936747188,1255822.0493500317,3972262.275089337,1096727.4987971415,4245129.839176778,3877377.0520191174,2357961.855959459,1249841.1831122572,2437700.85352264,1926323.1278654495,4296635.25463441,4345640.544863636,5635350.421149825,1854758.1910607484,1272704.0126262875,1650179.0060053817,7323202.131457206,38251184.462281786,1726533.8519464945,1429086.1157705386,2103606.0394938285,1587206.8051413468,6203449.155233368,5424122.512821805,2038050.280236294,3867034.0743068685,2471630.8928090334,905893.9166549444,1726072.139632164,3160005.426178752,4694146.358946092,1996994.0625100387,19769825.525579423,2046633.1322193285,1415349.5550080552,5333635.497275972,9218770.908572435,2711795.7427992895,2720939.1452275496,14035574.62306073,1438068.924148221,4046512.807787955,3515893.9894202347,1271697.9500928763,2485562.358956745,1126926.564182656,2281734.109521557,3798301.9979831157,2104884.7031846503,1429868.3190627843,2463390.5440477044,845447.4762405527,14065258.847929548,1119358.734718572,2847732.7144900267,11157044.39701745,1433789.4549114835,1393901.4169568704,4495197.624316007,1167579.13640106,1100793.9610406486,2278860.77574195,2033712.1870684507,12210016.99815045,2498976.4615990273,1691063.8503349125,3517018.162432448,19863338.339874156,1750178.3360773115,1117194.7695376328,4057423.0293920077,1411476.368164931,1532691.8582398104,4074487.926048589,5183236.229764533,1587015.3209122643,1893658.4972352555,15538623.066679051,2536668.438008038,13452842.721087033,2401669.019903599,4057846.4393834304,793044.7679123061,4985898.384766686,1890153.5634735732,2629810.196324024,4909153.356200056,2151930.904198811,6386073.485817886,1322618.4309433743,2999307.9093246786,1045276.289159738,1549576.1060899058,3532997.712704222,1765760.4265822954,4676594.615223529,4107560.4921956225,1024884.992671265,25113338.75025954,2015895.7731097124,4460945.998820424,2013011.2522386792,18346805.839841947,2128285.6606054576,1626370.8568891764,4307948.212502686,2030200.4303968777,2369433.371800202,1514016.070744036,1026146.6242228705,3768929.6255651014,2433766.178046936,9872270.536823288,43449547.876608856,36953200.62505436,2775598.6395641956,1373935.9869416016,1011024.944740939,4731974.835667063,5047369.479435669,1013110.1665301681,2442327.6392263053,659259.7281648931,6898566.993783492,10158405.581558881,2785211.460369903,1171994.0434839018,4728134.858040083,1259817.215930116,3215647.698225787,1402883.8236606643,1133934.825010465,1626592.3350473754,1157146.0336645888,5581413.390429059,1231442.5155996985,1829628.1929022765,3199270.9386113947,5077393.009782229,3183288.786929137,16249335.833695285,16061436.732818477,7593558.058199177,1549913.4135461939,1018986.9758786187,1968249.070969661,1082418.498340326,2709206.514266029,1109613.773995068,1343926.3877110472,926225.5090618981,6484583.924337824,834605.1990545201,1425722.9699879237,1472486.6828751145,1116965.0897545712,664409.625468611,1153279.8421350848,951639.7767000508,1016654.7973863576,1258403.5786964614,889565.2386761284,2813586.5623742095,2858265.0275311647,1111084.472450274,2199659.4331540978,3263587.7828856367,1068329.5826530834,1099169.4650256652,1091863.1314348998,3527583.474950865,3117907.1296753786,4128895.6197044835,6866477.38935928,1700827.132184152,4027328.9676078632,2025849.4475868237,4134944.53861933,3328371.6194788963,5223945.322516078,731643.2337648713,1051158.1787706735,1421806.7117299018,810809.0116863903,4653849.592445182,1697090.656475509,84803012.83445886,945726.0845852601,5059946.800783672,4510902.3464821,3200394.7423593774,6037904.658943383,1603542.9281225621,3207635.972287191,3732824.9273285363,7791726.963889303,1515729.3552137234,2017646.4815101153,2352335.848864606,934454.9999014335,768473.7657461234,1034647.8007733184,11019589.837196615,61088878.091853596,1251417.1434784546,897267.0826836298,1091405.1396627224,1287784.5870123743,4557382.041503617,4250666.598201603,1123281.850811351,2313290.7677537515,2492851.384025466,1295183.3945085746,1809887.614722474,3380901.0200939877,10836452.986550877,23458100.666612912,1302315.725744966,10203880.124617497,2013151.4335221911,1757472.380303603,1508498.8128799377,3129536.192154533,2179497.7780136163,1044004.2494872273,2002801.6337585375,4871414.62049196,945947.5627434594,3088458.4344342505,17400810.40957758,2365601.7212648396,1467558.9417125848,3914149.8192248577,7510366.647761354,14255351.24536413,3238325.964829521,2046854.6103775273,1794328.0482864806,1645176.769325548,2257964.8150468795,3196231.59625749,1758302.6634456308,2911644.1706295535,1048646.5235742824,6130346.624251442,1738817.8667113883,2615391.0212973454,1750600.7853996437,2650076.703064589,7014579.815000681,991519.2384253684,5770459.788925168,2082926.6177949067,1118963.6059522866,3445413.300958173,1569899.4200797454,3060039.7913678936,9057536.85141036,1723178.5361022034,1482058.5082282035,5589667.012072783,3184366.057262754,1201793.3707891882,4045079.13802482,2486907.9726791484,1654666.0743653395,1557503.377700221,1658167.254550759,3923530.220839683,5126647.286824089,4071706.4835698595,1318431.2786866408,278427.125098235,1991283.1532943603,4466947.577198473,4324047.147845812,3756853.6978948163,5008583.60823248,2360743.3372441414,2623972.287920368,1821463.1278572662,2094265.0262800734,9768536.921726042,3736129.5720445574,3777242.4831213,4135547.9946600953,4185615.6336188456,7288710.753069764,4080932.8417129014,1192752.0796765701,2842895.255187205,1438614.1504576146,9446400.352921557,1729647.4679416725,3340512.9162749522,4301115.830004936,2414577.6552489097,1037307.399026301,11796646.370447889,3601827.124582374,1362696.5403259525,3723552.8557691155,1113479.162176794,1666548.5948451897,990872.0229281792,1685246.3112615487,2665414.544028415,1748419.4812841646,1629558.9364343286,7966551.16581705,3266119.1129676467,2618182.575685995,1407993.3447876833,3208956.25642391,2425927.23240321,1289970.805657053,5226590.574745006,6891679.167845506,2124277.3413629145,2460270.878440674,3563498.3481009565,3528953.4198055617,2094527.8736224212,29216482.711520866,978084.6412407551,3255314.4391472647,1554847.655908811,3311869.329132388,1975104.2246294636,938845.6105196716,5882150.8417072445,2306761.935185302,1057536.4271704573,2297031.4824861963,7355832.310874477,1156024.181722606,2612507.7174095544,1260038.6940883151,13508014.271441296,2487174.44371713,1438867.492856432,12665222.848910611,1542774.3417202241,6204712.39348053,2805175.6853635055,4921538.561022853,3050912.672011803,1778991.0106704324,5068831.226948412,1084731.1944978456,2435948.9891526327,542662.1807386675,2898255.196346812,4996851.16866069,2122749.98334191,1396188.3272621413,1321708.3322879332,1203966.582357509,12499130.779028993,5057535.646471119,1564118.4249172115,8695695.748319201,3956530.1857179813,13811449.472530939,13917198.76992634,6705997.851601279,894142.9265669859,779004.5559923968,1062615.3251684867,2075368.5076962728,1033284.6634490378,6432268.09793965,5560579.108294273,1079622.0229599294,3955501.3723056377,7874265.398133805,3429815.62049333,4441562.084150145,1444797.6257567846,1404500.129237111,2428126.88964791,2414851.705984116,1121820.7518935204,4316018.03685804,12474982.102678837,41623206.9188587,2484599.425414807,11118555.035776615,3053136.6207496454,9157486.657784462,1523956.9504370755,1764699.7129394875,49949138.05278226,10309508.116696522,1807666.4956438476,1937675.8219318297,2096225.5624637625,1167789.4083263273,845164.674819971,13508235.749599496,1254221.5724964668,1308039.5774581537,1721315.8669343144,1033506.1416072368,9328859.756747635,1477675.69276,6546613.740617879,1448095.616573901,11252453.67187117,1790782.372631502,1039592.4195331773,956586.4864575297,2235888.679718481,1017646.8772749102,2887043.667860264,7952204.209776456,814750.4851315753,3921315.2543562073,1011246.422899138,2029863.461839559,4897627.104185581,1362252.3506976771,12630069.433751125,472702.1711074454,3529001.3411024706,1937454.343773631,4243255.085933674,1090756.6377129615,1184819.8275774478,1258366.3828768935,2604597.9408446606,35106457.36173623,1298076.6425400588,2505560.0942031858,9919202.812556818,2227889.308151522,11465840.479677865,1020669.1512807594,3478825.913082475,1181175.5158800315,2116814.849572356,2735756.2553445725,3144769.6140524866,4614878.2090315,1072126.0852147206,1260869.378904232,746986.077772235,874421.019030092,1270470.993477008,2414799.1334071085,2383510.786234089,2524904.368979851,4655042.590340187,6477154.717985591,1292501.4730479638,2041722.9714126207,12967477.250589484,1054113.1919573513,2897223.8428362133,2150501.6840491886,1045386.2441210274,19349956.264941067,3522470.504147515,4390506.716819081,1871705.2375847846,2713706.996198208,1076390.4081288972,939169.9319413959,1058319.032136592,1037630.6849117908,1288971.1700669916,14203344.300420698,4883515.404289229,7343857.247108943,2252562.71295789,1845313.3950190893,4869494.865894721,2217652.5540537443,2487129.4508373477,8676964.335339408,1781012.4330484213,1043588.7911349284,7236386.550696423,2209099.601386321,1133523.8620466692,907181.6111168531,10074322.971244222,1568742.740492405,9106402.238118842,1109835.2521532667,1947827.3570796107,2632230.7577570104,4502146.00149308,5729076.896851317,4234674.725648332,2993762.390757748,6605348.895061519,2319960.8461889355,1302635.071522691,3344274.926179135,3133890.8080045963,62894586.300533295,1362424.8342527798,1097148.0426476765,1063224.573711725,16866967.44189845,1495673.0287863915,2298335.565680832,1398877.450847648,1219585.5892361365,10101159.415807154,1228567.225537991,8074768.326394235,1365876.7681613434,1873284.7226840032,1513203.0021627967,3961051.263118576,7289178.808075523,11722960.22682688,1532671.2221007189,2673833.691513866,2287419.3043238577,5999198.582104059,1381084.8307268112,20279494.929964162,8854796.10184718,1071796.307922264,1586440.4098602412,1813323.8172239878,1416820.2534632618,2742245.896973556,2223955.328707719,2665636.0221866136,67955031.4977059,2943193.356844297,1437749.0044577487,2576364.3003093987,2345509.404112312,5109277.7630587695,1399099.732353625,847389.1909500365,1674861.4172012103,1325104.4897340275,1116743.6115963722,3681849.823248321,2954048.6903551444,1443815.6281102328,4034575.8736358597,1438585.6741862237,44752522.256701194,1207415.9095698611,13646190.162863895,1965501.003361132,3067035.527985097,1845150.21088323,1365655.2900031442,618984.2459685751,1237411.2489809468,653497.4162118305,4053603.8751056655,8404338.46799869,1134156.3031686642,12300082.606343172,1510112.6181268883,1258587.8610350927,2438709.1792978784,31117199.508904964,5860637.83477486,5642595.447809729,47056687.397124015,2010418.300167601,1323746.0862491995,2333577.1262191855,3083315.486139961,2117305.725497523,30286482.71806141,4033527.0087549225,6288333.1033235025,2117036.3277305546,7980692.194833304,1258182.1005382626,3307242.2856507027,50832568.11540837,11109752.271743318,2236110.1578766797,4779392.436200398,4149602.8585349205,1120050.6143471934,2164911.940524355,1353827.2781891767,3166134.9872501194,1591620.0076479672,1225133.9561677813,1190158.3242577142,3741151.024551497,3694571.416056731,1736191.6295461175,1354100.7369985795,1598856.8563361836,5465561.861762286,12998737.471554717,1530260.0055146022,1187974.7098042397,1148443.58785393,10269252.18179192,3681659.160520641,1061741.4640019205,3502517.094182183,1713838.003506965,19911928.035100043,6969302.963253939,3973796.11563201,1122440.7631102805,1175638.756855207,1238230.1016170867,906960.1329586543,5037306.791897041,2476390.88777188,1800206.1920842582,5069052.70510661,927842.209016515,1757453.8038844094,2232674.9218276823,3101590.6847296427,2130983.2191051953,5226369.096586807,2053244.7705125518,1264233.3280346247,2078908.866743794,1967397.1071663268,1135056.676952448,2203306.5412580846,4282833.058275694,12829915.280687131,1587236.7990704633,4559059.296196978,2713928.4743564073,1199093.3605815032,2013372.9116803901,1700805.3904051448,2557738.2617719155,2092323.0799809592,2709427.9924242278,645600.6097426717,19054187.405437376,1932653.2290019533,2066141.1637112945,2408194.9353645,1970957.2688590472,1491376.8707427692,2910205.1747949296,1264093.8247464742,3257008.8873857856,1842211.1628386702,1893658.4972352555,1139872.268423734,2125386.3983025104,7017196.155968865,1353794.3002173505,1366700.3605349197,6447802.036944908,7343195.4908688795,3335648.038268528,1201583.0988639207,1376669.5219700802,1117281.3027208983,1965739.005997173,1562503.5660522713,1869186.4603773039,1208393.250953528,11766706.464175578,1149577.047702733,32827874.840143032,50200159.8246,1132230.8220169358,1726547.8676941553,1071574.829764065,10189876.836165808,1647048.6985462522,1045754.5050654674,2089304.6818293862,4754828.9860254545,1880414.031455073,1275976.5329707772,2030084.939997758,1724179.3254695653,1408749.2877142977,5780938.532833604,2314061.764813066,1604733.665684634,1174808.875387068,4855268.473987725,14584188.285920233,2678809.013008049,6118021.877535341,1234376.61109781,1186163.5593515188,3335992.26317377,986604.2960396295,2583630.3730061366,10818581.077924797,977059.1563232136,14868368.734257605,2336478.358920346,1781233.9112066203,2424442.9176717387,4306167.581913209,1544887.4638580538,12544653.304660948,1664877.8886609755,982289.7443754771,1067275.8175116242,5052632.722313252,5922255.945830122,2975130.0229820497,1094692.0644697933,2486878.834434423,1541662.8926633948,3986128.194468674,8244311.592668857,118369682.39192808,4327973.997771515,2489581.336226787,673602.7500244202,2267732.252286326,3243323.493212371,81151636.42559767,6104763.94168444,1459937.6060499412,11705552.783448625,2013232.7303968784,1210149.2470325707,1757122.3287687125,2354135.93849302,4170118.0312000667,9256392.834754743,1168430.2253133948,54829874.39014015,842029.7942079962,2213684.35472491,1210334.4278721397,1442351.6016630684,700937.3981895559,11710209.501030492,1656249.8857013085,5215113.427088465,12244120.114218023,995433.7939996722,2805678.3294365057,16245898.074172124,2577251.231591856,6984890.572624394,6576905.309111012,5715884.903522233,69365671.17483723,23922013.329514615,11092263.127112582,1329417.6248675825,2765514.3998488714,3438094.983766776,3357786.8754235143,3439884.505863883,2252341.234799691,6571365.969649544,12819858.63533321,1024663.514513066,1495894.5069445905,1638667.537505739,2091804.7414790958,950223.7151539028,1912853.9962614316,6395620.422554337,2329340.7213680674,3255881.6823458266,15647078.12939158,1068551.0608112821,7672418.736208042,4836682.846015599,3565789.1151068495,1931657.7169810627,1777308.0319205138,1099390.9431838645,6627294.014921369,10688341.459309462,14565672.522393454,1120223.563915582,1355126.9167539657,3788158.0519063612,11844967.80057576,1414099.129689381,1665099.3668191745,1201501.8019892336,11122830.696586885,3090025.258481022,1734130.991577232,1875162.6019051853,2048137.066643523,4433113.509301327,4953033.311330341,1151598.068406833,1556866.916003135,2282711.4924882306,4334315.125423634,7544606.215407386,1125638.819999582,300270.769888469,4301337.308163134,1927302.0052780723,2140230.9918561582,1236266.760300144,70460138.37338224,952237.4777856688,1491155.3925845702,1254081.3912129551,653718.8943700294,19227069.530811876,1727838.8160766843,4579310.411161939,1075280.1640937347,19608232.669558927,3921299.8680317337,3127766.748052399,1079843.5011181284,1196764.4135478153,2742024.4188153567,2846049.7357401084,5623949.182536974,3087072.818396884,6605570.373219717,22864172.25042324,903273.4706401131,1530038.527356403,3785755.3714349265,1444938.0252366243,6035394.7841412015,896599.3087263426,1308320.743372955,8570214.060250264,3031400.491228861,2167430.5355642335,1765129.9299713653,951861.2548582498,2927675.501611643,1166890.768972271,1099100.5794055762,1063423.6394040613,8440955.122049294,631998.0389173029,1373121.7856865604,2537930.4712335323,2776018.380066953,1576503.409072102,5179993.157854836,16589670.504897851,2034961.2310150864,9635735.769845694,864402.1764759449,18458670.497891955,1794481.0424984794,793740.3685131348,2053291.2199399972,1744646.6107545283,5037326.7562249135,1422567.6860872428,1096506.0206389423,8277045.079407552,2911865.6487877523,3057435.469877707,1700583.9122469458,2238013.4185079485,4852243.370159291,2174958.356417509,4682075.301556751,1343210.9809503169,2935977.4622255783,4893629.49378388,1153855.204545892,1608728.0289169408,1967049.964631746,5253818.619062149,1304336.3813519129,1681865.6771044377,4629155.662009403,3790970.4038034505,1855447.4895714868,5013661.314775278,19715255.389690123,3758839.003937602,2930460.9671105556,1514605.9698075461,11315646.095950596,1135676.671052537,52325640.02262678,1266266.3583194339,31387785.71207237,1050387.181711359,1371063.4756512777,2877311.366984892,1209352.3555397796,3804406.4123314843,6098694.501840951,2632217.1447167676,7374711.4482715465,2459552.974467038,25369538.086052727,4990671.213148855,1395454.6173084308,2561481.730211282,1896010.0702434585,1362371.4327257723,1912632.5181032326,1010909.0739507726,3488981.191355426,15490527.908743232,1194107.9275407784,1068551.0608112821,1596752.8508699776,1835063.5240751272,2864964.207802928,5471634.674500218,1425371.3117574775,1148869.502830822,15891260.26441893,1205560.36950901,3611051.3074493813,31242083.44490808,628401.1505794325,5228154.636945257,7166379.964919469,1392365.5465011033,995655.2721578712,2017867.959668314,2128064.182447259,7613965.070197348,1726326.3895359566,1107522.2609753232,1136458.0881136765,1353949.6638764108,7871094.561127411,12138487.813992634,3525728.107128872,5401348.504339437,4027334.854211284,3238547.4429877196,30059931.814440776,15996235.66883963,630995.8902934727,123140069.6713638,2606526.482271987,1334265.8887294466,1090744.8328416818,3367733.9806551486,4234896.203806533,2365670.360825093,2371800.2875961345,2994513.9366881223,1010715.0751297595,814776.8776128553,1006650.6212556968,54912362.76101638,2895180.84098379,7395335.357677916,1431270.1318979019,1391588.024105038,976365.7429373614,2953827.212196945,1148514.0801695748,20253216.798397355,5320798.908623383,8155345.7581053935,2605557.537642792,2949353.340476994,3101369.206571444,3314357.0562090324,2693613.5597039354,6931945.486823497,3619029.4642636385,1871635.3419808906,3698561.8097428484,735745.6756568984,1180729.4541820392,15538401.588520851,943442.7341365116,5607420.381484656,8233637.53545881,4260093.492500134,2107345.3467557416,5047590.9575938685,568567.3481123825,2113393.4624876813,4818829.537489754,14203861.243677646,1103584.7737823962,3109524.3853571997,47809001.34674885,5113961.424801815,5710255.600611292,2356665.6687677028,4370876.67769247,80008028.00728011,6519801.06997839,2401672.5130395056,2921065.282098219,1783546.9023845638,7144055.073302707,9345744.89185548,76118598.65382653,2608501.7462704275,2937439.662705265,1806282.3472592353,1299884.1072879224,1131757.7076398013,15996457.146997828,4345419.066705436,14658787.059991999,1855376.9972558422,2875838.884415063,1472974.0637426437,996417.3875593399,2519987.668617919,23280970.92248309,4448881.728065532,13750648.949794516,2623750.809762169,1047626.8818643093,8058196.334593503,1207637.3877280601,6879764.078209747,1050468.0769121572,3209177.7345821094,3188009.1639385037,1196034.731932276,3338507.789738507,9468779.52765923,1598635.3781779846,4478789.446562059,4664583.651432227,2034739.7528568874,14918611.621304091,2483413.2000202616,1137871.107181725,10504925.456067286,1729425.9897834738,1896286.9930399703,2317140.680060075,1973011.1490747167,1425944.4481461227,2270648.5312427254,8773436.351906192,855067.3537907929,12830136.75884533,1554490.2834524966,6316608.6791462675,1008815.3499872019,1286884.2132285908,2548573.444221382,1097428.8068885892,5047316.430724915,1529345.8865412462,31688557.857340507,4265372.820041848,2242990.2557465043,1209573.8336979786,1527238.4828640507,2139901.5282232994,6671387.8958718125,3220474.7778197452,977280.6344814124,4942048.103248362,822727.3507176177,30558837.958912075,1435549.2381704263,6849645.081269734,19992382.540039584,1488860.806043289,27019340.735123917,4391694.575322923,1006544.0206397958,1817201.1836673617,3454577.892486226,4448602.972194064,843371.4731860743,37234297.69492934,8147196.21746503,1454178.6347131573,5409184.276953804,1131536.2294816023,1086008.572703849,1874941.1237469865,2766191.7646257095,20710004.781304322,3877106.717967314,1398948.344837182,2176279.2010776433,16796439.6429258,756941.7684497689,1369239.9139439582,1373084.0946814886,6442090.31589381,9545481.427236002,1965279.5252029328,5681046.501657282,26834006.733139288,1715525.930058133,904282.2618223887,2703666.632705355,1744354.224062324,6197691.690751901,1756900.8506105135,9393673.515844531,1408409.7781264463,1172215.5216421005,968387.0357749581,2274600.733504512,1514384.4916493471,1332370.6296848156,1041075.5292429817,12429087.500694457,2133777.026310152,5508701.2902748985,3025777.7116528917,5238172.8465522565,4080711.3635547026,9803148.231622186,1965418.9031386669,3603762.9280468374,7823922.433405905,2457309.47221753,3092443.048400645,1359625.888116584,1331029.3079540785,443055.1690714774,4877743.6871786155,14821389.973526247,1751994.104013714,10041519.32843511,1056648.7875594576,4576369.834635971,5401900.4412640985,7763912.597576134,2276484.4228020813,3157701.1741047017,5667807.308304263,3513593.409496889,1408093.4273769695,976558.7987215756,1697532.8720216262,2318606.048620377,7614604.415824725,2115119.2026087786,2204413.61242391,3394008.1499447757,3070059.828465753,4988411.593285875,2649237.746229942,1410312.6283390082,1445919.879372656,1564944.025574475,1325335.9978845709,5555993.236652002],"xaxis":"x","y":[2450488,1188444,1205617,579888.3125,3637973,2228675.5,5662234,783104.125,620104,21069722,3537977,2918553.25,5448975,770333,5907068,6388260,369291.5,1784385.25,873923712,1889990,1275236.125,646505.1875,1058986,533370.625,1384693.5,2196718,1295685.75,5702077,378714.4063,727374.6875,2989919.75,1066901,4155007.75,5253241,533329,1271021,1057256,9341427,2659770,1054995,3874632,13951571,1344512,6508595,1286817.375,398357,729535,5117308.5,1860901,827618,2017423,242805,1212883.75,1079477,4547129,8819864,499887,620483,4735249,478982,7046345.5,1191803,811703.6875,792734.8125,4420650.5,1847401,1073156.375,727703.5,729096.875,858897,4897760,1030150,1784796.5,870633.5,1614322,552015.625,6345009.5,2147013,4235405,2788860,1691963,4946798.5,7482832,4024810,1093653,630040,1582655.875,1835804,722062.875,994194.875,1964681.25,969682.8125,6695413,764906,3291518,13158205,3344833,44984468,12704014,524144.6875,25377650,6962255,872114.5,466672.6875,1381407,2919980.75,3481727.5,2992693,464155.1875,2372126.25,5965171,1811506.375,670779,15552193,5337334,896403,4039667,4784731,749207,8381834.5,25970248,900973,1436773.875,1200381,2225265.5,5348309,966812.125,9791557,777094,5471735,662424,939413.625,17924416,454296,452744,11441,4188660.5,1686262.25,1767480,342726.0938,1987334.75,666385,8611054,410433.0938,1501111.75,1210229,4268054,914670,1509492,1185469,10750010,572654.3125,9290214,4145920.5,1332591.5,549509,1134195.5,1682454.875,2364351,7475577.5,3418139,1035592.813,567608.625,6015128,7380088,102673696,2177075,875789,1972234,618876.1875,6010407,1189402,2916511,3757006,974305.625,505601,1325597.625,4189569.25,3229864.5,1508561.875,15878689,5528223,1605522,1747090.375,19490284,0,14585813,5696695,541950.875,5968153.5,1346952,612158.1875,1076084,731092,2804534,690529.5,2190460,1829122.375,7240274,947884,9162048,2470566,690557.6875,25959642,703847.375,207572,3155391,948975,666687.5,1247361,986353.125,53166156,1419980,1171380,888308,15590743,830945,1074350.375,3805877,2108012,659450,3693616,4696639,2433777,804620.5,5258024.5,2420605.5,13171370,761504.875,4054994.25,858932,3267564,776237.375,12095818,3556291,887403,6797528,2055534,1983256,566183,1989287,1045413,1169932.75,2964206,3782378.25,802312.5,10062436,1034941.313,12086616,1138520,14168729,2173143.75,1441631,1742351.25,1222949.25,2478960,1913558.375,473874.0938,3518459.5,2920918,19645206,36667044,98960776,13010578,2280352.5,1228041,4961405,3157579,809291.8125,2288635,6293359,5454482,7739699,2077039.375,3577120,1574750.875,708967,2458760.5,748335.3125,686949,1465795.75,1244150,2557533,1686405,2186148.5,1194835.5,3744483,4476997,8355988.5,6914839.5,1738724,90558.70313,3087318,965425.1875,913899,2292169,490743,601590,1117658.75,1082531,326778.6875,1821047,230365,809870.8125,387810,678608,1297872,684234.875,1114384.625,742630,2454812,10420096,694241,1966424,3663534.5,515025,592534,600060,2001643.75,1866800,2360288.5,6859198,955641,4358944.5,1034827.313,6596057.5,1784233,1886382,1100575,1632731,835085,738990,4385512,1227772,77164568,549834,6609358,1789710,1375366,5104947,1883864,2137686,2208196,3004271,866096.6875,1679127,498745,3717064,1446455,710825.1875,8259071,16644664,924438.125,542882,711118.375,219483,1041652.813,0,318205,905750.375,1024033,1696448,694479,5457975,5885427.5,13093350,1270139,10213132,1200811,1076644.75,581589,1967637,2912264,967697,4700396,7369591,489557.0938,1529510.25,8320719,1130960,807358,3837228,4988493.5,12719042,2996958,5238803,958725,1529309,1256575.875,11319836,5767765,6036588,683274,16371764,2126288,920068.875,942089,1497496.5,1358022,529807,2912140,1017787.125,685888.1875,5252131,1817550,7338511,17109052,961476.1875,688641.8125,2211671,3864699,647906.1875,1938887,4547379,1750419,648634.875,1739693.25,2962476,3893073.5,2338238,809257,719471.1875,934357,408513,3258819.5,1911591,2890253,468396.9063,1868497.125,1072487.875,2151376.5,8168547,2472350.25,1417778,2337651.25,5917000,5653656,3059758,2266983,2676594.75,1379739,7637986,1128179.125,1472548,3371455,3091559,7003615,4059424.75,1318096,2156407,1765813.625,455798.9063,641713,1027394.313,648879.3125,1947717,896485,2248776,3404992,2158629,1380293.625,7067404,32579658,1489363.875,1359579,3017709.25,10154608,3546459,892227,2439484.75,3040924.25,317581,32381836,1401315.875,1921222,1494727,1703504.875,2563985.5,1762174,4078339,6454983,684155,10403123,3240830,508014.5938,1022957.688,654625.5,20311228,1481698,1093757.75,18661606,293217,922041,8381744,2127534,2694853,362874,9586556,609494,1133028.875,502667.6875,1457118,2265836,962238,815800,1259456,999924.3125,10317852,4022735,877788,6578370,2477499,51168308,11788719,12783255,1677877,989079.5,1431784,928409,1521202,4685346,2148565,538740,1889761.125,3605749.75,1443367.75,4206839.5,578854.125,739662,3740373.75,1385697,608755.375,1634178.25,8696206,137635696,1364344,3590731,3605855.5,5357833,1176342.25,716940.625,47148456,6320060,1045640,1275231.75,783345.6875,1929011,2307938,22055140,759060.8125,924121,2361088,1493131.75,5261096,755618.1875,2735128,688375.1875,8493420,1858224.625,551219,562617,10303305,657385.8125,1645748.25,4760734,1612105.75,3366528,1245923.75,1093743,2468322.5,816510.875,7682562.5,1607972.25,2512443,2069880,1634334,1334125.125,625938,3158151,405062,40062888,287132.5,1192100,4934559,956481,7520554,818317,2003474,827211,1967129,2429021,1006989.688,2616226,454124.3125,872555,268269,1085439,1055624,3191228.25,53401,1012341,2140476,3265995,811900,713326.875,6935723,204499.0938,12525174,2189735,1585441,14361382,1750362.25,1824054.5,1137513.625,1150926,593824,555864,1761137.5,497991,2470642,4321141.5,2274873.5,12731388,1898509.75,1245753,4486477,990361.5,4560492,5707529.5,3643112,1198630,3151107.75,966376,1104074.625,895310.5,6750112,2225134,3775187.5,431390.4063,976537,5415438,306721,2188452,2234448,3928695.25,4983934,2023197,2028125.625,1593610.125,3436032,85357952,591249,505949.0938,833747,25476332,1186682,1307883,965786,876871,1888344.25,758875,8651835,2442220,3863772,3947209.25,2014500.375,4424263,6127771.5,522791.0938,2093011,422133,2646130,680395.875,35383012,6313678,681354.1875,841242,1507168,784584,2025533,7439009,2090543.75,37980712,3239754,730211.1875,599390,3895403,861972.875,162960,580688,930439.375,1453543,642052,1482997,819354.625,4294550,2024367.75,1525624,157606480,869699,5903033,8926736,3289053,1820292,2277316,1844825,94186,869835,2491238.25,4407932,637685,9155568,4912520,3999505.25,6983736,18848780,3303036,6361236.5,25397086,862541,752441,1551204,2056762,802755,136241424,13962749,3312125,2114860.25,3568237,1109924,3542542,46510960,17044842,10960107,2471623,3132453,597761,1107290.625,658297.5,9029489,1015172,2701872,493969.6875,1886007,3226884,1216543,1469556.25,789209.8125,2081072.625,6884914,1623657,393493,645927,5672838.5,2401890,624993,3485865.5,3005926,8024064.5,9373179,1872413,145573,1834570,714898,876237,2015117.25,958242.875,1976856,10077532,462465.5,694189,1056099,1418915.875,2485521,2756494,1271753.25,1176615,1096646.25,1943933.625,534039,849504,2322314,0,2466100,3629065.75,1188994.625,1822834.25,1169948.875,2122581.25,2400507.25,7175646,3996541.5,350264.5938,14709151,2067352.5,415586,1559199.25,832168,1207869.875,1041304,577033.875,1677881,796042.375,813641,1085183,871355.5,5169726,661383,828531,6102883,2227640,2890379,507603.3125,602191.1875,466904,2767381,847419.375,1618695,436848,4425832.5,439065.9063,28450436,72587024,781682.5,5876439,642133,2407904.5,6058494,611536,1926134.875,2556131,1639849.25,696522,1054581.375,872326.375,677536.3125,3751417,818836.125,1334876,1287159.25,2334260,16163194,1172785,2887392,474468.5938,1153690.125,1228963.75,447947,1332912.625,0,443365,8504904,929688,3573328.25,1293336,2075833,1059605.625,1507270,1419453,586827.5,1360340,13659926,1971236,1364214.75,609508,1790148.625,2245568.25,2855545,3944933.5,286685536,4767227,815115.5,1261634.5,1845150.625,1698074.625,61576184,1226435,716071.625,5249190,1128208.125,863703,2301043.75,1080203,2376794,11325513,941564.8125,41655852,387279,1734533,1540235,570982,1689631,8909924,700899,686019.1875,8580279,938908,4167253.75,4222682,6118300.5,7323351,3185005.25,7256842,53079164,9569715,3644982.25,238255,1614026.25,1655612,1570758.25,11594121,1929118,3021669,6814269,841841,1258619.125,803938,938096.1875,961749,2058532.875,2220053.25,1313366,1297633.75,65336980,541542.6875,3608955.75,1983509,3218824,363808.1875,770175.8125,592221,3791402,23364636,49102164,1625308.625,497528,4202335,9986032,709990.8125,1423463,567521,8748273,3373582,846389.3125,1000804.625,2998208.25,2153603.5,2660396,1289073,1247972,4253369.5,2105451,13586070,935580.3125,1082920.125,3321035.75,3288836,3560614.25,512166,51625548,2615973.5,1251906,555153.125,810636.875,48729588,548292.6875,24473814,785052.875,13300859,571309,3570161.75,522972.1875,2473302,2098850,6881405.5,2306723.5,2876857,5089903,10564729,5039523,1730886,1633399.375,1696978.75,1739286.75,395346.3125,349139,6040837,945020.1875,1764767.25,914023,1232822.875,1625695.75,2545405,5807879,1392015,6283226,2662054.25,4133811,1582794.5,1259959,964015,6424029,12448381,873711.125,13911761,1475475.5,10498344,1788148,618366,776991,1331122.125,5903142,634751,520405,11026945,5981430,2203026.5,2166711,1427974.625,2654755,1050982,2581741.75,1639136.25,1402048.5,3165109,729397.1875,937635,868859.8125,4409881.5,705262.125,3542708.5,2729167.75,2628864,813038,2773779,21781324,2161828,1199392,832986.875,602245,1074989,35685224,342493,94178648,1136676,741287.1875,882896.5,1505709,0,1803753.25,3220635.25,8201974,2035595,12265516,3306123,313528.0938,241478.5938,2465266,1270582.125,1982967,824516,533078,19344782,1172065,511180.6875,255971.2031,758008.875,1911611,2503948,636641,677279,7838600,499928,2921368,22591288,964923.8125,2278160,17186624,591622.8125,919746.3125,1717130.875,1815350,3024478.5,5975230,860364.1875,600543,3275779.25,7261732,8921778,1976400,5572503,6159684,2960377.25,13270687,7370743,1323038.75,291614432,1425599,689803,439266.9063,3020001.5,2168701.5,1234095.5,2219744,3045139,638201,1943772,855393,41213584,3674705,2712983.75,1201630.125,648022,556700.6875,816846,196410,9977330,7419765,25453690,1120197,2483320,1358226,1441665.375,33268410,5535626,6471427,1437569,2128963.5,1581425.75,1231546,5037781,1581036.75,5392617.5,4180298,4817062,687953.6875,3743228.5,578789,1519952.125,2355732,7969653,431357,5094197,65047284,10513817,2599171.75,842593,10105379,45074876,2391566.25,1004253,1827424.125,652991.6875,3534690.75,7657568,64715652,1073507,9671619,1540258.5,8163413,520206.6875,6744317,6577130,63668488,695346,2666634,676032.625,571644,1103594.75,16614404,2885740.5,9851937,1743198,572082,40613740,861007.8125,47102428,529033.5,41680640,1398253,987989,4590246.5,7976729,786346,9146727,1688612.5,801264,18628834,1108310.875,500237,17673018,978211,1921890,2926089.5,8839812,1634669.375,1115020,5121586,804002,16246106,3612526.75,10340282,881386.1875,920430.625,1183623,323739.4063,2993032.25,788055,19150024,1929828,2248134.5,669117,495152.6875,941257.3125,21795830,1513700.625,445943.8125,2843033.5,460010,16020642,812013,5838129,18967014,1038351.875,20657852,1808401.375,588492.5,1380283,2345752.5,14833476,1541879,19827916,4396566,899266.125,8354235,466985,1280263.75,1011296,1325972.75,11821841,5481549,537141,1037696,17750994,792015,697397.875,1736775,3616057,8070420,9159185,3730953.5,18576900,1356972.25,656980,1487769.5,706961,1916309,4038090,4027134.75,868163,3767135,517029,1208168.75,864765,783208.625,535649.875,14751117,1896858,1757601.25,1116705,1920352.125,2755578,4828599,873648,13440924,1730431,3441643.25,2178074.5,690750,573395,936616.5,2762846,6821833,897514,18074716,228212,2549892,22530168,19991412,3215953,3245111.5,6057734,3234787.75,1928391.25,416566.1875,741850,1045337,11963428,3035962.25,1855556,5040382,2096835.5,3002337,797170.3125,892455.875,1751483.875,8054334,686055,1381590.125],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","SiteEnergyUse_predRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","print(ScoresRidge)\n","figRidge.show()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[10589992.743607046,10589992.711005785,10589992.67779782,10589992.64397186,10589992.6095164,10589992.574419731,10589992.538669918,10589992.502254803,10589992.465162013,10589992.427378926,10589992.388892706,10589992.349690262,10589992.30975827,10589992.269083148,10589992.227651069,10589992.18544795,10589992.142459437,10589992.098670918,10589992.054067502,10589992.008634027,10589991.962355044,10589991.915214822,10589991.867197333,10589991.818286248,10589991.768464942,10589991.717716474,10589991.66602359,10589991.613368716,10589991.559733953,10589991.505101059,10589991.449451463,10589991.39276625,10589991.335026141,10589991.276211511,10589991.216302356,10589991.155278314,10589991.09311864,10589991.029802196,10589990.965307454,10589990.899612492,10589990.832694976,10589990.76453215,10589990.695100844,10589990.62437745,10589990.552337926,10589990.47895778,10589990.404212067,10589990.328075368,10589990.250521803,10589990.171525009,10589990.09105812,10589990.009093788,10589989.925604148,10589989.840560809,10589989.75393486,10589989.665696856,10589989.575816793,10589989.48426412,10589989.391007708,10589989.296015855,10589989.199256267,10589989.100696048,10589989.000301689,10589988.898039063,10589988.793873405,10589988.6877693,10589988.57969068,10589988.469600797,10589988.357462227,10589988.243236853,10589988.126885839,10589988.008369626,10589987.887647934,10589987.764679719,10589987.639423173,10589987.51183572,10589987.381873984,10589987.249493787,10589987.11465012,10589986.977297155,10589986.83738819,10589986.694875663,10589986.549711134,10589986.401845256,10589986.251227751,10589986.097807433,10589985.94153214,10589985.782348754,10589985.620203156,10589985.455040231,10589985.286803832,10589985.115436776,10589984.940880803,10589984.763076583,10589984.58196367,10589984.397480499,10589984.209564367,10589984.018151388,10589983.8231765,10589983.62457343,10589983.42227467,10589983.21621145,10589983.00631373,10589982.792510161,10589982.574728075,10589982.352893434,10589982.12693085,10589981.896763502,10589981.662313167,10589981.42350015,10589981.180243284,10589980.932459878,10589980.680065718,10589980.422975015,10589980.16110039,10589979.89435283,10589979.622641677,10589979.345874574,10589979.063957456,10589978.776794512,10589978.484288136,10589978.186338907,10589977.882845571,10589977.573704977,10589977.258812059,10589976.938059796,10589976.611339172,10589976.278539158,10589975.939546647,10589975.594246428,10589975.24252116,10589974.8842513,10589974.519315097,10589974.14758853,10589973.76894528,10589973.383256663,10589972.990391616,10589972.590216631,10589972.18259572,10589971.767390365,10589971.344459474,10589970.913659323,10589970.474843536,10589970.027862988,10589969.572565805,10589969.108797267,10589968.6363998,10589968.155212883,10589967.665073019,10589967.165813666,10589966.657265186,10589966.139254792,10589965.611606477,10589965.07414097,10589964.52667566,10589963.969024546,10589963.400998164,10589962.82240353,10589962.233044077,10589961.632719574,10589961.021226076,10589960.39835585,10589959.763897289,10589959.117634857,10589958.459349021,10589957.788816158,10589957.105808487,10589956.410094004,10589955.70143638,10589954.979594901,10589954.244324382,10589953.495375073,10589952.732492587,10589951.95541781,10589951.163886812,10589950.35763076,10589949.536375819,10589948.699843075,10589947.847748425,10589946.979802486,10589946.095710505,10589945.195172248,10589944.277881904,10589943.343527976,10589942.391793186,10589941.42235436,10589940.434882317,10589939.42904177,10589938.404491188,10589937.360882716,10589936.297862023,10589935.2150682,10589934.112133645,10589932.988683904,10589931.844337601,10589930.678706251,10589929.49139417,10589928.28199832,10589927.050108181,10589925.795305612,10589924.5171647,10589923.215251636,10589921.88912455,10589920.538333375,10589919.16241968,10589917.760916533,10589916.333348325,10589914.879230632,10589913.398070026,10589911.88936393,10589910.352600437,10589908.787258139,10589907.192805955,10589905.568702951,10589903.914398152,10589902.229330363,10589900.512927976,10589898.764608778,10589896.98377976,10589895.169836914,10589893.322165022,10589891.440137463,10589889.523115989,10589887.570450518,10589885.58147892,10589883.555526774,10589881.491907172,10589879.389920454,10589877.248854006,10589875.067981988,10589872.846565122,10589870.583850417,10589868.279070932,10589865.93144552,10589863.540178541,10589861.104459634,10589858.623463418,10589856.096349211,10589853.522260776,10589850.900326004,10589848.229656646,10589845.50934799,10589842.738478584,10589839.916109908,10589837.041286072,10589834.113033487,10589831.13036055,10589828.092257293,10589824.99769507,10589821.8456262,10589818.634983607,10589815.36468049,10589812.033609938,10589808.640644556,10589805.18463612,10589801.66441516,10589798.078790585,10589794.426549286,10589790.706455734,10589786.91725156,10589783.057655144,10589779.126361182,10589775.122040253,10589771.043338383,10589766.888876587,10589762.65725042,10589758.347029494,10589753.956757022,10589749.48494933,10589744.930095347,10589740.290656125,10589735.565064328,10589730.751723692,10589725.849008529,10589720.855263159,10589715.768801372,10589710.587905884,10589705.310827747,10589699.935785795,10589694.460966028,10589688.884521049,10589683.20456942,10589677.41919506,10589671.526446618,10589665.524336819,10589659.410841819,10589653.183900533,10589646.841413964,10589640.381244509,10589633.801215265,10589627.099109303,10589620.272668958,10589613.319595074,10589606.23754626,10589599.02413813,10589591.676942507,10589584.193486655,10589576.571252443,10589568.807675552,10589560.900144622,10589552.846000409,10589544.642534923,10589536.286990546,10589527.776559126,10589519.10838109,10589510.279544502,10589501.28708411,10589492.127980415,10589482.799158674,10589473.297487926,10589463.619779944,10589453.762788268,10589443.723207105,10589433.497670298,10589423.082750233,10589412.474956747,10589401.670736013,10589390.666469377,10589379.458472233,10589368.042992841,10589356.416211113,10589344.57423741,10589332.513111306,10589320.228800328,10589307.717198668,10589294.974125903,10589281.995325645,10589268.776464224,10589255.3131293,10589241.600828486,10589227.634987924,10589213.410950858,10589198.923976174,10589184.169236906,10589169.141818736,10589153.836718459,10589138.248842414,10589122.37300492,10589106.203926647,10589089.736232992,10589072.964452406,10589055.883014712,10589038.486249384,10589020.76838381,10589002.723541506,10588984.345740328,10588965.628890637,10588946.566793445,10588927.153138515,10588907.381502464,10588887.245346796,10588866.738015939,10588845.852735221,10588824.582608853,10588802.920617828,10588780.859617848,10588758.39233717,10588735.51137444,10588712.209196506,10588688.478136167,10588664.31038991,10588639.698015617,10588614.63293023,10588589.106907371,10588563.11157494,10588536.63841268,10588509.6787497,10588482.223761965,10588454.264469747,10588425.791735042,10588396.79625895,10588367.268579012,10588337.199066533,10588306.577923834,10588275.395181492,10588243.640695516,10588211.304144539,10588178.375026893,10588144.842657711,10588110.696165957,10588075.924491437,10588040.516381748,10588004.460389206,10587967.744867736,10587930.357969705,10587892.287642729,10587853.521626445,10587814.047449231,10587773.852424892,10587732.923649315,10587691.247997059,10587648.812117942,10587605.602433564,10587561.605133789,10587516.806173211,10587471.191267546,10587424.745890029,10587377.455267733,10587329.304377869,10587280.27794406,10587230.36043254,10587179.536048371,10587127.788731553,10587075.10215318,10587021.4597115,10586966.844527949,10586911.239443181,10586854.627013033,10586796.989504484,10586738.308891552,10586678.566851182,10586617.744759118,10586555.823685693,10586492.78439166,10586428.60732394,10586363.27261139,10586296.760060508,10586229.049151123,10586160.119032117,10586089.948517036,10586018.51607977,10585945.799850155,10585871.777609598,10585796.426786683,10585719.72445275,10585641.647317484,10585562.171724495,10585481.273646895,10585398.92868287,10585315.112051254,10585229.798587123,10585142.96273737,10585054.578556318,10584964.619701326,10584873.059428435,10584779.870588005,10584685.025620406,10584588.496551722,10584490.254989479,10584390.272118438,10584288.518696398,10584184.965050058,10584079.58107094,10583972.33621133,10583863.199480332,10583752.139439933,10583639.124201179,10583524.121420402,10583407.098295543,10583288.021562546,10583166.85749188,10583043.57188511,10582918.13007163,10582790.496905465,10582660.63676222,10582528.513536155,10582394.090637399,10582257.330989279,10582118.19702585,10581976.650689553,10581832.65342905,10581686.166197246,10581537.149449484,10581385.563141963,10581231.366730344,10581074.519168563,10580914.978907928,10580752.703896374,10580587.651578048,10580419.778893102,10580249.042277796,10580075.397664849,10579898.800484134,10579719.205663644,10579536.56763082,10579350.840314183,10579161.977145333,10578969.931061324,10578774.654507408,10578576.099440169,10578374.21733108,10578168.959170483,10577960.275471997,10577748.116277402,10577532.431162003,10577313.169240437,10577090.279173056,10576863.709172776,10576633.40701252,10576399.320033161,10576161.39515211,10575919.578872446,10575673.8172927,10575424.05611724,10575170.240667356,10574912.315892959,10574650.226385025,10574383.91638873,10574113.329817304,10573838.410266666,10573559.101030812,10573275.345118005,10572987.085267764,10572694.263968715,10572396.823477257,10572094.705837136,10571787.852899909,10571476.206346286,10571159.70770846,10570838.298393358,10570511.91970687,10570180.512879072,10569844.019090477,10569502.379499285,10569155.535269715,10568803.427601378,10568445.997759754,10568083.18710775,10567714.93713839,10567341.189508628,10566961.886074299,10566576.968926258,10566186.380427655,10565790.063252408,10565387.960424881,10564980.01536074,10564566.171909053,10564146.37439555,10563720.567667197,10563288.697137881,10562850.708835427,10562406.549449774,10561956.16638241,10561499.50779701,10561036.522671312,10560567.160850184,10560091.373099886,10559609.11116354,10559120.327817762,10558624.976930413,10558123.013519548,10557614.393813422,10557099.0753116,10556577.016847117,10556048.17864966,10555512.522409748,10554970.011343848,10554420.610260384,10553864.285626639,10553301.005636437,10552730.740278596,10552153.461406073,10551569.142805742,10550977.760268763,10550379.291661428,10549773.716996452,10549161.018504616,10548541.180706657,10547914.19048537,10547280.03715776,10546638.712547239,10545990.211055677,10545334.529735247,10544671.668359969,10544001.629496804,10543324.418576207,10542640.043961985,10541948.517020399,10541249.85218828,10540544.067040114,10539831.182353918,10539111.222175749,10538384.21388274,10537650.188244473,10536909.179482568,10536161.22532829,10535406.367078077,10534644.64964676,10533876.12161835,10533100.835294215,10532318.846738482,10531530.215820475,10530735.006254058,10529933.285633678,10529125.125466917,10528310.601203464,10527489.792260239,10526662.782042569,10525829.657961193,10524990.511445012,10524145.437949311,10523294.536959408,10522437.911989478,10521575.670576494,10520707.924269035,10519834.788610948,10518956.383119617,10518072.83125877,10517184.260405745,10516290.80181301,10515392.590563938,10514489.76552269,10513582.469278159,10512670.848081892,10511755.051779967,10510835.233738735,10509911.550764475,10508984.163016886,10508053.233916456,10507118.930045724,10506181.42104448,10505240.879498912,10504297.480824867,10503351.40314519,10502402.82716138,10501451.936019575,10500498.915171113,10499543.952227738,10498587.236811748,10497628.960401159,10496669.316170251,10495708.498825638,10494746.704438167,10493784.130270947,10492820.974603789,10491857.436554402,10490893.71589667,10489930.01287642,10488966.528024994,10488003.461971099,10487041.015251307,10486079.388119664,10485118.780356843,10484159.391079318,10483201.418549037,10482245.059984107,10481290.511370927,10480337.967278395,10479387.620674603,10478439.66274662,10477494.282723892,10476551.667705785,10475612.002493815,10474675.469429158,10473742.248235907,10472812.515870746,10471886.446379438,10470964.210760798,10470045.976838594,10469131.909141952,10468222.168794762,10467316.913414577,10466416.297021514,10465520.469957616,10464629.578817131,10463743.766388156,10462863.171606068,10461987.929519095,10461118.171266457,10460254.024069397,10459395.6112354,10458543.052175932,10457696.462437952,10456855.953749401,10456021.634078927,10455193.607709974,10454371.975329366,10453556.834130578,10452748.277931614,10451946.3973077,10451151.279738678,10450363.009771096,10449581.669194972,10448807.33723509,10448040.09075667,10447280.004485339,10446527.151241077,10445781.602185994,10445043.427085642,10444312.694583526,10443589.472488556,10442873.828074992,10442165.828394575,10441465.540600352,10440773.03228182,10440088.371810853,10439411.628697965,10438742.873958373,10438082.180487346,10437429.62344423,10436785.28064466,10436149.232960291,10435521.564725485,10434902.364150357,10434291.723739479,10433689.740715738,10433096.517448539,10432512.161885869,10431936.787989464,10431370.516172465,10430813.473738927,10430265.795324465,10429727.623337507,10429199.108400373,10428680.409789648,10428171.695875172,10427673.144557009,10427184.94369986,10426707.291564224,10426240.397233786,10425784.481038418,10425339.774972279,10424906.523106419,10424484.98199539,10424075.421077337,10423678.123067075,10423293.3843417,10422921.515318211,10422562.84082279,10422217.70045124,10421886.448920215,10421569.456408849,10421267.10889043,10420979.808453728,10420707.973613717,10420452.039611299,10420212.45870182,10419989.700432008,10419784.25190517,10419596.618034313,10419427.321783032,10419276.904393908,10419145.925604217,10419034.963848796,10418944.616449852,10418875.499793563,10418828.249493342,10418803.520539565,10418801.987435713,10418824.344320685,10418871.305077298,10418943.60342673,10419041.993008891,10419167.247448588,10419320.160407338,10419501.545620814,10419712.236921756,10419953.088248277,10420224.97363748,10420528.787204262,10420865.443105226,10421235.875487601,10421641.038423058,10422081.905826325,10422559.471358497,10423074.748314934,10423628.76949762,10424222.587071883,10424857.27240735,10425533.915903043,10426253.626796419,10427017.532956332,10427826.780659718,10428682.534351876,10429585.97639027,10430538.306771656,10431540.742842466,10432594.518992277,10433700.886330286,10434861.112344615,10436076.480544409,10437348.290084545,10438677.85537289,10440066.50566003,10441515.584611345,10443026.449861396,10444600.472550558,10446239.036843823,10447943.539431807,10449715.389013883,10451556.005763466,10453466.820775501,10455449.275496142,10457504.821134737,10459634.918058144,10461841.035167541,10464124.649257837,10466487.244359791,10468930.311065104,10471455.34583458,10474063.850289693,10476757.33048768,10479537.296180546,10482405.260058187,10485362.736976026,10488411.243167443,10491552.295441372,10494787.41036548,10498118.103435282,10501545.888229635,10505072.275553077,10508698.772565356,10512426.881898742,10516258.100763515,10520193.920042092,10524235.823372353,10528385.286220582,10532643.774944574,10537012.74584734,10541493.644221952,10546087.903387975,10550796.943719963,10555622.17166851,10560564.978774272,10565626.740675414,10570808.816108918,10576112.545906123,10581539.25198291,10587090.236324884,10592766.779967904,10598570.141974267,10604501.55840486,10610562.241287563,10616753.377582137,10623076.12814185,10629531.626672043,10636120.978685869,10642845.260457318,10649705.517971773,10656702.76587424,10663837.986415306,10671112.128395176,10678526.106105719,10686080.798270825,10693777.04698521,10701615.656651817,10709597.39291806,10717722.981611114,10725993.107672587,10734408.414092775,10742969.500844968,10751676.923820157,10760531.193762634,10769532.775207046,10778682.085417476,10787979.493329301,10797425.318494555,10807019.830031747,10816763.245581046,10826655.73026595,10836697.395662613,10846888.298778139,10857228.44103928,10867717.767293025,10878356.164820794,10889143.46236798,10900079.429190751,10911163.774122128,10922396.144659486,10933776.126075732,10945303.240556529,10956976.946366018,10968796.637043675,10980761.64063488,10992871.21895806,11005124.566911131,11017520.81182025,11030059.012833724,11042738.160364144,11055557.175581735,11068514.909961991,11081610.144890606,11094841.591328762,11108207.889541728,11121707.608893823,11135339.247712448,11149101.233224252,11162991.921565894,11177009.597872186,11191152.476443993,11205418.700998247,11219806.345002241,11234313.41209417,11248937.836591724,11263677.484090317,11278530.152152287,11293493.571088249,11308565.404831434,11323743.251905676,11339024.646487392,11354407.059561659,11369887.90017222,11385464.516764913,11401134.19862382,11416894.177399062,11432741.628724929,11448673.673926711,11464687.381814376,11480779.770560859,11496947.80966257,11513188.421979332,11529498.48585086,11545874.837286469,11562314.272224613,11578813.548858508,11595369.39002401,11611978.485645574,11628637.495236073,11645343.050446006,11662091.757657481,11678880.20061828,11695704.943111163,11712562.531653415,11729449.4982217,11746362.36299709,11763297.637125155,11780251.825486014,11797221.429469157,11814202.949747931,11831192.889048595,11848187.754908865,11865184.062421026,11882178.33695462,11899167.116853993,11916146.956105959,11933114.426972983,11950066.12258756,11966998.659503356,11983908.680199148,12000792.855531497,12017647.887132468,12034470.509748753,12051257.49351891,12068005.646185441,12084711.815238811,12101372.88999064,12117985.803573536,12134547.534865212,12151055.110334862,12167505.605809864,12183896.14816118,12200223.916906025,12216486.145726558,12232680.123903673,12248803.197664995,12264852.771446621,12280826.309068155,12296721.334820881,12312535.434469158,12328266.256165134,12343911.511277292,12359468.975133333,12374936.48767811,12390311.954047587,12405593.345059784,12420778.697623977,12435866.11506942,12450853.767395075,12465739.891441967,12480522.79098973,12495200.836779311,12509772.46646353,12524236.184487643,12538590.561901812,12552834.236107705,12566965.910541352,12580984.354294514,12594888.40167684,12608676.951721143,12622348.967634125,12635903.476194972,12649339.567104146,12662656.392284855,12675853.165139562,12688929.15976395,12701883.710120752,12714716.209175887,12727426.107999135,12740012.91483189,12752476.194124153,12764815.565543165,12777030.702955881,12789121.333387533,12801087.23595845,12812928.240801267,12824644.227960633,12836235.126277393,12847700.912259301,12859041.608940162,12870257.284729213,12881348.0522527,12892314.067189246,12903155.527100801,12913872.670260798,12924465.774481047,12934935.155938914,12945281.168006202,12955504.200081151,12965604.676424826,12975583.055003233]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[14980469.226253428,14980469.217785697,14980469.209160391,14980469.200374573,14980469.191425249,14980469.182309384,14980469.173023876,14980469.163565565,14980469.153931245,14980469.14411763,14980469.134121388,14980469.12393912,14980469.113567367,14980469.103002597,14980469.092241224,14980469.081279587,14980469.070113957,14980469.058740543,14980469.04715547,14980469.03535481,14980469.023334542,14980469.011090588,14980468.998618782,14980468.985914879,14980468.972974565,14980468.95979344,14980468.946367022,14980468.932690743,14980468.918759964,14980468.904569939,14980468.890115844,14980468.875392776,14980468.860395718,14980468.845119577,14980468.829559157,14980468.813709166,14980468.797564227,14980468.78111884,14980468.764367413,14980468.747304259,14980468.729923578,14980468.712219454,14980468.694185872,14980468.6758167,14980468.657105697,14980468.6380465,14980468.618632635,14980468.59885749,14980468.578714348,14980468.55819637,14980468.537296567,14980468.516007844,14980468.494322967,14980468.472234555,14980468.449735101,14980468.426816959,14980468.403472336,14980468.379693305,14980468.35547177,14980468.330799505,14980468.305668123,14980468.280069077,14980468.253993668,14980468.227433031,14980468.200378142,14980468.172819797,14980468.144748636,14980468.11615511,14980468.0870295,14980468.057361908,14980468.027142258,14980467.99636026,14980467.965005467,14980467.933067214,14980467.900534643,14980467.867396701,14980467.833642121,14980467.799259434,14980467.764236938,14980467.728562757,14980467.692224741,14980467.655210545,14980467.617507596,14980467.579103075,14980467.53998392,14980467.500136849,14980467.45954831,14980467.418204509,14980467.376091396,14980467.333194658,14980467.28949971,14980467.24499171,14980467.199655527,14980467.153475754,14980467.106436692,14980467.058522355,14980467.00971647,14980466.960002434,14980466.90936336,14980466.85778204,14980466.805240944,14980466.75172221,14980466.697207658,14980466.641678756,14980466.585116643,14980466.527502079,14980466.46881551,14980466.409036966,14980466.348146155,14980466.286122374,14980466.222944558,14980466.158591222,14980466.093040511,14980466.026270146,14980465.958257442,14980465.888979282,14980465.81841214,14980465.74653202,14980465.673314512,14980465.598734736,14980465.52276735,14980465.445386538,14980465.366566017,14980465.286279008,14980465.204498226,14980465.121195892,14980465.036343697,14980464.949912826,14980464.861873906,14980464.77219703,14980464.680851737,14980464.58780699,14980464.493031181,14980464.396492112,14980464.298156999,14980464.197992425,14980464.095964376,14980463.99203819,14980463.886178568,14980463.778349549,14980463.66851451,14980463.55663614,14980463.442676447,14980463.326596724,14980463.208357543,14980463.08791874,14980462.965239428,14980462.840277925,14980462.71299181,14980462.583337843,14980462.451272003,14980462.316749435,14980462.179724466,14980462.04015056,14980461.897980329,14980461.753165495,14980461.605656885,14980461.455404414,14980461.302357078,14980461.146462899,14980460.987668958,14980460.82592135,14980460.661165154,14980460.49334444,14980460.32240224,14980460.148280526,14980459.970920194,14980459.790261036,14980459.606241733,14980459.418799829,14980459.227871701,14980459.033392552,14980458.835296374,14980458.633515943,14980458.427982781,14980458.218627138,14980458.005377974,14980457.788162936,14980457.566908307,14980457.341539022,14980457.111978618,14980456.878149215,14980456.639971476,14980456.397364605,14980456.1502463,14980455.89853274,14980455.64213854,14980455.38097674,14980455.114958752,14980454.843994364,14980454.567991678,14980454.286857095,14980454.000495285,14980453.708809134,14980453.411699757,14980453.109066408,14980452.800806483,14980452.486815479,14980452.166986953,14980451.841212492,14980451.509381663,14980451.171382003,14980450.827098949,14980450.476415832,14980450.119213806,14980449.755371831,14980449.384766623,14980449.007272616,14980448.622761918,14980448.231104262,14980447.832166981,14980447.42581494,14980447.01191051,14980446.59031351,14980446.160881165,14980445.723468062,14980445.277926084,14980444.824104387,14980444.36184933,14980443.891004428,14980443.411410306,14980442.922904639,14980442.425322095,14980441.91849429,14980441.402249727,14980440.876413733,14980440.34080841,14980439.795252573,14980439.239561688,14980438.673547804,14980438.09701951,14980437.509781845,14980436.911636252,14980436.302380523,14980435.681808682,14980435.049710982,14980434.405873794,14980433.750079527,14980433.082106605,14980432.401729342,14980431.7087179,14980431.00283819,14980430.28385182,14980429.551515985,14980428.805583417,14980428.045802288,14980427.271916125,14980426.483663728,14980425.680779085,14980424.862991292,14980424.03002444,14980423.181597557,14980422.317424497,14980421.437213823,14980420.540668767,14980419.627487086,14980418.697360976,14980417.749976974,14980416.78501586,14980415.80215254,14980414.80105595,14980413.781388935,14980412.742808156,14980411.684963964,14980410.60750029,14980409.510054542,14980408.392257456,14980407.253733002,14980406.094098259,14980404.912963279,14980403.709930966,14980402.48459695,14980401.236549448,14980399.965369148,14980398.670629054,14980397.351894345,14980396.00872226,14980394.640661936,14980393.24725426,14980391.82803173,14980390.382518306,14980388.910229245,14980387.410670958,14980385.883340847,14980384.327727139,14980382.74330873,14980381.129555019,14980379.485925723,14980377.811870724,14980376.106829893,14980374.370232888,14980372.601499002,14980370.800036965,14980368.965244746,14980367.096509397,14980365.19320681,14980363.25470157,14980361.280346718,14980359.269483563,14980357.221441468,14980355.135537658,14980353.01107698,14980350.847351704,14980348.64364129,14980346.399212182,14980344.11331756,14980341.785197102,14980339.41407678,14980336.999168597,14980334.539670357,14980332.034765381,14980329.48362232,14980326.885394834,14980324.23922138,14980321.54422492,14980318.799512673,14980316.004175832,14980313.157289281,14980310.257911326,14980307.30508342,14980304.297829848,14980301.235157441,14980298.1160553,14980294.939494465,14980291.704427615,14980288.409788769,14980285.05449295,14980281.637435878,14980278.157493634,14980274.613522336,14980271.004357796,14980267.328815177,14980263.58568867,14980259.773751099,14980255.89175361,14980251.938425282,14980247.912472758,14980243.8125799,14980239.637407383,14980235.385592328,14980231.055747911,14980226.646462962,14980222.15630158,14980217.583802728,14980212.927479804,14980208.185820254,14980203.357285127,14980198.440308673,14980193.433297884,14980188.334632087,14980183.142662473,14980177.855711674,14980172.47207329,14980166.990011442,14980161.407760296,14980155.723523606,14980149.935474232,14980144.04175365,14980138.040471485,14980131.929704998,14980125.707498588,14980119.37186331,14980112.920776352,14980106.35218051,14980099.663983691,14980092.85405836,14980085.920241037,14980078.860331753,14980071.672093494,14980064.35325168,14980056.90149359,14980049.314467816,14980041.589783717,14980033.72501083,14980025.717678323,14980017.565274388,14980009.265245713,14980000.814996855,14979992.211889666,14979983.453242706,14979974.536330663,14979965.45838373,14979956.216587014,14979946.808079945,14979937.229955655,14979927.47926037,14979917.552992802,14979907.44810354,14979897.161494426,14979886.690017955,14979876.030476626,14979865.179622358,14979854.134155855,14979842.890725978,14979831.445929155,14979819.796308722,14979807.93835435,14979795.868501402,14979783.583130319,14979771.078566037,14979758.351077337,14979745.396876294,14979732.21211762,14979718.792898111,14979705.135256063,14979691.235170648,14979677.088561371,14979662.69128749,14979648.03914747,14979633.127878416,14979617.953155525,14979602.51059159,14979586.79573644,14979570.804076456,14979554.531034084,14979537.971967343,14979521.122169375,14979503.976867974,14979486.531225193,14979468.780336913,14979450.719232462,14979432.34287423,14979413.646157358,14979394.623909391,14979375.270889988,14979355.581790652,14979335.551234499,14979315.173776055,14979294.443901045,14979273.35602629,14979251.904499572,14979230.083599571,14979207.88753583,14979185.310448758,14979162.346409708,14979138.989421021,14979115.233416222,14979091.072260186,14979066.499749374,14979041.509612164,14979016.095509168,14978990.251033667,14978963.9697121,14978937.245004568,14978910.07030547,14978882.438944172,14978854.344185757,14978825.779231861,14978796.737221563,14978767.211232383,14978737.194281366,14978706.679326233,14978675.659266647,14978644.126945553,14978612.075150657,14978579.49661596,14978546.384023447,14978512.730004838,14978478.527143518,14978443.767976515,14978408.444996685,14978372.550654937,14978336.077362664,14978299.017494287,14978261.36338994,14978223.107358284,14978184.241679538,14978144.758608576,14978104.65037829,14978063.909203038,14978022.527282318,14977980.49680458,14977937.809951283,14977894.458901083,14977850.435834266,14977805.732937355,14977760.342407916,14977714.256459652,14977667.46732762,14977619.967273764,14977571.748592608,14977522.80361726,14977473.124725614,14977422.704346836,14977371.53496811,14977319.609141618,14977266.919491861,14977213.45872319,14977159.219627697,14977104.195093345,14977048.378112432,14976991.76179036,14976934.339354735,14976876.104164753,14976817.04972098,14976757.169675393,14976696.457841853,14976634.908206873,14976572.51494074,14976509.272409061,14976445.17518463,14976380.218059694,14976314.3960586,14976247.704450853,14976180.13876453,14976111.69480016,14976042.368644966,14975972.156687528,14975901.05563289,14975829.062518083,14975756.174728055,14975682.390012065,14975607.706500512,14975532.122722182,14975455.637621965,14975378.250579005,14975299.9614253,14975220.770464763,14975140.678492712,14975059.686815834,14974977.797272585,14974895.012254054,14974811.334725253,14974726.768246874,14974641.316997487,14974554.985796142,14974467.780125478,14974379.706155144,14974290.770765789,14974200.98157331,14974110.346953625,14974018.876067769,14973926.578887403,14973833.466220671,14973739.54973844,14973644.842000863,14973549.356484253,14973453.107608315,14973356.110763611,14973258.382339306,14973159.9397512,14973060.801469944,14972960.987049444,14972860.51715548,14972759.413594432,14972657.69934215,14972555.398572877,14972452.536688242,14972349.140346253,14972245.2374903,14972140.857378064,14972036.030610332,14971930.789159693,14971825.166399032,14971719.197129797,14971612.917609997,14971506.365581874,14971399.58029918,14971292.602554064,14971185.47470345,14971078.240694888,14970970.94609183,14970863.638098221,14970756.36558243,14970649.179100353,14970542.130917748,14970435.275031615,14970328.667190693,14970222.364914838,14970116.427513406,14970010.916102435,14969905.893620605,14969801.424843937,14969697.576399114,14969594.416775407,14969492.016335081,14969390.44732228,14969289.783870306,14969190.102007153,14969091.479659379,14968993.99665412,14968897.734719247,14968802.777481616,14968709.210463347,14968617.121076029,14968526.598612897,14968437.734238854,14968350.620978344,14968265.353700975,14968182.029104963,14968100.74569824,14968021.603777317,14967944.70540378,14967870.15437851,14967798.056213485,14967728.518101333,14967661.648882475,14967597.55900995,14967536.360511987,14967478.166952215,14967423.0933877,14967371.256324735,14967322.773672491,14967277.764694583,14967236.34995862,14967198.651283782,14967164.791686589,14967134.895324882,14967109.087440172,14967087.494298458,14967070.243129674,14967057.462065801,14967049.280077994,14967045.826912638,14967047.233026702,14967053.629522478,14967065.14808191,14967081.920900717,14967104.080622558,14967131.760273363,14967165.093196195,14967204.212986762,14967249.253429908,14967300.348437287,14967357.63198653,14967421.238062132,14967491.300598375,14967567.953424565,14967651.330212845,14967741.56442891,14967838.78928593,14967943.137701947,14968054.742261086,14968173.735178864,14968300.248271925,14968434.412932508,14968576.360107899,14968726.220285296,14968884.123482227,14969050.199242946,14969224.576641057,14969407.384288635,14969598.750352107,14969798.802575251,14970007.668309432,14970225.474551488,14970452.347989338,14970688.415055677,14970933.80198985,14971188.6349082,14971453.039883006,14971727.1430302,14972011.07060599,14972304.949112535,14972608.905412741,14972923.06685434,14973247.56140322,14973582.517786123,14973928.065642662,14974284.335686777,14974651.459877428,14975029.571598621,14975418.805848652,14975819.299438413,14976231.191198714,14976654.622196458,14977089.735959396,14977536.678709485,14977995.599604312,14978466.650986679,14978949.988641804,14979445.772061963,14979954.164718263,14980475.334339198,14981009.453195516,14981556.698391218,14982117.252160065,14982691.302167315,14983279.041816195,14983880.670558605,14984496.394209657,14985126.425265435,14985770.983223615,14986430.294906236,14987104.594784254,14987794.125303196,14988499.137209412,14989219.889876347,14989956.651630266,14990709.700074762,14991479.32241359,14992265.8157711,14993069.487509742,14993890.655544054,14994729.648650415,14995586.806772182,14996462.48131932,14997357.035462225,14998270.844418943,14999204.295735285,15000157.789557267,15001131.738895245,15002126.569879316,15003142.722005261,15004180.64837067,15005240.81590059,15006323.70556226,15007429.812568463,15008559.646568917,15009713.731829353,15010892.607397767,15012096.827257447,15013326.960466392,15014583.591282664,15015867.319275381,15017178.759420976,15018518.542184386,15019887.313584857,15021285.73524614,15022714.484430773,15024174.25405817,15025665.752706401,15027189.704597402,15028746.849565443,15030337.94300874,15031963.755824052,15033625.074324217,15035322.700138438,15037057.450095413,15038830.15608911,15040641.664927332,15042492.83816294,15044384.55190787,15046317.696629927,15048293.176932465,15050311.911317024,15052374.831929058,15054482.884286819,15056637.026993683,15058838.231433889,15061087.481452089,15063385.773016714,15065734.113867521,15068133.523147505,15070585.031019367,15073089.678266905,15075648.515881546,15078262.604634292,15080933.014633473,15083660.8248685,15086447.122740053,15089293.003577013,15092199.570140436,15095167.93211503,15098199.2055884,15101294.512518546,15104454.980189886,15107681.740658328,15110975.930185677,15114338.688663905,15117771.159029562,15121274.48666891,15124849.818814106,15128498.303930875,15132221.091098197,15136019.32938034,15139894.167191822,15143846.751655616,15147878.227955215,15151989.73868085,15156182.423170537,15160457.416846205,15164815.850545548,15169258.849850018,15173787.534409428,15178403.017263658,15183106.404161997,15187898.792880505,15192781.272537999,15197754.922911074,15202820.813748669,15207980.004086696,15213233.541563138,15218582.461734254,15224027.787392184,15229570.527884582,15235211.678436728,15240952.219476525,15246793.115962949,15252735.316718323,15258779.753764994,15264927.341666697,15271178.976875234,15277535.537082732,15283997.88058006,15290566.845621698,15297243.249797497,15304027.88941177,15310921.538869984,15317924.95007351,15325038.851822753,15332263.949228879,15339600.923134651,15347050.429544477,15354613.099064002,15362289.536349561,15370080.319567597,15377985.999864358,15386007.100845981,15394144.118069183,15402397.518542662,15410767.740239315,15419255.191619422,15427860.251164809,15436583.266924068,15445424.556068856,15454384.404461278,15463463.066232342,15472660.763371466,15481977.685326975,15491413.988617498,15500969.796454262,15510645.198374085,15520440.249883052,15530354.972110722,15540389.351474728,15550543.33935571,15560816.85178236,15571209.769126553,15581721.935808409,15592353.16001105,15603103.21340524,15613971.830883477,15624958.710303653,15636063.512242239,15647285.859756842,15658625.338158194,15670081.49479157,15681653.83882771,15693341.84106322,15705144.9337307,15717062.51031861,15729093.925401181,15741238.494478505,15753495.4938271,15765864.160361316,15778343.691505805,15790933.24507966,15803631.939192474,15816438.852152927,15829353.022390429,15842373.448390376,15855499.088643692,15868728.861611295,15882061.645704221,15895496.279280184,15909031.560657334,15922666.248146057,15936399.060099687,15950228.67498501,15964153.731473472,15978172.828554034,15992284.525668645,16006487.34287122,16020779.761011234,16035160.22194272,16049627.128759842,16064178.846059855,16078813.700234447,16093529.979790434,16108325.935700668,16123199.781786054,16138149.695129534,16153173.81652277,16168270.250946451,16183437.068084678,16198672.30287436,16213973.95608997,16229339.99496436,16244768.353845987,16260256.934893006,16275803.608804457,16291406.21558886,16307062.565370318,16322770.439232137,16338527.590097997,16354331.743650533,16370180.599287052,16386071.831112135,16402003.088966642,16417971.999492647,16433976.167233707,16450013.175769666,16466080.588885304,16482175.951771822,16498296.792260207,16514440.622085359,16530604.938179802,16546787.223995686,16562984.950853685,16579195.57931735,16595416.560591374,16611645.337942157,16627879.348138915,16644116.022913612,16660352.790437877,16676587.07681496,16692816.307584822,16709037.909240276,16725249.31075218,16741447.945101527,16757631.250816368,16773796.673511244,16789941.66742711,16806063.696969412,16822160.23824209,16838228.780575365,16854266.82804495,16870271.900980547,16886241.53746133,16902173.294796273,16918064.750987157,16933913.506171994,16949717.1840469,16965473.433264278,16981179.928805213,16996834.37332432,17012434.498464856,17027978.06614252,17043462.869795926,17058886.735602237,17074247.523656163,17089543.12911092,17104771.483279597,17119930.554695524,17135018.350130495,17150032.915569507,17164972.337141,17179834.742001563,17194618.299174245,17209321.220339656,17223941.760579146,17238478.21906945,17252928.93972846,17267292.311811462,17281566.770457834,17295750.79718782,17309842.92034942,17323841.71551532,17337745.80583003,17351553.862307407,17365264.604078893,17378876.798592784,17392389.261765096,17405800.858082518,17419110.5006581,17432317.15124043,17445419.82017704,17458417.566332966,17471309.496965252,17484094.767554596,17496772.58159499,17509342.190342553,17521802.892524704,17534154.03401084,17546395.007445764,17558525.251847193,17570544.252168544,17582451.538828507,17594246.687208552,17605929.31711998,17617499.09224174,17628955.719530575,17640298.948604785,17651528.57110317,17662644.4200205,17673646.369021,17684534.331731237,17695308.261013903,17705968.148223802,17716514.02244756,17726945.949728373,17737264.032277174,17747468.40767157,17757559.2480439,17767536.759259667,17777401.18008765,17787152.781362932,17796791.865144096,17806318.763865635,17815733.83948701,17825037.48263908,17834230.111769397,17843312.172287054,17852284.13570839,17861146.4988043,17869899.782750208,17878544.5322796,17887081.314841874,17895510.719765484]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[6199516.260960665,6199516.204225871,6199516.146435249,6199516.087569148,6199516.027607553,6199515.966530079,6199515.904315961,6199515.84094404,6199515.776392781,6199515.710640224,6199515.643664024,6199515.575441405,6199515.505949173,6199515.435163699,6199515.363060915,6199515.289616314,6199515.2148049185,6199515.138601294,6199515.060979533,6199514.981913244,6199514.901375545,6199514.819339056,6199514.735775885,6199514.650657617,6199514.563955319,6199514.475639509,6199514.385680158,6199514.294046689,6199514.200707941,6199514.105632179,6199514.0087870825,6199513.910139725,6199513.809656565,6199513.707303445,6199513.603045555,6199513.496847461,6199513.388673053,6199513.278485552,6199513.1662474945,6199513.051920725,6199512.935466373,6199512.816844846,6199512.696015815,6199512.572938199,6199512.447570154,6199512.31986906,6199512.189791499,6199512.057293246,6199511.922329257,6199511.784853647,6199511.644819673,6199511.502179731,6199511.356885328,6199511.208887063,6199511.05813462,6199510.904576751,6199510.748161251,6199510.588834936,6199510.426543647,6199510.261232206,6199510.092844413,6199509.921323018,6199509.746609711,6199509.568645095,6199509.387368669,6199509.202718802,6199509.014632724,6199508.823046484,6199508.627894955,6199508.429111796,6199508.226629419,6199508.020378994,6199507.810290402,6199507.596292225,6199507.378311703,6199507.156274739,6199506.930105847,6199506.699728141,6199506.465063299,6199506.226031553,6199505.982551637,6199505.734540781,6199505.481914672,6199505.224587436,6199504.962471583,6199504.695478019,6199504.423515973,6199504.146492999,6199503.864314916,6199503.576885805,6199503.284107955,6199502.985881841,6199502.6821060795,6199502.372677411,6199502.057490647,6199501.736438642,6199501.409412265,6199501.076300342,6199500.736989641,6199500.39136482,6199500.039308395,6199499.680700689,6199499.315419802,6199498.943341566,6199498.564339507,6199498.17828479,6199497.785046191,6199497.384490037,6199496.9764801795,6199496.560877926,6199496.13754201,6199495.706328534,6199495.267090924,6199494.819679884,6199494.3639433365,6199493.899726376,6199493.426871215,6199492.945217129,6199492.454600401,6199491.954854287,6199491.445808922,6199490.927291276,6199490.399125124,6199489.861130946,6199489.313125892,6199488.754923699,6199488.186334646,6199487.607165491,6199487.017219387,6199486.4162958255,6199485.804190582,6199485.18069561,6199484.545599014,6199483.898684949,6199483.239733562,6199482.5685209,6199481.884818857,6199481.188395073,6199480.479012873,6199479.756431181,6199479.020404438,6199478.270682506,6199477.507010623,6199476.729129253,6199475.936774067,6199475.129675793,6199474.307560173,6199473.47014784,6199472.617154228,6199471.748289487,6199470.863258368,6199469.9617601475,6199469.0434884885,6199468.108131378,6199467.155370993,6199466.184883598,6199465.196339444,6199464.189402645,6199463.163731077,6199462.118976249,6199461.054783194,6199459.970790349,6199458.866629424,6199457.741925275,6199456.596295802,6199455.42935179,6199454.24069678,6199453.029926972,6199451.796631026,6199450.540389975,6199449.260777063,6199447.957357593,6199446.6296888,6199445.277319677,6199443.899790842,6199442.496634381,6199441.067373662,6199439.611523216,6199438.128588542,6199436.61806595,6199435.079442391,6199433.512195283,6199431.915792332,6199430.289691348,6199428.633340071,6199426.946175977,6199425.227626092,6199423.477106798,6199421.694023623,6199419.877771069,6199418.0277323695,6199416.1432793075,6199414.223772005,6199412.268558674,6199410.2769754445,6199408.248346095,6199406.181981856,6199404.0771811595,6199401.933229409,6199399.74939873,6199397.5249477355,6199395.25912127,6199392.951150151,6199390.600250918,6199388.205625555,6199385.766461234,6199383.281930026,6199380.751188647,6199378.173378134,6199375.547623597,6199372.873033893,6199370.148701338,6199367.373701401,6199364.547092393,6199361.667915139,6199358.735192665,6199355.747929866,6199352.705113169,6199349.605710191,6199346.448669399,6199343.232919738,6199339.957370287,6199336.620909883,6199333.222406747,6199329.760708113,6199326.234639816,6199322.643005933,6199318.984588336,6199315.258146323,6199311.462416171,6199307.596110733,6199303.657918991,6199299.646505612,6199295.560510518,6199291.3985484,6199287.1592082875,6199282.8410530435,6199278.442618894,6199273.962414947,6199269.398922668,6199264.750595393,6199260.015857791,6199255.193105349,6199250.280703832,6199245.276988725,6199240.180264684,6199234.988804974,6199229.700850857,6199224.314611055,6199218.828261108,6199213.239942774,6199207.547763422,6199201.749795379,6199195.844075289,6199189.828603472,6199183.701343234,6199177.4602201935,6199171.103121597,6199164.627895607,6199158.032350581,6199151.314254339,6199144.471333428,6199137.50127235,6199130.401712801,6199123.170252884,6199115.804446297,6199108.301801533,6199100.659781043,6199092.8758004,6199084.947227416,6199076.871381285,6199068.645531707,6199060.266897935,6199051.732647909,6199043.039897264,6199034.1857084,6199025.167089506,6199015.980993559,6199006.624317328,6198997.093900328,6198987.386523792,6198977.4989095945,6198967.427719162,6198957.1695523895,6198946.720946499,6198936.078374906,6198925.238246048,6198914.196902205,6198902.950618292,6198891.495600637,6198879.82798572,6198867.943838913,6198855.839153184,6198843.509847773,6198830.951766864,6198818.160678203,6198805.13227174,6198791.862158169,6198778.345867542,6198764.578847775,6198750.55646316,6198736.273992867,6198721.726629388,6198706.909476961,6198691.817549999,6198676.4457714455,6198660.78897112,6198644.841884049,6198628.599148749,6198612.055305494,6198595.2047945075,6198578.041954216,6198560.561019375,6198542.756119218,6198524.621275546,6198506.150400821,6198487.337296193,6198468.1756494725,6198448.65903314,6198428.780902263,6198408.53459238,6198387.913317379,6198366.910167312,6198345.518106191,6198323.72996972,6198301.5384630365,6198278.93615834,6198255.91549257,6198232.468764966,6198208.588134636,6198184.265618052,6198159.493086538,6198134.262263678,6198108.564722713,6198082.391883862,6198055.735011635,6198028.58521207,6198000.933429941,6197972.770445911,6197944.086873656,6197914.873156902,6197885.119566464,6197854.816197188,6197823.952964893,6197792.519603208,6197760.505660402,6197727.900496147,6197694.693278218,6197660.8729791455,6197626.42837284,6197591.3480311185,6197555.620320203,6197519.233397152,6197482.175206262,6197444.433475361,6197405.995712089,6197366.849200108,6197326.980995231,6197286.377921527,6197245.026567336,6197202.913281233,6197160.024167924,6197116.345084108,6197071.861634231,6197026.55916619,6196980.422767,6196933.437258361,6196885.587192176,6196836.856845999,6196787.230218404,6196736.691024312,6196685.222690208,6196632.80834935,6196579.430836839,6196525.0726846615,6196469.7161166435,6196413.343043365,6196355.93505693,6196297.473425756,6196237.939089207,6196177.312652211,6196115.574379766,6196052.704191397,6195988.681655528,6195923.485983755,6195857.096025088,6195789.490260088,6195720.646794922,6195650.543355357,6195579.157280674,6195506.46551749,6195432.444613526,6195357.070711274,6195280.3195416005,6195202.166417268,6195122.58622637,6195041.553425707,6194959.0420340635,6194875.025625418,6194789.477322083,6194702.369787741,6194613.675220448,6194523.365345486,6194431.411408248,6194337.784166935,6194242.453885251,6194145.390324992,6194046.562738577,6193945.939861498,6193843.489904687,6193739.180546839,6193632.978926646,6193524.851634946,6193414.764706864,6193302.683613798,6193188.573255438,6193072.39795164,6192954.121434272,6192833.706839039,6192711.116697159,6192586.312927079,6192459.2568260785,6192329.909061837,6192198.229663975,6192064.178015513,6191927.712844315,6191788.7922144905,6191647.373517735,6191503.413464693,6191356.868076218,6191207.692674674,6191055.84187517,6190901.269576807,6190743.928953893,6190583.772447162,6190420.751754989,6190254.817824592,6190085.920843259,6189914.010229584,6189739.034624713,6189560.941883628,6189379.6790664485,6189195.192429778,6189007.42741809,6188816.328655195,6188621.839935695,6188423.904216601,6188222.463608944,6188017.459369523,6187808.83189271,6187596.520702395,6187380.464443988,6187160.600876615,6186936.866865376,6186709.198373782,6186477.530456351,6186241.797251352,6186001.93197372,6185757.866908183,6185509.533402591,6185256.861861415,6184999.781739554,6184738.221536305,6184472.108789639,6184201.370070747,6183925.930978843,6183645.716136319,6183360.649184171,6183070.652777805,6182775.648583166,6182475.557273274,6182170.298525118,6181859.791016984,6181543.952426204,6181222.699427374,6180895.947691011,6180563.611882749,6180225.605662996,6179881.841687194,6179532.231606573,6179176.686069551,6178815.114723707,6178447.426218379,6178073.528207969,6177693.327355897,6177306.729339256,6176913.638854251,6176513.959622363,6176107.594397343,6175694.444972978,6175274.4121917905,6174847.395954533,6174413.295230664,6173972.008069727,6173523.431613733,6173067.462110525,6172603.994928198,6172132.924570587,6171654.144693868,6171167.5481242705,6170673.026876994,6170170.472176315,6169659.774476927,6169140.823486579,6168613.508189985,6168077.716874112,6167533.337154852,6166980.256005044,6166418.3597840285,6165847.5342686325,6165267.664685686,6164678.635746079,6164080.331680442,6163472.636276387,6162855.4329174645,6162228.604623752,6161592.034094207,6160945.603750736,6160289.195784068,6159622.6922014225,6158945.974876014,6158258.925598463,6157561.426130057,6156853.358257941,6156134.603852275,6155405.044925337,6154664.5636926275,6153913.042635955,6153150.364568605,6152376.412702451,6151591.070717229,6150794.222831777,6149985.753877417,6149165.549373349,6148333.4956041835,6147489.4796995055,6146633.389715519,6145765.114718765,6144884.544871913,6143991.571521522,6143086.087287894,6142167.9861569,6141237.163573755,6140293.516538755,6139336.943704887,6138367.345477347,6137384.624114819,6136388.683832526,6135379.430907024,6134356.773782573,6133320.623179128,6132270.892201813,6131207.496451792,6130130.3541384945,6129039.386193058,6127934.516382907,6126815.671427357,6125682.781114134,6124535.778416674,6123374.599612068,6122199.18439959,6121009.476019525,6119805.421372271,6118586.971137507,6117354.079893253,6116106.706234666,6114844.812892355,6113568.366850104,6112277.339461722,6110971.706566822,6109651.4486054,6108316.550730893,6106967.002921543,6105602.800089832,6104223.942189729,6102830.434321501,6101422.286833874,6099999.515423217,6098562.141229546,6097110.1909290515,6095643.696822844,6094162.696921703,6092667.2350265,6091157.360804008,6089633.129857805,6088094.603794031,6086541.850281624,6084974.943106794,6083393.962221412,6081798.993785061,6080190.130200382,6078567.470141498,6076931.118575176,6075281.186774479,6073617.792324586,6071941.059120564,6070251.117356759,6068548.103507589,6066832.160299504,6065103.436673805,6063362.0877401745,6061608.274720646,6059842.164883827,6058063.931469201,6056273.7536013145,6054471.81619369,6052658.309842361,6050833.43070889,6048997.38039274,6047150.365792989,6045292.598959287,6043424.296932024,6041545.68157174,6039656.979377742,6037758.421296057,6035850.242516672,6033932.682260316,6032005.983554758,6030070.393000937,6028126.160528954,6026173.539144308,6024212.784664514,6022244.155446425,6020267.912104607,6018284.317221047,6016293.635046671,6014296.131194965,6012292.072328273,6010281.725837143,6008265.3595132865,6006243.241216684,6004215.63853738,6002182.818452601,6000145.046979772,5998102.588826148,5996055.707035706,5994004.662633955,5991949.714271493,5989891.117866979,5987829.126250293,5985763.988806727,5983695.951122935,5981625.254635524,5979552.136283064,5977476.828162384,5975399.557190005,5973320.544769539,5971240.006465918,5969158.15168734,5967075.183375706,5964991.297706519,5962906.683798954,5960821.523437037,5958735.990802699,5956650.252221521,5954564.465921972,5952478.781808915,5950393.341252067,5948308.276890251,5946223.712452016,5944139.762593374,5942056.532753242,5939974.119027251,5937892.608060391,5935812.07695914,5933732.593223491,5931654.214699335,5929576.989551672,5927500.956258914,5925426.143628719,5923352.570835551,5921280.2474802295,5919209.173671681,5917139.340130982,5915070.728317823,5913003.310579459,5910937.050322089,5908871.902204673,5906807.812355088,5904744.7186084455,5902682.550767456,5900621.230884549,5898560.673565535,5896500.786294469,5894441.469779385,5892382.618318511,5890324.12018652,5888265.8580404,5886207.709344426,5884149.546813696,5882091.238875731,5880032.650149483,5877973.641941227,5875914.072756659,5873853.798828542,5871792.674659295,5869730.553577758,5867667.288309513,5865602.731559983,5863536.736609644,5861469.157920587,5859399.851753686,5857328.676795698,5855255.4947954845,5853180.171208626,5851102.575849753,5849022.583551756,5846940.074831256,5844854.936559531,5842767.0626382185,5840676.354679069,5838582.722687109,5836486.085746444,5834386.372708118,5832283.522879293,5830177.486713174,5828068.226499014,5825955.717051567,5823839.946399438,5821720.916471707,5819598.64378226,5817473.160111298,5815344.513183456,5813212.767342011,5811078.004218694,5808940.323398545,5806799.843079423,5804656.700725577,5802511.053714925,5800363.079979515,5798212.978638733,5796060.970624874,5793907.2993005635,5791752.231067666,5789596.055967239,5787439.088270102,5785281.667057626,5783124.156792311,5780966.947877744,5778810.45720748,5776655.128702507,5774501.433836748,5772349.872150262,5770200.971749672,5768055.28979531,5765913.412974723,5763775.957961966,5761643.571862261,5759516.932641487,5757396.749540024,5755283.763470399,5753178.747398191,5751082.506705679,5748995.87953762,5746919.737128592,5744854.984111322,5742802.558805353,5740763.433485438,5738738.614629021,5736729.1431421805,5734736.0945632765,5732760.579243753,5730803.742505331,5728866.764772876,5726950.861682344,5725057.284162972,5723187.318493108,5721342.286328937,5719523.544705358,5717732.486008381,5715970.537918282,5714239.1633228855,5712539.860200233,5710874.16147004,5709243.634813262,5707649.882459134,5706094.540939118,5704579.280807141,5703105.806325614,5701675.8551166905,5700291.197778261,5698953.637464308,5697665.009429147,5696427.180535219,5695242.048724104,5694111.542450501,5693037.6200789455,5692022.269243058,5691067.506167259,5690175.3749508355,5689347.946814393,5688587.319308663,5687895.615485859,5687274.983033643,5686727.593371992,5686255.64071319,5685861.341085247,5685546.931319191,5685314.66800058,5685166.826385759,5685105.699283402,5685133.595901831,5685252.840662832,5685465.7719825525,5685774.741020182,5686182.110395145,5686690.252873568,5687301.55002479,5688018.3908487,5688843.17037472,5689778.288233288,5690826.147200611,5691989.1517176,5693269.706383734,5694670.214426759,5696193.076148978,5697840.687350968,5699615.437733478,5701519.709278302,5703555.874608834,5705726.295331035,5708033.320355455,5710479.284201042,5713066.505281222,5715797.284172977,5718673.901869358,5721698.618016028,5724873.669132276,5728201.266816991,5731683.595940073,5735322.812819564,5739121.04338511,5743080.381327961,5747202.886237997,5751490.581728182,5755945.453546793,5760569.447677926,5765364.468430659,5770332.376517464,5775474.98712233,5780794.067959238,5786291.337321703,5791968.462124086,5797827.055935587,5803868.67700785,5810094.826297288,5816506.945483304,5823106.414983833,5829894.551969619,5836872.608378975,5844041.7689347975,5851403.149165901,5858957.793434866,5866706.672974753,5874650.683937368,5882790.645455776,5891127.29772417,5899661.3000982,5908393.229219284,5917323.577166454,5926452.749639586,5935781.064178001,5945308.748418706,5955035.938398538,5964962.676904885,5975088.911879543,5985414.494880658,5995939.179607593,6006662.620493841,6017584.371373036,6028703.884223314,6040020.5079951575,6051533.487527991,6063241.962560688,6075144.966841195,6087241.427340219,6099530.163574145,6112009.887041818,6124679.200780014,6137536.599041997,6150580.467103487,6163809.081200026,6177220.608599479,6190813.10781313,6204584.528948498,6218532.714206577,6232655.3985259645,6246950.210375815,6261414.672699217,6276046.204008144,6290842.11963067,6305799.633110734,6320915.857760159,6336187.808362334,6351612.4030263005,6367186.46518965,6382906.725768064,6398769.825448948,6414772.317126031,6430910.668471454,6447181.264641316,6463580.411110345,6480104.336630781,6496749.19631031,6513511.074803405,6530385.989610144,6547369.894476186,6564458.682887324,6581648.191651736,6598934.204562784,6616312.456135033,6633778.635405958,6651328.389795586,6668957.329016286,6686661.029024769,6704435.03600822,6722274.870396663,6740176.030893361,6758133.998515315,6776144.240635862,6794202.215021458,6812303.373854895,6830443.167737246,6848617.049661083,6866820.478947639,6885048.925140754,6903297.871850801,6921562.820541855,6939839.294255775,6958122.841267067,6976409.038662698,6994693.495841344,7012971.857926898,7031239.809091286,7049493.075782098,7067727.429850788,7085938.691577563,7104122.732589424,7122275.478668163,7140392.912445482,7158471.075982703,7176506.073232906,7194494.072383664,7212431.308078883,7230314.083518526,7248138.77243541,7265901.820948488,7283599.749292343,7301229.153422996,7318786.706500238,7336269.160247179,7353673.346187775,7370996.176763439,7388234.646330076,7405385.832037051,7422446.894589857,7439415.078898409,7456287.714613108,7473062.216550969,7489736.085014211,7506306.906004026,7522772.351332074,7539130.178632732,7555378.231278918,7571514.438204571,7587536.813636939,7603443.456741838,7619232.551185133,7634902.364613778,7650451.2480596965,7665877.635269965,7681180.041966549,7696357.065039135,7711407.381674339,7726329.748424729,7741123.000221008,7755786.049330776,7770317.884267033,7784717.568649876,7798984.240024505,7813117.10863877,7827115.456183388,7840978.634497893,7854706.064245328,7868297.233558636,7881751.696661599,7895069.072467135,7908249.043155669,7921291.352736229,7934195.805592788,7946962.265018393,7959590.65173941,7972080.942432206,7984433.16823454,7996647.413253702,8008723.813073528,8020662.553262196,8032463.867882703,8044128.038007777,8055655.390240984]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[8776811.588883396,8776811.596569836,8776811.604399327,8776811.612374531,8776811.620498158,8776811.62877297,8776811.637201784,8776811.645787464,8776811.65453293,8776811.663441157,8776811.67251517,8776811.681758061,8776811.691172967,8776811.700763093,8776811.710531697,8776811.720482105,8776811.730617698,8776811.740941921,8776811.75145829,8776811.762170374,8776811.773081822,8776811.78419634,8776811.795517711,8776811.807049781,8776811.818796478,8776811.830761788,8776811.842949787,8776811.855364615,8776811.868010493,8776811.880891725,8776811.894012691,8776811.907377852,8776811.920991754,8776811.934859022,8776811.948984379,8776811.96337262,8776811.978028646,8776811.992957434,8776812.008164067,8776812.023653714,8776812.039431639,8776812.055503212,8776812.071873898,8776812.088549262,8776812.105534974,8776812.122836815,8776812.140460666,8776812.15841252,8776812.176698482,8776812.195324771,8776812.21429772,8776812.233623784,8776812.253309537,8776812.273361668,8776812.293786999,8776812.314592477,8776812.335785177,8776812.357372308,8776812.37936121,8776812.40175936,8776812.424574379,8776812.447814023,8776812.471486194,8776812.49559895,8776812.520160483,8776812.545179153,8776812.570663465,8776812.596622089,8776812.623063851,8776812.649997747,8776812.677432934,8776812.705378743,8776812.733844684,8776812.762840431,8776812.792375851,8776812.82246099,8776812.853106076,8776812.884321537,8776812.916117987,8776812.948506242,8776812.981497316,8776813.015102433,8776813.049333023,8776813.084200727,8776813.119717406,8776813.155895138,8776813.192746233,8776813.230283225,8776813.268518878,8776813.307466201,8776813.34713844,8776813.387549093,8776813.428711902,8776813.470640872,8776813.513350263,8776813.556854604,8776813.601168696,8776813.64630761,8776813.6922867,8776813.739121612,8776813.786828276,8776813.835422918,8776813.884922074,8776813.935342578,8776813.986701587,8776814.03901657,8776814.092305327,8776814.146585984,8776814.201877011,8776814.258197216,8776814.31556576,8776814.374002162,8776814.433526302,8776814.494158432,8776814.55591918,8776814.618829558,8776814.682910973,8776814.748185229,8776814.814674532,8776814.882401505,8776814.951389194,8776815.021661071,8776815.093241049,8776815.166153485,8776815.240423184,8776815.316075424,8776815.39313594,8776815.471630964,8776815.551587204,8776815.633031866,8776815.715992669,8776815.800497845,8776815.886576148,8776815.974256877,8776816.063569872,8776816.154545523,8776816.2472148,8776816.341609236,8776816.437760962,8776816.535702702,8776816.635467792,8776816.737090187,8776816.840604478,8776816.946045902,8776817.053450344,8776817.16285437,8776817.274295218,8776817.387810824,8776817.50343983,8776817.621221602,8776817.741196232,8776817.863404566,8776817.987888211,8776818.114689548,8776818.243851744,8776818.375418782,8776818.509435453,8776818.64594739,8776818.785001073,8776818.926643856,8776819.070923965,8776819.217890535,8776819.367593613,8776819.520084176,8776819.67541416,8776819.833636465,8776819.99480497,8776820.158974579,8776820.326201199,8776820.496541793,8776820.67005438,8776820.846798062,8776821.026833052,8776821.210220672,8776821.397023398,8776821.587304873,8776821.78112992,8776821.978564579,8776822.179676117,8776822.38453306,8776822.593205208,8776822.805763671,8776823.022280883,8776823.242830623,8776823.467488062,8776823.696329761,8776823.929433718,8776824.166879384,8776824.408747692,8776824.655121092,8776824.90608357,8776825.161720678,8776825.42211957,8776825.68736903,8776825.95755949,8776826.232783081,8776826.513133654,8776826.798706809,8776827.089599937,8776827.385912243,8776827.687744796,8776827.99520054,8776828.308384355,8776828.627403073,8776828.95236553,8776829.28338259,8776829.620567197,8776829.964034399,8776830.313901402,8776830.670287596,8776831.033314614,8776831.403106354,8776831.779789042,8776832.163491257,8776832.554343984,8776832.952480663,8776833.35803723,8776833.771152163,8776834.191966534,8776834.620624054,8776835.05727113,8776835.502056899,8776835.9551333,8776836.416655114,8776836.886780022,8776837.365668658,8776837.85348467,8776838.350394756,8776838.856568756,8776839.372179687,8776839.897403806,8776840.432420675,8776840.977413226,8776841.532567816,8776842.098074298,8776842.67412609,8776843.260920234,8776843.858657466,8776844.4675423,8776845.087783074,8776845.719592044,8776846.363185454,8776847.0187836,8776847.686610915,8776848.366896056,8776849.059871966,8776849.765775973,8776850.484849855,8776851.217339942,8776851.963497192,8776852.72357728,8776853.497840688,8776854.2865528,8776855.089983994,8776855.90840973,8776856.742110655,8776857.5913727,8776858.456487179,8776859.337750886,8776860.235466212,8776861.149941243,8776862.08148987,8776863.0304319,8776863.997093167,8776864.981805654,8776865.9849076,8776867.006743629,8776868.047664862,8776869.10802906,8776870.188200725,8776871.288551249,8776872.409459036,8776873.551309643,8776874.714495916,8776875.899418123,8776877.106484096,8776878.336109398,8776879.58871744,8776880.864739653,8776882.164615633,8776883.488793302,8776884.837729076,8776886.211888006,8776887.611743968,8776889.037779821,8776890.490487589,8776891.97036862,8776893.477933789,8776895.013703676,8776896.578208731,8776898.171989502,8776899.795596806,8776901.449591937,8776903.134546861,8776904.851044439,8776906.599678619,8776908.38105467,8776910.195789384,8776912.044511324,8776913.92786103,8776915.846491266,8776917.801067254,8776919.792266922,8776921.820781145,8776923.887314007,8776925.99258305,8776928.137319548,8776930.32226877,8776932.548190255,8776934.815858098,8776937.126061229,8776939.47960371,8776941.877305033,8776944.32000042,8776946.80854114,8776949.343794823,8776951.926645784,8776954.55799535,8776957.238762204,8776959.969882729,8776962.752311353,8776965.58702091,8776968.475003015,8776971.417268425,8776974.414847432,8776977.468790248,8776980.580167402,8776983.750070153,8776986.979610898,8776990.269923601,8776993.622164227,8776997.037511181,8777000.517165763,8777004.06235263,8777007.674320256,8777011.354341436,8777015.103713762,8777018.92376013,8777022.815829247,8777026.78129617,8777030.821562828,8777034.93805858,8777039.132240763,8777043.405595278,8777047.759637166,8777052.1959112,8777056.715992512,8777061.321487194,8777066.01403296,8777070.795299772,8777075.666990533,8777080.630841743,8777085.688624209,8777090.842143752,8777096.093241936,8777101.443796813,8777106.895723673,8777112.450975837,8777118.11154544,8777123.879464244,8777129.756804477,8777135.745679678,8777141.848245565,8777148.066700919,8777154.403288513,8777160.860296018,8777167.440056965,8777174.144951724,8777180.97740849,8777187.939904304,8777195.0349661,8777202.265171764,8777209.633151235,8777217.141587608,8777224.793218287,8777232.590836147,8777240.537290744,8777248.635489522,8777256.888399074,8777265.299046433,8777273.870520374,8777282.605972763,8777291.50861993,8777300.581744093,8777309.828694772,8777319.2528903,8777328.857819311,8777338.647042295,8777348.624193199,8777358.792981025,8777369.157191513,8777379.720688846,8777390.487417378,8777401.461403443,8777412.64675717,8777424.047674369,8777435.668438444,8777447.513422372,8777459.587090712,8777471.894001674,8777484.438809242,8777497.226265334,8777510.261222035,8777523.54863387,8777537.09356014,8777550.90116732,8777564.976731509,8777579.325640935,8777593.953398554,8777608.865624668,8777624.068059647,8777639.5665667,8777655.367134718,8777671.475881204,8777687.89905524,8777704.64304058,8777721.714358773,8777739.119672397,8777756.865788367,8777774.959661309,8777793.408397049,8777812.21925618,8777831.399657698,8777850.957182772,8777870.89957857,8777891.234762205,8777911.970824786,8777933.116035549,8777954.678846128,8777976.667894894,8777999.09201145,8778021.960221197,8778045.281750057,8778069.06602929,8778093.32270045,8778118.061620455,8778143.292866806,8778169.026742913,8778195.273783596,8778222.044760685,8778249.350688802,8778277.20283126,8778305.612706134,8778334.592092484,8778364.153036743,8778394.30785924,8778425.069160938,8778456.449830301,8778488.463050364,8778521.122305978,8778554.441391228,8778588.434417054,8778623.115819074,8778658.500365578,8778694.603165753,8778731.439678105,8778769.025719097,8778807.377472006,8778846.511495996,8778886.444735445,8778927.194529472,8778968.778621724,8779011.21517042,8779054.522758609,8779098.720404726,8779143.827573372,8779189.864186402,8779236.850634245,8779284.807787538,8779333.75700902,8779383.720165733,8779434.719641516,8779486.778349798,8779539.91974671,8779594.167844493,8779649.547225256,8779706.083055034,8779763.801098201,8779822.727732208,8779882.889962671,8779944.315438827,8780007.032469312,8780071.07003834,8780136.457822232,8780203.226206332,8780271.40630229,8780341.029965745,8780412.129814407,8780484.739246508,8780558.892459696,8780634.624470308,8780711.971133078,8780790.969161242,8780871.65614709,8780954.070582928,8781038.251882484,8781124.240402736,8781212.077466201,8781301.805383645,8781393.467477249,8781487.108104218,8781582.772680847,8781680.507707037,8781780.360791257,8781882.380675955,8781986.617263446,8782093.121642228,8782201.94611376,8782313.14421969,8782426.770769518,8782542.881868739,8782661.534947373,8782782.788788978,8782906.703560058,8783033.340839922,8783162.76365094,8783295.036489211,8783430.225355629,8783568.397787342,8783709.622889573,8783853.971367812,8784001.51556036,8784152.329471193,8784306.488803148,8784464.070991404,8784625.155237256,8784789.822542107,8784958.155741734,8785130.239540739,8785306.160547186,8785486.007307397,8785669.870340867,8785857.842175277,8786050.017381549,8786246.492608959,8786447.366620192,8786652.740326386,8786862.716822041,8787077.401419824,8787296.901685158,8787521.327470597,8787750.79094991,8787985.406651827,8788225.291493377,8788470.564812813,8788721.34840199,8788977.76653817,8789239.94601522,8789508.01617408,8789782.108932443,8790062.358813616,8790348.90297444,8790641.881232215,8790941.43609054,8791247.712763999,8791560.859201595,8791881.026108855,8792208.36696851,8792543.038059676,8792885.19847541,8793235.010138586,8793592.637815977,8793958.249130415,8794332.014571011,8794714.10750123,8795104.7041648,8795503.983689312,8795912.128087435,8796329.322255595,8796755.753970066,8797191.613880346,8797637.095499685,8798092.395192722,8798557.712160068,8799033.24841976,8799519.208785487,8800015.800841486,8800523.234913977,8801041.724039111,8801571.483927254,8802112.732923597,8802665.691964934,8803230.584532587,8803807.636601353,8804397.076584429,8804999.135274224,8805614.045779029,8806242.043455435,8806883.36583652,8807538.252555689,8808206.945266197,8808889.687556276,8809586.724859877,8810298.304363018,8811024.674905727,8811766.086879598,8812522.792120978,8813295.043799799,8814083.096304163,8814887.205120632,8815707.626710402,8816544.618381338,8817398.438156057,8818269.344636086,8819157.596862271,8820063.454171535,8820987.176050166,8821929.021983763,8822889.251304042,8823868.123032682,8824865.895722428,8825882.827295654,8826919.174880639,8827975.194645807,8829051.141632173,8830147.269584294,8831263.830780037,8832401.075859403,8833559.25365282,8834738.611009149,8835939.39262382,8837161.840867402,8838406.195615008,8839672.694076901,8840961.570630679,8842273.056655467,8843607.380368477,8844964.766664399,8846345.436957981,8847749.609030288,8849177.496878985,8850629.310573176,8852105.256113103,8853605.535295269,8855130.345583297,8856679.879985018,8858254.326936213,8859853.870191356,8861478.688721871,8863128.956622194,8864804.843024148,8866506.512019884,8868234.122593861,8869987.828564182,8871767.778533567,8873574.115850408,8875406.978580078,8877266.499486854,8879152.806026703,8881066.020351151,8883006.259322464,8884973.634540347,8886968.252380295,8888990.214043753,8891039.615620231,8893116.548161382,8895221.097767184,8897353.345684174,8899513.368415825,8901701.237844918,8903917.021367958,8906160.78204146,8908432.578740023,8910732.466326032,8913060.495830769,8915416.71464678,8917801.16673121,8920213.892819818,8922654.930651428,8925124.315202435,8927622.078931035,8930148.252030782,8932702.862693094,8935285.937378228,8937897.501094334,8940537.57768406,8943206.190118248,8945903.360796193,8948629.111851962,8951383.465466194,8954166.444182841,8956978.071230324,8959818.37084644,8962687.368606513,8965585.091754192,8968511.569534212,8971466.833526665,8974450.917982038,8977463.860156517,8980505.700646931,8983576.483724734,8986676.257668426,8989805.075093906,8992962.993282098,8996150.074503347,8999366.38633804,9002612.001992892,9005887.000612393,9009191.467584955,9012525.494843177,9015889.181157913,9019282.632425528,9022705.96194807,9026159.290705867,9029642.74762218,9033156.469819602,9036700.602867795,9040275.301022336,9043880.727454314,9047517.054470468,9051184.463723583,9054883.146412995,9058613.303474942,9062375.14576266,9066168.894216035,9069994.780020727,9073853.04475668,9077743.940535901,9081667.7301295,9085624.68708398,9089615.095826702,9093639.251760619,9097697.461348273,9101790.04218509,9105917.323062096,9110079.644018088,9114277.356381385,9118510.82280129,9122780.417269347,9127086.525130587,9131429.543084867,9135809.879178515,9140227.952786373,9144684.19458454,9149179.046513839,9153712.961734345,9158286.40457108,9162899.850451076,9167553.785832055,9172248.708122859,9176985.125595871,9181763.557291599,9186584.53291563,9191448.592728123,9196356.287426049,9201308.178018343,9206304.83569414,9211346.841684258,9216434.787116097,9221569.2728621,9226750.909381894,9231980.316558288,9237258.123527205,9242584.96850167,9247961.49858998,9253388.36960811,9258866.245886462,9264395.800070997,9269977.71291884,9275612.673088372,9281301.376923867,9287044.528234672,9292842.838068994,9298697.024482224,9304607.812299863,9310575.932874972,9316602.12384019,9322687.128854211,9328831.697342733,9335036.584233819,9341302.549687587,9347630.358820215,9354020.78142214,9360474.591670433,9366992.567835221,9373575.491980124,9380224.149656583,9386939.32959203,9393721.823371785,9400572.425114604,9407491.931141805,9414481.139639858,9421540.85031635,9428671.864049299,9435874.98252964,9443151.007896885,9450500.742367826,9457924.987858228,9465424.545597443,9473000.215735849,9480652.796945097,9488383.086011024,9496191.877419293,9504079.962933563,9512048.131166263,9520097.167141838,9528227.851852473,9536440.961806217,9544737.268567497,9553117.538289974,9561582.531241698,9570133.001322554,9578769.695573935,9587493.353680663,9596304.707465064,9605204.480373245,9614193.38695348,9623272.132326726,9632441.411649227,9641701.909567177,9651054.299663423,9660499.243896177,9670037.392029746,9679669.381057197,9689395.834615007,9699217.36238959,9709134.559515808,9719148.005967304,9729258.265938789,9739465.887220168,9749771.400562583,9760175.319036327,9770678.137380715,9781280.331345873,9791982.357026523,9802784.650187852,9813687.625583487,9824691.676265733,9835797.17288812,9847004.463000508,9858313.870336808,9869725.694095626,9881240.208214007,9892857.660634559,9904578.2725663,9916402.237739578,9928329.721655412,9940360.860829815,9952495.762033494,9964734.501527544,9977077.124295752,9989523.643274156,10002074.038578637,10014728.25673131,10027486.209886625,10040347.775058085,10053312.7933466,10066381.06917159,10079552.369505944,10092826.423116105,10106202.91980856,10119681.509684112,10133261.802401425,10146943.366451284,10160725.728443224,10174608.372406185,10188590.739104852,10202672.225373574,10216852.183469577,10231129.920447497,10245504.697557094,10259975.729666155,10274542.184710674,10289203.183174301,10303957.7975992,10318805.052130418,10333743.922095865,10348773.333624098,10363892.163301948,10379099.237874176,10394393.333987199,10409773.177979002,10425237.445717199,10440784.762487257,10456413.7029328,10472122.791049812,10487910.500236562,10503775.253400873,10519715.423126398,10535729.331899308,10551815.252396861,10567971.407839023,10584195.972404283,10600487.071710678,10616842.783362819,10633261.137565637,10649740.117805345,10666277.66159801,10682871.661305899,10699519.965021629,10716220.377519948,10732970.661276834,10749768.537555344,10766611.687557546,10783497.753641589,10800424.340602888,10817389.01701811,10834389.316650532,10851422.739915146,10868486.75540168,10885578.801453598,10902696.2878009,10919836.59724442,10936997.087389145,10954175.092423994,10971367.924945232,10988572.877820665,11005787.226091638,11023008.228909642,11040233.131504351,11057459.167179769,11074683.559335046,11091903.523506504,11109116.269427352,11126319.00310148,11143508.92888768,11160683.251590736,11177839.178555597,11194973.921761038,11212084.699909171,11229168.740507072,11246223.28193703,11263245.575511783,11280232.887511235,11297182.501197267,11314091.718803199,11330957.86349464,11347778.281298593,11364550.342997614,11381271.445986122,11397939.016085986,11414550.509318614,11431103.413630977,11447595.25057306,11464023.576924415,11480385.986267665,11496680.11050688,11512903.62132895,11529054.231606267,11545129.696739087,11561127.815936228,11577046.433432793,11592883.439643877,11608636.772253308,11624304.417236667,11639884.409817923,11655374.835359313,11670773.830184089,11686079.582332015,11701290.332247563,11716404.373401029,11731420.052842738,11746335.771690818,11761149.985553037,11775861.20488342,11790467.99527434,11804968.97768506,11819362.828607671,11833648.280171564,11847824.12018759,11861889.192133268,11875842.39508032,11889682.683566047,11903409.067410063,11917020.611477919,11930516.435393315,11943895.713200577,11957157.67297912,11970301.5964117,11983326.818308245,11996232.726087194,12009018.759216044,12021684.408613179,12034229.216012752,12046652.773294562,12058954.721780848,12071134.751501905,12083192.600432374,12095128.053700184,12106940.942769907,12118631.14460251,12130198.580793185,12141643.216689242,12152965.060489679,12164164.162328277,12175240.613341905,12186194.544725731,12197026.126776908,12207735.56792843,12218323.113774655,12228789.046090022,12239133.681842398,12249357.37220253,12259460.501550924,12269443.486483501,12279306.774817273,12289050.844597295,12298676.203106068,12308183.385876441,12317572.95570921],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[17073628.86246494,17073628.8987033,17073628.935616065,17073628.973215777,17073629.01151522,17073629.050527416,17073629.090265628,17073629.130743373,17073629.17197441,17073629.213972755,17073629.256752692,17073629.300328758,17073629.34471578,17073629.389928844,17073629.435983323,17073629.48289488,17073629.53067945,17073629.579353303,17073629.628932964,17073629.67943531,17073629.730877496,17073629.783277027,17073629.836651713,17073629.891019695,17073629.946399465,17073630.00280985,17073630.060270034,17073630.11879954,17073630.178418282,17073630.239146527,17073630.301004916,17073630.3640145,17073630.428196676,17073630.493573282,17073630.560166538,17073630.62799909,17073630.697094,17073630.767474763,17073630.8391653,17073630.912189994,17073630.986573674,17073631.062341623,17073631.139519602,17073631.218133852,17073631.29821111,17073631.379778594,17073631.462864038,17073631.54749569,17073631.63370232,17073631.72151324,17073631.810958315,17073631.902067944,17073631.994873114,17073632.089405365,17073632.185696844,17073632.28378029,17073632.383689046,17073632.485457093,17073632.58911901,17073632.694710057,17073632.802266132,17073632.9118238,17073633.023420304,17073633.137093596,17073633.25288232,17073633.37082584,17073633.490964256,17073633.613338415,17073633.73798992,17073633.864961147,17073633.994295284,17073634.12603627,17073634.260228932,17073634.39691886,17073634.536152545,17073634.677977324,17073634.822441403,17073634.969593912,17073635.119484864,17073635.27216524,17073635.42768693,17073635.586102806,17073635.747466747,17073635.911833595,17073636.07925923,17073636.249800585,17073636.423515636,17073636.600463435,17073636.78070415,17073636.964299053,17073637.151310556,17073637.341802247,17073637.535838887,17073637.73348644,17073637.934812095,17073638.1398843,17073638.348772787,17073638.56154855,17073638.778283935,17073638.999052636,17073639.223929696,17073639.452991556,17073639.686316103,17073639.923982654,17073640.166072015,17073640.412666466,17073640.663849864,17073640.91970758,17073641.180326615,17073641.445795555,17073641.716204654,17073641.99164584,17073642.27221274,17073642.558000747,17073642.849107012,17073643.145630486,17073643.44767199,17073643.75533419,17073644.068721686,17073644.387941007,17073644.71310066,17073645.0443112,17073645.38168521,17073645.72533738,17073646.075384527,17073646.43194565,17073646.79514196,17073647.165096924,17073647.541936293,17073647.92578817,17073648.316783044,17073648.715053827,17073649.1207359,17073649.533967156,17073649.954888087,17073650.38364175,17073650.820373904,17073651.265233003,17073651.718370263,17073652.17993971,17073652.65009824,17073653.129005663,17073653.616824772,17073654.113721393,17073654.61986441,17073655.135425866,17073655.660581015,17073656.195508346,17073656.740389697,17073657.29541026,17073657.860758696,17073658.43662715,17073659.023211375,17073659.620710738,17073660.22932832,17073660.84927099,17073661.48074944,17073662.123978317,17073662.779176243,17073663.446565893,17073664.126374096,17073664.818831917,17073665.524174687,17073666.24264213,17073666.97447844,17073667.719932325,17073668.47925715,17073669.252710965,17073670.040556636,17073670.843061913,17073671.66049953,17073672.493147288,17073673.341288146,17073674.20521035,17073675.085207485,17073675.981578592,17073676.8946283,17073677.824666888,17073678.772010393,17073679.73698074,17073680.719905853,17073681.721119735,17073682.740962602,17073683.779781003,17073684.83792793,17073685.915762942,17073687.013652276,17073688.131968983,17073689.27109305,17073690.431411516,17073691.61331865,17073692.817216013,17073694.043512665,17073695.29262524,17073696.56497815,17073697.861003667,17073699.18114212,17073700.52584202,17073701.895560205,17073703.290762026,17073704.711921457,17073706.15952131,17073707.63405336,17073709.136018522,17073710.66592702,17073712.224298567,17073713.81166254,17073715.42855814,17073717.075534612,17073718.753151383,17073720.461978298,17073722.202595785,17073723.975595064,17073725.781578325,17073727.621158976,17073729.494961802,17073731.403623205,17073733.347791407,17073735.32812669,17073737.3453016,17073739.400001157,17073741.492923148,17073743.624778282,17073745.796290506,17073748.008197192,17073750.261249438,17073752.55621226,17073754.89386491,17073757.27500112,17073759.700429343,17073762.170973063,17073764.68747105,17073767.25077766,17073769.861763127,17073772.52131382,17073775.230332587,17073777.98973905,17073780.800469875,17073783.663479153,17073786.579738673,17073789.550238285,17073792.57598619,17073795.658009335,17073798.797353707,17073801.99508473,17073805.25228759,17073808.570067625,17073811.9495507,17073815.391883545,17073818.898234203,17073822.469792377,17073826.10776986,17073829.813400924,17073833.587942734,17073837.4326758,17073841.34890438,17073845.337956924,17073849.40118654,17073853.539971426,17073857.75571534,17073862.04984809,17073866.423825994,17073870.87913238,17073875.417278074,17073880.039801937,17073884.74827135,17073889.544282757,17073894.42946219,17073899.40546584,17073904.473980594,17073909.6367246,17073914.89544784,17073920.251932755,17073925.7079948,17073931.265483063,17073936.926280897,17073942.692306563,17073948.565513834,17073954.547892686,17073960.641469944,17073966.84830999,17073973.1705154,17073979.610227723,17073986.16962813,17073992.850938182,17073999.656420562,17074006.58837984,17074013.649163228,17074020.841161385,17074028.166809205,17074035.62858662,17074043.229019456,17074050.970680255,17074058.856189128,17074066.888214666,17074075.069474783,17074083.402737655,17074091.890822638,17074100.536601193,17074109.34299785,17074118.312991194,17074127.449614838,17074136.755958453,17074146.23516877,17074155.89045065,17074165.72506813,17074175.742345523,17074185.94566851,17074196.33848527,17074206.92430764,17074217.706712235,17074228.68934168,17074239.875905782,17074251.27018279,17074262.87602062,17074274.69733815,17074286.73812648,17074299.002450287,17074311.49444917,17074324.21833897,17074337.178413216,17074350.3790445,17074363.82468595,17074377.51987267,17074391.469223246,17074405.67744126,17074420.14931685,17074434.889728256,17074449.903643455,17074465.196121756,17074480.772315502,17074496.637471702,17074512.79693381,17074529.256143417,17074546.020642087,17074563.09607311,17074580.488183375,17074598.202825256,17074616.245958492,17074634.62365215,17074653.34208658,17074672.40755544,17074691.82646774,17074711.60534992,17074731.750847954,17074752.269729525,17074773.1688862,17074794.45533567,17074816.136224005,17074838.21882798,17074860.710557394,17074883.618957486,17074906.951711353,17074930.716642406,17074954.921716888,17074979.575046465,17075004.68489077,17075030.259660084,17075056.307918023,17075082.83838425,17075109.859937266,17075137.381617278,17075165.412629005,17075193.96234465,17075223.040306877,17075252.65623181,17075282.820012122,17075313.541720174,17075344.831611168,17075376.70012641,17075409.157896563,17075442.215745024,17075475.884691283,17075510.175954416,17075545.10095657,17075580.671326526,17075616.898903348,17075653.795740053,17075691.374107357,17075729.64649748,17075768.625628043,17075808.324445955,17075848.756131448,17075889.9341021,17075931.872017007,17075974.583780937,17076018.083548583,17076062.385728925,17076107.5049896,17076153.456261367,17076200.254742645,17076247.915904112,17076296.45549341,17076345.88953984,17076396.234359257,17076447.506558903,17076499.72304242,17076552.901014898,17076607.057987977,17076662.21178509,17076718.380546708,17076775.58273574,17076833.83714294,17076893.162892465,17076953.579447474,17077015.1066158,17077077.76455575,17077141.573781922,17077206.55517122,17077272.729968805,17077340.11979425,17077408.74664774,17077478.632916365,17077549.801380467,17077622.275220174,17077696.078021888,17077771.233784985,17077847.766928516,17077925.70229807,17078005.065172683,17078085.881271858,17078168.176762663,17078251.978266962,17078337.312868685,17078424.208121236,17078512.69205497,17078602.793184765,17078694.54051775,17078787.963561,17078883.092329465,17078979.957353897,17079078.589688946,17079179.02092128,17079281.28317785,17079385.409134246,17079491.43202311,17079599.38564273,17079709.3043656,17079821.223147206,17079935.17753482,17080051.2036764,17080169.33832965,17080289.61887107,17080412.083305165,17080536.77027372,17080663.7190652,17080792.96962417,17080924.56256088,17081058.539160863,17081194.94139469,17081333.811927747,17081475.194130138,17081619.132086635,17081765.670606725,17081914.855234735,17082066.73226001,17082221.34872721,17082378.7524466,17082538.99200449,17082702.11677367,17082868.176923983,17083037.22343284,17083209.30809592,17083384.483537856,17083562.803222954,17083744.32146597,17083929.093442984,17084117.175202202,17084308.623674903,17084503.496686336,17084701.85296667,17084903.752161942,17085109.254845064,17085318.422526788,17085531.317666695,17085748.00368418,17085968.5449694,17086193.00689428,17086421.455823388,17086653.95912488,17086890.585181348,17087131.403400626,17087376.484226577,17087625.8991498,17087879.720718265,17088138.02254786,17088400.879332885,17088668.36685643,17088940.562000647,17089217.542756904,17089499.38823583,17089786.17867721,17090077.995459743,17090374.92111065,17090677.03931507,17090984.43492535,17091297.193970077,17091615.403662946,17091939.15241136,17092268.52982488,17092603.62672334,17092944.53514481,17093291.34835314,17093644.160845377,17094003.068358764,17094368.167877488,17094739.557639055,17095117.337140348,17095501.60714329,17095892.46968017,17096290.028058533,17096694.386865657,17097105.651972618,17097523.930537913,17097949.33101058,17098381.96313287,17098821.93794238,17099269.367773693,17099724.366259474,17100187.04833098,17100657.530218024,17101135.929448318,17101622.364846196,17102116.956530724,17102619.825913128,17103131.09569354,17103650.8898571,17104179.333669234,17104716.553670324,17105262.67766952,17105817.83473784,17106382.15520043,17106955.77062804,17107538.81382764,17108131.418832235,17108733.72088972,17109345.85645094,17109967.963156793,17110600.1798244,17111242.646432366,17111895.504105076,17112558.89509601,17113232.962770067,17113917.85158489,17114613.70707121,17115320.675812114,17116038.905421298,17116768.544520307,17117509.742714617,17118262.65056879,17119027.419580463,17119804.202153258,17120593.15156868,17121394.421956874,17122208.168266322,17123034.546232447,17123873.712345168,17124725.82381532,17125591.038540084,17126469.515067313,17127361.412558783,17128266.890752498,17129186.109923884,17130119.230846044,17131066.414748993,17132027.823277965,17133003.618450783,17133993.96261433,17134999.01840009,17136018.94867898,17137053.916515253,17138104.085119706,17139169.617802195,17140250.677923415,17141347.428846136,17142460.033885807,17143588.656260718,17144733.45904168,17145894.605101347,17147072.257063165,17148266.577250186,17149477.727633644,17150705.869781416,17151951.164806575,17153213.773315955,17154493.85535888,17155791.57037621,17157107.07714971,17158440.533751905,17159792.09749646,17161161.92488929,17162550.17158043,17163956.99231679,17165382.54089596,17166826.97012111,17168290.431757182,17169773.07648848,17171275.053877715,17172796.512326773,17174337.59903921,17175898.459984664,17177479.239865344,17179080.082084704,17180701.128718413,17182342.520487852,17184004.39673616,17185686.89540711,17187390.153026793,17189114.30468844,17190859.484040327,17192625.823277105,17194413.453134473,17196222.5028876,17198053.10035313,17199905.371895153,17201779.442435198,17203675.43546624,17205593.473071102,17207533.675945133,17209496.163423426,17211481.053512633,17213488.46292747,17215518.507132065,17217571.30038617,17219646.95579645,17221745.585372698,17223867.30008936,17226012.209952194,17228180.42407019,17230372.050732844,17232587.197492786,17234825.971253816,17237088.47836431,17239374.82471613,17241685.115848873,17244019.45705963,17246377.953518074,17248760.710386988,17251167.832948036,17253599.426732894,17256055.597659558,17258536.45217379,17261042.097395577,17263572.64127061,17266128.192726538,17268708.861833975,17271314.759972032,17273945.999998365,17276602.696423374,17279284.965588633,17281992.92584912,17284726.69775927,17287486.40426248,17290272.17088396,17293084.125926636,17295922.40066994,17298787.129571155,17301678.450469147,17304596.504790142,17307541.43775536,17310513.398590155,17313512.540734384,17316539.022053774,17319593.00505192,17322674.65708258,17325784.150562122,17328921.663181536,17332087.378117993,17335281.48424534,17338504.17634346,17341755.655305974,17345036.128346056,17348345.80919998,17351684.918328155,17355053.683113143,17358452.338054564,17361881.124960303,17365340.293133926,17368830.099557806,17372350.809071742,17375902.69454669,17379486.03705333,17383101.126025174,17386748.25941581,17390427.743850157,17394139.894769263,17397885.036568474,17401663.502728667,17405475.635940246,17409321.78821967,17413202.321018267,17417117.605323035,17421068.021749254,17425053.96062465,17429075.82206483,17433134.01603996,17437228.962432202,17441361.09108399,17445530.8418368,17449738.664560273,17453985.019171547,17458270.375644602,17462595.21400955,17466960.024341587,17471365.30673968,17475811.57129465,17480299.3380467,17484829.136932254,17489401.507719927,17494016.999935653,17498676.172776848,17503379.59501549,17508127.84489018,17512921.509986985,17517761.187109135,17522647.48213547,17527581.009867642,17532562.393866003,17537592.266274262,17542671.267632782,17547800.04668059,17552979.260146096,17558209.572526526,17563491.655856073,17568826.18946282,17574213.859714452,17579655.359752823,17585151.389217336,17590702.65395731,17596309.865733318,17601973.741907585,17607695.005123485,17613474.382974274,17619312.607661113,17625210.41564048,17631168.54726105,17637187.74639024,17643268.760030374,17649412.337924834,17655619.23215408,17661890.196721867,17668225.98713174,17674627.35995399,17681095.072383236,17687629.881786857,17694232.545244403,17700903.819078304,17707644.458375998,17714455.216503784,17721336.844612617,17728290.091136113,17735315.701281115,17742414.416510966,17749586.97402197,17756834.10621326,17764156.540150438,17771554.99702338,17779030.191598568,17786582.831666313,17794213.617483336,17801923.24121112,17809712.386350464,17817581.727172725,17825531.928148255,17833563.6433725,17841677.515990306,17849874.177619018,17858154.24777081,17866518.333275,17874967.027700786,17883500.910781126,17892120.547838323,17900826.489211995,17909619.26969011,17918499.407943696,17927467.405965958,17936523.748516534,17945668.902571466,17954903.316779807,17964227.420927376,17973641.62540854,17983146.320706725,17992741.876884293,18002428.643082723,18012206.947033603,18022077.094581343,18032039.369218234,18042094.03163263,18052241.319270894,18062481.445913862,18072814.601268496,18083240.950575337,18093760.634232458,18104373.76743648,18115080.43984131,18125880.7152351,18136774.63123596,18147762.19900699,18158843.40299101,18170018.20066549,18181286.52231799,18192648.27084251,18204103.321557034,18215651.522042476,18227292.692003284,18239026.623149846,18250853.079102747,18262771.795318987,18274782.479040083,18286884.80926208,18299078.43672724,18311362.98393735,18323738.045188278,18336203.186625652,18348757.946321122,18361401.83436899,18374134.333002575,18386954.89672996,18399862.952488407,18412857.899816945,18425939.111046337,18439105.931505878,18452357.679746043,18465693.647776447,18479113.10131801,18492615.280068636,18506199.39798147,18519864.64355469,18533610.180132035,18547435.14621294,18561338.655771356,18575319.79858232,18589377.640555058,18603511.224071838,18617719.568331398,18632001.669696044,18646356.50204136,18660783.01710761,18675280.144851893,18689846.793800022,18704481.851397455,18719184.184358176,18733952.639010977,18748786.041642137,18763683.198834024,18778642.897798814,18793663.90670676,18808744.975008614,18823884.83375155,18839082.195888363,18854335.756579515,18869644.19348789,18885006.167066008,18900420.32083567,18915885.281659942,18931399.660007752,18946962.050210956,18962571.03071441,18978225.164319165,18993922.998419233,19009663.06523244,19025443.882025864,19041263.95133649,19057121.76118778,19073015.785302892,19088944.483315405,19104906.30097841,19120899.670372885,19136923.010116346,19152974.72557284,19169053.209065262,19185156.840091214,19201283.985543407,19217432.99993585,19233602.225636978,19249789.993110802,19265994.621167418,19282214.417223874,19298447.677576702,19314692.687687125,19330947.722480174,19347211.046658687,19363480.91503334,19379755.572869662,19396033.25625293,19412312.192471955,19428590.60042252,19444866.691031206,19461138.66770041,19477404.726775013,19493663.058031373,19509911.84518901,19526149.26644535,19542373.49503383,19558582.699805465,19574775.04583401,19590948.695044663,19607101.8068662,19623232.53890625,19639339.04764948,19655419.489178155,19671472.019914616,19687494.797385015,19703485.981003586,19719443.732876573,19735366.218625028,19751251.60822532,19767098.076866377,19782903.805822447,19798666.983340092,19814385.805538148,19830058.47731928,19845683.21329155,19861258.238698617,19876781.790356982,19892252.117598608,19907667.483217318,19923026.164417285,19938326.453761913,19953566.660121325,19968745.109616812,19983860.146560416,19998910.13438791,20013893.456583478,20028808.517594293,20043653.743733324,20058427.5840687,20073128.51129786,20087755.022605024,20102305.640500247,20116778.913638644,20131173.41761815,20145487.75575456,20159720.5598323,20173870.490829714,20187936.239617612,20201916.527629893,20215810.107505098,20229615.76369795,20243332.313059848,20256958.605387535,20270493.523939054,20283935.98591634,20297284.942913868,20310539.381332718,20323698.322759684,20336760.824311044,20349725.97894066,20362592.91571228,20375360.800035868,20388028.833867896,20400596.25587577,20413062.341566302,20425426.403378624,20437687.790741656,20449845.890096553,20461900.1248845,20473849.955500327,20485694.879212484,20497434.430049933,20509068.178656667,20520595.732114494,20532016.733734887,20543330.862820607,20554537.83439809,20565637.398921307,20576629.34194812,20587513.483790066,20598289.679136492,20608957.81665411,20619517.818562955,20629969.640189815,20640313.26950014,20650548.726609576,20660676.063276183,20670695.362374343,20680606.73735163,20690410.33166951,20700106.318229187,20709694.898783483,20719176.303336017,20728550.789528575,20737818.6420179,20746980.171842836,20756035.715782877,20764985.635709226,20773830.317929205,20782570.172525138,20791205.6326886,20799737.154050916,20808165.214010973,20816490.311061,20824712.96411141,20832833.711815342,20840853.111893836,20848771.740462348,20856590.191359334,20864309.075477704,20871929.02009976,20879450.668236233],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[14355053.475377856,14355053.406680075,14355053.336703835,14355053.265425337,14355053.19282035,14355053.118864188,14355053.043531707,14355052.966797294,14355052.88863486,14355052.809017831,14355052.72791913,14355052.645311192,14355052.561165929,14355052.475454727,14355052.388148453,14355052.299217416,14355052.208631381,14355052.11635955,14355052.02237055,14355051.92663243,14355051.829112636,14355051.729778012,14355051.628594786,14355051.525528552,14355051.420544276,14355051.313606257,14355051.204678135,14355051.093722884,14355050.980702778,14355050.865579385,14355050.748313567,14355050.62886546,14355050.507194448,14355050.383259157,14355050.257017465,14355050.128426442,14355049.99744237,14355049.864020718,14355049.728116117,14355049.58968237,14355049.448672414,14355049.305038292,14355049.158731185,14355049.009701341,14355048.857898101,14355048.703269843,14355048.545764012,14355048.385327043,14355048.221904399,14355048.055440513,14355047.885878792,14355047.71316159,14355047.537230188,14355047.358024769,14355047.175484408,14355046.989547048,14355046.800149461,14355046.607227275,14355046.41071489,14355046.2105455,14355046.00665105,14355045.798962211,14355045.587408394,14355045.371917661,14355045.152416758,14355044.928831056,14355044.701084543,14355044.469099794,14355044.232797937,14355043.992098639,14355043.746920068,14355043.497178877,14355043.24279015,14355042.983667415,14355042.71972257,14355042.450865887,14355042.177005963,14355041.8980497,14355041.613902248,14355041.324467031,14355041.02964563,14355040.729337834,14355040.423441537,14355040.111852761,14355039.794465564,14355039.471172057,14355039.141862333,14355038.806424445,14355038.46474435,14355038.1167059,14355037.762190774,14355037.401078464,14355037.0332462,14355036.658568945,14355036.27691932,14355035.888167594,14355035.492181608,14355035.08882675,14355034.677965904,14355034.259459393,14355033.833164966,14355033.398937697,14355032.956629982,14355032.506091459,14355032.04716898,14355031.579706535,14355031.103545232,14355030.618523194,14355030.12447556,14355029.621234382,14355029.1086286,14355028.586483963,14355028.054622991,14355027.512864884,14355026.961025493,14355026.398917241,14355025.826349054,14355025.243126305,14355024.649050757,14355024.043920478,14355023.427529775,14355022.799669132,14355022.160125142,14355021.50868042,14355020.845113538,14355020.169198953,14355019.480706925,14355018.779403431,14355018.065050116,14355017.33740418,14355016.596218295,14355015.841240548,14355015.072214328,14355014.288878273,14355013.490966134,14355012.67820672,14355011.850323798,14355011.007035999,14355010.148056712,14355009.273094002,14355008.381850505,14355007.474023318,14355006.549303915,14355005.607378017,14355004.647925524,14355003.670620352,14355002.675130384,14355001.661117302,14355000.62823651,14354999.576136997,14354998.50446122,14354997.412844999,14354996.30091737,14354995.168300485,14354994.014609447,14354992.839452226,14354991.642429486,14354990.423134467,14354989.181152847,14354987.916062605,14354986.62743387,14354985.314828768,14354983.977801299,14354982.615897164,14354981.228653608,14354979.815599283,14354978.376254078,14354976.910128944,14354975.416725757,14354973.895537116,14354972.346046196,14354970.767726561,14354969.16004198,14354967.522446277,14354965.854383098,14354964.155285753,14354962.424577026,14354960.661668966,14354958.865962686,14354957.03684818,14354955.173704099,14354953.275897536,14354951.342783833,14354949.373706345,14354947.367996218,14354945.324972179,14354943.243940273,14354941.124193678,14354938.965012409,14354936.765663119,14354934.525398819,14354932.243458658,14354929.919067625,14354927.55143632,14354925.139760686,14354922.6832217,14354920.180985142,14354917.63220129,14354915.036004627,14354912.39151357,14354909.697830144,14354906.954039698,14354904.159210594,14354901.31239389,14354898.412623009,14354895.458913425,14354892.450262325,14354889.385648273,14354886.264030863,14354883.084350362,14354879.84552736,14354876.546462407,14354873.186035624,14354869.76310636,14354866.276512753,14354862.725071404,14354859.107576916,14354855.422801526,14354851.669494674,14354847.846382583,14354843.952167843,14354839.98552894,14354835.945119858,14354831.829569578,14354827.637481643,14354823.367433682,14354819.017976925,14354814.587635724,14354810.07490704,14354805.478259964,14354800.796135165,14354796.02694439,14354791.169069927,14354786.220864046,14354781.180648467,14354776.046713766,14354770.81731883,14354765.49069024,14354760.065021709,14354754.538473446,14354748.909171546,14354743.175207358,14354737.334636865,14354731.38547999,14354725.325719966,14354719.153302642,14354712.86613579,14354706.462088417,14354699.938990036,14354693.294629954,14354686.526756495,14354679.63307629,14354672.611253476,14354665.458908929,14354658.173619447,14354650.752916975,14354643.194287723,14354635.495171381,14354627.65296022,14354619.664998246,14354611.528580297,14354603.240951125,14354594.79930451,14354586.200782292,14354577.442473425,14354568.521413013,14354559.4345813,14354550.178902678,14354540.751244655,14354531.148416799,14354521.367169708,14354511.404193876,14354501.25611863,14354490.919510985,14354480.390874524,14354469.666648202,14354458.743205177,14354447.616851605,14354436.283825427,14354424.74029507,14354412.982358228,14354401.006040532,14354388.807294236,14354376.38199689,14354363.725949952,14354350.834877409,14354337.70442435,14354324.330155535,14354310.707553918,14354296.832019147,14354282.698866056,14354268.30332313,14354253.64053086,14354238.70554023,14354223.493311027,14354207.998710187,14354192.216510111,14354176.14138693,14354159.76791878,14354143.090583984,14354126.10375925,14354108.801717825,14354091.178627621,14354073.228549277,14354054.945434239,14354036.323122753,14354017.355341855,14353998.03570334,14353978.357701628,14353958.314711677,14353937.89998679,14353917.106656447,14353895.92772402,14353874.356064532,14353852.384422319,14353830.005408669,14353807.211499423,14353783.995032536,14353760.348205566,14353736.263073193,14353711.731544591,14353686.74538085,14353661.296192283,14353635.37543574,14353608.974411821,14353582.084262118,14353554.695966309,14353526.800339296,14353498.38802822,14353469.44950949,14353439.975085713,14353409.954882583,14353379.378845729,14353348.236737493,14353316.518133678,14353284.21242021,14353251.308789758,14353217.796238301,14353183.663561642,14353148.899351824,14353113.491993554,14353077.429660492,14353040.70031154,14353003.29168704,14352965.191304887,14352926.38645664,14352886.864203496,14352846.611372255,14352805.614551181,14352763.860085817,14352721.334074711,14352678.022365103,14352633.910548495,14352588.983956203,14352543.227654798,14352496.626441473,14352449.16483936,14352400.827092761,14352351.597162288,14352301.458719937,14352250.39514409,14352198.38951442,14352145.424606735,14352091.482887724,14352036.546509624,14351980.597304812,14351923.6167803,14351865.58611216,14351806.486139834,14351746.297360396,14351684.999922687,14351622.573621385,14351558.997890979,14351494.251799634,14351428.314043006,14351361.162937902,14351292.776415894,14351223.132016843,14351152.206882268,14351079.977748675,14351006.420940777,14350931.512364585,14350855.227500431,14350777.541395888,14350698.428658564,14350617.86344881,14350535.819472328,14350452.26997268,14350367.187723653,14350280.545021595,14350192.313677533,14350102.465009304,14350010.969833503,14349917.798457326,14349822.920670347,14349726.305736143,14349627.922383823,14349527.738799457,14349425.722617384,14349321.84091141,14349216.060185883,14349108.346366702,14348998.664792135,14348886.980203608,14348773.256736333,14348657.457909824,14348539.546618335,14348419.48512114,14348297.235032745,14348172.757312957,14348046.012256855,14347916.959484661,14347785.557931462,14347651.765836876,14347515.540734569,14347376.839441681,14347235.61804815,14347091.8319059,14346945.435617961,14346796.383027483,14346644.627206607,14346490.120445278,14346332.814239942,14346172.65928216,14346009.605447097,14345843.60178194,14345674.596494231,14345502.536940102,14345327.369612403,14345149.04012879,14344967.493219702,14344782.672716264,14344594.521538131,14344402.981681244,14344207.994205536,14344009.499222564,14343807.435883064,14343601.742364518,14343392.355858585,14343179.212558547,14342962.247646706,14342741.395281712,14342516.588585919,14342287.759632671,14342054.83943359,14341817.757925859,14341576.443959488,14341330.825284602,14341080.828538727,14340826.379234085,14340567.401744952,14340303.819295019,14340035.5539448,14339762.526579108,14339484.656894594,14339201.863387337,14338914.063340554,14338621.172812356,14338323.106623648,14338019.77834614,14337711.100290464,14337396.98349446,14337077.337711591,14336752.071399538,14336421.09170898,14336084.304472553,14335741.61419403,14335392.924037723,14335038.135818122,14334677.149989793,14334309.865637558,14333936.180466924,14333555.990794895,14333169.191541025,14332775.67621888,14332375.336927822,14331968.064345207,14331553.747718956,14331132.274860602,14330703.532138698,14330267.404472783,14329823.77532778,14329372.526708936,14328913.539157283,14328446.691745665,14327971.862075353,14327488.92627328,14326997.758989887,14326498.23339766,14325990.221190337,14325473.59258285,14324948.216311991,14324413.959637875,14323870.68834617,14323318.266751222,14322756.557699941,14322185.422576698,14321604.721309036,14321014.31237439,14320414.052807787,14319803.798210563,14319183.40276007,14318552.719220553,14317911.598955072,14317259.891938582,14316597.4467722,14315924.110698665,14315239.72961904,14314544.148110706,14313837.20944664,14313118.755616024,14312388.627346246,14311646.664126314,14310892.70423164,14310126.584750365,14309348.141611138,14308557.20961242,14307753.622453362,14306937.212766245,14306107.81215055,14305265.251208661,14304409.359583236,14303539.96599625,14302656.898289789,14301759.983468546,14300849.047744093,14299923.916580934,14298984.414744327,14298030.366349969,14297061.594915465,14296077.923413629,14295079.174327688,14294065.169708284,14293035.731232382,14291990.680264005,14290929.837916877,14289853.025118884,14288760.06267844,14287650.771352662,14286524.971917382,14285382.485239023,14284223.13234823,14283046.734515289,14281853.113327306,14280642.09076712,14279413.489293844,14278167.131925143,14276902.842321005,14275620.444869157,14274319.764771914,14273000.628134513,14271662.862054799,14270306.294714188,14268930.755469956,14267536.074948566,14266122.085140143,14264688.619493896,14263235.513014413,14261762.602358771,14260269.725934258,14258756.723996712,14257223.438749257,14255669.714441366,14254095.397468103,14252500.336469399,14250884.38242926,14249247.388774665,14247589.211474132,14245909.709135681,14244208.743104057,14242486.177557103,14240741.879601005,14238975.719364326,14237187.570090573,14235377.308229156,14233544.8135245,14231689.969103158,14229812.661558716,14227912.78103422,14225990.221302066,14224044.879840955,14222076.657909848,14220085.460618697,14218071.196995636,14216033.780050522,14213973.126834609,14211889.158496091,14209781.800331328,14207650.981831577,14205496.63672501,14203318.703013785,14201117.123006051,14198891.84334261,14196642.815018153,14194369.993396811,14192073.338221895,14189752.813619671,14187408.388097027,14185040.034532858,14182647.730163103,14180231.456559278,14177791.19960043,14175326.949438423,14172838.700456489,14170326.451220997,14167790.204426397,14165229.966833353,14162645.749200005,14160037.56620647,14157405.436372567,14154749.381968832,14152069.428920949,14149365.606707677,14146637.948252404,14143886.489808528,14141111.27083877,14138312.333888715,14135489.724454762,14132643.490846733,14129773.684045441,14126880.357555566,14123963.56725408,14121023.371234678,14118059.829648519,14115073.00454178,14112062.959690347,14109029.760432212,14105973.47349798,14102894.16684003,14099791.909460874,14096666.771241207,14093518.822768278,14090348.135165162,14087154.779921493,14083938.828726348,14080700.353303878,14077439.425252369,14074156.115887363,14070850.49608955,14067522.636158101,14064172.60567012,14060800.473346945,14057406.306927945,14053990.173052596,14050552.137151383,14047092.263346426,14043610.614362333,14040107.251448048,14036582.234310372,14033035.62105974,14029467.468168983,14025877.830445625,14022266.761018347,14018634.3113382,14014980.5311951,14011305.46875018,14007609.170584412,14003891.681764074,14000153.045923403,13996393.305364901,13992612.501177609,13988810.6733737,13984987.861043682,13981144.102530424,13977279.43562224,13973393.897765148,13969487.526294453,13965560.35868569,13961612.43282496,13957643.787298612,13953654.46170221,13949644.496968623,13945613.935715102,13941562.82260911,13937491.204752557,13933399.132084234,13929286.657799965,13925153.83879012,13921000.736093996,13916827.415370543,13912633.947384872,13908420.408509964,13904186.88124293,13899933.45473507,13895660.225335067,13891367.297144558,13887054.782585192,13882722.8029764,13878371.489123018,13874000.981911786,13869611.432915857,13865203.005006326,13860775.872969802,13856330.224131016,13851866.25897944,13847384.191798879,13842884.251298977,13838366.681247573,13833831.741102843,13829279.706644084,13824710.870600168,13820125.543274434,13815524.053165076,13810906.747579811,13806273.993243843,13801626.176900005,13796963.705900038,13792287.00878594,13787596.535860399,13782892.75974522,13778176.175926847,13773447.303287927,13768706.684624,13763954.887144411,13759192.50295651,13754420.149532303,13749638.47015669,13744848.134356525,13740049.838309683,13735244.305233447,13730432.285751449,13725614.558238564,13720791.929143114,13715965.233285755,13711135.334134547,13706303.12405568,13701469.524539374,13696635.486400528,13691801.989953749,13686970.04516236,13682140.691761142,13677314.999352468,13672494.067475654,13667679.0256493,13662871.033386467,13658071.280182574,13653280.985475931,13648501.398580872,13643733.798593426,13638979.494269598,13634239.823876282,13629516.15501487,13624809.884417702,13620122.437717449,13615455.269189635,13610809.861468418,13606187.725235904,13601590.398885187,13597019.448157357,13592476.465752749,13587963.070916727,13583480.909000255,13579031.650995633,13574616.993047625,13570238.655940404,13565898.384560581,13561597.947336672,13557339.135655401,13553123.763255144,13548953.665596869,13544830.699212985,13540756.741034413,13536733.687696256,13532763.45482245,13528847.976289736,13524989.203471329,13521189.104460662,13517449.66327552,13513772.879042992,13510160.765165545,13506615.34846862,13503138.668330094,13499732.775791958,13496399.73265462,13493141.610554153,13489960.490022864,13486858.459533604,13483837.61452813,13480900.05642997,13478047.891642122,13475283.230530052,13472608.186390363,13470024.874405565,13467535.410585392,13465141.910695126,13462846.489171367,13460651.258025754,13458558.325737141,13456569.796132732,13454687.76725871,13452914.330240965,13451251.568136428,13449701.554775702,13448266.353597522,13446948.016475784,13445748.58253975,13444670.076988144,13443714.509897865,13442883.875028063,13442180.148620289,13441605.288195577,13441161.231349204,13440849.894543963,13440673.171902806,13440632.934001679,13440731.02666345,13440969.269753795,13441349.45597991,13441873.349693025,13442542.68569552,13443359.16805364,13444324.468916688,13445440.227343582,13446708.048137708,13448129.500691,13449706.117838064,13451439.394721271,13453330.787667692,13455381.713078698,13457593.546333084,13459967.620704494,13462505.22629397,13465207.608978381,13468075.969375435,13471111.46182608,13474315.193394827,13477688.222888833,13481231.559896205,13484946.16384424,13488832.943078104,13492892.753960494,13497126.399992839,13501534.630958447,13506118.142088134,13510877.573248692,13515813.508154666,13520926.473603781,13526216.938736398,13531685.314319404,13537331.95205478,13543157.143913267,13549161.121493403,13555344.055406265,13561706.054686284,13568247.16622834,13574967.37425163,13581866.599790527,13588944.700212844,13596201.46876591,13603636.634150788,13611249.860125123,13619040.745135067,13627008.821976747,13635153.557487814,13643474.352269616,13651970.540440632,13660641.389421748,13669486.099754076,13678503.804950062,13687693.571378587,13697054.398184907,13706585.217246247,13716284.89316395,13726152.223293057,13736185.937810363,13746384.699821766,13756747.105510125,13767271.684324486,13777956.899211833,13788801.146892395,13799802.75817958,13810959.99834565,13822271.067534177,13833734.101220336,13845347.17072015,13857108.283749623,13869015.3850348,13881066.356973693,13893259.020350976,13905591.135106279,13918060.401156846,13930664.45927531,13943400.892023163,13956267.224740427,13969260.926592087,13982379.411671415,13995620.04016063,14008980.11954879,14022456.905907076,14036047.605221136,14049749.374780351,14063559.32462351,14077474.519040326,14091491.978128102,14105608.679402646,14119821.55946241,14134127.515704682,14148523.408092514,14163006.060970817,14177572.264930114,14192218.778716004,14206942.331182512,14221739.623287179,14236607.330125647,14251542.103003427,14266540.571542287,14281599.345818661,14296715.018531352,14311884.167195635,14327103.356360836,14342369.13984829,14357678.063006612,14373026.664980981,14388411.480993222,14403829.044629315,14419275.890130948,14434748.554687751,14450243.580726685,14465757.518195173,14481286.926834587,14496828.378440492,14512378.459106369,14527933.77144734,14543490.936800562,14559046.59739899,14574597.418515244,14590140.09057247,14605671.331219016,14621187.887364013,14636686.537170887,14652164.092006044,14667617.398340061,14683043.33959876,14698438.837961778,14713800.856106345,14729126.39889404,14744412.514998572,14759656.298472665,14774854.890252339,14790005.47959701,14805105.305463985,14820151.65781612,14835141.878861465,14850073.364224043,14864943.56404488,14879749.98401273,14894490.18632395,14909161.790571252,14923762.474561118,14938289.97505988,14952742.088468557,14967116.671426732,14981411.64134585,14995624.976872459,15009754.718282076,15023798.967804452,15037755.88988109,15051623.711356083,15065400.721601386,15079085.272577662,15092675.778832141,15106170.717434766,15119568.627854178,15132868.111775123,15146067.832858795,15159166.516447974,15172162.94921856,15185055.978779372,15197844.513222067,15210527.520623049,15223104.028499255,15235573.123219846,15247933.949375669,15260185.709108556,15272327.661402425,15284359.121338168,15296279.459314378,15308088.100235831,15319784.522671828,15331368.257986259,15342838.889441436,15354196.051277587,15365439.427769965,15376568.75226542,15387583.80620033,15398484.4181017,15409270.462573225,15419941.859268054,15430498.571849994,15440940.606944773,15451268.013083039,15461480.879636638,15471579.335749693,15481563.549265992,15491433.725654082,15501190.106931476,15510832.970589252,15520362.628518393,15529779.425939001,15539083.740333643],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[5565509.830995497,5565509.771556407,5565509.711011158,5565509.649339167,5565509.586519469,5565509.522530699,5565509.457351102,5565509.390958518,5565509.323330381,5565509.2544436855,5565509.184275019,5565509.112800521,5565509.039995895,5565508.965836381,5565508.890296765,5565508.813351373,5565508.734974035,5565508.655138108,5565508.5738164475,5565508.490981403,5565508.406604812,5565508.320657985,5565508.23311171,5565508.1439362075,5565508.053101172,5565507.960575713,5565507.866328372,5565507.770327114,5565507.67253929,5565507.572931655,5565507.471470349,5565507.368120873,5565507.262848088,5565507.155616205,5565507.046388758,5565506.93512863,5565506.821797975,5565506.706358274,5565506.588770274,5565506.468993996,5565506.346988724,5565506.2227129685,5565506.096124488,5565505.967180237,5565505.835836382,5565505.7020482635,5565505.565770397,5565505.426956455,5565505.285559241,5565505.141530681,5565504.994821811,5565504.845382752,5565504.6931627,5565504.538109897,5565504.380171632,5565504.219294217,5565504.0554229515,5565503.88850212,5565503.718474979,5565503.545283722,5565503.368869471,5565503.189172245,5565503.006130957,5565502.819683376,5565502.629766114,5565502.436314608,5565502.239263093,5565502.03854457,5565501.83409081,5565501.625832301,5565501.413698238,5565501.197616513,5565500.977513659,5565500.753314857,5565500.524943877,5565500.292323089,5565500.055373408,5565499.814014283,5565499.568163662,5565499.3177379705,5565499.0626520645,5565498.802819238,5565498.538151149,5565498.268557835,5565497.993947636,5565497.714227206,5565497.42930145,5565497.13907351,5565496.843444723,5565496.542314588,5565496.235580745,5565495.923138914,5565495.604882889,5565495.280704479,5565494.950493486,5565494.6141376505,5565494.271522642,5565493.922531987,5565493.567047059,5565493.204947012,5565492.836108757,5565492.460406916,5565492.077713775,5565491.6878992515,5565491.290830834,5565490.88637355,5565490.474389917,5565490.054739892,5565489.62728083,5565489.191867427,5565488.748351683,5565488.296582843,5565487.836407336,5565487.367668755,5565486.890207769,5565486.403862084,5565485.90846639,5565485.403852305,5565484.889848298,5565484.366279678,5565483.832968479,5565483.289733427,5565482.7363898875,5565482.17274978,5565481.598621537,5565481.013810015,5565480.418116452,5565479.811338382,5565479.193269574,5565478.563699961,5565477.922415574,5565477.269198457,5565476.603826603,5565475.926073873,5565475.235709934,5565474.532500151,5565473.816205538,5565473.086582657,5565472.343383548,5565471.5863556415,5565470.815241654,5565470.029779539,5565469.229702366,5565468.414738234,5565467.5846102,5565466.7390361605,5565465.877728766,5565465.000395332,5565464.106737725,5565463.196452262,5565462.269229615,5565461.324754721,5565460.362706631,5565459.382758454,5565458.3845772,5565457.367823697,5565456.332152469,5565455.277211608,5565454.202642674,5565453.108080549,5565451.993153337,5565450.857482227,5565449.7006813595,5565448.522357706,5565447.322110925,5565446.099533243,5565444.854209292,5565443.585715999,5565442.29362241,5565440.977489563,5565439.6368703535,5565438.271309339,5565436.880342631,5565435.46349771,5565434.020293276,5565432.550239083,5565431.05283577,5565429.527574699,5565427.973937777,5565426.391397279,5565424.77941568,5565423.137445457,5565421.464928919,5565419.761298006,5565418.025974108,5565416.258367849,5565414.457878919,5565412.62389585,5565410.755795799,5565408.852944367,5565406.914695355,5565404.940390569,5565402.929359584,5565400.880919509,5565398.794374785,5565396.669016917,5565394.504124257,5565392.298961752,5565390.052780689,5565387.764818457,5565385.43429827,5565383.06042893,5565380.642404529,5565378.1794042075,5565375.6705918405,5565373.115115805,5565370.512108639,5565367.860686794,5565365.15995029,5565362.408982466,5565359.606849625,5565356.75260074,5565353.845267126,5565350.883862124,5565347.867380752,5565344.794799375,5565341.665075367,5565338.477146735,5565335.229931791,5565331.922328768,5565328.553215453,5565325.121448813,5565321.625864592,5565318.06527696,5565314.438478051,5565310.744237624,5565306.981302597,5565303.148396646,5565299.244219779,5565295.267447884,5565291.216732303,5565287.090699358,5565282.887949903,5565278.607058843,5565274.246574665,5565269.805018941,5565265.280885841,5565260.672641609,5565255.978724073,5565251.197542099,5565246.327475073,5565241.366872325,5565236.314052624,5565231.167303578,5565225.924881064,5565220.585008652,5565215.145877009,5565209.605643279,5565203.962430481,5565198.214326877,5565192.359385322,5565186.395622614,5565180.321018843,5565174.1335167,5565167.831020797,5565161.411396969,5565154.872471532,5565148.212030608,5565141.427819334,5565134.517541141,5565127.478856968,5565120.3093845,5565113.006697347,5565105.568324266,5565097.99174831,5565090.274405991,5565082.41368645,5565074.406930555,5565066.251430046,5565057.944426602,5565049.483110941,5565040.864621899,5565032.086045434,5565023.144413718,5565014.036704098,5565004.759838112,5564995.3106804835,5564985.6860380415,5564975.882658713,5564965.897230394,5564955.726379896,5564945.366671782,5564934.814607285,5564924.066623097,5564913.119090227,5564901.9683127785,5564890.610526754,5564879.0418987805,5564867.258524874,5564855.256429133,5564843.031562431,5564830.579801091,5564817.896945525,5564804.978718839,5564791.82076545,5564778.418649631,5564764.767854078,5564750.8637784,5564736.70173764,5564722.276960717,5564707.584588868,5564692.61967407,5564677.377177414,5564661.851967442,5564646.038818498,5564629.932409016,5564613.527319771,5564596.818032125,5564579.79892623,5564562.464279206,5564544.808263256,5564526.824943805,5564508.508277561,5564489.852110554,5564470.850176139,5564451.496092986,5564431.783363008,5564411.705369247,5564391.25537377,5564370.42651547,5564349.21180787,5564327.604136888,5564305.59625852,5564283.180796547,5564260.35024015,5564237.096941521,5564213.413113394,5564189.290826591,5564164.722007454,5564139.698435306,5564114.211739813,5564088.253398332,5564061.814733202,5564034.886909,5564007.460929724,5563979.527635982,5563951.077702066,5563922.101633032,5563892.589761707,5563862.532245648,5563831.919064049,5563800.740014612,5563768.984710336,5563736.642576296,5563703.702846329,5563670.154559679,5563635.9865576085,5563601.187479916,5563565.745761426,5563529.64962842,5563492.887094991,5563455.445959355,5563417.313800116,5563378.477972426,5563338.925604138,5563298.643591861,5563257.618596974,5563215.837041549,5563173.285104253,5563129.948716146,5563085.813556431,5563040.865048135,5562995.08835373,5562948.46837068,5562900.989726906,5562852.636776224,5562803.3935936475,5562753.2439707015,5562702.171410584,5562650.159123312,5562597.190020767,5562543.24671168,5562488.311496543,5562432.366362423,5562375.392977742,5562317.37268694,5562258.286505081,5562198.115112382,5562136.838848664,5562074.437707724,5562010.8913316075,5561946.179004837,5561880.279648536,5561813.1718144845,5561744.833679071,5561675.243037206,5561604.377296095,5561532.213468991,5561458.728168813,5561383.897601703,5561307.6975605115,5561230.103418176,5561151.090121025,5561070.632182005,5560988.703673807,5560905.278221924,5560820.328997618,5560733.8287108,5560645.749602827,5560556.0634392155,5560464.741502276,5560371.7545836475,5560277.072976771,5560180.666469255,5560082.5043351855,5559982.555327323,5559880.787669247,5559777.169047386,5559671.666603022,5559564.246924136,5559454.876037259,5559343.519399178,5559230.141888602,5559114.707797748,5558997.180823843,5558877.52406056,5558755.699989386,5558631.670470903,5558505.396736037,5558376.839377203,5558245.958339404,5558112.712911284,5557977.061716074,5557838.962702554,5557698.373135894,5557555.249588471,5557409.54793066,5557261.2233215235,5557110.230199543,5556956.522273215,5556800.052511694,5556640.773135371,5556478.635606419,5556313.590619343,5556145.588091478,5555974.577153522,5555800.506140011,5555623.3225798365,5555442.973186738,5555259.403849809,5555072.559624051,5554882.384720908,5554688.822498838,5554491.815453953,5554291.305210677,5554087.232512443,5553879.5372124845,5553668.158264659,5553453.033714392,5553234.1006896505,5553011.295392048,5552784.553088066,5552553.808100351,5552318.993799168,5552080.042593965,5551836.8859251235,5551589.454255833,5551337.677064139,5551081.482835205,5550820.799053747,5550555.552196678,5550285.667725996,5550011.070081902,5549731.682676168,5549447.427885789,5549158.227046893,5548864.000449018,5548564.667329625,5548260.145869021,5547950.353185588,5547635.2053314475,5547314.61728844,5546988.502964591,5546656.775190988,5546319.345719094,5545976.1252185805,5545627.023275633,5545271.948391787,5544910.8079833165,5544543.508381201,5544169.954831678,5543790.0514974445,5543403.701459448,5543010.806719447,5542611.268203199,5542204.985764403,5541791.8581894385,5541371.783202829,5540944.657473603,5540510.376622434,5540068.835229738,5539619.926844604,5539163.543994764,5538699.578197466,5538227.919971409,5537748.458849711,5537261.083393974,5536765.681209441,5536262.13896135,5535750.342392458,5535230.176341782,5534701.524764641,5534164.270753972,5533618.296563014,5533063.483629344,5532499.712600371,5531926.863360264,5531344.815058369,5530753.446139197,5530152.634373961,5529542.256893744,5528922.190224317,5528292.310322659,5527652.492615196,5527002.61203783,5526342.543077773,5525672.159817239,5524991.335979007,5524299.944973919,5523597.8599503515,5522884.953845651,5522161.099439647,5521426.169410194,5520680.036390804,5519922.57303047,5519153.652055571,5518373.1463340195,5517580.928941603,5516776.873230573,5515960.852900487,5515132.74207135,5514292.415359046,5513439.747953108,5512574.6156967925,5511696.895169543,5510806.463771768,5509903.199811985,5508986.982596348,5508057.692520526,5507115.211163905,5506159.421386183,5505190.207426269,5504207.4550034925,5503211.051421134,5502200.885672175,5501176.848547338,5500138.832745282,5499086.732984958,5498020.446120098,5496939.871255732,5495844.909866705,5494735.465918133,5493611.445987701,5492472.759389754,5491319.318301065,5490151.0378882,5488967.836436406,5487769.635479873,5486556.359933263,5485327.938224398,5484084.302427997,5482825.388400261,5481551.135914204,5480261.488795572,5478956.395059182,5477635.807045488,5476299.681557218,5474947.979995886,5473580.668497992,5472197.718070649,5470799.104726496,5469384.80961763,5467954.8191683125,5466509.125206247,5465047.725092147,5463570.621847343,5462077.824279166,5460569.347103833,5459045.211066557,5457505.443058561,5455950.076230753,5454379.15010371,5452792.710673699,5451190.810514385,5449573.508873931,5447940.871767163,5446292.972062477,5444629.889563096,5442951.711082465,5441258.530513301,5439550.448890083,5437827.574444552,5436090.022653952,5434337.916281614,5432571.385409595,5430790.567463002,5428995.607225709,5427186.656847105,5425363.875839587,5423527.431066461,5421677.496719977,5419814.254289185,5417937.89251732,5416048.607348504,5414146.601863417,5412232.086203773,5410305.277485351,5408366.399699345,5406415.683601884,5404453.366591514,5402479.692574516,5400494.9118179185,5398499.280790096,5396493.061988881,5394476.523757131,5392449.940085704,5390413.59040387,5388367.759357153,5386312.736572695,5384248.816412167,5382176.297712432,5380095.483514034,5378006.680777716,5375910.2000891995,5373806.355352463,5371695.463471786,5369577.844022902,5367453.818913581,5365323.712034066,5363187.848897733,5361046.556272467,5358900.161803217,5356748.993626283,5354593.379975816,5352433.648783218,5350270.1272699535,5348103.141534478,5345933.016133962,5343760.073661455,5341584.634319275,5339407.015489343,5337227.531301242,5335046.492198799,5332864.204505986,5330680.969993009,5328497.085443383,5326312.842222869,5324128.525851168,5321944.415577199,5319760.783958878,5317577.896448268,5315396.010982986,5313215.377584758,5311036.237965979,5308858.825145174,5306683.363072197,5304510.066264034,5302339.1394520225,5300170.777241324,5298005.163783426,5295842.472462442,5293682.86559597,5291526.494151228,5289373.49747712,5287224.0030529415,5285078.126254302,5282935.970136876,5280797.625238523,5278663.169400281,5276532.667606721,5274406.17184606,5272283.720990456,5270165.34069679,5268051.043328253,5265940.82789698,5263834.680027947,5261732.571944271,5259634.462474064,5257540.297078855,5255450.007903669,5253363.513848688,5251280.720662456,5249201.521056522,5247125.794841354,5245053.409083352,5242984.218282718,5240918.064571915,5238854.7779344255,5236794.176443447,5234736.066520171,5232680.243211234,5230626.49048491,5228574.581545595,5226524.279166076,5224475.336037118,5222427.4951338,5220380.490098104,5218334.045637143,5216287.8779365225,5214241.695088196,5212195.197532253,5210148.078512049,5208100.024542052,5206050.715887837,5203999.827057589,5201947.0273045655,5199891.981139867,5197834.348854983,5195773.7870535,5193709.949191411,5191642.486125476,5189571.046669086,5187495.278155092,5185414.827005104,5183329.339304738,5181238.461384351,5179141.840404797,5177039.124947759,5174929.965610228,5172814.015602742,5170690.931351004,5168560.373100481,5166422.005523703,5164275.498329887,5162120.526876617,5159956.772783289,5157783.924546063,5155601.6781540755,5153409.737706695,5151207.816031606,5148995.635303545,5146772.927663489,5144539.435838172,5142294.913759744,5140039.127185465,5137771.854317313,5135492.886421358,5133202.028446847,5130899.099644852,5128583.9341864325,5126256.3817801885,5123916.308289148,5121563.5963468775,5119198.145972772,5116819.875186385,5114428.720620774,5112024.638134723,5109607.603423773,5107177.612629955,5104734.682950106,5102278.853242679,5099810.184632882,5097328.761116052,5094834.690159092,5092328.103299814,5089809.156744028,5087278.031960183,5084734.9362713685,5082180.103444452,5079613.794276154,5077036.297175768,5074447.928744324,5071849.034349887,5069239.988698701,5066621.196401905,5063993.092537455,5061356.143206957,5058710.846087034,5056057.730974875,5053397.360327569,5050730.329794836,5048057.268744749,5045378.840781999,5042695.744258277,5040008.712774319,5037318.515673124,5034625.958523885,5031931.883596106,5029237.170323418,5026542.735756538,5023849.535004845,5021158.561666009,5018470.848243095,5015787.466548544,5013109.52809443,5010438.184468369,5007774.627694407,5005120.090578248,5002475.84703612,4999843.21240654,4997223.54374429,4994618.24009577,4992028.7427549865,4989456.535499311,4986903.144804154,4984370.140035655,4981859.133620439,4979371.781191459,4976909.781708894,4974474.877555022,4972068.854601933,4969693.542250906,4967350.813442205,4965042.584633978,4962770.815748921,4960537.510087233,4958344.714204423,4956194.517752327,4954089.053281754,4952030.4960050145,4950021.0635165395,4948063.015469744,4946158.653208167,4944310.3193489,4942520.397316178,4940791.310823014,4939125.52329859,4937525.537259159,4935993.893620047,4934533.170946382,4933145.984640031,4931834.986060277,4930602.861575643,4929452.331544319,4928386.149220579,4927407.099584582,4926517.998092995,4925721.689347826,4925021.045680966,4924418.965651934,4923918.372456417,4923522.212243278,4923233.4523378005,4923055.079369092,4922990.097299659,4923041.5253554005,4923212.395854426,4923505.751933288,4923924.64516952,4924472.133099563,4925151.276631513,4925965.13735234,4926916.774729686,4928009.2432085425,4929245.589203655,4930628.847988732,4932162.0404841015,4933848.169944777,4935690.218551426,4937691.143907159,4939853.875443578,4942181.310739983,4944676.311760169,4947341.701011757,4950180.257633495,4953194.713416543,4956387.748766202,4959761.988611122,4963319.998267508,4967064.279266337,4970997.265152087,4975121.317261943,4979438.720494891,4983951.67908052,4988662.312357743,4993572.650574012,4998684.630715913,5004000.092382313,5009520.773711474,5015248.307373738,5021184.216641539,5027329.911548594,5033686.685150174,5040255.709896342,5047038.034129984,5054034.57872135,5061246.133850627,5068673.35594984,5076316.764815098,5084176.740899827,5092253.522799262,5100547.204935968,5109057.735455708,5117784.914342333,5126728.3917598585,5135887.666629124,5145262.085445847,5154850.8413460525,5164652.973424143,5174667.366308059,5184892.749995138,5195327.699951467,5205970.637476606,5216819.830334724,5227873.393652291,5239129.291081578,5250585.336228339,5262239.19434116,5274088.384259113,5286130.280613497,5298362.116278609,5310780.98506573,5323383.844653675,5336167.51974859,5349128.705464922,5362263.970918869,5375569.763024988,5389042.410486055,5402678.127965772,5416473.020433446,5430423.087669298,5444524.228918786,5458772.247683895,5473162.856639185,5487691.682660128,5502354.271951113,5517146.095260433,5532062.553169468,5547098.981443322,5562250.656430185,5577512.800496833,5592880.5874877805,5608349.148195803,5623913.575831781,5639568.931482081,5655310.249541963,5671132.54311391,5687030.809360042,5703000.034798272,5719035.200532193,5735131.287405206,5751283.281069786,5767486.176963334,5783734.985182472,5800024.735248222,5816350.480754983,5832707.3038967,5849090.319864234,5865494.681108364,5881915.581463435,5898348.260127177,5914788.005492708,5931230.158829236,5947670.117808548,5964103.339874726,5980525.345455156,5996931.721011229,6013318.121927666,6029680.2752398,6046013.982198549,6062315.120673267,6078579.647392965,6094803.600026844,6110983.099105359,6127114.349783379,6143193.643447364,6159217.359168679,6175181.965005518,6191084.019156104,6206920.1709661065,6222687.161793389,6238381.825733432,6254001.090208902,6269541.976427077,6285001.5997088635,6300377.169693383,6315665.990422135,6330865.460306836,6345973.071985184,6360986.412068724,6375903.1607872145,6390721.091533741,6405438.070314983,6420052.055110988,6434561.0951488,6448963.330094303,6463256.98916659,6477440.390179146,6491511.938512086,6505470.126019643,6519313.52987704,6533040.811370794,6546650.714636469,6560142.065347785,6573513.76936088,6586764.811317541,6599894.253210959,6612901.232917644,6625784.962698896,6638544.727675199,6651179.884276793,6663689.858673556,6676074.145187198,6688332.30468874,6700463.962984032,6712468.809190053,6724346.594104553,6736097.128571523,6747720.281844866,6759215.9799525365],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[7178959.960313533,7178959.881519301,7178959.801258715,7178959.719504482,7178959.636228807,7178959.551403383,7178959.464999362,7178959.376987364,7178959.28733748,7178959.196019209,7178959.103001519,7178959.00825278,7178958.911740778,7178958.8134326935,7178958.713295109,7178958.611293979,7178958.507394618,7178958.401561704,7178958.293759254,7178958.183950617,7178958.072098457,7178957.958164747,7178957.8421107475,7178957.723897,7178957.603483322,7178957.480828764,7178957.3558916235,7178957.228629434,7178957.098998911,7178956.966955998,7178956.832455792,7178956.695452572,7178956.55589975,7178956.41374988,7178956.26895464,7178956.121464795,7178955.971230202,7178955.818199785,7178955.662321511,7178955.50354239,7178955.341808432,7178955.177064656,7178955.009255048,7178954.838322557,7178954.664209072,7178954.4868553905,7178954.306201219,7178954.122185135,7178953.934744584,7178953.743815829,7178953.549333966,7178953.351232871,7178953.1494452,7178952.943902342,7178952.734534416,7178952.521270246,7178952.304037332,7178952.0827618055,7178951.857368453,7178951.62778064,7178951.393920308,7178951.1557079535,7178950.913062594,7178950.6659017345,7178950.4141413495,7178950.157695842,7178949.896478033,7178949.630399112,7178949.359368622,7178949.08329443,7178948.802082665,7178948.515637734,7178948.223862255,7178947.9266570285,7178947.62392102,7178947.315551309,7178947.001443063,7178946.6814895,7178946.355581836,7178946.023609287,7178945.685459,7178945.3410160085,7178944.990163217,7178944.632781356,7178944.268748926,7178943.897942179,7178943.520235056,7178943.135499156,7178942.743603679,7178942.344415414,7178941.937798649,7178941.523615156,7178941.101724138,7178940.671982177,7178940.234243181,7178939.788358347,7178939.334176098,7178938.871542041,7178938.400298904,7178937.920286502,7178937.431341658,7178936.93329816,7178936.4259867165,7178935.909234863,7178935.382866953,7178934.846704052,7178934.300563899,7178933.744260856,7178933.177605821,7178932.600406175,7178932.012465714,7178931.413584584,7178930.803559219,7178930.182182263,7178929.5492424965,7178928.904524783,7178928.247809973,7178927.578874843,7178926.897492015,7178926.203429893,7178925.496452561,7178924.776319709,7178924.042786572,7178923.2956038285,7178922.534517507,7178921.75926893,7178920.969594591,7178920.165226097,7178919.345890047,7178918.511307971,7178917.6611962095,7178916.795265821,7178915.913222506,7178915.014766478,7178914.099592382,7178913.167389178,7178912.217840042,7178911.250622261,7178910.265407113,7178909.261859768,7178908.239639175,7178907.198397915,7178906.137782138,7178905.057431391,7178903.956978537,7178902.836049586,7178901.694263615,7178900.531232608,7178899.346561332,7178898.139847211,7178896.910680169,7178895.65864252,7178894.383308796,7178893.0842456315,7178891.761011593,7178890.413157038,7178889.040223969,7178887.641745869,7178886.217247545,7178884.766244971,7178883.288245121,7178881.782745804,7178880.24923548,7178878.6871931115,7178877.096087977,7178875.475379475,7178873.824516951,7178872.142939531,7178870.430075897,7178868.6853441205,7178866.908151452,7178865.097894118,7178863.25395712,7178861.375714043,7178859.4625268085,7178857.513745494,7178855.528708075,7178853.506740246,7178851.447155152,7178849.349253173,7178847.212321691,7178845.035634847,7178842.818453282,7178840.560023895,7178838.259579606,7178835.916339063,7178833.529506392,7178831.098270943,7178828.621806994,7178826.099273486,7178823.529813718,7178820.912555085,7178818.246608769,7178815.531069423,7178812.765014894,7178809.947505892,7178807.077585675,7178804.154279728,7178801.176595447,7178798.143521755,7178795.054028829,7178791.907067701,7178788.701569918,7178785.436447189,7178782.110591003,7178778.722872273,7178775.272140927,7178771.757225553,7178768.1769329645,7178764.530047842,7178760.815332285,7178757.031525408,7178753.177342924,7178749.251476694,7178745.252594295,7178741.179338567,7178737.030327157,7178732.804152055,7178728.499379109,7178724.114547562,7178719.648169528,7178715.098729516,7178710.464683908,7178705.744460442,7178700.936457684,7178696.039044481,7178691.050559411,7178685.969310242,7178680.7935733255,7178675.521593065,7178670.151581275,7178664.681716616,7178659.110143952,7178653.4349737605,7178647.65428146,7178641.76610679,7178635.768453137,7178629.659286864,7178623.436536645,7178617.098092733,7178610.641806284,7178604.065488621,7178597.366910496,7178590.543801355,7178583.593848551,7178576.514696591,7178569.303946343,7178561.959154209,7178554.477831341,7178546.8574427655,7178539.095406562,7178531.189093003,7178523.135823635,7178514.932870425,7178506.577454817,7178498.066746817,7178489.3978640335,7178480.567870723,7178471.573776808,7178462.412536868,7178453.081049126,7178443.576154416,7178433.894635125,7178424.033214107,7178413.988553609,7178403.757254137,7178393.335853339,7178382.720824831,7178371.908577043,7178360.895452008,7178349.67772413,7178338.251598982,7178326.613212,7178314.7586272275,7178302.683835982,7178290.38475555,7178277.857227799,7178265.097017815,7178252.099812496,7178238.8612191025,7178225.376763832,7178211.641890302,7178197.651958049,7178183.402241005,7178168.887925917,7178154.104110759,7178139.045803102,7178123.707918479,7178108.085278695,7178092.17261011,7178075.964541904,7178059.455604307,7178042.6402267935,7178025.512736241,7178008.067355067,7177990.298199319,7177972.199276762,7177953.76448486,7177934.987608818,7177915.86231953,7177896.382171471,7177876.540600629,7177856.330922315,7177835.746329001,7177814.7798880655,7177793.424539555,7177771.673093851,7177749.518229336,7177726.952489993,7177703.968283002,7177680.557876216,7177656.713395701,7177632.426823128,7177607.6899931915,7177582.494590946,7177556.832149122,7177530.6940453695,7177504.071499462,7177476.955570471,7177449.337153856,7177421.206978536,7177392.555603905,7177363.373416766,7177333.650628259,7177303.377270691,7177272.54319436,7177241.138064271,7177209.1513568275,7177176.572356478,7177143.390152268,7177109.59363436,7177075.171490493,7177040.11220237,7177004.4040419925,7176968.035067936,7176930.9931215495,7176893.265823108,7176854.840567877,7176815.704522149,7176775.844619167,7176735.247555019,7176693.899784436,7176651.787516542,7176608.896710514,7176565.2130711945,7176520.722044595,7176475.408813372,7176429.258292188,7176382.255123006,7176334.383670331,7176285.6280163545,7176235.971956003,7176185.398991939,7176133.892329475,7176081.434871383,7176028.009212633,7175973.597635074,7175918.182101975,7175861.744252531,7175804.265396251,7175745.7265072735,7175686.108218581,7175625.390816126,7175563.554232886,7175500.578042776,7175436.441454536,7175371.123305465,7175304.602055072,7175236.855778667,7175167.862160792,7175097.598488625,7175026.041645205,7174953.168102628,7174878.953915086,7174803.374711846,7174726.405690077,7174648.021607633,7174568.196775668,7174486.9050511895,7174404.11982947,7174319.814036381,7174233.960120595,7174146.5300456835,7174057.495282114,7173966.826799101,7173874.495056391,7173780.469995904,7173684.721033257,7173587.217049192,7173487.926380876,7173386.816813087,7173283.855569268,7173179.009302506,7173072.244086334,7172963.52540547,7172852.818146394,7172740.08658784,7172625.294391134,7172508.404590444,7172389.379582909,7172268.181118595,7172144.770290423,7172019.107523882,7171891.15256671,7171760.8644783655,7171628.201619472,7171493.121641068,7171355.581473784,7171215.537316881,7171072.944627189,7170927.758107907,7170779.931697309,7170629.418557319,7170476.171062003,7170320.140785882,7170161.278492245,7169999.534121234,7169834.856777903,7169667.194720139,7169496.495346507,7169322.705183932,7169145.769875375,7168965.634167321,7168782.241897246,7168595.535980957,7168405.458399834,7168211.9501880305,7168014.951419545,7167814.401195273,7167610.237629915,7167402.397838872,7167190.8179250695,7166975.4329657,7166756.176998907,7166532.983010464,7166305.782920376,7166074.5075694285,7165839.086705735,7165599.448971273,7165355.521888329,7165107.231846024,7164854.504086763,7164597.262692734,7164335.430572377,7164068.929446925,7163797.679836924,7163521.601048814,7163240.611161569,7162954.627013359,7162663.564188332,7162367.337003423,7162065.8584952755,7161759.04040729,7161446.793176743,7161129.025922051,7160805.646430203,7160476.561144301,7160141.675151299,7159800.892169916,7159454.114538766,7159101.243204663,7158742.17771121,7158376.816187585,7158005.055337647,7157626.790429269,7157241.915284032,7156850.322267204,7156451.902278072,7156046.544740653,7155634.137594782,7155214.567287581,7154787.718765432,7154353.475466317,7153911.719312715,7153462.330704944,7153005.188515074,7152540.170081386,7152067.151203394,7151586.006137526,7151096.607593388,7150598.826730764,7150092.533157253,7149577.594926679,7149053.87853826,7148521.248936558,7147979.569512283,7147428.702103925,7146868.507000333,7146298.842944184,7145719.567136435,7145130.53524183,7144531.6013953565,7143922.618209875,7143303.436784833,7142673.906716154,7142033.876107306,7141383.191581643,7140721.698296029,7140049.2399557745,7139365.658830957,7138670.795774155,7137964.490239657,7137246.58030412,7136516.9026888525,7135775.292783633,7135021.584672176,7134255.61115933,7133477.203799913,7132686.192929421,7131882.4076965,7131065.676097313,7130235.82501177,7129392.6802418195,7128536.066551619,7127665.80770987,7126781.7265341645,7125883.644937516,7124971.383977058,7124044.763904972,7123103.604221679,7122147.723731344,7121176.940599734,7120191.072414446,7119189.936247581,7118173.348720856,7117141.126073242,7116093.084231095,7115029.038880859,7113948.805544373,7112852.199656744,7111739.0366469165,7110609.132020839,7109462.301447342,7108298.3608466815,7107117.126481821,7105918.415052356,7104702.043791233,7103467.8305640835,7102215.593971394,7100945.153453251,7099656.329396858,7098348.943246692,7097022.817617329,7095677.776408871,7094313.644924983,7092930.249993461,7091527.420089342,7090104.985460423,7088662.778255238,7087200.6326533025,7085718.384997684,7084215.873929734,7082692.940525922,7081149.4284366835,7079585.184027185,7078000.056519881,7076393.898138763,7074766.564255172,7073117.913535043,7071447.808087413,7069756.11361411,7068042.699560389,7066307.439266364,7064550.210119116,7062770.89370522,7060969.375963517,7059145.547337947,7057299.30293021,7055430.542652037,7053539.171376814,7051625.0990903955,7049688.2410407225,7047728.517886149,7045745.855842066,7043740.186825638,7041711.4485983085,7039659.584905854,7037584.545615589,7035486.286850514,7033364.771120035,7031219.9674469195,7029051.85149024,7026860.405663903,7024645.619250438,7022407.488509749,7020146.016782437,7017861.214587393,7015553.099713263,7013221.697303498,7010867.039934601,7008489.167687201,7006088.128209727,7003663.9767741645,7001216.776323714,6998746.597511936,6996253.518733072,6993737.626143247,6991199.01367221,6988637.783025346,6986054.043675645,6983447.912845368,6980819.515477151,6978168.984194289,6975496.459249975,6972802.088465307,6970086.027155813,6967348.438046404,6964589.491174523,6961809.36378144,6959008.240191531,6956186.311679515,6953343.776325568,6950480.8388583185,6947597.710485729,6944694.608713908,6941771.757153915,6938829.3853167,6935867.728396277,6932887.027041359,6929887.527115607,6926869.479446803,6923833.139565199,6920778.767431369,6917706.627153938,6914616.986697578,6911510.117581712,6908386.294570403,6905245.795353946,6902088.900222695,6898915.891733749,6895727.054371087,6892522.674199834,6889303.038515352,6886068.435487871,6882819.153803455,6879555.482302053,6876277.709613502,6872986.123792313,6869681.011952105,6866362.659900633,6863031.351776291,6859687.369687067,6856330.993352898,6852962.499752411,6849582.16277504,6846190.252879541,6842787.036759885,6839372.777019559,6835947.731855318,6832512.1547513595,6829066.2941849725,6825610.393344658,6822144.689861699,6818669.415556194,6815184.796198509,6811691.051287085,6808188.393843553,6804677.030226049,6801157.159961589,6797628.975598377,6794092.662578838,6790548.399134154,6786996.356201049,6783436.697361511,6779869.578806108,6776295.149321508,6772713.550302762,6769124.915790865,6765529.372536059,6761927.040087275,6758318.030908108,6754702.450519564,6751080.3976699,6747451.964531699,6743817.236926333,6740176.294575891,6736529.211382596,6732876.055735672,6729216.890845586,6725551.775105492,6721880.7624796955,6718203.902918887,6714521.242801805,6710832.825403014,6707138.691386337,6703438.879323515,6699733.426237574,6696022.368170338,6692305.740773497,6688583.57992259,6684855.922353217,6681122.806318762,6677384.272268876,6673640.363547941,6669891.127112664,6666136.614268005,6662376.881420518,6658611.990848241,6654842.011486203,6651067.019726608,6647287.100232754,6643502.346765712,6639712.863022786,6635918.7634867625,6632120.174284953,6628317.234057029,6624510.094830638,6620698.922903785,6616883.899732987,6613065.222826173,6609243.1066393275,6605417.78347589,6601589.504387877,6597758.54007779,6593925.181800263,6590089.742262529,6586252.556522688,6582413.982884866,6578574.403790274,6574734.226703244,6570893.884991327,6567053.838798487,6563214.575910532,6559376.6126118405,6555540.494532504,6551706.79748499,6547876.128289452,6544049.125586815,6540226.46063875,6536408.838113693,6532596.996858027,6528791.710651588,6524993.788946612,6521204.077589283,6517423.459523003,6513652.855472527,6509893.224608093,6506145.565188663,6502410.915183398,6498690.352870466,6494984.997412287,6491296.009406287,6487624.59141026,6483971.988441354,6480339.488447763,6476728.4227521345,6473140.166465706,6469576.138872165,6466037.803780204,6462526.669843737,6459044.290848718,6455592.26596547,6452172.239965439,6448785.903401267,6445434.992749028,6442121.29051152,6438846.625281404,6435612.871763074,6432421.950752009,6429275.8290704815,6426176.51945835,6423126.080417793,6420126.616010732,6417180.275607759,6414289.253587355,6411455.788984217,6408682.165085498,6405970.708973812,6403323.791015822,6400743.824295355,6398233.263989876,6395794.606689341,6393430.389656355,6391143.190026703,6388935.623949333,6386810.345664913,6384770.046522184,6382817.453931382,6380955.330254063,6379186.471628774,6377513.706732078,6375939.895474543,6374467.92763141,6373100.721407728,6371841.221937918,6370692.399719756,6369657.248982969,6368738.785992708,6367940.047288318,6367264.087857934,6366713.979249621,6366292.807619849,6366003.6717202915,6365849.680824056,6365833.952592617,6365959.610884867,6366229.783509845,6366647.599924868,6367216.188880933,6367938.676017409,6368818.181408185,6369857.817061594,6371060.684376575,6372429.871557655,6373968.450991513,6375679.47658798,6377565.98108846,6379630.973344915,6381877.435572607,6384308.320579977,6386926.548979033,6389735.00637986,6392736.54057278,6395933.958701899,6399330.024433758,6402927.45512493,6406728.9189923685,6410737.0322904335,6414954.35649848,6419383.395522946,6424026.592917886,6428886.329127846,6433964.918757028,6439264.607868604,6444787.571318056,6450535.910124326,6456511.648882562,6462716.733222125,6469153.02731351,6475822.311427705,6482726.279551458,6489866.537061827,6497244.598463278,6504861.885190504,6512719.72348,6520819.342313399,6529161.871435311,6537748.339448457,6546579.671988632,6555656.689981969,6564980.107986858,6574550.532622699,6584368.4610876,6594434.279766954,6604748.262934741,6615310.571549245,6626121.252144783,6637180.235820906,6648487.337330402,6660042.254267344,6671844.566356294,6683893.734843646,6696189.101992039,6708729.890678597,6721515.2040976845,6734544.025568793,6747815.218450001,6761327.526157455,6775079.57229111,6789069.860867001,6803296.77665611,6817758.585629884,6832453.435512369,6847379.356438742,6862534.261720088,6877915.948714045,6893522.099800929,6909350.283464837,6925397.955479112,6941662.46019553,6958141.031936379,6974830.7964885775,6991728.772698848,7008831.874168868,7026136.911049192,7043640.591930652,7061339.525831852,7079230.224281185,7097309.103491753,7115572.48662742,7134016.606158088,7152637.606302214,7171431.545554356,7190394.399295526,7209522.062483895,7228810.3524233,7248255.011606901,7267851.7106331065,7287596.051190854,7307483.569111129,7327509.737481493,7347669.969820278,7367959.623306927,7388374.002064888,7408908.360493325,7429557.906643764,7450317.805637762,7471183.183121474,7492149.128752974,7513210.699718077,7534362.924270293,7555600.805290538,7576919.323862061,7598313.442856096,7619778.110523625,7641308.2640886335,7662898.833338233,7684544.744204946,7706240.922336535,7727982.296648678,7749763.802855894,7771580.386976087,7793427.008804146,7815298.645350129,7837190.29423755,7859096.977057423,7881013.742673826,7902935.670476749,7924857.873578213,7946775.501947709,7968683.7454831265,7990577.837013534,8012453.055230259,8034304.727542913,8056128.232857162,8077919.004271196,8099672.53168808,8121384.364341267,8143050.113230815,8164665.45346804,8186226.126526452,8207727.942397148,8229166.7816469185,8250538.597377597,8271839.417085358,8293065.344418906,8314212.5608356325,8335277.327155103,8356255.985009387,8377144.958189938,8397940.753890978,8418639.963849451,8439239.265381878,8459735.422318593,8480125.285835981,8500405.795187617,8520573.978335235,8540626.952480748,8560561.924500579,8580376.191283807,8600067.139975714,8619632.248128455,8639069.083760738,8658375.305328457,8677548.661608392,8696586.991497178,8715488.223727759,8734250.376505775,8752871.557068294,8771349.961167369,8789683.872481083,8807871.661954641,8825911.787074199,8843802.791076193,8861543.30209486,8879132.032250779,8896567.77668317,8913849.4125288,8930975.897850268,8947946.2705165,8964759.647038234,8981415.22136128,8997912.263620311,9014250.11885594,9030428.205697771,9046446.015016105,9062303.108544933,9077999.117478816,9093533.74104617,9108906.745061474,9124117.96045883,9139167.281809269,9154054.665824085,9168780.12984653,9183343.75033398,9197745.661332792,9211986.052947838,9226065.169808766,9239983.309534883,9253740.821200524,9267338.103802683,9280775.60473264,9294053.818253178,9307173.283982994,9320134.585389785,9332938.348293427,9345585.239380585,9358075.964732058,9370411.268364059,9382591.930784538],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","FigRMSEGRidRidge = visuRMSEGrid(Ridge(), 'Ridge', alphasridge, 'alpha',\n","                                GridRidge)\n","FigRMSEGRidRidge.show()\n","if write_data is True:\n","    FigRMSEGRidRidge.write_image('./Figures/ConsoGraphRMSERidge.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.3 Modèle Lasso"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      1.0\n","               R²          RMSE           MAE\n","Lasso()  0.883546  9.865173e+06  3.280866e+06\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLasso=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4815895.497897476,2172894.7235930483,1281136.0893816885,1181058.3822034146,5103161.675761552,1977013.890776019,6683416.909094835,834753.3923284942,701166.9228880752,8410564.241618419,218974.39058369212,2856418.2191780205,3850473.9557904582,1253329.6068963031,11156689.952950506,3409850.2669114755,1343387.812717299,29669.7578968429,855461100.0357366,3210219.766516117,2284830.270908502,1153375.773265357,956575.5131050735,903807.3650617718,4670415.399706084,3770942.7228799807,577943.0103639094,8044352.872451615,2258781.6377233667,1078380.8777522922,2715303.359815425,1534847.279910184,6740038.5421512555,4334065.46062899,792329.5333209685,4981404.80757015,2150147.064587757,21772792.24189562,1977682.766549605,739057.5417302498,3245663.391844266,7452611.108605692,1376308.2268227832,15969783.02277665,616291.5130333945,1309590.4814088023,1020306.3849870968,215299.8137896154,1018535.9682282528,1133537.2644829715,3830547.959985927,1333284.4243772961,929640.4381223542,2857469.4750698595,5035332.960444719,12516633.547678342,1179938.2891658763,5979442.122012163,1613110.7848925386,848378.0584087791,8068541.499605779,1351452.0389042306,1382404.5388507417,1351381.7270909504,1316058.3172616172,3565218.1395594347,1924681.2003102303,1724523.3390936696,1972133.6042791784,1405355.798814407,3114101.1284112837,2055642.9977378224,3611260.132147895,755427.5657661725,4073751.745435075,930246.2434799371,4437451.822301891,2136313.871946045,3153644.9209594624,935911.1693892351,2304367.6532649277,5594374.715688372,1920518.5774650723,3699419.817712904,993820.6746446693,1249764.0449882103,3190849.624636636,342411.74123237026,1664696.9026730778,1531037.4263252881,1660845.172603619,1192770.7477731288,11366287.982853081,1331686.7280186764,6287848.847709056,17805925.503986932,3766172.169839683,43956771.38503432,4777155.60454641,948804.4771528165,19317533.44231234,16382041.011728683,2069829.6952988016,1246138.6983096674,1785149.6562429115,3797787.929750122,3736142.9661249276,8402490.805953976,800594.0512370113,4364022.947443202,9813397.121307446,4526699.547903007,710505.2477706973,20618501.14938261,3883006.7354419352,-34499.190888721496,11188630.426912086,7665351.09035532,723093.8945073488,8373741.928819876,43744679.688472874,1725954.795872794,3139720.9361969884,2271925.23323813,726950.4796385991,2192820.639030824,1696001.5686561214,9324444.853019515,1389781.8860166557,3078263.3033253863,871427.3370297437,782892.515431324,2775866.712025447,981335.1104539754,231614.8348781336,3280981.7234607106,4717558.206291232,1498494.1500745215,4549808.611277003,2896810.8316533277,258376.8791533066,1098200.6865971542,3543230.0546893757,1084040.2111432315,2242643.021327354,43621.45781542221,5982302.542271752,1672295.9059786457,1441040.45981463,1040483.626915127,3082198.033367153,840007.272014155,4833361.286821821,2516948.1129275076,1210513.9867992538,1114895.9604252514,2274334.67246899,1389847.5582604231,4903761.749505216,5086846.402981348,5797066.286501428,1849929.4084554084,1141960.8534799353,1724615.2476802836,5777327.713278703,43097012.53726322,1020498.0226669437,1249960.8943821283,1748028.6602010373,1161118.0297190407,6741817.85521071,6175319.83198758,2182408.1530077592,3257279.006285508,2571045.2124714023,584303.6300087576,1126879.6039175831,3085177.6089681564,3793664.625429251,1737062.8194292092,20610638.802957565,1900921.3582464745,1513729.1291952126,5682077.951719368,10285520.647888806,2111891.9447737546,3084064.4483467997,19628607.92757945,1252691.0473233366,4535181.465853665,3586006.580887271,1125809.5239724603,2527246.8379622702,1165024.0500581935,2271919.314030759,4293143.819142964,1515105.240263272,1242776.4679165245,2546972.1272234325,500895.77924473444,11667199.500619194,1086929.3967027217,3143900.0079020774,16884053.009061772,1094711.806631891,1394339.5994735048,4115780.151137607,1214173.2471176034,1125299.4655394636,1689225.523666719,1439813.0325600717,14053090.383380605,1972427.736596141,1613402.247224438,3757264.232419515,16081990.659946918,1372443.2273902209,1051669.533079965,3314122.4822896756,1002477.0486864992,1330936.8350905403,3939897.1462141955,4734827.057010435,1307836.3556622022,1896960.1056415676,18113426.084818132,2300528.454037051,12607308.10088824,1972768.9935862157,4326558.746549449,143016.276821855,5888603.437869947,1799264.0146219444,2266914.7753223996,4709713.652467409,2123884.4620953905,4700116.583479784,1121240.842083433,2739188.040109292,566284.4535134451,1378510.5372813852,3980519.504742685,855291.7688208977,4895841.805655225,3565336.3597331373,846607.8684840105,26837425.03454013,1857828.474253272,5318691.4870529715,1955929.7921440974,16009276.737119474,2321653.620842599,1516323.7522262842,4115854.4885522267,1652613.9393541338,2378716.0999533986,1471363.109308429,848133.1883927509,3809909.545460048,2651542.1642552326,12802234.964920813,49007732.197311334,44567219.51845633,3158278.2108020466,1175153.9519623714,775630.383869057,6224314.583553872,3373924.7043316877,925830.64060889,2466845.694680616,168340.1674987222,5161684.266360966,8614133.92444442,2974852.84115138,1032593.6842519492,5402919.56531053,1045313.806626138,1915801.220008066,1303610.6611415907,986579.8670049505,1508461.4058012404,1006512.2389169519,6491726.415184995,1189795.7248158923,2008000.0373521547,3453530.591996613,4348778.287464711,2958326.625627826,19415317.185565334,17958544.747925475,8702843.448477097,937395.3841133942,1034524.4578659679,1598158.2430398753,830224.029712121,1696772.16163396,1050634.1800373571,1240460.8918152805,516894.2950309438,7362087.248694373,556136.7433471167,1526270.648444855,1413877.7264681472,991277.251102394,597708.2571051796,1009968.1056056351,523202.1999689366,288680.3904415511,1388097.2939411285,546056.6384170048,2923830.0142347487,2227122.557125528,772036.44611152,1576740.4704495594,2550066.9792643557,813804.1404773323,851089.7382465396,719044.68977634,3651431.492813767,3503445.5578174596,3953004.9888475765,7271357.424208479,1485038.7634526808,2321576.886708406,2056779.7360795792,4045936.3758799774,3318343.9119625324,4270270.0880663805,220979.35089959158,979961.0242657233,1149063.8350215743,368685.6780393843,5070109.047740419,1629045.2529524264,89269604.97308895,591049.5262169826,5559757.876775702,5162811.914006278,3330170.753652106,6159709.0556836855,1741256.0155823072,3528605.477969446,3941735.956468795,8717473.956640068,1346582.961705795,708065.6874567538,2379333.652733455,334425.8849516718,-201263.75130978692,627245.0959971633,11705453.149293775,74090887.29880503,933569.3495515797,822105.4501675386,1010670.3463963172,1359502.3384225818,4837041.599142555,4746596.655148169,1160617.570321833,2310839.364014455,2442600.7061618697,1175166.3519904716,1598460.575524161,3796633.7157445485,12615392.227494348,24946142.021635175,988127.8095519932,10729104.677973477,1956099.2721339576,1732310.2954925452,1345971.3086378374,2157919.937884372,2421233.197802001,549608.5732598391,2134150.878694009,5328176.861314857,583187.1797919387,3522753.5986048393,19516856.425575696,1886938.990623174,989179.653112262,4254417.414625871,8241984.061830532,12466614.507791517,3258813.581588086,1893059.0118214306,1598081.9166893228,1791591.5725707372,2080317.7160600927,2620451.290878363,1834902.8331146617,3245578.2057344243,1062252.909311531,7120729.496864315,1904804.2057972394,2677731.2455347576,1767592.2833737498,2811474.130926308,8283269.583790068,806335.9985099996,4023002.339713666,1624466.4159530639,336190.84326448664,1980179.4378797214,1700580.818015899,2926269.7104745796,12317680.773064654,1575483.5655108404,1225027.7309560895,6373586.084701043,3372104.5874145366,967032.5244142502,4412711.725155142,1629298.8831581513,1836398.847857489,1397087.998684085,623267.4707874423,3005794.9614954144,5125220.244516961,4129801.1634319955,1034486.9177080635,-156896.5948511595,1929660.3937157923,5121495.478087316,2956931.5432354202,4152200.824126151,6086141.287875856,2648585.614401444,2495073.84161697,1342382.4544980382,1568366.285662922,9277132.940904308,4527639.639672525,2674207.230410248,4589906.308829099,4589447.297622001,8026663.750854351,4598593.571267887,1049560.1563414007,2857675.674432539,1029757.3084398089,9753195.095205184,1738915.3047212705,3885405.852793542,4059026.619046482,1505713.6997990226,645395.6062308624,11717181.891323995,4157193.2710370836,1322673.043558618,3011312.364488607,860260.1308024288,1461880.3343309974,360632.12390214857,1025311.3693233146,2651230.5736795706,1729495.4025564068,1815691.3956953473,8497876.445650358,2322043.55706882,1777212.769014607,1499230.0061830636,2462051.5284938756,2633934.718413237,708740.3614586282,6251196.246920709,7285386.025199341,2002127.928547029,2495517.491392216,4009264.836127408,2900666.313164743,2334859.6800670745,27988500.307543725,623974.944273991,3759517.9894195125,616913.1339568221,2863916.432870546,2088887.0710233694,390941.5571845891,5699156.079539655,2497993.2934420425,987672.3638043548,2374146.4972774973,8055283.847841052,911697.3077249494,2672594.4023706964,1037451.4602010942,15292703.893017964,2529195.8578456608,1391316.2756997845,12088578.803973163,1667786.4399779828,7117179.540211938,2913661.2148431465,5297531.766786002,3023841.45452154,1945243.6564823054,2944637.015445154,927092.3905640789,2365675.263868862,450514.88591174036,2786553.21424657,5555279.549900005,2107675.569791265,1071261.6908615185,1119689.1192494403,1063118.555530203,14353333.679814672,4982567.313007376,956838.452495334,8607701.521808036,3791329.603694701,16025263.12975266,17116751.520017378,7069026.345039561,197557.15123137506,35759.111390378326,951435.8419493351,1706474.359359773,864893.2342854463,6300354.140954509,4529687.642045166,941973.5239512287,3767191.243733334,6449333.907216413,3071387.258837598,3231187.2890663897,1260826.0868366181,89793.25411966257,2750324.3853585655,2340168.525394927,1057262.3727453463,4682765.223962417,10469902.39136448,49908687.87182636,2545951.752810089,8148299.300508214,3548020.667297,9610880.22955313,1653415.1086563477,1395508.475118252,49079473.91384605,10565403.235484734,1380281.3158651777,2043635.9137307405,1295100.6192861707,1214427.467102393,30997.43117333483,15284841.54659292,1123877.1318950253,1103614.9231379894,377384.06168868183,857030.8878604027,10240081.695315719,1450634.1260640337,7343623.052934315,1230575.8639794206,12891023.53955904,1838013.7666349406,779060.7425560253,854344.9772513635,2173491.4712779503,960571.2456281562,2455756.9266334036,7785907.318735806,388435.4645983684,3453998.667988555,767768.0374440134,1758690.982645283,4599994.030142985,1100174.9857003212,11879342.65592865,-341818.59337022156,3695312.0512123066,2051498.2601557844,4743859.746462539,942717.7978321288,861182.9207778976,673735.6965699759,2922828.444024479,28637472.735736147,1325422.0779522522,2962329.0929258605,14708399.140508512,2298187.426005524,9833028.479997078,1036558.2177442883,3909151.3078237083,950235.5323146591,1979993.8462524232,2287167.856085224,3151234.151731806,3143409.981020473,903722.6250668417,1233503.0890963331,405015.84565693466,268017.9906804883,1058194.2858555005,1497851.3533739788,2497324.4966172883,2194822.5998863876,3827637.52334204,7446563.900504915,648205.053714287,1712645.801654195,15106484.508483289,975403.53764295,2618553.6906069457,862423.4265809138,1059332.1083630575,22734807.01940786,3102282.5091969674,3759623.2631430523,1689627.738107342,2709616.430186352,917008.3311674069,573782.9024128704,1073947.0286118733,963606.2052442301,1150742.8670412297,15015170.451951852,5605173.160644413,5148360.08350428,1598282.90184094,1443131.8164902553,5027470.614019675,2056811.2453008103,1621436.5367331074,8720744.02869199,1955817.6827451095,1064268.1960864037,8075973.6853576545,1124324.2212860358,331630.7575900918,406861.5801725546,8582623.66875272,1355475.2150969724,10164909.511431314,1042771.8336123133,1970580.6881323045,218002.36658253428,1306406.8883057134,6448674.843343724,4735392.50271396,2636704.4437795714,7227716.986139424,1744808.5464511893,951204.876999076,3599220.133367687,2713354.1860292195,74923930.27601498,1475044.0181992983,840515.7119837354,892960.6457107295,14508146.35126836,1517382.0721118893,2718865.959452246,1306897.0478404136,1090132.140809431,9005181.38874159,1009091.0440339716,6791596.918386111,1445786.1263647866,910442.150614531,2097994.193404237,4209532.813551095,7299504.9723900575,13874101.865915446,1303188.6363810764,2747294.038286925,2563075.416383148,7020932.269418981,1090338.3401721106,23513460.689892303,8511578.922600044,681689.8307440439,1627120.3655462607,1901423.729134725,1235131.3952693755,2922907.2242592797,1584689.6938420914,2643368.2272545267,70901751.46837257,2605102.242084903,1089641.1581830294,1274169.4552530986,3553862.860678951,5971579.775587119,1485952.8839616147,-28760.255351858214,495175.8829502624,1237256.608815002,999139.5975274378,3066199.000898326,3517932.720219561,1260127.4996473847,4225495.847850688,605323.0270302417,57027291.01668328,733632.4514832865,8438303.946320761,1890359.5091717122,3510627.109721056,2206044.386344867,1453648.4727898305,-145172.4566458529,1227419.4989769103,170319.1378183593,3994913.0605166145,8050106.928062798,978717.5205799066,8023965.2869034335,970708.2276832082,665873.350144932,1217213.3241641824,31393020.597074628,6794649.996373611,6149070.940644003,59677903.421696626,2046253.5036048093,1174808.613879374,1061252.5941370942,2236347.62695302,1988563.8133268398,38210913.740963235,4196718.390098341,5104830.80770554,1972131.4998273794,5864834.384455488,1395959.6403661724,3431609.6376617607,46513315.9563483,7518158.388020217,2165629.124852907,4775587.578771078,4115263.5067251194,739702.2315215976,2142284.718162713,972467.1299469171,2885186.1443542484,1316708.9254006513,1088710.0339990682,952965.6852558674,4369779.663813517,2791324.9873970416,1772340.2030294724,1244631.624670299,1447084.5956927948,5602077.235387318,13904723.914226351,1972626.2574198665,1051914.4030959932,1004121.0459554638,11808099.26522151,3973334.8514430053,899297.6722270332,4390383.579503,1246940.2620775597,21456117.0845922,8289403.503716156,4140773.9877787232,1159600.690382673,1037000.1639883104,1206132.0707339377,414723.9265975985,4071895.3745788587,2321111.149670359,1060424.9331381638,2936774.6690201107,594022.9777883617,1876292.562325505,2128045.8449816476,2944811.8407168346,1985556.5985399752,6259058.593345753,2076609.2772152107,1144111.5175798514,2307846.094787847,2273522.7655975297,1081394.7981969528,1999080.695832848,4060151.736199052,11776614.02153963,1299974.0092371583,3188443.941242215,2701754.0837613083,221817.65726173623,1948236.9257089137,841089.3005184578,2542423.8830065588,924661.1110331819,1688909.815208916,169342.82377337106,22218245.11336358,1377930.999329375,2300539.9821204175,2152114.0337271695,1998544.88645921,1058907.324298298,1273130.0889787832,847709.4128513962,3206366.3544796417,1647842.7668166906,1896960.1056415676,755101.3364451395,1831417.8981148554,7055616.728909599,1077170.2899144301,1225304.5993416843,7892393.380087918,8680529.460406445,4744457.733561224,966778.3044294603,1178458.8117646417,809160.4272435177,1945952.278096524,1314933.3843002457,2062420.5963670705,1003230.9405705549,14296776.973298864,1184278.502010755,37427436.082562275,51988481.03999733,820819.9475593739,1652843.3215342094,689552.1771690878,8845187.48216221,1702795.1012619468,905832.8734580055,1190588.2453548678,5449590.529952902,1765767.5666490993,965810.4248190457,1750828.636220239,1692058.797900787,1302677.8415340958,4801798.91697451,2311771.5039586853,1555778.4129498727,1027866.7176393161,5477563.851414494,18948316.642954286,2936232.869505302,7097698.691346796,902725.5464885833,1041594.5968179791,3463786.011073568,808456.3976620552,2637681.5244595297,12874160.802606562,831402.7635709941,10533124.842441669,2066627.5949267573,1947955.3363200657,2359894.471110173,4915286.388815699,1475293.8908698047,24397471.07275564,1118707.0451164893,655665.4959983821,946051.5734361205,7773080.896513479,5592464.631679354,2523167.0566464863,1071793.3063132013,2541133.5857425756,1377936.7598299012,4441443.779876307,8112498.685191065,136835264.15080082,3122076.1618779935,2294306.964401234,64358.087534196675,2349217.4585830667,3435254.356740847,88177319.36001158,7183258.489987981,1372589.0170146243,12828800.832881512,1948067.4457190537,1263269.6396268106,1825345.6867910177,2045463.0399270991,3909674.237198435,7564863.822853969,926696.2868275624,54051263.70198748,421316.2271693144,1940455.6644527393,1265864.6440249137,1260562.9033981245,37532.104321886785,19732693.103372686,1524603.2179503124,5819159.894112077,11841902.616228469,-12208.142586009111,3093056.0109440675,15374510.226994669,3550207.674278774,7712940.368567299,7652491.438153617,5981352.842519552,67235273.05115868,21780654.58832066,10740752.506922318,1222919.7128647673,3090185.6136963447,3865781.551607442,2175261.827315491,3754070.5175344357,1606145.2482659838,4634091.216841172,14035502.252458913,854470.2149090543,1509519.7256868454,1308298.7302667997,2136520.071308725,210648.69819830125,1383180.2474838714,7335519.903688844,2143326.6403071843,3103054.6945417398,16903528.158518244,805941.7940522884,8916172.91183853,5369765.22789475,2543637.23297874,1954591.570261179,1756291.7140577398,843227.3918214957,6980552.233676299,12724830.458436422,11696793.677047407,948713.1666800296,1157163.6246735365,4143817.965415774,8672882.636050692,1504087.2428766382,1110844.6986914456,974810.130844364,13283916.586726602,3538421.8280581017,1423714.6033614823,1695734.5307697167,1803349.780647957,4218064.571832821,5510433.619478579,1101393.4370004372,644666.1220657695,1440838.9338576635,3831457.5854467154,10351996.820867512,436067.6354047721,-186837.62477623438,4051164.272621438,1937635.3032079758,2271230.6881664493,1016841.1683296522,69634875.59932737,127360.3731302917,1066769.6707233416,1123707.651905165,162456.79139331542,18996185.163338486,1082274.6009980782,5152059.057784852,907535.9248386924,21043529.534625325,4449979.076654977,2165099.822531362,934111.1775261848,1192934.1208983068,2930769.570684324,2954948.0654775123,6078862.3555426225,2154931.898444807,7219854.639714381,28218305.252630733,-186605.98399261525,1980488.6038449104,4184516.2687773686,1159195.3063010839,5355098.119262968,537936.5244961765,1290872.0656639542,9975468.74734191,3062860.082173487,1760659.0694726007,1545150.7479445478,515339.85354389274,2005948.2673439246,1147620.940783122,1131382.272070977,901331.4321053536,10007727.261000367,-186662.42787674908,611941.2296971756,2395512.8652189136,2971868.468225382,1069686.1515274537,4428720.996177662,16830666.984745614,2067795.9354204813,8799579.760925524,400944.20195397385,21924156.287416924,1963971.195361839,426409.12234759796,2004743.68807041,1822327.2787349077,1711451.7166309934,1242080.0748536368,847869.6184391987,7884296.183033416,3237715.859309381,2550859.579097107,848951.6469435017,2219828.609459617,5482036.598042337,1773176.1047398692,5455089.506488667,775430.2184806508,2444464.0553987203,4540115.408015028,1285894.7903531832,1373690.4101146387,1756742.2536645613,5167024.425572747,1184466.2282699873,1486589.3625811974,5110733.536497137,2833160.7562909033,1765434.28354142,5490274.623819919,20841425.85259317,4064434.5161637864,2369090.3426539833,1073476.9987305033,13251435.939467654,1082742.101148461,68156634.51490948,1240028.0687059443,40357023.77000222,979028.8843214931,1452056.885989608,3273119.377035667,1077760.1015496484,4769536.654979848,7277644.079950836,2789689.854151589,4427050.51640936,551072.4997500023,30372894.138794765,5198093.8757232055,1388087.3129521315,2610903.6860616445,1795382.7505554878,1254630.944072041,1391042.5939089153,104267.0895077819,4020761.879199757,17013289.659975182,869383.0294043492,805941.7940522884,1241670.6390788362,2013035.6524263185,3266321.704337818,5571233.166030114,1245469.6746508374,846995.3137582766,14778710.900865441,1035875.9025999326,2260314.63713656,36826726.03387413,-42361.53731376538,6032354.4573038835,5032330.519489212,601178.1233588646,-20070.48901105253,700203.3410317099,2329515.9672676427,8705458.073417908,1660705.6679592533,1039975.4137796229,989630.506822431,272963.514232012,8418209.252261382,12290820.386501469,4060188.674350515,3901619.2983550616,7350930.87026675,3250951.2351630423,17111641.66507932,19142105.53449166,96505.14803353138,140442880.17141503,3185851.768925521,1407568.3886513493,889230.9689426101,3492208.756468504,4727530.156288915,2640801.6381697683,1696442.05192339,2907826.312868326,744147.8646448588,417742.88548308075,926151.1274851626,54223142.9010572,2942166.8368653916,6880244.517376548,1244471.267815125,1298084.0883676917,758999.2635527416,3525795.066644605,1097664.877223516,18580544.087158345,5955064.372876376,9373917.941431347,2672321.7092824588,3368348.6582335583,2952674.1871418785,4973542.461145106,2437973.887113565,6612359.1338434145,2068279.1180327262,1534239.073104872,3892181.532538183,321127.9536410505,910831.7614369041,18121288.431243174,726422.4056977588,6340383.045688801,10014672.091205236,4130017.242350142,1724833.7532162794,3366062.357906644,-562213.4909135159,2349538.172293415,4419374.24182602,14801020.390170326,762969.2666540083,3121250.557384529,40843962.27774909,5704996.647837898,6216358.047978617,2371409.9872859693,3066082.444872048,80258001.77986807,5887486.178319767,2500312.912353611,2792860.8059732523,1950751.7561527565,7803897.638211964,11033120.41293549,88105535.22975641,2769339.880342642,966601.0265072696,1115332.1300658023,884744.1604364091,834491.8641341422,19134243.188066617,5094708.749406392,15146296.654138107,1671890.4522733674,3014094.7563052448,1294891.5647984848,913779.08822538,2467278.517789952,24992838.693310853,4705864.086990889,14222770.802733704,2502936.188042014,576500.4885262349,6436130.233817305,725770.1050582426,4093319.8143729926,877537.9666334668,2454189.1820688318,3002674.7020584075,968200.4112398229,3830708.543675177,9412946.128581576,1454946.9421178387,4085486.8357221093,4338975.518191197,2075658.2818455251,13912227.19409431,2803881.7751518236,983208.7403111975,12494949.325294737,1746777.6511463143,1561192.6474762796,1969820.420124561,2265048.2640112946,1518408.302019811,2173225.539808412,9286723.993740559,374147.4996950361,11768751.675114585,1580362.3614483392,6437997.777448025,563254.7152465077,1256825.0608055354,2509969.0021312325,934313.7632365779,5616292.346249617,1487439.7687132221,32649704.1644216,4398672.255400817,2319304.240372771,1069897.7551246046,1103607.9881703495,1862336.9549591441,7860180.042592401,2779004.8118329747,823540.4171459505,5774466.056641443,66656.75211579306,28065958.44274976,1148293.629113716,5634122.052434273,18930055.32892695,985327.5823572951,21363900.754206292,4507511.2576229945,537088.7512875353,1520719.6920574042,3971037.9752792846,5079361.492103659,363846.44187273714,45895084.924168296,8036069.904544892,1181760.8692568792,4874285.257022002,842354.2105591858,546152.1795301626,1703596.8771947606,2979808.2410218185,25472918.213227723,4769293.258121366,1493899.9703815884,1302516.8367731443,16372142.736188214,-300069.5000629402,1169476.3723020602,1275712.7297061672,7366285.9571575485,9978019.4984589,1898221.855596756,4676280.853929078,30928483.40631194,1802477.526168076,229012.31715636654,1327143.8315623451,1724580.482850466,1169525.1154254791,1833208.0332160615,11238765.381052619,1318421.6871508958,1024731.3378269055,647179.1214796379,2264062.886813086,1081339.3451555471,751066.7698763816,772723.7160397219,11671330.484561743,2008477.7121353939,5269965.48102507,3198453.596758201,5753580.26117093,4606455.917692931,9452966.286132952,1711473.1530403714,3905408.410352933,7430906.639649024,1850259.391162964,1902546.6273016797,1072524.4681333594,1131409.6414750349,18527.791050672065,4516932.480616922,12729276.850360107,1733817.1422978379,9664685.264337577,1114074.2725404284,4098995.070601966,2702734.956100668,3061353.306289729,2507421.1719990405,3049583.3595347344,6055801.353486011,3855470.868464114,841472.4386860633,402971.9191118961,1481055.98369097,2231937.0787658477,6288321.000825655,1798204.940899783,3024636.0904325433,2200682.9854558674,3319236.1805469682,3909023.5188594675,2635370.5950004305,1125674.9180579276,1449100.1093017433,1663529.4903551955,1298936.702951851,6162821.006450757],"xaxis":"x","y":[2450488,1188444,1205617,579888.3125,3637973,2228675.5,5662234,783104.125,620104,21069722,3537977,2918553.25,5448975,770333,5907068,6388260,369291.5,1784385.25,873923712,1889990,1275236.125,646505.1875,1058986,533370.625,1384693.5,2196718,1295685.75,5702077,378714.4063,727374.6875,2989919.75,1066901,4155007.75,5253241,533329,1271021,1057256,9341427,2659770,1054995,3874632,13951571,1344512,6508595,1286817.375,398357,729535,5117308.5,1860901,827618,2017423,242805,1212883.75,1079477,4547129,8819864,499887,620483,4735249,478982,7046345.5,1191803,811703.6875,792734.8125,4420650.5,1847401,1073156.375,727703.5,729096.875,858897,4897760,1030150,1784796.5,870633.5,1614322,552015.625,6345009.5,2147013,4235405,2788860,1691963,4946798.5,7482832,4024810,1093653,630040,1582655.875,1835804,722062.875,994194.875,1964681.25,969682.8125,6695413,764906,3291518,13158205,3344833,44984468,12704014,524144.6875,25377650,6962255,872114.5,466672.6875,1381407,2919980.75,3481727.5,2992693,464155.1875,2372126.25,5965171,1811506.375,670779,15552193,5337334,896403,4039667,4784731,749207,8381834.5,25970248,900973,1436773.875,1200381,2225265.5,5348309,966812.125,9791557,777094,5471735,662424,939413.625,17924416,454296,452744,11441,4188660.5,1686262.25,1767480,342726.0938,1987334.75,666385,8611054,410433.0938,1501111.75,1210229,4268054,914670,1509492,1185469,10750010,572654.3125,9290214,4145920.5,1332591.5,549509,1134195.5,1682454.875,2364351,7475577.5,3418139,1035592.813,567608.625,6015128,7380088,102673696,2177075,875789,1972234,618876.1875,6010407,1189402,2916511,3757006,974305.625,505601,1325597.625,4189569.25,3229864.5,1508561.875,15878689,5528223,1605522,1747090.375,19490284,0,14585813,5696695,541950.875,5968153.5,1346952,612158.1875,1076084,731092,2804534,690529.5,2190460,1829122.375,7240274,947884,9162048,2470566,690557.6875,25959642,703847.375,207572,3155391,948975,666687.5,1247361,986353.125,53166156,1419980,1171380,888308,15590743,830945,1074350.375,3805877,2108012,659450,3693616,4696639,2433777,804620.5,5258024.5,2420605.5,13171370,761504.875,4054994.25,858932,3267564,776237.375,12095818,3556291,887403,6797528,2055534,1983256,566183,1989287,1045413,1169932.75,2964206,3782378.25,802312.5,10062436,1034941.313,12086616,1138520,14168729,2173143.75,1441631,1742351.25,1222949.25,2478960,1913558.375,473874.0938,3518459.5,2920918,19645206,36667044,98960776,13010578,2280352.5,1228041,4961405,3157579,809291.8125,2288635,6293359,5454482,7739699,2077039.375,3577120,1574750.875,708967,2458760.5,748335.3125,686949,1465795.75,1244150,2557533,1686405,2186148.5,1194835.5,3744483,4476997,8355988.5,6914839.5,1738724,90558.70313,3087318,965425.1875,913899,2292169,490743,601590,1117658.75,1082531,326778.6875,1821047,230365,809870.8125,387810,678608,1297872,684234.875,1114384.625,742630,2454812,10420096,694241,1966424,3663534.5,515025,592534,600060,2001643.75,1866800,2360288.5,6859198,955641,4358944.5,1034827.313,6596057.5,1784233,1886382,1100575,1632731,835085,738990,4385512,1227772,77164568,549834,6609358,1789710,1375366,5104947,1883864,2137686,2208196,3004271,866096.6875,1679127,498745,3717064,1446455,710825.1875,8259071,16644664,924438.125,542882,711118.375,219483,1041652.813,0,318205,905750.375,1024033,1696448,694479,5457975,5885427.5,13093350,1270139,10213132,1200811,1076644.75,581589,1967637,2912264,967697,4700396,7369591,489557.0938,1529510.25,8320719,1130960,807358,3837228,4988493.5,12719042,2996958,5238803,958725,1529309,1256575.875,11319836,5767765,6036588,683274,16371764,2126288,920068.875,942089,1497496.5,1358022,529807,2912140,1017787.125,685888.1875,5252131,1817550,7338511,17109052,961476.1875,688641.8125,2211671,3864699,647906.1875,1938887,4547379,1750419,648634.875,1739693.25,2962476,3893073.5,2338238,809257,719471.1875,934357,408513,3258819.5,1911591,2890253,468396.9063,1868497.125,1072487.875,2151376.5,8168547,2472350.25,1417778,2337651.25,5917000,5653656,3059758,2266983,2676594.75,1379739,7637986,1128179.125,1472548,3371455,3091559,7003615,4059424.75,1318096,2156407,1765813.625,455798.9063,641713,1027394.313,648879.3125,1947717,896485,2248776,3404992,2158629,1380293.625,7067404,32579658,1489363.875,1359579,3017709.25,10154608,3546459,892227,2439484.75,3040924.25,317581,32381836,1401315.875,1921222,1494727,1703504.875,2563985.5,1762174,4078339,6454983,684155,10403123,3240830,508014.5938,1022957.688,654625.5,20311228,1481698,1093757.75,18661606,293217,922041,8381744,2127534,2694853,362874,9586556,609494,1133028.875,502667.6875,1457118,2265836,962238,815800,1259456,999924.3125,10317852,4022735,877788,6578370,2477499,51168308,11788719,12783255,1677877,989079.5,1431784,928409,1521202,4685346,2148565,538740,1889761.125,3605749.75,1443367.75,4206839.5,578854.125,739662,3740373.75,1385697,608755.375,1634178.25,8696206,137635696,1364344,3590731,3605855.5,5357833,1176342.25,716940.625,47148456,6320060,1045640,1275231.75,783345.6875,1929011,2307938,22055140,759060.8125,924121,2361088,1493131.75,5261096,755618.1875,2735128,688375.1875,8493420,1858224.625,551219,562617,10303305,657385.8125,1645748.25,4760734,1612105.75,3366528,1245923.75,1093743,2468322.5,816510.875,7682562.5,1607972.25,2512443,2069880,1634334,1334125.125,625938,3158151,405062,40062888,287132.5,1192100,4934559,956481,7520554,818317,2003474,827211,1967129,2429021,1006989.688,2616226,454124.3125,872555,268269,1085439,1055624,3191228.25,53401,1012341,2140476,3265995,811900,713326.875,6935723,204499.0938,12525174,2189735,1585441,14361382,1750362.25,1824054.5,1137513.625,1150926,593824,555864,1761137.5,497991,2470642,4321141.5,2274873.5,12731388,1898509.75,1245753,4486477,990361.5,4560492,5707529.5,3643112,1198630,3151107.75,966376,1104074.625,895310.5,6750112,2225134,3775187.5,431390.4063,976537,5415438,306721,2188452,2234448,3928695.25,4983934,2023197,2028125.625,1593610.125,3436032,85357952,591249,505949.0938,833747,25476332,1186682,1307883,965786,876871,1888344.25,758875,8651835,2442220,3863772,3947209.25,2014500.375,4424263,6127771.5,522791.0938,2093011,422133,2646130,680395.875,35383012,6313678,681354.1875,841242,1507168,784584,2025533,7439009,2090543.75,37980712,3239754,730211.1875,599390,3895403,861972.875,162960,580688,930439.375,1453543,642052,1482997,819354.625,4294550,2024367.75,1525624,157606480,869699,5903033,8926736,3289053,1820292,2277316,1844825,94186,869835,2491238.25,4407932,637685,9155568,4912520,3999505.25,6983736,18848780,3303036,6361236.5,25397086,862541,752441,1551204,2056762,802755,136241424,13962749,3312125,2114860.25,3568237,1109924,3542542,46510960,17044842,10960107,2471623,3132453,597761,1107290.625,658297.5,9029489,1015172,2701872,493969.6875,1886007,3226884,1216543,1469556.25,789209.8125,2081072.625,6884914,1623657,393493,645927,5672838.5,2401890,624993,3485865.5,3005926,8024064.5,9373179,1872413,145573,1834570,714898,876237,2015117.25,958242.875,1976856,10077532,462465.5,694189,1056099,1418915.875,2485521,2756494,1271753.25,1176615,1096646.25,1943933.625,534039,849504,2322314,0,2466100,3629065.75,1188994.625,1822834.25,1169948.875,2122581.25,2400507.25,7175646,3996541.5,350264.5938,14709151,2067352.5,415586,1559199.25,832168,1207869.875,1041304,577033.875,1677881,796042.375,813641,1085183,871355.5,5169726,661383,828531,6102883,2227640,2890379,507603.3125,602191.1875,466904,2767381,847419.375,1618695,436848,4425832.5,439065.9063,28450436,72587024,781682.5,5876439,642133,2407904.5,6058494,611536,1926134.875,2556131,1639849.25,696522,1054581.375,872326.375,677536.3125,3751417,818836.125,1334876,1287159.25,2334260,16163194,1172785,2887392,474468.5938,1153690.125,1228963.75,447947,1332912.625,0,443365,8504904,929688,3573328.25,1293336,2075833,1059605.625,1507270,1419453,586827.5,1360340,13659926,1971236,1364214.75,609508,1790148.625,2245568.25,2855545,3944933.5,286685536,4767227,815115.5,1261634.5,1845150.625,1698074.625,61576184,1226435,716071.625,5249190,1128208.125,863703,2301043.75,1080203,2376794,11325513,941564.8125,41655852,387279,1734533,1540235,570982,1689631,8909924,700899,686019.1875,8580279,938908,4167253.75,4222682,6118300.5,7323351,3185005.25,7256842,53079164,9569715,3644982.25,238255,1614026.25,1655612,1570758.25,11594121,1929118,3021669,6814269,841841,1258619.125,803938,938096.1875,961749,2058532.875,2220053.25,1313366,1297633.75,65336980,541542.6875,3608955.75,1983509,3218824,363808.1875,770175.8125,592221,3791402,23364636,49102164,1625308.625,497528,4202335,9986032,709990.8125,1423463,567521,8748273,3373582,846389.3125,1000804.625,2998208.25,2153603.5,2660396,1289073,1247972,4253369.5,2105451,13586070,935580.3125,1082920.125,3321035.75,3288836,3560614.25,512166,51625548,2615973.5,1251906,555153.125,810636.875,48729588,548292.6875,24473814,785052.875,13300859,571309,3570161.75,522972.1875,2473302,2098850,6881405.5,2306723.5,2876857,5089903,10564729,5039523,1730886,1633399.375,1696978.75,1739286.75,395346.3125,349139,6040837,945020.1875,1764767.25,914023,1232822.875,1625695.75,2545405,5807879,1392015,6283226,2662054.25,4133811,1582794.5,1259959,964015,6424029,12448381,873711.125,13911761,1475475.5,10498344,1788148,618366,776991,1331122.125,5903142,634751,520405,11026945,5981430,2203026.5,2166711,1427974.625,2654755,1050982,2581741.75,1639136.25,1402048.5,3165109,729397.1875,937635,868859.8125,4409881.5,705262.125,3542708.5,2729167.75,2628864,813038,2773779,21781324,2161828,1199392,832986.875,602245,1074989,35685224,342493,94178648,1136676,741287.1875,882896.5,1505709,0,1803753.25,3220635.25,8201974,2035595,12265516,3306123,313528.0938,241478.5938,2465266,1270582.125,1982967,824516,533078,19344782,1172065,511180.6875,255971.2031,758008.875,1911611,2503948,636641,677279,7838600,499928,2921368,22591288,964923.8125,2278160,17186624,591622.8125,919746.3125,1717130.875,1815350,3024478.5,5975230,860364.1875,600543,3275779.25,7261732,8921778,1976400,5572503,6159684,2960377.25,13270687,7370743,1323038.75,291614432,1425599,689803,439266.9063,3020001.5,2168701.5,1234095.5,2219744,3045139,638201,1943772,855393,41213584,3674705,2712983.75,1201630.125,648022,556700.6875,816846,196410,9977330,7419765,25453690,1120197,2483320,1358226,1441665.375,33268410,5535626,6471427,1437569,2128963.5,1581425.75,1231546,5037781,1581036.75,5392617.5,4180298,4817062,687953.6875,3743228.5,578789,1519952.125,2355732,7969653,431357,5094197,65047284,10513817,2599171.75,842593,10105379,45074876,2391566.25,1004253,1827424.125,652991.6875,3534690.75,7657568,64715652,1073507,9671619,1540258.5,8163413,520206.6875,6744317,6577130,63668488,695346,2666634,676032.625,571644,1103594.75,16614404,2885740.5,9851937,1743198,572082,40613740,861007.8125,47102428,529033.5,41680640,1398253,987989,4590246.5,7976729,786346,9146727,1688612.5,801264,18628834,1108310.875,500237,17673018,978211,1921890,2926089.5,8839812,1634669.375,1115020,5121586,804002,16246106,3612526.75,10340282,881386.1875,920430.625,1183623,323739.4063,2993032.25,788055,19150024,1929828,2248134.5,669117,495152.6875,941257.3125,21795830,1513700.625,445943.8125,2843033.5,460010,16020642,812013,5838129,18967014,1038351.875,20657852,1808401.375,588492.5,1380283,2345752.5,14833476,1541879,19827916,4396566,899266.125,8354235,466985,1280263.75,1011296,1325972.75,11821841,5481549,537141,1037696,17750994,792015,697397.875,1736775,3616057,8070420,9159185,3730953.5,18576900,1356972.25,656980,1487769.5,706961,1916309,4038090,4027134.75,868163,3767135,517029,1208168.75,864765,783208.625,535649.875,14751117,1896858,1757601.25,1116705,1920352.125,2755578,4828599,873648,13440924,1730431,3441643.25,2178074.5,690750,573395,936616.5,2762846,6821833,897514,18074716,228212,2549892,22530168,19991412,3215953,3245111.5,6057734,3234787.75,1928391.25,416566.1875,741850,1045337,11963428,3035962.25,1855556,5040382,2096835.5,3002337,797170.3125,892455.875,1751483.875,8054334,686055,1381590.125],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","SiteEnergyUse_predLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","print(ScoresLasso)\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[10589994.454081696,10589994.361778982,10589994.271636125,10589994.167412002,10589994.07221976]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[14980469.662824878,14980469.62259005,14980469.582079716,14980469.536834547,14980469.500350365]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[6199519.245338513,6199519.100967913,6199518.961192534,6199518.797989456,6199518.644089153]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[8776811.20590027,8776811.273505108,8776811.341290172,8776811.408640873,8776811.475873321],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[17073626.97338111,17073627.104236174,17073627.2350913,17073627.36594649,17073627.496801723],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[14355057.038757114,14355056.753335515,14355056.468024429,14355056.170521395,14355055.891470246],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[5565512.967211687,5565512.83445284,5565512.712668646,5565512.557910366,5565512.393316441],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[7178964.0851583,7178963.843365272,7178963.601106076,7178963.334040883,7178963.103637064],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","FigRMSEGRidLasso = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso, 'alpha',\n","                                GridLasso, None, None)\n","FigRMSEGRidLasso.show()\n","if write_data is True:\n","    FigRMSEGRidLasso.write_image('./Figures/ConsoGraphRMSELasso.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.896e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.721e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.721e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.896e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.896e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.896e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.761e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.906e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.761e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.134e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.906e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.008e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.762e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.135e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.008e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.762e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.136e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.908e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.009e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.763e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.909e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.136e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.725e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.010e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.906e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.764e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.137e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.910e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.725e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.011e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.906e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.764e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.139e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.911e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.721e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.725e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.011e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.906e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.140e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.912e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.765e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.012e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.725e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.766e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.913e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.712e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.726e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.013e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.142e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.914e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.698e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.014e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.745e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.726e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.115e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.893e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.720e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.768e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.143e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.663e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.991e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.726e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.755e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.016e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.086e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.717e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.873e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.725e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.769e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.145e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.570e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.727e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.017e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.751e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.965e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.678e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.007e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.821e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.710e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.146e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.770e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.918e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.741e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.304e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.889e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.727e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.564e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.682e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.695e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.758e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.780e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.131e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.148e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.919e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.178e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.727e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.729e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.083e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.868e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.046e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.720e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.274e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.660e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.772e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.150e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.921e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.017e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.710e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.219e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.672e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.012e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.576e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.335e+16, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.960e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.773e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.152e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.923e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.345e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.657e+16, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.823e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.565e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.695e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.022e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.928e+14, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.452e+16, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.353e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.752e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.154e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.729e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.924e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.111e+14, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.324e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.024e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.267e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.705e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.361e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+16, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.201e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.119e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.746e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.894e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.746e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.776e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.926e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.156e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.829e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.729e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.026e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.585e+15, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.358e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.170e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.688e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.330e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.733e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.105e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.883e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.777e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.158e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.928e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.730e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e+14, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.077e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.979e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.707e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.072e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.858e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.160e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.930e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.440e+16, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.730e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.029e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.322e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.996e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.780e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.803e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.162e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.932e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.731e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.850e+15, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.031e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.328e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.127e+14, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.880e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.537e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.004e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.721e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.165e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.782e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.935e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.910e+14, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.033e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.732e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.180e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.334e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.268e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.720e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.784e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.168e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.937e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.732e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.036e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.297e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.425e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.128e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.785e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.901e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.381e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.170e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.733e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.940e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.218e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.751e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.715e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.787e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.173e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.041e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.909e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.121e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.908e+16, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.746e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.895e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.709e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.789e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.176e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.945e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.735e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.043e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+14, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.826e+15, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.737e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.887e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.994e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.697e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.792e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.180e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.735e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.046e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.509e+14, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.092e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.673e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.794e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.183e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.951e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.049e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.684e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.051e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.839e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.796e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.187e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.624e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.954e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.964e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.737e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.960e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.616e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.770e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.924e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.190e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.515e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.958e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.738e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.055e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.743e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.615e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.255e+17, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.801e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.961e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.740e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.058e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.209e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.178e+17, tolerance: 8.188e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.686e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.804e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.198e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.965e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.741e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.062e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.248e+17, tolerance: 9.833e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.181e+17, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.296e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.202e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.807e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.969e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.065e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.030e+17, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.302e+16, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.497e+16, tolerance: 1.032e+14\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.973e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.207e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.810e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.069e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.743e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.984e+16, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.813e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.212e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.390e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.977e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.745e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.073e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.136e+15, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.816e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.216e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.968e+14, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.982e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.746e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.329e+15, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.820e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.221e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.748e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.081e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.387e+14, tolerance: 9.165e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.823e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.991e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.750e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.827e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.232e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.996e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.751e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.090e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.831e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.238e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.001e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.753e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.095e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.835e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.244e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.755e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.839e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.012e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.250e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.105e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.843e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.256e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.111e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.848e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.263e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.024e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.117e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.852e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.122e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.857e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.036e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.284e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.862e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.128e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.770e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.867e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.043e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.773e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.299e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.873e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.141e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.148e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.777e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.065e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.879e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.780e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.316e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.885e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.162e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.784e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.307e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.891e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.080e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.072e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.787e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.155e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.324e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.333e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.791e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.904e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.097e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.796e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.352e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.911e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.343e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.195e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.918e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.186e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.800e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.805e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.373e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.363e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.926e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.204e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.125e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.815e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.384e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.395e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.222e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.951e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.157e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.146e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.821e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.827e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.419e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.960e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.407e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.833e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.243e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.969e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.839e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.444e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.989e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.431e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.979e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.265e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.853e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.846e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.289e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.302e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.011e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.231e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.861e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.869e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.500e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.023e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.329e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.260e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.035e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.877e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.315e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.246e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.886e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.516e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.532e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.344e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.359e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.895e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.905e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.276e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.565e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.075e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.548e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.391e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.915e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.325e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.375e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.308e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.925e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.601e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.104e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.583e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.426e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.936e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.408e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.361e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.119e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.342e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.948e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.639e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.619e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.960e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.463e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.444e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.399e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.380e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.972e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.658e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.679e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.440e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.985e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.504e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.483e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.998e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.419e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.206e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.700e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.547e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.012e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.225e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.525e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.484e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.461e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.027e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.766e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.743e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.245e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.593e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.570e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.530e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.266e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.042e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.506e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.057e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.789e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.812e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.287e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.617e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.642e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.073e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.578e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.553e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.089e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.861e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.331e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.354e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.668e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.603e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.886e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.912e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.377e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.749e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.681e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.721e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.402e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.159e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.655e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.964e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.938e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.426e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.777e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.177e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.451e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.736e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.709e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.196e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.991e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.477e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.503e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.793e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.836e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.866e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.216e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.235e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.073e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.529e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.556e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.928e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.255e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.897e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.850e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.821e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.128e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.959e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.991e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.583e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.611e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.296e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.909e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.880e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.317e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.185e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.156e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.638e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.056e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.968e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.666e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.338e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.939e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.023e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.359e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.241e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.088e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.380e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.121e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.028e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.722e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.998e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.401e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.297e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.750e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.269e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.187e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.423e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.087e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.778e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.154e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.058e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.444e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.353e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.325e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.252e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.834e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.146e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.487e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.117e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.408e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.861e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.380e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.285e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.204e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.889e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.317e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.508e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.175e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.529e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.461e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.434e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.381e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.260e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.942e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.232e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.549e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.349e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.570e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.512e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.969e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.487e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.443e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.994e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.314e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.287e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.610e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.562e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.020e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.537e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.502e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.045e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.366e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.630e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.340e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.473e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.586e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.069e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.416e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.667e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.560e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.531e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.092e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.391e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.686e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.654e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.115e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.614e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.632e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.138e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.704e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.439e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.462e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.587e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.696e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.160e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.675e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.666e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.738e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.641e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.181e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.506e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.485e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.735e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.716e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.201e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.220e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.714e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.547e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.690e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.785e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.770e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.527e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.754e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.772e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.239e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.737e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.759e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.585e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.257e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.800e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.567e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.814e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.805e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.275e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.801e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.292e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.620e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.828e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.789e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.603e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.841e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.780e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.836e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.308e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.821e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.839e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.820e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.652e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.323e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.637e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.865e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.865e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.338e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.851e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.874e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.351e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.682e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.667e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.877e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.857e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.888e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.878e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.890e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.365e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.890e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.898e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.708e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.377e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.906e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.908e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.695e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.389e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.913e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.902e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.917e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.401e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.921e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.935e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.732e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.926e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.934e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.720e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.961e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.753e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.935e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.422e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.948e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.743e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.943e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.953e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.432e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.944e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.984e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.773e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.950e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.441e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.973e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.763e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.958e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.970e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.449e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.962e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.458e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.005e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.790e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.995e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.964e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.971e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.781e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.985e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.977e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.465e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.473e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.015e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.024e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.977e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.805e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.798e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.998e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.480e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.991e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.040e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.032e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.486e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.819e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.812e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.009e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.492e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.004e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.498e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.055e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.831e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.048e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.825e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.015e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.020e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.503e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.508e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.068e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.062e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.841e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.836e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.011e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.029e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.079e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.025e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.517e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.851e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.846e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.015e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.074e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.037e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.522e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.090e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.033e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.526e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.855e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.085e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.859e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.024e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.529e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.044e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.041e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.855e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.094e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.533e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.099e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.866e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.863e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.050e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.536e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.047e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.872e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.106e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.033e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.103e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.035e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.869e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.056e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.542e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.053e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.113e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.110e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.037e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.545e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.878e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.875e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.061e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.039e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.547e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.119e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.549e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.883e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.058e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.881e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.041e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.043e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.116e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.063e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.065e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.552e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.122e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.125e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.887e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.554e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.885e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.047e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.069e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.555e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.067e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.129e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.857e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.891e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.557e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.889e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.127e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.049e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.072e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.857e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.559e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.070e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.133e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.560e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.857e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.894e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.131e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.893e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.073e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.075e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.857e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.562e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.857e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.135e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.137e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.563e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.897e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.053e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.896e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.054e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.076e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.857e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.077e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.565e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.140e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.858e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.138e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.900e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.566e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.898e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.055e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.056e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.079e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.858e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.567e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.078e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.858e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.143e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.568e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.902e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.141e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.901e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.058e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.081e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.858e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.080e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.569e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.145e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.570e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.859e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.144e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.904e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.903e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.059e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.059e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.859e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.082e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.083e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.571e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.905e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.060e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.859e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.146e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.571e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.147e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.084e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.905e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.572e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.860e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.083e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.149e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.860e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.061e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.148e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.573e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.907e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.906e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.062e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.860e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.085e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.574e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.861e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.085e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.574e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.150e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.908e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.062e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.150e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.907e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.086e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.575e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.861e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.086e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.575e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.862e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.152e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.909e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.151e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.909e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.087e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.087e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.862e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.576e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.863e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.153e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.152e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.576e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.910e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.910e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.088e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.088e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.863e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.864e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.154e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.153e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.911e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.911e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.065e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.065e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.089e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.865e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.089e+17, tolerance: 8.188e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+17, tolerance: 9.165e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.865e+15, tolerance: 6.139e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.155e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.154e+17, tolerance: 1.032e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.065e+17, tolerance: 6.139e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.911e+17, tolerance: 9.833e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha       0.24094\n","1  elasticnet__l1_ratio       0.20000\n","                    R²          RMSE           MAE\n","ElasticNet()  0.849277  1.122320e+07  3.303228e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predEN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4255766.036592869,2629439.333664701,1415056.0258855964,1168154.2283554457,5122122.760404942,1973149.6804927934,6127929.929018833,1157169.3436315313,1668316.352964988,6794053.908942068,754538.2671424504,3736984.83417629,3909448.529328311,1147383.7525518392,11194985.398233794,5552963.393112533,1484962.3268434075,781909.5836419039,723777781.4933372,3028166.726972217,3093716.529095265,1413230.2752418048,1155523.8655809842,1123556.8539912696,4150592.053231975,3253759.227359418,989391.6162954557,7689577.454773939,2034266.7092648766,1352960.6144771527,2668473.6506249337,1803315.9125443543,5992060.50358762,8512176.463611107,1260443.190418933,3216717.265738738,2183497.914833682,23673792.505872287,2376779.1927740597,1525612.1030665536,2993848.732673089,7645215.8469786495,1580648.8574037063,16735182.238688193,1402387.007736505,1259707.9493966438,1205440.1887854838,452479.1596258837,1293125.1198349365,1474653.65826709,3552999.043918798,1311305.8492473722,1364072.3523790988,2681876.624240466,4877655.625286428,11814165.713245776,1155512.9426509738,5012635.028965516,2490521.7193992753,1156377.6861451943,8594911.715787344,1293349.9973262912,2045286.3593516806,2077630.9259507575,1365754.67632484,3084184.630178597,2052261.9095239216,1872233.11541554,2071225.14790629,1425777.4900959432,3673486.206687266,1940663.829529122,4226573.918996811,1540417.8056497928,3748449.808206427,1144804.4632099946,3959201.138185995,1924104.3453513854,3423018.82422546,1077515.8586796925,2460292.693159879,6207464.570622699,1750811.8919307021,3352109.506092224,1797285.711742816,1211628.423408119,2785244.12758833,809061.0894779717,1824153.5894270153,1787515.434055218,1680704.3736145657,1355782.3265420762,9430882.364562124,1352670.5589027898,6994521.72786019,18153237.204650465,5094878.7334110495,36451338.52059948,3853851.6792643545,1494497.291520291,16843613.69101948,15640126.959734026,2060630.9015863137,1220456.035663076,2544979.8623718666,5250353.353316668,2748668.7422013646,7283071.196488631,1040609.4564643246,4171682.8862709436,10749792.857473008,3078166.183859908,1336269.7626628948,19714969.78395783,4578281.030830206,705088.226828652,10313669.534226246,9179881.64081626,999904.2629284838,7747210.313686423,35700748.69265851,1784275.779119694,2683892.913821612,2300411.6196880103,1124054.7558047941,3486405.56728108,2536020.749813489,8287355.533402645,1591476.965947864,3591991.795092757,1174901.2428999804,1115491.3409332633,3665989.3230392924,1263228.644043205,672401.0984632922,2844016.862423613,4455801.55825979,1681142.1562425082,3474241.7849460132,3117606.204191651,822233.4227927127,1446255.4113305253,3055358.7918772334,1179293.2851682098,2746166.9917750685,846166.4540137874,8104102.786005628,1729411.6802241663,1543563.2008719225,1310763.3595389419,4119355.149897156,1161391.624950637,4192435.816842826,3927931.0319345896,2437985.877944506,1292276.461656204,2492304.5422525,2017097.2097180125,4237272.038127469,4274387.520294973,5609562.168721682,1883907.9979882657,1318562.6908869133,1655164.6783051,7439022.83344339,37369477.5179189,1779352.7640252437,1479109.8018103773,2173581.2633289304,1669714.7945692968,6159964.779663538,5366869.69715981,2063377.1879899944,3904233.997290775,2484366.5129525107,967223.67940814,1825987.0744279954,3196875.084860594,4767847.832458902,2024871.8322074285,19720392.330632865,2063652.9001653905,1423764.0090373748,5320045.745466676,9074502.309071723,2736226.1983546983,2697505.3829224445,13720742.297628026,1493045.0124352202,4002028.614915006,3534688.775831634,1318247.0566808572,2505600.8715342213,1143527.1118096416,2312799.1106265173,3758289.955402535,2190341.9460690534,1485077.1589781982,2484850.7097525476,894245.5896475259,14185200.31085854,1144076.5662936927,2834699.9675039896,11166838.807271648,1495658.8770550624,1416924.3195881413,4584195.484784857,1183025.8725367584,1123343.5653236478,2340687.4582537855,2095404.164769053,12008572.966587886,2582619.3094229973,1736646.3161609164,3478962.1614151383,20209858.381388027,1802864.6148904383,1153278.5841375915,4155799.417102795,1491739.872359558,1584737.0326112565,4114494.2665037606,5273022.312536,1628595.6245455686,1921704.2259254204,15221393.098370235,2602462.9061294496,13336929.305103753,2468729.2891965904,4052467.0479513966,866240.51800901,4895246.637844579,1932298.472245397,2640293.8503815853,4972402.519185563,2181438.6283254707,6536251.779050217,1375663.9094922906,3094776.0916711073,1130632.4560812972,1611643.8254511193,3495308.2482620617,1836747.0826265356,4676445.807802314,4196316.231997602,1077588.4686622978,24808470.274371557,2068471.537152965,4451720.578001792,2046461.8287372892,18390741.88585661,2133564.3119108723,1659703.277598819,4308350.579233203,2072515.327525219,2397975.240955783,1554977.5825658632,1078814.2922710702,3769308.765116707,2427275.3784631463,9788833.25808492,42952566.2507368,36126134.69098461,2745406.1632290184,1430732.3123226136,1061677.6418944667,4684205.438247148,5234694.263128065,1052148.1364138531,2468800.605018199,708246.3885339508,7049112.509821653,10283043.75778475,2787952.909458048,1215314.878967263,4706688.43239741,1314645.134300055,3305742.6330787158,1444858.8421128844,1178335.86676929,1665125.8242738522,1206095.643648624,5567770.9415992815,1264283.7220431091,1831423.7823637533,3191231.9904021164,5160157.541397045,3226214.809025643,15852886.384576123,15783959.902533013,7468620.156896432,1631538.528763725,1038651.0919479872,1991700.4809600683,1182223.9167634428,2810224.7997457506,1140705.3997447183,1382367.3963183272,991054.9510025992,6391121.204785328,885529.0558594419,1433843.0031539495,1486765.9492644896,1145861.6234106747,699344.7993115298,1197131.8287704699,1002125.0907091298,1123450.2562950463,1283136.0008012485,937473.6049975951,2824315.009815094,2881279.4323372827,1184133.4629879873,2254617.560816152,3375737.7812284837,1128592.3510130157,1158556.9281163458,1161736.3090468626,3519046.7275786316,3081043.1078479253,4141861.2511396203,6801363.1584996525,1757137.898777573,4163341.3060158454,2050143.299600148,4183375.738309322,3334587.566122482,5345441.141426493,815196.2507067746,1083908.9058715885,1466260.5330411368,889842.7223059149,4635546.775118593,1727209.5439120983,83072780.24198881,1020451.0599695272,5251526.494814748,4445457.747684013,3205299.780617699,6060324.972007774,1606616.0306792855,3181775.7565797344,3731463.0233290205,7656265.250677994,1568501.2656863327,2162216.20714319,2345998.79329147,997720.3555159168,877345.9163088892,1097318.7708866235,10912396.201463632,59453505.392877445,1325690.5315483762,956184.278015852,1118798.189190938,1299819.6219281466,4551824.4076136425,4211815.1323590055,1139985.8436065211,2343426.090365403,2526682.719840363,1325431.973349642,1861640.8203963924,3347528.4020933667,10624681.783363406,23137713.630298976,1350890.903194513,10178059.188985955,2046598.0313604863,1789383.3774895796,1556268.5878514517,3246688.235529827,2181653.7666065022,1112316.6814402342,2008380.9376783734,4858361.232668523,1025873.6066445606,3054843.764484351,17182924.928742476,2451596.0574245946,1547395.7016083528,3893641.8045343803,7497324.586798542,14499683.396268487,3244911.2136605824,2069075.4468404236,1847985.0484499412,1647068.209768781,2296922.1017192416,3301313.677575779,1770983.0385886233,2890868.6924607903,1072676.1894943807,6018938.552166825,1738051.5620643464,2636878.8953628563,1779772.6270416593,2656653.5806961847,6864073.811279071,1040319.6649246882,5864675.70460474,2147533.2792970985,1239774.27346,3518728.8526358656,1573927.4011120168,3047296.368989307,8836607.99701783,1768989.6254695803,1529511.0701681224,5507604.346396074,3184570.9481710372,1263475.491036754,4020855.0546003357,2527058.9473710326,1659657.9478924528,1609089.647399025,1746583.375244933,3929020.063795633,5171951.851188004,4085253.9292655927,1365358.0275543514,337078.41798555106,2025350.4221417615,4402226.459191403,4450980.087701393,3746448.5412325277,5029595.738661736,2347532.7426178707,2643913.3150808294,1921556.4344784904,2171501.4714352223,9838078.367721902,3737724.20227558,3793741.2479738332,4147934.387668684,4215171.72753779,7521299.242884959,4044363.6032341416,1240691.1099406504,2871998.9127033954,1514332.130120614,9446715.390506681,1750706.741770926,3302188.515006507,4289159.529460704,2468240.9262368274,1084536.0574461678,11668591.177070707,3548184.424351866,1388109.212015456,3738464.2543709096,1177667.8384226728,1713164.0505263659,1054397.5507648312,1756527.3735854889,2694348.148995814,1775380.0655531383,1649704.6145775507,7944438.281813288,3309908.6566717187,2679649.059208326,1416865.0000563858,3335898.222231573,2424866.2743043546,1360474.4342180756,5223154.305720309,6875867.736688922,2178199.9508450422,2481078.2644236144,3530150.561547637,3583296.539338464,2083665.7184266173,29028148.22419266,1039475.2985035626,3220422.241863059,1680373.603463452,3411660.660008207,1986838.6381029505,994869.9405346822,5945783.550595421,2303875.801846706,1090106.12522705,2347209.18389297,7273318.063555196,1219005.3345629484,2634150.589579301,1320067.6809750884,13299209.004910123,2507167.2017009864,1507436.0455898305,12636618.594356705,1547572.193523406,6105193.35567271,2816142.8524232768,4875175.437692857,3082904.6746873376,1782291.8563505115,5125605.639037116,1130528.7460271595,2476602.6975626377,581052.8210649751,2907342.8575353273,4950821.007536293,2122667.064090476,1473018.2864254867,1364941.7486293835,1251587.3197964064,12276960.790061943,5102573.683939638,1650961.675971371,8701324.070315719,3992416.076650263,13543240.577960566,13702034.5644161,6690249.119524319,967468.9848748804,851589.0006888206,1101765.943470146,2121445.429997597,1080542.383313877,6453545.2662008,5652898.740677471,1104915.5545217104,4005367.1920242365,7964413.12891647,3454379.105420917,4593049.403600905,1499582.7383486738,1495461.557913782,2412105.419035724,2456104.202771496,1157773.2707030913,4301357.860465808,12553861.493569609,40539461.45467356,2523500.524978022,11121718.047404576,3024269.8976606675,9104351.308976326,1533764.6022122318,1811718.862560424,50136267.172791585,10254171.879955169,1882196.0922764395,1950472.5377093637,2156048.0263100797,1183230.1764715537,934593.9322041518,13304631.551585156,1300415.9602125145,1361498.8366798074,1811808.178959039,1085964.9299889104,9238668.300850928,1521156.3113971704,6465390.17744988,1494446.5304365465,11374139.978506066,1808990.33228321,1100670.8132576402,982663.2942908041,2255455.5776710557,1055259.5606901739,2962382.7362821903,7911359.8026666185,862748.1760105763,3916744.14416118,1067100.1885695,2072479.698403831,4891432.149953559,1422077.1227963571,12660689.489595976,554301.7457323093,3533424.4092006492,1945049.9910343306,4279248.138551004,1140860.5951360867,1255776.042789585,1309909.1724842745,2585378.1946763867,34855320.53127828,1316728.3959224056,2526536.7538256533,9716440.266313216,2251586.392234797,11762987.170973703,1040285.5234263507,3442934.2919981796,1238235.4626865638,2151814.5448961305,2777585.596980732,3176423.590934841,4680978.9267524015,1123488.7526796712,1287668.0301742316,787694.44691305,968065.8755352302,1324996.5336630235,2473663.472911861,2392446.0499469386,2531092.540572113,4701690.331578555,6369903.15385599,1398853.3757410946,2091525.6712616174,12704015.130363166,1091987.4036989622,2898862.0067779887,2221008.810663186,1103827.2143326257,19120027.046546105,3547999.452893076,4418634.325133513,1928766.1948339234,2741269.9526871643,1122424.6899469406,1008968.2323223741,1082074.1704949706,1070765.3527330824,1315134.7477338817,14083213.211781982,4812702.562881755,7639602.726879427,2357381.7027838887,1909108.5269564202,4883078.171961461,2281872.3448107583,2532481.494046066,8603094.842409402,1779048.5516466296,1062554.6523190527,7154792.739530631,2284345.3225528393,1221644.8732394397,986265.4160135507,10204521.365855195,1618371.3745585354,9018783.921643041,1146127.9464197517,1969128.2970508053,2864931.884566008,4607385.036858268,5657057.167065345,4191069.7905743155,3006886.5718703037,6550457.700369205,2360644.069310303,1357701.661241842,3358035.6693142503,3181733.493352992,61758897.82769367,1371220.1385478128,1161800.2328202277,1114839.8861066648,16981983.572081055,1515807.4240291298,2275590.8009488513,1435758.8246115332,1261555.6695426293,10217816.715441953,1280579.837207213,8216416.046082991,1380902.7257890878,1949547.441982347,1493179.3784279046,3958419.1366338995,7260351.256216235,11458028.548661143,1577935.2290979042,2659290.6004348644,2271303.6480358136,5908624.448596071,1451677.9580055585,20060597.61377664,8896935.357983362,1138149.215120409,1603998.6225491576,1824442.5681934278,1467192.0722806435,2746206.805448182,2254446.719084571,2699770.695670847,67295929.11985093,3025449.137200417,1490085.0143404037,2624649.6985983765,2286948.739592634,5018057.575640549,1413182.747486766,972067.004281271,1769374.8187554292,1353355.3445228168,1140439.0767356416,3769918.324914106,2917260.9111520583,1469033.2626099256,4013373.1357465982,1497701.3382588108,44171733.451653615,1274736.2391288574,13956490.249157917,2005507.3822137597,3028571.416855486,1843571.0283130421,1375480.1791140547,702104.1588612897,1254030.5283616795,702251.1801478192,4043099.3260493265,8453475.118167978,1183758.4134443235,12328669.076483104,1574553.5874444097,1315331.7191593077,2530328.815302316,30830726.814626135,5756884.705135886,5606237.015092863,45869055.5722864,2029942.7683082453,1344872.3223738703,2464987.615525564,3188563.3506486905,2161795.8922957233,29797267.50745131,3993857.376891289,6387655.313931143,2157237.091571164,8015843.882245515,1277713.454126215,3358223.6639801394,50086903.47059113,11333849.613367781,2260878.1243460886,4757589.46314981,4185625.285929712,1194155.235279086,2188920.4615087155,1419099.3896188021,3200898.5004110252,1651569.9064059313,1272153.9158991463,1252170.6733114067,3691455.612103401,3753086.280145381,1760945.3528913956,1397460.3292403445,1649269.4212421263,5464481.120002737,12914964.733503323,1535937.8969935547,1230841.9780117157,1192432.8382701748,10069107.975159522,3667749.753962232,1108191.5158228585,3450742.7114237677,1799035.1323217484,19517323.729174364,6909680.8202671455,3984390.2066278933,1139168.6278673394,1218856.147170384,1265671.3065279238,980842.8693385175,5077750.921461916,2529896.466254859,1838407.053811816,5131028.185712149,989294.2761490159,1792868.3132320882,2273890.6511868164,3123493.24528052,2151224.6504585426,5217731.759045276,2135881.815895603,1304936.205030859,2073697.3686803947,1980612.721809964,1165426.1758549656,2259355.513526955,4309797.811977282,12948750.524332717,1634018.1712206018,4707889.346287847,2746692.4993621977,1293733.1291671295,2052020.5780355195,1790500.5645821341,2573911.962142514,2196137.495033367,2815647.346420784,699045.7154980008,18692916.253010202,2027323.2267894875,2056084.687229234,2477329.5050223814,1991601.7298783027,1556284.084418018,2996522.2007691795,1322775.8886285003,3318860.8521182965,1899716.4870119388,1921704.2259254204,1234154.9130605073,2172033.5645049764,7025692.669252482,1415116.4644151898,1421749.2424626148,6300337.24871543,7183364.336166971,3263977.512606,1263271.1871019586,1433388.263474954,1183306.5273905087,1968866.9315087502,1588282.9783930837,1864720.0016375137,1253932.514881473,11503861.177485049,1170742.0781961875,32257931.93266487,49914957.4107337,1188781.9609126989,1761201.3894036748,1132726.6684453757,10326055.782408137,1657769.5512868988,1094503.6245410354,2202873.9220848866,4687668.554786951,1913940.094692042,1340564.3462013993,2077902.2450788645,1757035.254480303,1450913.4394964187,5797915.110921739,2344175.204792986,1635772.3767762603,1223257.1741714398,4799257.046207469,14713026.444615852,2704603.6689692885,6012170.964065731,1299729.6312501878,1234289.5866503934,3340801.7251075227,1035186.9097892914,2606092.849200729,10565318.013206095,1014526.4469684574,15193232.404285867,2402747.3174639614,1784471.0983216628,2460215.8397202496,4246533.816504862,1582832.0994113989,11829214.9139455,1771631.0905304851,1042381.92875655,1092077.5317480634,4904874.766531758,5917611.306166611,3012532.15092338,1124469.9953305274,2511182.1520211822,1593698.750977769,3929571.697345022,8196353.9417377515,115456133.8577052,4397714.669248717,2524891.362789494,742263.93835028,2285160.9259264925,3247062.3939655945,80200008.48956862,5980082.26153348,1500293.3097540445,11525938.674537435,2051884.3754123226,1229339.880846655,1775043.559031687,2421139.7053441163,4240342.9777913205,9402569.724149082,1231059.2667158789,54473937.013254374,886432.1737056298,2262735.0064913435,1224567.6726118296,1489080.2922895825,776487.0369668705,11405146.571250517,1699826.1527010412,5162888.491853951,12180960.169275831,1090466.1684573237,2793839.1805449035,16072880.84912327,2529538.8633052506,6858911.756772623,6458030.25110096,5693732.304558668,69167009.47024865,23668369.959197253,10999589.723188374,1368270.4248174424,2752790.437706857,3403099.0723577235,3439964.813213051,3424589.3327210853,2351959.1561088557,6630560.1906581465,12595128.798085663,1072165.9219872644,1521229.9707041632,1715949.5900298618,2114226.633814315,1024382.6771977204,1983868.3723566956,6296082.606536482,2387019.9145212546,3274659.1046579126,15545021.501017172,1134014.8976880491,7558534.738453722,4789991.267793534,3671467.3870441355,1938391.0725867474,1808656.0486719487,1163979.4747913792,6574051.477328563,10433567.533515885,14523161.464275148,1160946.4122560173,1404100.8781493877,3813882.665906936,12050154.570347968,1427756.4281688402,1777053.6372055183,1257984.8430501223,10854227.689859975,3050908.6470597866,1808703.5764269875,1941233.6907647436,2113797.4523881646,4402566.360294461,4903039.445047007,1181498.085392206,1696082.557903051,2321058.40963546,4323422.632954551,7420710.604048393,1189217.0257249551,384120.0552284722,4294582.076135737,1954392.8554926894,2167761.021190861,1291763.0936029668,70177864.01485428,1034985.3214670741,1550861.537742985,1300279.7575893176,707673.7268228526,18978525.7911091,1811175.9121251982,4525923.939704027,1126553.3117016028,19248987.355706375,3872589.5145176244,3225919.110594893,1110338.1011967435,1238450.640781431,2740784.2587731485,2861064.1598254265,5605055.2978576105,3178199.2108765384,6555880.247044239,22433308.900454395,974138.7522600645,1530515.3503185213,3760099.132526308,1502324.5132754245,6053978.199904454,959749.7235448635,1333772.6181264007,8457445.072775196,3046302.908893211,2257704.508528573,1814535.9385959643,1007547.6373841632,3040979.6923109554,1190116.6104903785,1116490.8911050465,1109825.947301222,8249967.078422318,710735.3770162566,1478320.460273684,2589689.417838322,2773813.39489841,1639868.2505719876,5203372.101132988,16552653.214656543,2058996.4701079503,9719419.298912749,942670.2976152368,18016563.905174688,1797342.2462137751,864478.2876418512,2085593.4558752896,1752529.3560711204,5177532.098788319,1472776.3798317187,1155969.0782756037,8326024.560635141,2896291.2391358237,3075968.4237236865,1785078.0179071007,2270285.281708638,4791110.447068497,2225435.700780266,4602980.081447831,1407089.77615486,2975880.933522291,4904856.194654232,1170624.704612386,1667652.775337174,2013016.8878701841,5251185.4263200145,1349108.3980054255,1732619.1036854126,4593561.526890682,3903923.329391633,1893370.080263913,4975946.391479589,19456384.462622214,3762588.2412938448,2993281.5699280035,1579452.8087837019,11058188.00418355,1191786.3654076466,51044029.52016766,1292911.8311673147,30430005.19716093,1083159.7914440052,1385942.222847375,2849439.4090986466,1251612.8780492514,3743435.886802515,6004097.749138709,2639309.42418658,7768940.081045499,2646646.148988232,24717285.81557302,4975183.4722233005,1423640.7911835455,2584572.83473561,1904053.8067677987,1405496.2840089647,1978445.8256816624,1123712.3572115614,3438541.3126783175,15267682.011871263,1260691.7690721117,1134014.8976880491,1656215.7292571655,1836772.905629293,2832235.335517077,5470363.984812446,1475500.432295658,1210510.4065847439,15910267.823493369,1229865.7302739697,3681136.577822593,30961472.855684407,710510.7735036854,5156952.264726334,7396706.94186754,1484921.026136872,1095888.715132357,2167638.7538182233,2128141.765235839,7484414.651612046,1755778.8427286413,1143880.6031370019,1180787.5139868353,1420651.0613726135,7736190.254410026,12013339.948651213,3484324.770671131,5488085.301579964,3862592.731873615,3250333.760335616,30152994.981893547,15590811.759556953,697158.841855898,120339305.21622193,2600425.5689075147,1350189.0342581747,1133360.970261795,3381941.5339527912,4196492.337249349,2371429.997023094,2484500.2578398795,3020368.390436779,1072613.0728790676,866487.262355526,1040664.5730065554,54555733.74760436,2939692.385013881,7351021.943461352,1486439.185210168,1428676.2882052916,1031729.2170346011,2911838.3644770253,1178501.6276818728,20034080.36781813,5260367.027004216,8019670.412098463,2622190.316990814,2914229.314681643,3118070.698605487,3222139.8124137707,2727697.7516092565,6897685.693904244,3658394.607142873,1900495.902484229,3703379.724697603,794593.4268318554,1209950.7243180848,15215970.551695202,989562.4493437009,5587050.71107598,8165001.481787002,4292362.968102946,2158261.4167288784,5240116.809803098,701222.595171211,2107203.2139868457,4876718.034162141,14063953.035589747,1176846.6226469502,3094233.9264581986,47238399.83124651,5059400.05647711,5687603.064634525,2380362.559504622,4449430.4327709805,78960515.93714528,6497422.661600782,2428688.2075737757,2945153.111500668,1786718.4416044126,7032760.763021779,9141970.621296018,75038486.20529723,2611051.26017805,3200928.2031539995,1906197.429398689,1382195.9846749697,1187691.027138733,15596234.306231987,4268964.97361994,14534755.922416892,1907301.2908522147,2854561.9636805058,1526959.4656112615,1030721.7815131778,2558256.170169212,23203197.663553692,4446403.5772592,13580504.88272011,2638490.768405796,1135790.282005646,8050267.003975254,1280158.7858038908,7137202.13728857,1102445.447395742,3341320.7689066064,3244109.463973759,1252672.940745441,3297546.039416625,9448997.809879337,1643846.874567093,4460279.497643146,4664084.198047442,2053573.9234329169,14938211.69459496,2466411.2900512624,1187367.7829590426,10260564.64380291,1745284.1950958928,1934432.1039629844,2382201.739449093,1967027.9907374887,1439265.5498289827,2271387.271365124,8647341.465392208,919582.8620481072,12954173.071007747,1578162.66720049,6279582.106513115,1083529.172441361,1318151.8595175047,2580823.2625611527,1148073.3261667215,4999853.951887196,1561513.5959644802,31355942.335163776,4276895.41265405,2261121.16293223,1257035.4247242846,1591068.3116319883,2175686.2982996143,6535831.507235806,3301662.2315394236,1019948.9936434908,4855349.675159512,940407.5140545319,30346594.08723295,1497656.9297509172,6886237.762831047,19908910.88287492,1566823.3525374306,27192368.46106105,4376850.070359897,1082360.208648692,1878448.500171828,3410321.811423703,4390133.924413205,908048.7892974168,36474139.93736995,8080462.823856992,1502846.1576336478,5491993.019397169,1182268.4804637,1133851.6913812477,1935811.1440897104,2768234.36124749,20213947.74402782,3859274.2259393875,1407828.3021233312,2252658.875601978,16862910.118975595,896012.2982232217,1426169.5244455156,1410697.5419432935,6327655.004265233,9533482.441962484,2000084.8355387263,5809505.535142789,26322185.674810305,1723554.5955911754,976469.8051763377,2806948.5767262992,1771430.1894804265,6477093.688330838,1769621.0123566538,9196698.298736617,1445020.6029889262,1220737.4256422962,1023657.9483718066,2305834.1663630432,1574030.2621086687,1446345.4821443176,1107319.183541446,12510781.462602656,2177799.7005213653,5557133.425630619,3029682.301717163,5199293.13526975,4038941.056559108,9789363.822439127,2028219.661961724,3561327.352354346,7839812.081602688,2491769.3933665873,3160305.1726008793,1425620.7139161867,1383836.066884108,498080.35505686235,4950090.021844907,14724697.00041562,1778853.2324446605,10102701.281400697,1073736.4692708491,4599151.590574652,5564614.075967752,8069040.797478444,2272417.564162272,3302103.0684221215,5631370.520366238,3495661.7395894215,1470680.2893478367,1057563.0961171857,1753937.1371324444,2357382.8592065494,7773820.7464345815,2153121.6641446385,2175609.358162458,3480983.0206060465,3064716.639794259,5039852.534525734,2704587.0139658223,1480076.2049421233,1472673.7355344489,1585194.0979167288,1341001.311903718,5506985.406803664],"xaxis":"x","y":[2450488,1188444,1205617,579888.3125,3637973,2228675.5,5662234,783104.125,620104,21069722,3537977,2918553.25,5448975,770333,5907068,6388260,369291.5,1784385.25,873923712,1889990,1275236.125,646505.1875,1058986,533370.625,1384693.5,2196718,1295685.75,5702077,378714.4063,727374.6875,2989919.75,1066901,4155007.75,5253241,533329,1271021,1057256,9341427,2659770,1054995,3874632,13951571,1344512,6508595,1286817.375,398357,729535,5117308.5,1860901,827618,2017423,242805,1212883.75,1079477,4547129,8819864,499887,620483,4735249,478982,7046345.5,1191803,811703.6875,792734.8125,4420650.5,1847401,1073156.375,727703.5,729096.875,858897,4897760,1030150,1784796.5,870633.5,1614322,552015.625,6345009.5,2147013,4235405,2788860,1691963,4946798.5,7482832,4024810,1093653,630040,1582655.875,1835804,722062.875,994194.875,1964681.25,969682.8125,6695413,764906,3291518,13158205,3344833,44984468,12704014,524144.6875,25377650,6962255,872114.5,466672.6875,1381407,2919980.75,3481727.5,2992693,464155.1875,2372126.25,5965171,1811506.375,670779,15552193,5337334,896403,4039667,4784731,749207,8381834.5,25970248,900973,1436773.875,1200381,2225265.5,5348309,966812.125,9791557,777094,5471735,662424,939413.625,17924416,454296,452744,11441,4188660.5,1686262.25,1767480,342726.0938,1987334.75,666385,8611054,410433.0938,1501111.75,1210229,4268054,914670,1509492,1185469,10750010,572654.3125,9290214,4145920.5,1332591.5,549509,1134195.5,1682454.875,2364351,7475577.5,3418139,1035592.813,567608.625,6015128,7380088,102673696,2177075,875789,1972234,618876.1875,6010407,1189402,2916511,3757006,974305.625,505601,1325597.625,4189569.25,3229864.5,1508561.875,15878689,5528223,1605522,1747090.375,19490284,0,14585813,5696695,541950.875,5968153.5,1346952,612158.1875,1076084,731092,2804534,690529.5,2190460,1829122.375,7240274,947884,9162048,2470566,690557.6875,25959642,703847.375,207572,3155391,948975,666687.5,1247361,986353.125,53166156,1419980,1171380,888308,15590743,830945,1074350.375,3805877,2108012,659450,3693616,4696639,2433777,804620.5,5258024.5,2420605.5,13171370,761504.875,4054994.25,858932,3267564,776237.375,12095818,3556291,887403,6797528,2055534,1983256,566183,1989287,1045413,1169932.75,2964206,3782378.25,802312.5,10062436,1034941.313,12086616,1138520,14168729,2173143.75,1441631,1742351.25,1222949.25,2478960,1913558.375,473874.0938,3518459.5,2920918,19645206,36667044,98960776,13010578,2280352.5,1228041,4961405,3157579,809291.8125,2288635,6293359,5454482,7739699,2077039.375,3577120,1574750.875,708967,2458760.5,748335.3125,686949,1465795.75,1244150,2557533,1686405,2186148.5,1194835.5,3744483,4476997,8355988.5,6914839.5,1738724,90558.70313,3087318,965425.1875,913899,2292169,490743,601590,1117658.75,1082531,326778.6875,1821047,230365,809870.8125,387810,678608,1297872,684234.875,1114384.625,742630,2454812,10420096,694241,1966424,3663534.5,515025,592534,600060,2001643.75,1866800,2360288.5,6859198,955641,4358944.5,1034827.313,6596057.5,1784233,1886382,1100575,1632731,835085,738990,4385512,1227772,77164568,549834,6609358,1789710,1375366,5104947,1883864,2137686,2208196,3004271,866096.6875,1679127,498745,3717064,1446455,710825.1875,8259071,16644664,924438.125,542882,711118.375,219483,1041652.813,0,318205,905750.375,1024033,1696448,694479,5457975,5885427.5,13093350,1270139,10213132,1200811,1076644.75,581589,1967637,2912264,967697,4700396,7369591,489557.0938,1529510.25,8320719,1130960,807358,3837228,4988493.5,12719042,2996958,5238803,958725,1529309,1256575.875,11319836,5767765,6036588,683274,16371764,2126288,920068.875,942089,1497496.5,1358022,529807,2912140,1017787.125,685888.1875,5252131,1817550,7338511,17109052,961476.1875,688641.8125,2211671,3864699,647906.1875,1938887,4547379,1750419,648634.875,1739693.25,2962476,3893073.5,2338238,809257,719471.1875,934357,408513,3258819.5,1911591,2890253,468396.9063,1868497.125,1072487.875,2151376.5,8168547,2472350.25,1417778,2337651.25,5917000,5653656,3059758,2266983,2676594.75,1379739,7637986,1128179.125,1472548,3371455,3091559,7003615,4059424.75,1318096,2156407,1765813.625,455798.9063,641713,1027394.313,648879.3125,1947717,896485,2248776,3404992,2158629,1380293.625,7067404,32579658,1489363.875,1359579,3017709.25,10154608,3546459,892227,2439484.75,3040924.25,317581,32381836,1401315.875,1921222,1494727,1703504.875,2563985.5,1762174,4078339,6454983,684155,10403123,3240830,508014.5938,1022957.688,654625.5,20311228,1481698,1093757.75,18661606,293217,922041,8381744,2127534,2694853,362874,9586556,609494,1133028.875,502667.6875,1457118,2265836,962238,815800,1259456,999924.3125,10317852,4022735,877788,6578370,2477499,51168308,11788719,12783255,1677877,989079.5,1431784,928409,1521202,4685346,2148565,538740,1889761.125,3605749.75,1443367.75,4206839.5,578854.125,739662,3740373.75,1385697,608755.375,1634178.25,8696206,137635696,1364344,3590731,3605855.5,5357833,1176342.25,716940.625,47148456,6320060,1045640,1275231.75,783345.6875,1929011,2307938,22055140,759060.8125,924121,2361088,1493131.75,5261096,755618.1875,2735128,688375.1875,8493420,1858224.625,551219,562617,10303305,657385.8125,1645748.25,4760734,1612105.75,3366528,1245923.75,1093743,2468322.5,816510.875,7682562.5,1607972.25,2512443,2069880,1634334,1334125.125,625938,3158151,405062,40062888,287132.5,1192100,4934559,956481,7520554,818317,2003474,827211,1967129,2429021,1006989.688,2616226,454124.3125,872555,268269,1085439,1055624,3191228.25,53401,1012341,2140476,3265995,811900,713326.875,6935723,204499.0938,12525174,2189735,1585441,14361382,1750362.25,1824054.5,1137513.625,1150926,593824,555864,1761137.5,497991,2470642,4321141.5,2274873.5,12731388,1898509.75,1245753,4486477,990361.5,4560492,5707529.5,3643112,1198630,3151107.75,966376,1104074.625,895310.5,6750112,2225134,3775187.5,431390.4063,976537,5415438,306721,2188452,2234448,3928695.25,4983934,2023197,2028125.625,1593610.125,3436032,85357952,591249,505949.0938,833747,25476332,1186682,1307883,965786,876871,1888344.25,758875,8651835,2442220,3863772,3947209.25,2014500.375,4424263,6127771.5,522791.0938,2093011,422133,2646130,680395.875,35383012,6313678,681354.1875,841242,1507168,784584,2025533,7439009,2090543.75,37980712,3239754,730211.1875,599390,3895403,861972.875,162960,580688,930439.375,1453543,642052,1482997,819354.625,4294550,2024367.75,1525624,157606480,869699,5903033,8926736,3289053,1820292,2277316,1844825,94186,869835,2491238.25,4407932,637685,9155568,4912520,3999505.25,6983736,18848780,3303036,6361236.5,25397086,862541,752441,1551204,2056762,802755,136241424,13962749,3312125,2114860.25,3568237,1109924,3542542,46510960,17044842,10960107,2471623,3132453,597761,1107290.625,658297.5,9029489,1015172,2701872,493969.6875,1886007,3226884,1216543,1469556.25,789209.8125,2081072.625,6884914,1623657,393493,645927,5672838.5,2401890,624993,3485865.5,3005926,8024064.5,9373179,1872413,145573,1834570,714898,876237,2015117.25,958242.875,1976856,10077532,462465.5,694189,1056099,1418915.875,2485521,2756494,1271753.25,1176615,1096646.25,1943933.625,534039,849504,2322314,0,2466100,3629065.75,1188994.625,1822834.25,1169948.875,2122581.25,2400507.25,7175646,3996541.5,350264.5938,14709151,2067352.5,415586,1559199.25,832168,1207869.875,1041304,577033.875,1677881,796042.375,813641,1085183,871355.5,5169726,661383,828531,6102883,2227640,2890379,507603.3125,602191.1875,466904,2767381,847419.375,1618695,436848,4425832.5,439065.9063,28450436,72587024,781682.5,5876439,642133,2407904.5,6058494,611536,1926134.875,2556131,1639849.25,696522,1054581.375,872326.375,677536.3125,3751417,818836.125,1334876,1287159.25,2334260,16163194,1172785,2887392,474468.5938,1153690.125,1228963.75,447947,1332912.625,0,443365,8504904,929688,3573328.25,1293336,2075833,1059605.625,1507270,1419453,586827.5,1360340,13659926,1971236,1364214.75,609508,1790148.625,2245568.25,2855545,3944933.5,286685536,4767227,815115.5,1261634.5,1845150.625,1698074.625,61576184,1226435,716071.625,5249190,1128208.125,863703,2301043.75,1080203,2376794,11325513,941564.8125,41655852,387279,1734533,1540235,570982,1689631,8909924,700899,686019.1875,8580279,938908,4167253.75,4222682,6118300.5,7323351,3185005.25,7256842,53079164,9569715,3644982.25,238255,1614026.25,1655612,1570758.25,11594121,1929118,3021669,6814269,841841,1258619.125,803938,938096.1875,961749,2058532.875,2220053.25,1313366,1297633.75,65336980,541542.6875,3608955.75,1983509,3218824,363808.1875,770175.8125,592221,3791402,23364636,49102164,1625308.625,497528,4202335,9986032,709990.8125,1423463,567521,8748273,3373582,846389.3125,1000804.625,2998208.25,2153603.5,2660396,1289073,1247972,4253369.5,2105451,13586070,935580.3125,1082920.125,3321035.75,3288836,3560614.25,512166,51625548,2615973.5,1251906,555153.125,810636.875,48729588,548292.6875,24473814,785052.875,13300859,571309,3570161.75,522972.1875,2473302,2098850,6881405.5,2306723.5,2876857,5089903,10564729,5039523,1730886,1633399.375,1696978.75,1739286.75,395346.3125,349139,6040837,945020.1875,1764767.25,914023,1232822.875,1625695.75,2545405,5807879,1392015,6283226,2662054.25,4133811,1582794.5,1259959,964015,6424029,12448381,873711.125,13911761,1475475.5,10498344,1788148,618366,776991,1331122.125,5903142,634751,520405,11026945,5981430,2203026.5,2166711,1427974.625,2654755,1050982,2581741.75,1639136.25,1402048.5,3165109,729397.1875,937635,868859.8125,4409881.5,705262.125,3542708.5,2729167.75,2628864,813038,2773779,21781324,2161828,1199392,832986.875,602245,1074989,35685224,342493,94178648,1136676,741287.1875,882896.5,1505709,0,1803753.25,3220635.25,8201974,2035595,12265516,3306123,313528.0938,241478.5938,2465266,1270582.125,1982967,824516,533078,19344782,1172065,511180.6875,255971.2031,758008.875,1911611,2503948,636641,677279,7838600,499928,2921368,22591288,964923.8125,2278160,17186624,591622.8125,919746.3125,1717130.875,1815350,3024478.5,5975230,860364.1875,600543,3275779.25,7261732,8921778,1976400,5572503,6159684,2960377.25,13270687,7370743,1323038.75,291614432,1425599,689803,439266.9063,3020001.5,2168701.5,1234095.5,2219744,3045139,638201,1943772,855393,41213584,3674705,2712983.75,1201630.125,648022,556700.6875,816846,196410,9977330,7419765,25453690,1120197,2483320,1358226,1441665.375,33268410,5535626,6471427,1437569,2128963.5,1581425.75,1231546,5037781,1581036.75,5392617.5,4180298,4817062,687953.6875,3743228.5,578789,1519952.125,2355732,7969653,431357,5094197,65047284,10513817,2599171.75,842593,10105379,45074876,2391566.25,1004253,1827424.125,652991.6875,3534690.75,7657568,64715652,1073507,9671619,1540258.5,8163413,520206.6875,6744317,6577130,63668488,695346,2666634,676032.625,571644,1103594.75,16614404,2885740.5,9851937,1743198,572082,40613740,861007.8125,47102428,529033.5,41680640,1398253,987989,4590246.5,7976729,786346,9146727,1688612.5,801264,18628834,1108310.875,500237,17673018,978211,1921890,2926089.5,8839812,1634669.375,1115020,5121586,804002,16246106,3612526.75,10340282,881386.1875,920430.625,1183623,323739.4063,2993032.25,788055,19150024,1929828,2248134.5,669117,495152.6875,941257.3125,21795830,1513700.625,445943.8125,2843033.5,460010,16020642,812013,5838129,18967014,1038351.875,20657852,1808401.375,588492.5,1380283,2345752.5,14833476,1541879,19827916,4396566,899266.125,8354235,466985,1280263.75,1011296,1325972.75,11821841,5481549,537141,1037696,17750994,792015,697397.875,1736775,3616057,8070420,9159185,3730953.5,18576900,1356972.25,656980,1487769.5,706961,1916309,4038090,4027134.75,868163,3767135,517029,1208168.75,864765,783208.625,535649.875,14751117,1896858,1757601.25,1116705,1920352.125,2755578,4828599,873648,13440924,1730431,3441643.25,2178074.5,690750,573395,936616.5,2762846,6821833,897514,18074716,228212,2549892,22530168,19991412,3215953,3245111.5,6057734,3234787.75,1928391.25,416566.1875,741850,1045337,11963428,3035962.25,1855556,5040382,2096835.5,3002337,797170.3125,892455.875,1751483.875,8054334,686055,1381590.125],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN = np.logspace(-3, 3, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","SiteEnergyUse_predEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","print(ScoresEN)\n","figEN.show()\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[10584379.99564748,10583971.356977362,10583535.067522459,10583069.527560506,10582573.054985661,10582043.87784394,10581480.126422938,10580879.8252784,10580240.885688161,10579561.099107005,10578838.132241972,10578069.524362601,10577252.687393898,10576384.9092138,10575463.360398218,10574485.104446681,10573447.111311767,10572346.273879703,10571179.426941408,10569943.368178641,10568634.880773894,10567250.757422559,10565787.825747108,10564242.97533369,10562613.186803497,10560895.563396005,10559087.365559518,10557186.048909087,10555189.305731531,10553095.11001747,10550901.76566969,10548607.957373325,10546212.803245375,10543715.909432147,10541117.424064625,10538418.090157013,10535619.296327854,10532723.123682309,10529732.386209672,10526650.665142652,10523482.333926748,10520232.571383258,10516907.362941206,10513513.487344017,10510058.487770353,10506550.626092004,10502998.820036346,10499412.563001309,10495801.82727005,10492176.951984012,10488548.51804113,10484927.212888297,10481323.68901092,10477748.4203042,10474211.562144347,10470722.819615014,10467291.33059375,10463925.569155097,10460633.27464081,10457421.411981564,10454296.166990135,10451262.980513494,10448326.623234713,10445491.312218565,10442760.86854883,10440138.914276969,10437629.104308877,10435235.395892626,10432962.33285498,10430815.359301697,10428801.152586196,10426927.941810597,10425205.84813292,10423647.207405793,10422266.88183781,10421082.552908845,10420114.990518797,10419388.294119908,10418930.102443414,10418771.768613713,10418948.498952145,10419499.453453809,10420467.806267679,10421900.765719626,10423849.551935738,10426369.332008692,10429519.110563008,10433361.575094085,10437962.890798662,10443392.465820942,10449722.629851125,10457028.301902523,10465386.582576457,10474876.308872309,10485577.551216375,10497571.074411355,10510937.712649968,10525757.797784382,10542110.435157882,10560072.825133607,10579719.542435434,10601121.771125432,10624346.556789527,10649456.017489221,10676506.564117141,10705548.101611426,10736623.229457902,10769766.437793344,10805003.301249528,10842349.677646823,10881810.917444976,10923381.091268728,10967042.267337972,11012763.829309288,11060501.8894799,11110198.815131752,11161782.883822124,11215168.12730752,11270254.380104274,11326927.543881983,11385060.127570782,11444512.034349024,11505131.617200825,11566756.990550648,11629217.575884346,11692335.845695851,11755929.233461052,11819812.15037216,11883798.066826444,11947701.59212418,12011340.520591948,12074537.762980634,12137123.145126669,12198935.024443422,12259821.694392378,12319642.56156483,12378269.082108386,12435585.446520725,12491489.027828129,12545890.597439107,12598714.327750351,12649897.591611074,12699390.60882462,12747155.925940944,12793167.78624144,12837411.401653577,12879882.153465346,12920584.744627142,12959532.306927726,12996745.591908593,13032251.952857926,13066084.622237278,13098281.867148463,13128886.08403331,13157943.187096726,13185501.87776036,13211613.011026885,13236329.01743143,13259703.38074866,13281790.170590267,13302643.628200646,13322317.803119294,13340866.23790631,13358341.697801068,13374795.941981928,13390279.532997143,13404841.680924399,13418530.11887199,13431391.00654319,13443468.858733406,13454806.495806057,13465445.013388108,13475423.768731851,13484780.381399408,13493550.737964656,13501769.048995715,13509467.82861374,13516677.967649003,13523428.770643916,13529748.00359194,13535661.945909565,13541195.444822747,13546371.97137162,13551213.677350368,13555741.452600554,13559974.982166937,13563932.802905345,13567632.359203314,13571090.057536948,13574321.319642195,13577340.634126376,13580161.606387287,13582797.006742617,13585258.816702962,13587558.273347681,13589705.911784614,13591711.60569315,13593584.605965335,13595333.577472186,13596966.633992633]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[14979107.991214896,14979000.372259125,14978885.982938807,14978764.593447138,14978635.969142959,14978499.866255313,14978356.027279042,14978204.176449087,14978044.015765434,14977875.22209454,14977697.445879634,14977510.311944269,14977313.422762867,14977106.364407128,14976888.71517209,14976660.056667197,14976419.986960884,14976168.1352262,14975904.177278986,14975627.851444513,14975338.974326782,14975037.456261493,14974723.316464104,14974396.698087227,14974057.883544423,14973707.310480028,14973345.588723987,14972973.518421791,14972592.10935786,14972202.601365812,14971806.485214446,14971405.524103831,14971001.77477533,14970597.607701063,14970195.725950208,14969799.181700772,14969411.390485778,14969036.141326224,14968677.603157897,14968340.325745314,14968029.235822614,14967749.62659257,14967507.141041502,14967307.748555485,14967157.715220263,14967063.567549573,14967032.051195374,14967070.085142594,14967184.713150475,14967383.054343764,14967672.255267348,14968059.446131509,14968551.704321211,14969156.027775437,14969879.322815794,14970728.408034049,14971710.0382515,14972830.951046035,14974097.937295742,14975517.93850458,14977098.169256862,14978846.26696131,14980770.465805741,14982879.79338717,14985184.286465254,14987695.221830094,14990425.356561184,14993389.176652644,14996603.136433946,15000085.89812607,15003858.551741095,15007944.81851539,15012371.224538185,15017167.244027838,15022365.405399742,15028001.356252475,15034113.883983612,15040744.889528366,15047939.312923418,15055745.009394841,15064212.577240597,15073395.137544986,15083348.068068294,15094128.69401986,15105795.937766403,15118409.932616219,15132031.603194835,15146722.217486784,15162542.905502152,15179554.20660568,15197815.506516643,15217384.566375565,15238316.985846803,15260665.69836393,15284480.454538878,15309807.337873526,15336688.233916946,15365160.45349228,15395256.22896389,15427002.32705185,15460419.683097841,15495523.024466883,15532320.581346028,15570813.782686308,15610997.010241628,15652857.372903341,15696374.497315465,15741520.371064179,15788259.184451416,15836547.22779394,15886332.798119001,15937556.157835152,15990149.536022667,16044037.174377406,16099135.43996739,16155353.01732622,16212591.166064816,16270744.084963083,16329699.376176886,16389338.58612807,16449537.89905776,16510168.896495208,16571099.429727232,16632194.583066845,16693317.713616734,16754331.548017465,16815099.325320173,16875485.954192087,16935359.169678185,16994590.656971335,17053057.13427527,17110641.35048115,17167232.996801525,17222729.50742327,17277036.72971655,17330069.474268667,17381751.91095287,17432017.83077024,17480810.76757711,17528083.986050032,17573800.346140858,17617932.047666885,17660460.288565297,17701374.82211906,17740673.451900236,17778361.469395284,17814451.051101428,17848960.629236594,17881914.20307895,17913340.89266535,17943273.977596726,17971750.51639448,17998810.728353344,18024497.33678593,18048855.121711083,18071930.40597105,18093770.605708223,18114423.821641557,18133938.4711816,18152362.960700963,18169745.396693792,18186133.334101975,18201573.559745938,18216111.908560105,18229793.110183742,18242660.663383443,18254756.735771064,18266122.086318336,18276796.008246474,18286816.289975595,18296219.19194687,18305039.437273048,18313310.214324005,18321063.189509124,18328328.524259605,18335134.92045826,18341509.6328262,18347478.51714695,18353066.076189265,18358295.503167015,18363188.729597066,18367766.47453834,18372048.294628266,18376052.634417806,18379796.876582526,18383297.3916552,18386569.586986054,18389627.954689994,18392486.11838737,18395156.878585707,18397652.25658548,18399983.536824018,18402161.307597894,18404195.500126828,18406095.425941415,18407869.81259284,18409526.83769641,18411074.16133158,18412518.956830293,18413867.93999268]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[6189652.000080066,6188942.3416956,6188184.15210611,6187374.461673875,6186510.140828364,6185587.889432568,6184604.225566834,6183555.474107713,6182437.755610889,6181246.976119471,6179978.818604309,6178628.736780935,6177191.952024928,6175663.454020472,6174038.0056243455,6172310.152226165,6170474.23566265,6168524.412533207,6166454.6766038295,6164258.884912771,6161930.787221007,6159464.058583624,6156852.335030112,6154089.252580155,6151168.490062572,6148083.816311981,6144829.142395049,6141398.579396383,6137786.502105203,6133987.618669127,6129997.046124936,6125810.390642818,6121423.83171542,6116834.211163229,6112039.122179043,6107036.998613255,6101827.202169928,6096410.106038393,6090787.169261448,6084961.004539989,6078935.432030884,6072715.516173946,6066307.584840909,6059719.22613255,6052959.260320442,6046037.684634434,6038965.588877318,6031755.040860022,6024418.941389624,6016970.84962426,6009424.780814912,6001794.979645085,5994095.673700629,5986340.812832962,5978543.8014728995,5970717.231195978,5962872.622935998,5955020.18726416,5947168.611985878,5939324.885458548,5931494.164723407,5923679.694065679,5915882.780663686,5908102.831049961,5900337.450632404,5892582.606723843,5884832.85205657,5877081.6151326075,5869321.529276015,5861544.820477326,5853743.753431296,5845911.065105803,5838040.471727653,5830127.170783748,5822168.358275877,5814163.749565216,5806116.097053982,5798031.698711449,5789920.891963409,5781798.527832584,5773684.420663692,5765603.769362633,5757587.544467064,5749672.837419393,5741903.166105072,5734328.731401165,5727006.617931182,5720000.932701387,5713382.876095171,5707230.725036205,5701629.753185608,5696672.037429482,5692456.179306112,5689086.919380687,5686674.647893872,5685334.810949185,5685187.19138299,5686355.142076484,5688964.641351875,5693143.323215364,5699019.401773026,5706720.517783981,5716372.532233025,5728098.252292135,5742016.1179926535,5758238.830319511,5776871.961600339,5798012.50452251,5821747.418047641,5848152.127499707,5877289.03677095,5909206.024702302,5943934.998653279,5981490.484241171,6021868.33899241,6065044.612937282,6110974.601579433,6159592.169651955,6210809.384031661,6264516.501635896,6320582.356083804,6378855.1722028395,6439163.804674419,6501319.3980344515,6565117.4381519575,6630340.143374238,6696759.14160193,6764138.346552234,6832236.963974702,6900812.527277025,6969623.906908628,7038434.175480119,7107013.29345181,7175140.541463572,7242606.6590682035,7309215.64886099,7374786.253263902,7439153.062271209,7502167.288079144,7563697.208828183,7623628.309359846,7681863.135555262,7738320.929083944,7792937.029762827,7845662.120582644,7896461.333911871,7945313.2558292635,7992208.860017689,8037150.4107765,8080150.291151835,8121229.928119128,8160418.728080075,8197753.005943582,8233274.831280689,8267031.252482371,8299073.34954967,8329455.416345548,8358234.213221302,8385468.290315719,8411217.38047957,8435541.8597075,8458502.272136612,8480158.916066684,8500571.487042032,8519798.773780115,8537898.402610842,8554926.626077734,8570938.151425645,8585986.004839906,8600121.427491218,8613393.79966524,8625850.58950317,8637537.323139697,8648497.573289694,8658772.95166971,8668403.177533168,8677426.024401275,8685877.418151058,8693791.465098567,8701200.504016865,8708135.162222065,8714624.415107153,8720695.648114976,8726374.72028293,8731686.028618583,8736652.572678674,8741296.018824637,8745636.763716634,8749693.996686526,8753485.760698684,8757029.011667274,8760339.675950555,8763432.70588734,8766322.133279096,8769021.12075395,8771542.010976385,8773896.37368989,8776095.05059909,8778148.198114078,8780065.327992585]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[8778246.225170681,8778354.759612821,8778471.744255424,8778597.866565866,8778733.874559073,8778880.582878755,8779038.879541596,8779209.733395813,8779394.202341985,8779593.44235931,8779808.717375401,8780041.410013104,8780293.03324463,8780565.242982399,8780859.851637973,8781178.842685506,8781524.386272993,8781898.855931899,8782304.846440466,8782745.192894422,8783222.991027845,8783741.618803058,8784304.7592524,8784916.424481707,8785580.980751613,8786303.174339332,8787088.157963175,8787941.517358663,8788869.297556046,8789878.028317701,8790974.748109693,8792167.025895236,8793462.979949282,8794871.292805333,8796401.221360913,8798062.601109711,8799865.843385467,8801821.924528664,8803942.365869107,8806239.203496119,8808724.946910314,8811412.525836928,8814315.224732514,8817446.604846869,8820820.41410394,8824450.485526389,8828350.625457626,8832534.493397897,8837015.4758558,8841806.55719089,8846920.190968303,8852368.175785074,8858161.539907783,8864310.439238904,8870824.07302834,8877710.622108964,8884977.213044992,8892629.911946485,8900673.750410443,8909112.785410808,8917950.192757515,8927188.394618807,8936829.218191952,8946874.082907608,8957324.21172217,8968180.86169928,8979445.56184713,8991120.388122419,9003208.180326965,9015712.81181646,9028639.419172302,9041994.620993936,9055786.715320224,9070025.852174738,9084724.178262474,9099895.952262912,9115557.629559748,9131727.916595384,9148427.795623781,9165680.52087921,9183511.58840912,9201948.681068102,9221021.591068782,9240762.12195023,9261203.971744979,9282382.59881365,9304335.07126478,9327099.900827676,9350716.860962747,9375226.789395833,9400671.374405747,9427092.923639843,9454534.116035486,9483037.73356583,9512646.37428925,9543402.14510963,9575346.30952421,9608519.027070267,9642958.848952008,9678702.409968304,9715783.987596165,9754235.070789892,9794083.88498509,9835354.91242926,9878068.384717008,9922239.749012465,9967879.11762044,10014990.703241494,10063572.23718479,10113614.386265328,10165100.176062612,10218004.429026205,10272293.237438265,10327923.494142681,10384842.483336499,10442987.58096575,10502286.059502695,10562655.038703617,10624001.595404249,10686223.042345414,10749207.397870367,10812834.045370437,10876974.582890106,10941493.851916019,11006251.133385068,11071101.48295095,11135897.183456402,11200489.270755278,11264729.106398966,11328469.946196312,11391568.487317754,11453886.327146322,11515291.326139057,11575658.833931113,11634872.758755473,11692826.456815533,11749423.447661424,11804577.926426193,11858215.094733635,11910271.307728885,11960694.051457563,12009441.75687085,12056483.490475044,12101798.508531582,12145375.721036533,12187213.07435836,12227316.874325212,12265701.068417046,12302386.504917316,12337400.180297866,12370774.4991221,12402546.502482006,12432757.342129704,12461451.327229794,12488675.505297674,12514479.018853104,12538912.567045715,12562027.914863268,12583877.450184107,12604513.788028892,12623989.420635862,12642356.411414033,12659666.130403351,12675969.02857249,12691314.448095925,12705750.465654764,12719323.765784709,12732079.541334264,12744061.418184081,12755311.401501803,12765869.84095652,12775775.412484124,12785065.114372075,12793774.275614323,12801936.567067752,12809584.061055977,12816747.21485877,12823454.947667746,12829734.67592011,12835612.36203934,12841112.565835925,12846258.497764932,12851072.073347207,12855573.968160765,12859783.672898378,12863719.54806734,12867398.877978396,12870837.923733646,12874051.97497837,12877055.400230028,12879861.695639614,12882483.532076927,12884932.800462477,12887220.65529549,12889357.55635014,12891353.308531176,12893217.09989618,12894957.537864706,12896582.683645599,12898100.084922211],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[17080198.635836933,17080646.05444632,17081125.08076552,17081638.067077264,17082187.515529506,17082776.07899296,17083406.56127585,17084081.91709709,17084805.252323173,17085579.825039882,17086409.04803826,17087296.493230514,17088245.898367714,17089261.176212363,17090346.426043205,17091505.947070245,17092744.253057666,17094066.087239154,17095476.436503477,17096980.543853294,17098583.918295447,17100292.34157772,17102111.871493164,17104048.84176537,17106109.858747542,17108301.795278225,17110631.782029428,17113107.19658338,17115735.65032715,17118524.973109774,17121483.195504714,17124618.52848112,17127939.340317935,17131454.13063571,17135171.501564004,17139100.126236737,17143248.714722913,17147625.977895256,17152240.589720044,17157101.148624867,17162216.138772234,17167593.892173685,17173242.552766375,17179170.043576736,17185384.038845178,17191891.9412915,17198700.867666356,17205817.642885745,17213248.804707903,17221000.620281786,17229079.11576212,17237490.120099794,17246239.323863067,17255332.352590483,17264774.856942073,17274572.616385773,17284731.657679178,17295258.385787245,17306159.725748777,17317443.27412761,17329117.45445682,17341191.67778623,17353676.50198656,17366583.787646383,17379926.846720308,17393720.580506936,17407981.603557058,17422728.35012397,17437981.16024456,17453762.342730585,17470096.212631952,17487009.101414494,17504529.33784144,17522687.198621042,17541514.82776816,17561046.124131646,17581316.596811503,17602363.188453764,17624224.06685954,17646938.38508005,17670546.011437997,17695087.230032437,17720602.41362738,17747131.67068758,17774714.469026156,17803389.238862526,17833192.95932211,17864160.731867805,17896325.32790259,17929716.826992605,17964362.073493846,18000284.337762456,18037502.882869855,18076032.60727947,18115883.69599487,18157061.326772984,18199565.42254095,18243390.45848505,18288525.328085043,18334953.270592947,18382651.86041644,18431593.05436326,18481743.30372947,18533063.702154513,18585510.185234986,18639033.763341386,18693580.76973693,18749093.123171486,18805508.59548786,18862761.06183412,18920780.74831529,18979494.448143963,19038825.72380004,19098695.093288306,19159020.188848365,19219715.9247093,19280694.65467621,19341866.3512834,19403138.818639666,19464417.905571744,19525607.840704214,19586611.555810224,19647331.109619994,19707668.16722487,19767524.547471534,19826802.82563323,19885406.990732364,19943243.132142466,20000220.152210865,20056250.47482608,20111250.751772188,20165142.520238973,20217852.81801897,20269314.731103532,20319467.85332901,20368258.672930386,20415640.84242982,20461575.358227078,20506030.636073597,20548982.487557784,20590414.003852833,20630315.348260947,20668683.48292267,20705521.817380145,20740839.80879533,20774652.51743482,20806980.13036657,20837847.464382436,20867283.369153295,20895320.601851594,20921994.70300606,20947343.96216557,20971408.824087154,20994231.445400234,21015855.279278822,21036324.690628484,21055684.60336382,21073980.180532802,21091256.537340943,21107558.48654029,21122930.315170836,21137415.591267604,21151056.99886636,21163896.199443247,21175973.71779887,21187328.85033356,21197999.593648054,21208022.591432452,21217433.097667832,21226264.954250973,21234550.581256647,21242320.97816844,21249605.73453247,21256433.048615437,21262829.75277528,21268821.34090014,21274432.018519923,21279684.71838761,21284601.158860512,21289201.88315594,21293506.304479383,21297532.75199177,21301298.517146584,21304819.899997402,21308112.25513919,21311190.03700265,21314066.84427048,21316755.46322843,21319267.90990234,21321615.470866006,21323808.742633812,21325857.669577178,21327771.580325384,21329559.222629637,21331228.79668475,21332787.98691557,21334243.992246322,21335603.554879386,21336872.987617508,21338058.199768774],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[14343009.730154099,14342147.938225877,14341226.9650991,14340243.055695256,14339192.232551675,14338070.281514736,14336872.737162301,14335594.86839236,14334231.664696842,14332777.823693424,14331227.740497082,14329575.499475613,14327814.868840508,14325939.298387812,14323941.920538189,14321815.554652633,14319552.714457298,14317145.618320553,14314586.202114632,14311866.134469477,14308976.83437744,14305909.491306778,14302655.088186033,14299204.427787462,14295548.163125958,14291676.832483264,14287580.899564587,14283250.799118752,14278676.988132875,14273850.002632568,14268760.518882286,14263399.420350567,14257757.867444942,14251827.370375877,14245599.86385541,14239067.781054871,14232224.128041852,14225062.5531973,14217577.412347687,14209763.826373555,14201617.729247874,14193135.90426151,14184316.006291665,14175156.568155626,14165656.989437351,14155817.506614724,14145639.143935334,14135123.645206451,14124273.387523388,14113091.278903216,14101580.642798554,14089745.093477612,14077588.407346401,14065114.395760227,14052326.786525358,14039229.120622959,14025824.67164345,14012116.396238795,13998106.920396777,13983798.570164429,13969193.450380132,13954293.575720267,13939101.056092981,13923618.336797096,13907848.491582172,13891795.565177282,13875464.959986685,13858863.858922973,13842001.677008424,13824890.53022811,13807545.711728452,13789986.163657503,13772234.932918277,13754319.599719327,13736272.668132091,13718131.908573857,13699940.643775862,13681747.970714387,13663608.913186135,13645584.499576768,13627741.765142318,13610153.675612029,13592898.973448617,13576061.94880787,13559732.134529581,13544003.933034362,13528976.173641324,13514751.60735053,13501436.341711385,13489139.219884308,13477971.14683716,13468044.372416954,13459471.718990462,13452365.786532348,13446838.107707987,13442998.31728602,13440953.120382138,13440805.602444796,13442654.17423217,13446591.685814481,13452704.582600035,13461071.93894995,13471764.693201957,13484844.796297833,13500364.475800449,13518365.53615942,13538878.686724115,13561923.014131991,13587505.424968874,13615620.265561676,13646248.919264868,13679359.5605546,13714906.972882345,13752832.432370948,13793063.733716866,13835515.315868866,13880088.48963822,13926671.828028126,13975141.694346847,14025362.906438157,14077189.583848728,14130466.151827427,14185028.48944081,14240705.246729705,14297319.290365953,14354689.25877413,14412631.212014144,14470960.335875751,14529492.669119703,14588046.815390779,14646445.609229818,14704517.691307552,14762098.969597785,14819033.935134226,14875176.800764022,14930392.472002473,14984557.300509103,15037559.647659073,15089300.244865047,15139692.360088618,15188661.783367036,15236146.636617495,15282097.052022478,15326474.701142019,15369252.226516768,15410412.58356978,15449948.316142794,15487860.785501052,15524159.372364879,15558860.661131011,15591987.640377715,15623568.861904584,15653637.861814905,15682232.12527522,15709392.61880884,15735163.082026374,15759589.430892257,15782719.211281572,15804601.103358649,15825284.476313584,15844818.992183883,15863254.25685129,15880639.515822459,15897023.39205768,15912453.662886309,15926977.07292248,15940639.179853078,15953484.229995945,15965555.060605519,15976893.026023233,15987537.944919616,15997528.066045275,16006900.050090924,16015688.965445638,16023928.28573199,16031649.950214578,16038884.32176948,16045660.26842254,16052005.188843902,16057945.055383205,16063504.460522711,16068706.665858407,16073573.652837612,16078126.174589029,16082383.808277898,16086365.007505517,16090087.154349562,16093566.610709924,16096818.768685194,16099858.099757548,16102698.20261012,16105351.84944086,16107831.030671403,16110146.997979324,16112310.305607198,16114330.849923596,16116217.907228988,16117980.169814708,16119625.780295426,16121162.364245856],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[5555211.990198807,5554465.130088086,5553667.026905339,5552814.581919902,5551904.533244517,5550933.44347217,5549897.686216642,5548793.431960919,5547616.63380003,5546363.013825219,5545028.0510345865,5543606.971734142,5542094.74339641,5540486.072859379,5538775.4095681505,5536956.954302583,5535024.673524122,5532972.319161006,5530793.4533827165,5528481.477751159,5526029.666109898,5523431.200700291,5520679.211254577,5517766.817157798,5514687.173120229,5511433.519080057,5507999.23519912,5504377.90279094,5500563.371835715,5496549.835430276,5492331.911147192,5487904.7296328405,5483264.025687608,5478406.239064535,5473328.61540598,5468029.308187574,5462507.481325115,5456763.409583116,5450798.5676274225,5444615.721999269,5438218.999896379,5431613.948311989,5424807.573214175,5417808.356270713,5410626.24565719,5403272.61774054,5395760.206890594,5388103.001315296,5380316.103631444,5372415.555878341,5364418.129863348,5356341.084967985,5348201.896922562,5340017.962287979,5331806.285412282,5323583.1537866425,5315363.811843112,5307162.1409977665,5298990.355839489,5290858.724742664,5282775.326217681,5274745.846139788,5266773.424900848,5258858.55927627,5250999.063047693,5243190.088321575,5235424.207988524,5227691.5578319505,5219980.035183225,5212275.538476702,5204562.310358259,5196823.209950365,5189040.130089634,5181194.388447733,5173267.151633122,5165239.8761201035,5157094.761161775,5148815.209698521,5140386.294060537,5131795.224020055,5123031.815374429,5114088.957757251,5104963.08063324,5095654.616501443,5086168.460330693,5076514.423819267,5066707.682728975,5056769.215054008,5046726.226989731,5036612.5632997835,5026469.097879077,5016344.099608723,5006293.568575185,4996381.536195471,4986680.322632516,4977270.744759298,4968242.264548673,4959693.069337227,4951730.071032668,4944468.81037241,4938033.249269797,4932555.4315448105,4928174.986300922,4925038.464259971,4923298.4428909365,4923112.418307252,4924641.438319493,4928048.433814894,4933496.281969775,4941145.551512563,4951151.977829477,4963663.671561795,4978818.127384641,4996739.089152231,5017533.370781156,5041287.729355458,5068065.919314111,5097906.042515718,5130818.32546963,5166783.435994803,5205751.430420555,5247641.405474055,5292341.90103074,5339712.041050876,5389583.402130192,5441762.532177426,5496034.045781682,5552164.1594707165,5609904.562429131,5668996.465300882,5729174.730322054,5790171.9058693815,5851722.096092583,5913564.55043986,5975446.902883307,6037127.99567894,6098380.280514371,6158991.741310534,6218767.373042341,6277530.217300413,6335121.986462449,6391403.299102948,6446253.602850961,6499570.780262747,6551270.523899239,6601285.509130544,6649564.4117070865,6696070.811017304,6740782.016233774,6783687.849120055,6824789.407511248,6864097.782036199,6901633.131525697,6937423.085018561,6971501.976064327,7003909.722954951,7034690.873083501,7063893.723011216,7091569.515748706,7117771.71502668,7142555.354942397,7165976.462285806,7188091.548038054,7208957.163958192,7228629.519796376,7247164.1564605255,7264615.670387943,7281037.484406535,7296481.6604880355,7310998.749977249,7324637.677108917,7337445.651882827,7349468.108645547,7360748.667014323,7371329.100522396,7381249.382069857,7390547.616861647,7399260.120014214,7407421.433665658,7415064.366879173,7422220.0418778155,7428917.945235962,7435185.982830615,7441050.537516751,7446536.528635961,7451667.472597576,7456465.543887333,7460951.635961377,7465145.421574132,7469065.41216826,7472729.01602474,7476152.59493173,7479351.519183557,7482340.220766529,7485132.244627162,7487740.297951599,7490176.297413154,7492451.414368538,7494576.118003144,7496560.216442093],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[7165233.396876883,7164242.902513704,7163184.520586917,7162054.066544233,7160847.119043532,7159559.002361079,7158184.767918293,7156719.175545814,7155156.675278782,7153491.390617186,7151717.104264534,7149827.247359631,7147814.893120228,7145672.75562704,7143393.194203572,7140968.223522433,7138389.529246757,7135648.488745897,7132736.196265752,7129643.49192486,7126360.994058845,7122879.134724947,7119188.19854937,7115278.365476113,7111139.75827215,7106762.495799148,7102136.753041279,7097252.828693695,7092101.220805878,7086672.710597022,7080958.45470456,7074950.082506859,7068639.802827113,7062020.514279275,7055085.918136827,7047830.634196172,7040250.31416392,7032341.753207205,7024102.995484101,7015533.42521944,7006633.854806948,6997406.586332182,6987855.4577012975,6977985.863870141,6967804.750808102,6957320.579286864,6946543.256231815,6935484.032201148,6924155.36463171,6912570.747665829,6900744.510813326,6888691.590111019,6876427.277014787,6863966.951643408,6851325.808813685,6838518.585170726,6825559.298758011,6812461.010805192,6799235.620808566,6785893.705462315,6772444.411138526,6758895.408302374,6745252.915001228,6731521.794465466,6717705.729671796,6703807.4756797785,6689829.188164983,6675772.82446182,6661640.6115117185,6647435.573256642,6633162.109040022,6618826.613036684,6604438.1244950155,6590008.998066118,6575555.583393204,6561098.903455707,6546665.321285097,6532287.185137483,6518003.442487076,6503860.213512476,6489911.3143968675,6476218.722799224,6462852.972560377,6449893.470651011,6437428.724047278,6425556.465513656,6414383.665857849,6404026.420370411,6394609.696426864,6386266.929532186,6379139.456639797,6373375.776084644,6369130.626411313,6366563.880788434,6365839.255457261,6367122.838128845,6370581.446253862,6376380.831584572,6384683.753487517,6395647.948919891,6409424.032294725,6426153.359979255,6445965.915730191,6468978.212304528,6495291.331942327,6524989.041236607,6558136.134888536,6594776.914606856,6634933.966636341,6678607.12306043,6725772.76575263,6776383.347057076,6830367.275184564,6887629.037592274,6948049.670716614,7011487.524759392,7077779.295979379,7146741.376006738,7218171.466660974,7291850.429059797,7367544.385010046,7445007.013262982,7523982.003022469,7604205.645831781,7685409.506068987,7767323.12894352,7849676.7353206705,7932203.853616593,8014643.843973559,8096744.25890685,8178263.024317926,8258970.370340946,8338650.515784952,8417103.071608381,8494144.156230072,8569607.21039682,8643343.539427215,8715222.558980752,8785131.790426023,8852976.61451983,8918679.813611872,8982180.91720313,9043435.415851954,9102413.822388234,9159100.650959337,9213493.323774373,9265601.03478506,9315443.593817873,9363050.27196936,9408458.667142432,9451713.514272507,9492866.002598032,9531972.176184857,9569092.437242746,9604290.556033967,9637632.87433889,9669187.580749134,9699024.057468295,9727212.297110898,9753822.387041882,9778924.058070257,9802586.293777734,9824876.996401334,9845862.704973735,9865608.361332163,9884177.119614394,9901630.194948213,9918026.747190755,9933423.795770487,9947876.161913788,9961436.434788588,9974154.95835987,9986079.836018227,9997256.950307313,10007729.98372586,10017540.510738024,10026727.971058885,10035329.783752916,10043381.395929404,10050916.350502046,10057966.356831998,10064561.363262665,10070729.63069609,10076497.806487901,10081890.99805134,10086932.845661609,10091645.594040962,10096050.162383184,10100166.212544708,10104012.215189135,10107605.513723595,10110962.385909751,10114098.103070265,10117026.986843828,10119762.463469159,10122317.115601119,10124702.731681107,10126930.352899335,10129010.317799253,10130952.30458424],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=0.2<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN = visuRMSEGrid(ElasticNet(), 'EN', alphasEN, 'alpha', GridEN,\n","                             BestParametresEN, 'elasticnet__l1_ratio')\n","FigRMSEGRidEN.show()\n","if write_data is True:\n","    FigRMSEGRidEN.write_image('./Figures/ConsoGraphRMSEEN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                     25\n","                             R²          RMSE           MAE\n","KNeighborsRegressor()  0.272898  2.465043e+07  3.653788e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predkNN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[2098634.32,3024931.72,981678.04,1217615.629384,4227126.075,2483038.98002,4294200.36,544817.23876,1273281.097544,6806537.28,1440039,2162086.5375,1936040.9581239999,997267.611884,11075431.94,23518065.6,2512243.8075,1041033.8775200001,85896123.22,2679833.04,2034577.605,952038.146272,823417.12,1017737.3787720001,2911058.745,2267126.08,1696321.550004,4783686.14,2738116.3687520004,889727.063772,1864542.7625,2163163.24,3876911.945,8659256.84,912823.32,3222947.6,1573327.92,16789050.94,1522481.76,941436.8,2210603.6,6014393.16,884538.8,15658786,1421831.486272,1838757.52,1061271.52,1631080.986252,690841.72,966322.4,2693052.48,2767682.52,1141446.3385156,2166566.4,3994070.72,8725386.49,1159866.68,4821763.28,1856670.92,548564.76,8773382.02,2154927.28,2217567.08002,1493767.248772,1228304.098128,3014567.905624,1979994.353128,970002.795,1046783.39,1362998.72,6675299.495,3037580.4,2262096.0725,938390.395,2458756.48,894940.0825240001,4369080.148128,2992565.76,2261919.88,708598.04,1522481.76,4247702.145,1832835.08,1951877.56,3421132.72,1339506.04,4276001.3825,1243575.130632,890220.2675,982369.3425,3459610.815,968041.19,6105910.28,2613873.84,7239237.36,17602167.16,3853811.7075199997,55120911.28,3246245.72,897326.875,11226530.83,13064663.3,1937569.7025,1143762.5787560001,1964130.16,3156174.6675199997,3242141.57,5590614.2,1082630.577524,2603430.81,7924830.18,3706523.5206239996,1600445.68,21569983.36,8886825.455,1254131.2,6257013.455,7178455.08,661654.48,7201573.98,28718893.32,1059206.7959376,4101819.48,1412930.16,847073.628752,2507960.04,1407539.22,8052476.2,884538.8,4401053.28,574403.72,537863.7512559999,7603033.16,701770.32,767577.84,3945524,2806761.39,1034213.6759376,2304084.84,1857540.19,1916681.277504,921054.16,2703262.16,748503.6981312,1993357.0825,1629524.76,7335107.56,975810.2,921659.08,686769.56,4181577.635,544817.23876,2311560.300624,3122143.515,1678425.4675,985905.8,1535360.82,1576363.5775,3570669.92,4352791.338128,4251833.88,1541974.6759376002,998590.11,2413902.04,7452828.12,58075802.56,3451025.345,930702.52,1471317.2,1393704.7275,4369412.56,2848921.04,1924105.04,2302428.4925,1759147.0679688,931897.4925040001,2347464.1425,1998469.15,3712681.485,2675693.746252,19772975.7,2545295.8,2460205.88,3809721.94,7733595.56,2585576.56,4139631.915,11304914,899012.258752,2059290.9106239998,2095643.04,985667.2825,1644335.24,1215546.4,1271357.71002,2409551.468124,2174337.36252,915953.871252,2337759.505,1276252.72,15470509.3,1022581.8787560001,2401624.7275,17216525.92,845407.871252,1321443.76,5048962.76,1162616.88,997267.611884,1509567.6,1502171.49,9446450.9,2060187.64,813790.68,3846650.92,18144489.64,1103289.12,880029.8987560001,5061557.2,1129902.52,883437.6,2911438.92,4256775.2,1229899,1718047.3459376001,11363494.98,1953935.9475,10507663.71,1324984.60252,2344064.7925,1329800.92,4410111.831876,974138.0625,4170631.1325,5229266.72,1147595.96,6312693.7,837256.72,2203875.28,782008.9600040001,1197871.88,3893074.16,1715142.9325,2847292.2,3777791.855,1074451.8425240002,26056559.66,2401856.11002,7116036.715,1153146.52,30021927.94,2117675.0359416003,957517.12,2489509.145,1419347.875,1382406.74254,936571.0925,1074451.842524,2355521.595,2214792.56,15057057.12,26130890.88,15902501.84,3919547.68,979003.703752,1020386.12,3079331.6,6818086.84,1081100.833132,1492117.32752,1351268.6,7065361.16,9908524.16,2001957.065,1043527.4,2135327.2031239998,687512.48,2386774.07,1056041.195,915543.92,925690.9925,921285.0381280001,3697727.04,785089.3243792,3514261.5625,2012670.41,3513547.44,2471653.12,11347819.99,12908197.08,4545803.04,1176232.705,894824.6,1850412.5606479999,1291007.72,1815150.72,747765.64,959520.92,951581.4525,3508758.88,1189262.00938,2460205.88,2229006.68,935799.113752,1408693.965,1035082.48,1120031.96,1219375.52,829543.9018792,1172570.84,2120895.56,2949654.08,912823.32,2875038.64,3580735.02,551207.76,548564.76,1330278.12,2389776.64,2705935.8,2510013.8125,4297554.35,1059855.16,3495146.495,2003322.4375,3300125.64,2836668.2,7809278.56,2527088.68,676987.96,1247224.08,1285112.52,3210113.56,961429,83920179.92,722951.6,19708983.66,3410860.08,2855914,5603910.96,1915262.52,3755950.72,2057349,4544613.46502,824180.193752,3920466.88,2163900.84,1794538.566252,687085.76,966244.625,7684851.12,88955821.52,815051.735004,850333.48,940451.163756,2154927.28,2813172.415,2911058.745,1242203.16,1335926.24752,1501009.52,1364603.56,1146968.84,3751867.92,6909200.855,18563306.38,1187823.56,10980014.44,1153146.52,1069374.0759375999,954439.92,2312346.96,2185497.5159416003,811913.44,2043762.8,2981307.04,901540.390004,3015245.335624,14054786.26,1666538.64,2090198.36,2509343.04,5625682.77,19107842.16,2478494.16,2737604.6796896006,1118026.6,2324561.04,1457556.16004,8451884.095,1436671.8,2387879.84,1033568.1350080002,5233565.36,2797758.32,1564879.21,1036491.92,1951956.6729688002,6950360.52,1018616.44,4574384.84,2558142.93502,2049790.86002,2250580.535,2557077.8,3459531.72,7478492.56,1654312.855,985196.9610156,3836820.44,2699704.52,799180.5975,2402001.04,3245662.96,3050902.8275,876200.175,1382406.002544,5227599.32,4310184.47,2568944.38,1178465.24,1609298.082504,1137591.88,3396462.32,4106390.305,2082613.12,5232672.24,1661075.7962560002,2141435.5475,1848230.175,1407124.8675,9076995.4,3850010.815,5339590.36,2776938.285,7398283.32,15768508.18,2080070.285,992833.191252,2010236.0275,876358.08,10183787.4,1968678.947504,3753947.68,2808637.44,2581087.76,1342588.24,5982172.75,3521338.08,1333344.04,3014907.213124,609794.306256,971943.8,1774632.976252,1109044.487544,1749565.68,1023677.52,941102.4,6481557.24,2348357.25,1750520.6775,2460205.88,3295634.16,2381198.2834416004,858504.56,7715644.645,5993863.225,1502109,1667253.68,3556040.355,2164667.62,3034729.56,42685045.64,955175.6475,3823015.48,1874330.255,3605480.86,2367433.998752,1481212.8,4887135.36,2281522.48,717277.48,1622097.48,5442956.36,660325.892504,1564879.21,801390.4162720001,13138675.44,1644335.24,1266354.93,13248712.76,2508903.6,4691469.28,3207374.56,5178893.315,1799311.76,3706716.415,4735845.64,789329.36,1631615.29002,1437392.99,2247578.28,2964345.1,2238442.8,1071419,928035.92,984734.2112520001,13275526.2,5479130.68,1127994.08,7278923.44,2563382.76,7546850.22,10362297.32,5392223.56,1422080.2,1719651.840004,866745.462504,1228684.44,1011707.52,5465456.12,4372531.93,1024437.72,3874430.61,5846030.77,2431230.355,4422493.47,898924.446252,1938238.64,2180949.127504,1607535.56502,786999.5662560001,2454779.54,14171801.12,59901623.6,1465504.56,7226742.27,3022202.945624,11829903.35,2689733.3175,1227897.73,38172285.2,8357075.38,1223539.56,2268340.751252,1731493.9975,1162616.88,2373729.96,10808253.84,910779.535,813461.32,2985734.36,1089894.2100240001,8424772.56,1427524.915,4151153.16,900493.6460156,15890316.42,2207939.076252,552314.68,894824.12,1633430.2,1081100.833132,3073035.14,5420387.94,1472673.602508,3534190.32,1057623.560024,1170279.2,3504252.43,921776.3937520001,10985068.32,1924562.757504,1983468.48,1911011.96,2752517.16,939739.0275240002,912823.32,2167956.12,3111556.9375,36912564.14,2399989.655628,1453891.2,8574114.56,1162530.76,33155097.94,894824.6,3720131.2,662260.52,1178552.6,1494362.5475,2917707.87,3405910.68,1017737.3787720001,913809.88,1343892.64,720432.04,684464.64,1986326.455,1716455.16,4208055.68,3537122.96,4110022.64,1141354.48,1257545.33,8453131.44,824176.042504,3652120.653124,2818738.315,1343230.92,17349058.82,2583228.27,2802034.49,2506750.50502,1741091.28,853910.8,943214.96,1029919.979384,1007564.08,1693576.5156239998,8175304.67,2875804.005624,15949358.64,2659229.5725,1117623.24,3481799.155,1636224.978124,2823488.945,8611215.91,2877306.72,918072.16,3974702.815,2927019.64,1030318.332504,608372.668756,7937661.9,1577768.76,8484513.38,864930.5712560001,1792382.08,5739064.84,4962259.48,3087406.04,2082653.92,3731353.04,4425756.12,1922474.32,1169245.6735156,2050507.1981239999,2019520.84,34967495.84,2634296.32,544817.23876,1018366.0662720001,18057134.36,1970861.84,1925498.2,959686.96,1129083.4,6054595.09,974160.28,15654931.68,2623852.073752,1542080.16,2266995.0243760003,2344064.7925,7569826.36,7834641.11,893816.0260156,4279002.32,2014569.28,3248092.64,904265.0725,19487288.56,8076789.12,1065112.655,2010264.08,1602815.52,907509.32,2013499.485,2545704.718148,1676893.56752,37958992.72,2882703.4675,1168408.4244516,4589954.08,3218689.92,3908876.776876,2575621.8537520003,1004849.24,1511972.830024,1364603.56,1230210.6,2137734.44,4218383.040624,1932300.12,2054736.22,2674651.52,25890114.24,676299.48,17562186.98,975371.7025,2691025.52,1616798.32,2592902,1058831.04,1852915.24,1351268.6,2887679.015,6558549.36,923731.29438,10046575.89,973601.84,2630297.5425,2205784.1875,43814141.32,3897403.6,4190713.32,55730873.2,1729032.6,2331423.56,3296575.04,2186477.8,1383200.84,18479351.06,4440334.598128,5395467.76,1116433.52252,7113237.82,927649.6,2194405.36,69877249.04,13375111.7,1584573.1879688,2768869.96,2802093.52,704071.64,1718521.6925,802468.553752,2966391.015,932127.68,891252.94,764492.063752,3979403.28,2575228.92,1026192.04,998637.63,878962.5175,5622554.11,11262267.8,1352837.564396,1214309.6,998319.48,5784307.155,2457173.8,901124.56,3750382.075,1330035.08,13325583.91,7347738.2175,2409040.13,1242203.16,1043527.4,886401.08,642769.32,3122725.4375199997,1563797.51752,2631908.4,4925859.0525,887891.3725040001,1725045.88,1393071.76,1878794.885,2916614.4971896,6403407.88,2515735.255,985905.8,2550007.165004,1778514.2709376002,799744.96,1562126.96,3036441.64,9605643.66,1056359.885,7509634.795,1789098.1775200001,1096808.467528,1046783.39,1177766.69002,2046262.9329688,1507920.28,3360416.5875,1422836.636256,17105773.74,3447070.32,3042081.84,1938349.2275,1798003.2,1037549.9862520001,2873447.88,1147534.4235156,2197309.04,2455508.08502,1718047.3459376001,1334749.56,1278262.54254,5964796.96,803404.16,792400.88,6403842.32,6358489.32,2921406.675,799180.5975,921776.3937520001,642889.2,2796395,2772195.135,2868274.32,1043140.04,7183355.2,1217615.629384,29107423.56,35986610.6,933103.36,1023608.8634376,924087.28,7987691.8,2988988.5525,714340.48,2320155.47,2396981.3575,1663738.16,791504.6,1142849.105,1052225.5234376,1032136.765,4164748.61,1335926.24752,944601.52,886464.2981279999,5179051.19,17618859.22,1302074.92,4488582.765,789351.1637520001,993447.2062520001,2639893.03,1018616.44,1609800.9075,7644703.39,1040590.2,18704646.8,1814137.92,3579791.32,1475886.88,3570669.92,945185.5125,7392235.01,1166697.56,894408.631252,903041.44,2978171.56,3792487.94,2012458.1675,790333.68,1526462.19752,873711.215,4293984.64,7757930.23,85848138.8,3226781.04,1323679.7925200001,1982149.6925040002,1645155.9229688002,2283868.75,47230807.92,6242148.48,962417.3975,8724816.81,1046783.39,1480400.2975040001,2003403.592504,1464920.92,4807414.68,22251566.72,720859.5900040001,34756641.36,1560813.44,1334753.28,1698984.44,1096727.84,1329800.92,19417386.04,1053671.8,3420389.215,12781210.62,970596.24,2387196.0325,15218797.72,2048677.117504,6591371.36,5674865.005,3422450.510624,44839767.84,15186409.32,10521161.2,993652.68,2350580.9734376003,3635318.64,2277424.335,2239126.955,1582034.2,5769865.44,7189224.8,1018616.44,1618086.7619,1535302.315,1907843.61,1426013.52,2556792.39252,5894891.54,1542874.42,2264526.08,16660347.22,521042.71500799997,4809937.8825,2665357.84,4237866.8,2628760.086252,1126554.3034376,544817.23876,4471942.44,6459038.4,11782881.24,800348.3518792001,1025845.72,2596578.96,13739599.32,2231662.563752,2145492.533752,689722.8,6508194.76,2703262.16,1531999.725,1533397.9175,1558390.2425,4365516.234376,2649023.84,803587.4,3453535.8,2596260.729396,4301469.8,8982774.28,2624835.6225,2483129.4162520003,3488390.89,1733327.8,1015686.9325,690841.72,45772199.68,1226033.4375200002,900829.88,910779.535,1478145.448756,17778353.4,1213575.58252,2770544.36,967127.9925240001,16375135.88,4603889.72,3715921.71,1029133.4987560001,924509.4943799999,2095454.76,1862022.8225,3850640.945,2647017.24,4548676.03,15891938.84,1661740.24,1745985.8,2100420.83,1519752.23,4662637.22,873155.8775040001,1226675.48,7826463.56,2499616.35,2256584.15,1905709.2,1715288.2962520001,1584532.6506239998,966704.9643800001,1033436.56,901124.56,5912017.68,1757797.917504,1509853.1462720002,1346418.77252,2117427.32,1079284.52,3478193.16,16873902.78,1937569.7025,15669317.06,610350.888756,13100772.37,3579791.32,936222.72,1086481.44,3323537.02,5110337.88,881103.12,544372.88,5659664.28,2378738.9525,3760663.625,1063984.72,1220372.5325200001,3087365.32,1347582.49252,4651067.006876,1436148.893436,2247309.9975,3254502.48,1005647.934384,1060189,1121441.6975,3590120.7,968041.19,920666.6025,2921153.18,2857683.84,1051623.6,2893321,18821497.56,2124310.32,1930200.14502,1054665.401272,8430073.71,1743706.0975,31388195.88,913809.88,31315159.56,676987.96,2623852.073752,4632529.130624,1118582.52,3302327.1831239997,4291247.835,2054215.9279688,14246495.22,4221346.56,14442969.45,3190657.96,1370977.9106480002,1598009.6875,2626902.48,993720.7675,1183576.88,1186927.76,3181799.44,11038663.82,727799.24,521042.71500799997,1032169.761272,3514261.5625,3866950.28,4941375.44,930702.52,1050441.12,15738400.5,2077016.2,2426433.4,28488241,949818.11002,5681235.685,24900415.82,941910.125,1094485.915004,4263027.24,1826603.36,6348756.275,976940.52,864930.571256,963292.36,2045118.8956480003,6154891.44,9050580.58,3967962.48,5284851.96,2747449.36,2316300.375,31607480.7,9858538.8,1683847.906568,90064516.88,1565428.56,2765247.105628,913749.005004,2145820.1625,2911058.745,3274712.4346896,1658218.72,1858778.44,579750.48,1400876.92,1007564.08,34756641.36,1704669.08,3578848.485,915953.871252,953661.56,1082630.577524,3484730.12,799744.96,16570727.92,3396075.44,5250922.045,1719129.68,3867997.52,3912374.68,3388317.9381239996,1772448.46,3499424.1,5473391.6,2969655.92,2155687.5025,1372943.040632,1783139.12,14360870.88,1200697.73438,4938956.8225,7461810.16,4960827.4725,1335436.63002,8015336.28,1233810.56,2316685.475004,6123098.28,13314531.4,912823.32,3163041.3706239997,69877249.04,3068331.56,4004487.355,1506236.16,4044726.8975,60381386.16,4428593.155,1503908.16,1919876.86,3579791.32,5932043.02002,5210799.32,41375351.12,2246066.6,8925015.15,1394397.025,3139295.17,887567.215,9925245.03,4890073.84,13986144.6,1206923.16,3096533.96,868074.376252,1024255.16,1622605.32002,17102576.98,2806761.39,12549425.09,2239723.8,861929.48,6685570.66,733529.53626,24751188.14,1113388.846272,4603434.435,2711300.52,687616.8,2477268.354376,7339376.96,1054984.88,4301469.8,3173325.43,1729032.6,14376233.42,1853357.007504,893943.3643799999,7085266.15,1458312.32,1750700.68,1445605.64252,2765027.401252,2231662.563752,2184074.28,5332124.175,1105871.2,9397438.6,1702783.411276,4231056.52,572413.597512,1128727.1206272,1551581.92,831628.5600040001,3395860.61,912991.48,42606389.8,3074700.4,1639557.6879688,898628.8525,1034192.5512720001,1554460.965,5560110.95252,3055522.795,1137809.40438,4410111.831876,1465733.12,37879863.84,770613.52,4584418.92,22501067.06,1117929.04,38747837.14,2738418.36,621389.716264,1017725.56,2777491.664376,2260930.570624,1099632.84,46288828.24,7341875.355,1207863.0610155999,4175979.08,1221740.32,1410461.3737559998,1465271.72,2001957.065,17224434.56,3749457.14,2434368.2,1733536,17028712.5,1113954.8,979003.703752,981678.04,5689651.16,6472214.08,1166657.88,6005658.87,21313919.2,3526197.455,1416137.64,1891795.41002,1019980.44,7358792.36,1436671.8,5225975.495,959686.96,886464.2981279999,983940.88,1310411.05752,856344.16,1721179.325,537863.7512559999,17132063.48,1383200.84,3770082.24,2566480.52,4173816.845,2026171.28,7962079.44,1439920.92,3829098.04,4679008.11,3024467.435,1994788.925,921054.16,830917.24,1632300.487504,9356025.12,12202112.35,1023677.52,8016339.68,919408.52,2831525.68,10653158.9,20355503.28,2198690.16,6352773.99,4117195.92,2279035.305,1443102.315936,641866.9150119999,1059855.16,1404756.16,15403554.98,1347306.92,2451493.36,2827353.36,2036462.15,4234747.48,2341331.205,887063.575,1629594.4606480002,1656335.2644,2676468.16,4097627.365],"xaxis":"x","y":[2450488,1188444,1205617,579888.3125,3637973,2228675.5,5662234,783104.125,620104,21069722,3537977,2918553.25,5448975,770333,5907068,6388260,369291.5,1784385.25,873923712,1889990,1275236.125,646505.1875,1058986,533370.625,1384693.5,2196718,1295685.75,5702077,378714.4063,727374.6875,2989919.75,1066901,4155007.75,5253241,533329,1271021,1057256,9341427,2659770,1054995,3874632,13951571,1344512,6508595,1286817.375,398357,729535,5117308.5,1860901,827618,2017423,242805,1212883.75,1079477,4547129,8819864,499887,620483,4735249,478982,7046345.5,1191803,811703.6875,792734.8125,4420650.5,1847401,1073156.375,727703.5,729096.875,858897,4897760,1030150,1784796.5,870633.5,1614322,552015.625,6345009.5,2147013,4235405,2788860,1691963,4946798.5,7482832,4024810,1093653,630040,1582655.875,1835804,722062.875,994194.875,1964681.25,969682.8125,6695413,764906,3291518,13158205,3344833,44984468,12704014,524144.6875,25377650,6962255,872114.5,466672.6875,1381407,2919980.75,3481727.5,2992693,464155.1875,2372126.25,5965171,1811506.375,670779,15552193,5337334,896403,4039667,4784731,749207,8381834.5,25970248,900973,1436773.875,1200381,2225265.5,5348309,966812.125,9791557,777094,5471735,662424,939413.625,17924416,454296,452744,11441,4188660.5,1686262.25,1767480,342726.0938,1987334.75,666385,8611054,410433.0938,1501111.75,1210229,4268054,914670,1509492,1185469,10750010,572654.3125,9290214,4145920.5,1332591.5,549509,1134195.5,1682454.875,2364351,7475577.5,3418139,1035592.813,567608.625,6015128,7380088,102673696,2177075,875789,1972234,618876.1875,6010407,1189402,2916511,3757006,974305.625,505601,1325597.625,4189569.25,3229864.5,1508561.875,15878689,5528223,1605522,1747090.375,19490284,0,14585813,5696695,541950.875,5968153.5,1346952,612158.1875,1076084,731092,2804534,690529.5,2190460,1829122.375,7240274,947884,9162048,2470566,690557.6875,25959642,703847.375,207572,3155391,948975,666687.5,1247361,986353.125,53166156,1419980,1171380,888308,15590743,830945,1074350.375,3805877,2108012,659450,3693616,4696639,2433777,804620.5,5258024.5,2420605.5,13171370,761504.875,4054994.25,858932,3267564,776237.375,12095818,3556291,887403,6797528,2055534,1983256,566183,1989287,1045413,1169932.75,2964206,3782378.25,802312.5,10062436,1034941.313,12086616,1138520,14168729,2173143.75,1441631,1742351.25,1222949.25,2478960,1913558.375,473874.0938,3518459.5,2920918,19645206,36667044,98960776,13010578,2280352.5,1228041,4961405,3157579,809291.8125,2288635,6293359,5454482,7739699,2077039.375,3577120,1574750.875,708967,2458760.5,748335.3125,686949,1465795.75,1244150,2557533,1686405,2186148.5,1194835.5,3744483,4476997,8355988.5,6914839.5,1738724,90558.70313,3087318,965425.1875,913899,2292169,490743,601590,1117658.75,1082531,326778.6875,1821047,230365,809870.8125,387810,678608,1297872,684234.875,1114384.625,742630,2454812,10420096,694241,1966424,3663534.5,515025,592534,600060,2001643.75,1866800,2360288.5,6859198,955641,4358944.5,1034827.313,6596057.5,1784233,1886382,1100575,1632731,835085,738990,4385512,1227772,77164568,549834,6609358,1789710,1375366,5104947,1883864,2137686,2208196,3004271,866096.6875,1679127,498745,3717064,1446455,710825.1875,8259071,16644664,924438.125,542882,711118.375,219483,1041652.813,0,318205,905750.375,1024033,1696448,694479,5457975,5885427.5,13093350,1270139,10213132,1200811,1076644.75,581589,1967637,2912264,967697,4700396,7369591,489557.0938,1529510.25,8320719,1130960,807358,3837228,4988493.5,12719042,2996958,5238803,958725,1529309,1256575.875,11319836,5767765,6036588,683274,16371764,2126288,920068.875,942089,1497496.5,1358022,529807,2912140,1017787.125,685888.1875,5252131,1817550,7338511,17109052,961476.1875,688641.8125,2211671,3864699,647906.1875,1938887,4547379,1750419,648634.875,1739693.25,2962476,3893073.5,2338238,809257,719471.1875,934357,408513,3258819.5,1911591,2890253,468396.9063,1868497.125,1072487.875,2151376.5,8168547,2472350.25,1417778,2337651.25,5917000,5653656,3059758,2266983,2676594.75,1379739,7637986,1128179.125,1472548,3371455,3091559,7003615,4059424.75,1318096,2156407,1765813.625,455798.9063,641713,1027394.313,648879.3125,1947717,896485,2248776,3404992,2158629,1380293.625,7067404,32579658,1489363.875,1359579,3017709.25,10154608,3546459,892227,2439484.75,3040924.25,317581,32381836,1401315.875,1921222,1494727,1703504.875,2563985.5,1762174,4078339,6454983,684155,10403123,3240830,508014.5938,1022957.688,654625.5,20311228,1481698,1093757.75,18661606,293217,922041,8381744,2127534,2694853,362874,9586556,609494,1133028.875,502667.6875,1457118,2265836,962238,815800,1259456,999924.3125,10317852,4022735,877788,6578370,2477499,51168308,11788719,12783255,1677877,989079.5,1431784,928409,1521202,4685346,2148565,538740,1889761.125,3605749.75,1443367.75,4206839.5,578854.125,739662,3740373.75,1385697,608755.375,1634178.25,8696206,137635696,1364344,3590731,3605855.5,5357833,1176342.25,716940.625,47148456,6320060,1045640,1275231.75,783345.6875,1929011,2307938,22055140,759060.8125,924121,2361088,1493131.75,5261096,755618.1875,2735128,688375.1875,8493420,1858224.625,551219,562617,10303305,657385.8125,1645748.25,4760734,1612105.75,3366528,1245923.75,1093743,2468322.5,816510.875,7682562.5,1607972.25,2512443,2069880,1634334,1334125.125,625938,3158151,405062,40062888,287132.5,1192100,4934559,956481,7520554,818317,2003474,827211,1967129,2429021,1006989.688,2616226,454124.3125,872555,268269,1085439,1055624,3191228.25,53401,1012341,2140476,3265995,811900,713326.875,6935723,204499.0938,12525174,2189735,1585441,14361382,1750362.25,1824054.5,1137513.625,1150926,593824,555864,1761137.5,497991,2470642,4321141.5,2274873.5,12731388,1898509.75,1245753,4486477,990361.5,4560492,5707529.5,3643112,1198630,3151107.75,966376,1104074.625,895310.5,6750112,2225134,3775187.5,431390.4063,976537,5415438,306721,2188452,2234448,3928695.25,4983934,2023197,2028125.625,1593610.125,3436032,85357952,591249,505949.0938,833747,25476332,1186682,1307883,965786,876871,1888344.25,758875,8651835,2442220,3863772,3947209.25,2014500.375,4424263,6127771.5,522791.0938,2093011,422133,2646130,680395.875,35383012,6313678,681354.1875,841242,1507168,784584,2025533,7439009,2090543.75,37980712,3239754,730211.1875,599390,3895403,861972.875,162960,580688,930439.375,1453543,642052,1482997,819354.625,4294550,2024367.75,1525624,157606480,869699,5903033,8926736,3289053,1820292,2277316,1844825,94186,869835,2491238.25,4407932,637685,9155568,4912520,3999505.25,6983736,18848780,3303036,6361236.5,25397086,862541,752441,1551204,2056762,802755,136241424,13962749,3312125,2114860.25,3568237,1109924,3542542,46510960,17044842,10960107,2471623,3132453,597761,1107290.625,658297.5,9029489,1015172,2701872,493969.6875,1886007,3226884,1216543,1469556.25,789209.8125,2081072.625,6884914,1623657,393493,645927,5672838.5,2401890,624993,3485865.5,3005926,8024064.5,9373179,1872413,145573,1834570,714898,876237,2015117.25,958242.875,1976856,10077532,462465.5,694189,1056099,1418915.875,2485521,2756494,1271753.25,1176615,1096646.25,1943933.625,534039,849504,2322314,0,2466100,3629065.75,1188994.625,1822834.25,1169948.875,2122581.25,2400507.25,7175646,3996541.5,350264.5938,14709151,2067352.5,415586,1559199.25,832168,1207869.875,1041304,577033.875,1677881,796042.375,813641,1085183,871355.5,5169726,661383,828531,6102883,2227640,2890379,507603.3125,602191.1875,466904,2767381,847419.375,1618695,436848,4425832.5,439065.9063,28450436,72587024,781682.5,5876439,642133,2407904.5,6058494,611536,1926134.875,2556131,1639849.25,696522,1054581.375,872326.375,677536.3125,3751417,818836.125,1334876,1287159.25,2334260,16163194,1172785,2887392,474468.5938,1153690.125,1228963.75,447947,1332912.625,0,443365,8504904,929688,3573328.25,1293336,2075833,1059605.625,1507270,1419453,586827.5,1360340,13659926,1971236,1364214.75,609508,1790148.625,2245568.25,2855545,3944933.5,286685536,4767227,815115.5,1261634.5,1845150.625,1698074.625,61576184,1226435,716071.625,5249190,1128208.125,863703,2301043.75,1080203,2376794,11325513,941564.8125,41655852,387279,1734533,1540235,570982,1689631,8909924,700899,686019.1875,8580279,938908,4167253.75,4222682,6118300.5,7323351,3185005.25,7256842,53079164,9569715,3644982.25,238255,1614026.25,1655612,1570758.25,11594121,1929118,3021669,6814269,841841,1258619.125,803938,938096.1875,961749,2058532.875,2220053.25,1313366,1297633.75,65336980,541542.6875,3608955.75,1983509,3218824,363808.1875,770175.8125,592221,3791402,23364636,49102164,1625308.625,497528,4202335,9986032,709990.8125,1423463,567521,8748273,3373582,846389.3125,1000804.625,2998208.25,2153603.5,2660396,1289073,1247972,4253369.5,2105451,13586070,935580.3125,1082920.125,3321035.75,3288836,3560614.25,512166,51625548,2615973.5,1251906,555153.125,810636.875,48729588,548292.6875,24473814,785052.875,13300859,571309,3570161.75,522972.1875,2473302,2098850,6881405.5,2306723.5,2876857,5089903,10564729,5039523,1730886,1633399.375,1696978.75,1739286.75,395346.3125,349139,6040837,945020.1875,1764767.25,914023,1232822.875,1625695.75,2545405,5807879,1392015,6283226,2662054.25,4133811,1582794.5,1259959,964015,6424029,12448381,873711.125,13911761,1475475.5,10498344,1788148,618366,776991,1331122.125,5903142,634751,520405,11026945,5981430,2203026.5,2166711,1427974.625,2654755,1050982,2581741.75,1639136.25,1402048.5,3165109,729397.1875,937635,868859.8125,4409881.5,705262.125,3542708.5,2729167.75,2628864,813038,2773779,21781324,2161828,1199392,832986.875,602245,1074989,35685224,342493,94178648,1136676,741287.1875,882896.5,1505709,0,1803753.25,3220635.25,8201974,2035595,12265516,3306123,313528.0938,241478.5938,2465266,1270582.125,1982967,824516,533078,19344782,1172065,511180.6875,255971.2031,758008.875,1911611,2503948,636641,677279,7838600,499928,2921368,22591288,964923.8125,2278160,17186624,591622.8125,919746.3125,1717130.875,1815350,3024478.5,5975230,860364.1875,600543,3275779.25,7261732,8921778,1976400,5572503,6159684,2960377.25,13270687,7370743,1323038.75,291614432,1425599,689803,439266.9063,3020001.5,2168701.5,1234095.5,2219744,3045139,638201,1943772,855393,41213584,3674705,2712983.75,1201630.125,648022,556700.6875,816846,196410,9977330,7419765,25453690,1120197,2483320,1358226,1441665.375,33268410,5535626,6471427,1437569,2128963.5,1581425.75,1231546,5037781,1581036.75,5392617.5,4180298,4817062,687953.6875,3743228.5,578789,1519952.125,2355732,7969653,431357,5094197,65047284,10513817,2599171.75,842593,10105379,45074876,2391566.25,1004253,1827424.125,652991.6875,3534690.75,7657568,64715652,1073507,9671619,1540258.5,8163413,520206.6875,6744317,6577130,63668488,695346,2666634,676032.625,571644,1103594.75,16614404,2885740.5,9851937,1743198,572082,40613740,861007.8125,47102428,529033.5,41680640,1398253,987989,4590246.5,7976729,786346,9146727,1688612.5,801264,18628834,1108310.875,500237,17673018,978211,1921890,2926089.5,8839812,1634669.375,1115020,5121586,804002,16246106,3612526.75,10340282,881386.1875,920430.625,1183623,323739.4063,2993032.25,788055,19150024,1929828,2248134.5,669117,495152.6875,941257.3125,21795830,1513700.625,445943.8125,2843033.5,460010,16020642,812013,5838129,18967014,1038351.875,20657852,1808401.375,588492.5,1380283,2345752.5,14833476,1541879,19827916,4396566,899266.125,8354235,466985,1280263.75,1011296,1325972.75,11821841,5481549,537141,1037696,17750994,792015,697397.875,1736775,3616057,8070420,9159185,3730953.5,18576900,1356972.25,656980,1487769.5,706961,1916309,4038090,4027134.75,868163,3767135,517029,1208168.75,864765,783208.625,535649.875,14751117,1896858,1757601.25,1116705,1920352.125,2755578,4828599,873648,13440924,1730431,3441643.25,2178074.5,690750,573395,936616.5,2762846,6821833,897514,18074716,228212,2549892,22530168,19991412,3215953,3245111.5,6057734,3234787.75,1928391.25,416566.1875,741850,1045337,11963428,3035962.25,1855556,5040382,2096835.5,3002337,797170.3125,892455.875,1751483.875,8054334,686055,1381590.125],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors = np.linspace(1, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","SiteEnergyUse_predkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","print(ScoreskNN)\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[12700472.97299695,10933454.929447746,10838888.951255023,10521999.46622356,10588083.390614485,10591020.060253223,10413708.298868254,10423594.458353352,10308678.038933313,10285545.299466353,10265146.757717524,10309499.599267412,10257199.613497872,10267782.04060175,10292187.578669572,10319467.772771638,10334496.231569353,10349054.60235133,10385762.714548409,10414573.716713762,10452494.66839842,10491784.010052653,10523730.90194863,10547632.547929918,10573796.25897364,10584217.95832065,10593226.491747886,10626959.889818508,10633548.562548487,10651594.789650548,10665549.257344551,10693918.479648763,10707950.63553638,10722164.11411365,10745217.450101579,10765640.861178707,10790263.64763675,10814356.987879805,10829933.805597547,10846632.734315049,10859289.788971128,10871076.776185792,10885283.255028212,10900384.62409721,10914039.502404654,10930378.691523483,10944287.79049458,10959435.313903812,10966844.824430618,10986820.515029501]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[16886365.65910154,14904435.866173346,14974779.296747988,14886074.492527116,15165286.688980918,15058226.636829898,14941705.491751682,15019582.22304983,14868062.408303358,14827813.12362618,14855442.471704457,14920028.887142837,14885677.160439283,14913147.346754046,14973386.769776596,15013157.194687994,15065214.063311681,15109701.803978257,15177350.724477518,15236941.322791457,15287805.125857543,15344772.808379315,15391536.949202493,15433445.722476013,15464725.251713436,15473169.802084666,15481451.877567912,15515162.379340973,15533632.402859747,15559134.931899944,15576415.184593726,15607855.978622826,15628542.923849732,15657103.835798433,15682011.306752507,15705541.966444867,15739213.627462931,15771714.100697637,15789959.289997805,15812475.65913779,15833789.243704654,15852838.002084734,15876536.651768133,15891564.663238574,15911968.671701517,15933230.975313336,15947284.271772385,15963118.642255373,15981734.160657728,16003859.062446235]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[8514580.286892362,6962473.992722147,6702998.605762059,6157924.439920005,6010880.092248052,6123813.48367655,5885711.105984827,5827606.693656873,5749293.669563268,5743277.475306527,5674851.043730591,5698970.311391987,5628722.06655646,5622416.7344494555,5610988.387562549,5625778.350855282,5603778.399827027,5588407.400724404,5594174.704619299,5592206.110636067,5617184.2109392965,5638795.211725991,5655924.85469477,5661819.373383823,5682867.266233842,5695266.114556634,5705001.10592786,5738757.400296044,5733464.722237226,5744054.647401152,5754683.330095377,5779980.980674701,5787358.3472230295,5787224.392428866,5808423.5934506515,5825739.755912548,5841313.66781057,5856999.875061973,5869908.32119729,5880789.80949231,5884790.334237602,5889315.55028685,5894029.858288293,5909204.584955846,5916110.333107792,5927526.40773363,5941291.309216776,5955751.985552251,5951955.488203509,5969781.967612768]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[10392001.693450058,9437347.906911768,9267023.89479677,8615764.721824903,8419906.364910537,8672248.837325163,8871808.982708871,8826862.075399842,8409619.377883269,8494532.18894981,8486787.320375608,8659074.273793276,8523234.084464151,8640215.786817947,8656728.934307495,8784170.221188676,8885111.96893721,8950734.594277794,9059786.088994373,9138996.091122434,9220120.3035498,9285194.039568942,9365894.970702628,9430766.084757464,9486641.45404081,9496676.666022157,9546406.163298659,9629642.401315227,9686414.805867778,9728202.770989098,9772575.269004727,9815031.321517855,9818178.076237787,9849814.710519826,9898167.357798228,9929141.653620435,9954109.852609377,9985820.721387828,9999855.16302689,10018007.240530998,10038557.140539963,10051754.652116619,10075555.187873762,10102810.296600543,10129445.765087256,10151067.43209675,10170596.048718322,10178860.133101238,10196393.4240593,10231639.334309593],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[15005258.02231386,16388066.751465434,16856540.461158026,17011921.22337895,17144357.31701255,17085374.816434987,17055272.209005695,17238169.68318207,17255220.150811166,17245238.10702192,17379761.159537908,17505809.880004894,17505263.881365463,17591031.510880776,17703479.933872312,17784933.799516343,17872499.30106357,17942267.393206116,18027719.17569588,18124327.880149,18197058.325964224,18270484.57746622,18331988.650995385,18384226.849416927,18435700.553859685,18437163.468120564,18443702.721891735,18471100.291583363,18497548.498429596,18531733.8434808,18550653.806985743,18579969.197425872,18600171.412760742,18634681.957721133,18659757.1745534,18693673.951987192,18735588.531210992,18774582.22027192,18799480.778927036,18827661.049804684,18849417.640493266,18874563.532734178,18900584.75836683,18914282.706787422,18934095.35232471,18958166.324773468,18971582.09256258,18993677.09934546,19016251.04304893,19038370.42432214],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[19659820.507569924,14833147.392661074,14536001.659096142,14292079.15767054,14879675.374079442,14622559.21214709,14315176.95940197,14291496.132572139,13982825.671022963,13851205.08729459,13732268.13324406,13677643.38593911,13628026.021750059,13536663.119605795,13533896.155984268,13474412.63443153,13451696.741293995,13450050.78150345,13471346.093141641,13464759.998946575,13467986.901621485,13492781.46776492,13497806.510103019,13508467.12241268,13492979.065115018,13510488.878593978,13499111.715477763,13516225.206537578,13508259.249859776,13508642.405476421,13512321.25873125,13536518.682481773,13564517.491228884,13580689.553066352,13590580.05107874,13591198.83139832,13612937.494597927,13632763.294628512,13640925.547877204,13655636.623662772,13671054.847910743,13681015.402140481,13700006.25532264,13711137.239159556,13725948.810701288,13739861.175141107,13751147.99195896,13763151.169177603,13775477.059561571,13786730.289802866],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[10696461.157816207,7624472.509006385,6660179.057723724,5916905.355681358,5547535.928458918,5727656.122058519,5529533.422012307,5438292.657166172,5425237.951346204,5344938.484018407,5229768.912184504,5260974.710027473,5206653.239621212,5172780.784745279,5151181.89629568,5128852.030236353,5066095.274044349,5036400.989433462,5005516.061456389,4991021.007942718,5009473.428551012,5010508.867842389,5001691.947052399,4974867.468432862,4982955.99120811,4986580.408068869,4974857.897961024,4983069.141783274,4991737.152545168,4985585.118687943,5002918.378747398,5000150.664877248,4996703.044951593,4986693.69957348,4990722.734481248,5000391.216310926,5004301.765641562,5008744.017759208,5018327.027169516,5026515.32524138,5004222.49003282,5000574.947368365,4998636.687007344,5012420.027704015,5002117.746404504,5002651.653978641,5011588.528144049,5039227.958826816,5031166.17946666,5030741.337384603],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[7748823.483834695,6384240.087194067,6874699.683500451,6773326.87256205,6948941.968610983,6847261.313300363,6296749.921212424,6323151.743446546,6470487.043602958,6491812.630047044,6497148.263245539,6443995.746572306,6422820.840288478,6398219.000958965,6415650.972888105,6424970.178485282,6397077.872507646,6365819.253335831,6364446.153453764,6353763.605408081,6367834.382305577,6399951.097620791,6421272.430889729,6439835.214629668,6470704.230644581,6490180.3707976835,6502053.960110243,6534762.40787309,6483783.106040111,6503809.80961848,6489277.573253644,6537922.5319410665,6560183.152502896,6558940.649687463,6586859.932596283,6613798.652576662,6644380.594123893,6669874.685351561,6691080.510987089,6705343.432335404,6733196.825878842,6747475.346569307,6751633.3865704825,6761272.850234512,6778589.837505512,6800146.871627461,6816524.291088986,6822260.209067934,6814936.416016632,6846621.1893283],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","FigRMSEGRidkNN = visuRMSEGrid(KNeighborsRegressor(), 'kNN', n_neighbors,\n","                              'n neighbors', GridkNN)\n","FigRMSEGRidkNN.show()\n","if write_data is True:\n","    FigRMSEGRidkNN.write_image('./Figures/ConsoGraphRMSEkNN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                     464\n","1  randomforestregressor__max_features                    sqrt\n","                               R²          RMSE           MAE\n","RandomForestRegressor()  0.377635  2.280601e+07  3.103676e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRF=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[2383836.0223599137,2798706.9517780175,936919.9779094828,945965.7042780172,3875996.683997845,2226632.656924569,4696659.493534483,555845.9477947741,1272715.681634484,12912478.173141163,2800302.3577586208,2343049.0968480604,2625188.7727640085,1319357.8893529745,11216922.734913792,18732129.171336208,1459469.2921605604,1199540.216864224,126164395.32327586,1739039.8556034483,1717670.9370959052,1109295.9141971983,1088928.8460398708,694996.7859311424,1481269.1398168104,2188264.4587823274,934304.9399614229,6119897.937230604,1330053.064284698,623312.920065087,2694187.59375,1481901.9373653017,3535359.6963900863,6845482.683459052,765265.4487474138,2759100.122003448,1326325.1540948276,20617230.532327585,1479907.6248663792,1119672.0043105604,1496387.590786638,8868492.695043104,1193157.202182112,8436096.091594828,1390806.624256466,934476.8828517249,788141.2359913794,2041841.9132543104,1184216.0215292743,761959.1167834052,1922960.1877693965,2142901.4773719823,1570215.1668787845,1539808.959051724,3709324.211476293,6981306.515086207,1093688.0199355604,5246454.160560345,2633458.199353448,598395.8629759339,9020484.804418104,1957251.2961340519,2011046.3122306035,1502403.693426724,806830.9236976297,1664077.885775862,1569362.5275908615,1015629.0536099138,866526.8550646552,995251.2088504311,4982750.457300646,1046513.7738415948,2186510.5377155175,1664223.7629704739,1619803.5692349137,540385.1585607053,3655997.8198073274,2534989.7077047415,3059979.6306573274,1257809.1972926725,1553826.62109375,3766663.787176724,4073827.6376616377,2300832.8860452585,3128973.0486260774,1169867.296555573,3851536.6101831896,1150189.007143966,893562.9797952586,1021645.6695514655,4542555.009294181,791817.4673006465,8570289.612607758,1827068.9806034483,7269418.564116379,17040150.577586208,5427376.6659482755,56921871.385775864,3780273.550377155,784259.3053612069,18182628.56627155,15814703.148168104,1480939.935614224,1511951.4206227157,1688504.9024784483,3642580.6027747844,3343202.515321983,3817187.9313038792,796335.3599176728,2076721.3042834052,7055798.891702586,2498438.4193497845,1235504.09375,23153304.747844826,6515474.244477371,1059214.0901806036,4967510.468211207,8903883.47036638,626405.8987743534,6073220.4432920255,32064755.814655174,878942.2373553019,5746544.663927802,1883574.8108836208,1217025.928744612,3049534.7435344825,3294012.2067618533,6667164.0123922415,1160529.740167026,4314886.458782327,641391.1637931034,624850.6655189666,11184781.59375,729694.8311566811,592950.9550784483,5951746.837284483,2113800.4591864226,1147162.6970635776,2103057.2039331896,2586723.399919181,1538633.2065622841,829942.2584859914,2453986.643756681,690292.905719828,1790754.20703125,1237364.4752829743,18349286.632543102,968466.6969288794,1151417.5783943965,744673.1452047414,3838289.2405711208,653549.9773713363,3439927.8349946123,3513409.011450431,2484989.998114224,1024536.1161101294,1507324.9928609913,3665996.355872845,2799533.1961206896,7957528.09294181,3556119.802801724,927039.0497541811,735684.0525066811,2772907.703125,6838530.105603448,86743343.41594827,1885738.1823499994,1575286.8208512932,1424855.970635776,886242.379849138,5499284.665678879,2713248.775323276,2574233.7947703446,2546818.1004849137,1595308.627747715,769161.0794721983,1864787.0792025863,2511767.4104256467,4408131.071390086,2032147.5056241811,24001737.48060345,1727210.2273932472,2173454.900862069,4266883.242456896,12108898.49299569,2022280.5884967672,7067559.409886854,10434674.296336208,706224.4739359914,2656121.9357129293,2008995.46875,1271416.9764731324,1299394.794989224,636576.8069101294,1598392.3561648706,1978549.6852101292,2150485.180162284,1011957.1283019398,4027535.262122845,978730.4406655172,11071296.394396551,922655.6579019399,3135399.884294181,10789767.912715517,722238.2234856898,1309639.8730084072,3333756.3380926726,631143.1168172413,2659993.5297855567,1624270.4921885775,1264375.4388480603,28898689.83189655,2043179.9493534483,1018285.5998114224,4485960.6268857755,18043165.290948275,1044209.6782079741,1061558.0565329029,5292247.140894396,1248286.7005657328,766197.1896551724,3516036.6532866377,4745189.536907327,952023.6787950431,1177325.1198846987,8954150.053340517,1796250.7959396546,11904576.34913793,1250670.9117877157,2770169.734509698,1095142.422817888,4213991.753165517,1215510.3141443972,4130620.035290948,3922139.2672413792,967737.7828663794,7332473.525862069,1096876.7661637932,2435043.394935345,706002.5983787358,1382145.9888200432,2242879.047413793,1639798.5572301724,3027319.771551724,3707024.6608297415,800719.4603316811,22653735.577586208,1001920.1985452586,9068287.429317025,1398204.7145743535,25513260.490301725,1374770.8614704737,1171500.9019396552,2778553.2529633623,1132408.2784213363,1945424.4203198364,1309875.152790043,790551.7056846983,2646532.441877586,2196597.633671207,15333673.742456896,47106388.17025862,31458611.830818966,4473574.969019396,1196508.3698143316,817677.8460398706,5019436.290409483,7342095.239762931,1719664.0017515088,1603587.2480603447,2256348.4823549567,5956151.219827586,9187678.40463362,2662843.033674569,846022.885775862,2130939.8984375,621830.3801185344,2104339.690195043,689662.0953663794,888201.4346713362,910663.4652480604,772261.3785058194,5217239.879418104,1268658.5778221553,3288301.7943157325,1804223.4637661637,4207450.583243535,3465644.277478448,9452839.664331896,16959648.192887932,3610602.765625,997510.8576950433,813073.069190661,1550208.741077586,1011878.1994881466,2601436.5234375,887773.4305226293,759695.6846713362,933653.2257213364,3743322.5398706896,684018.0494479168,2487732.2477103453,1821083.177801724,1455781.5673247827,1196574.9940174208,882767.1551724138,1585086.1037176724,795694.2956640088,1233908.2268035782,1099507.951104526,2564147.316540948,2632379.039197198,688299.8542564656,1767892.7293911637,3230363.875538793,552639.1187635775,912808.8203125001,920218.7210398706,2078474.8372844828,4284624.83674569,2233789.3973599137,4060392.6086045243,971601.4166217672,4294430.856547413,1312175.0230334052,4588515.24299569,2550683.2122844825,5717534.373383621,4572719.716056035,1095505.4172952587,1093711.5454471982,994517.2723375718,4061346.328663793,1185763.872036638,48403332.02586207,557779.7052131466,11137069.377155172,5315468.806977371,3009752.775862069,5072825.8203125,3166982.174367026,3349945.4754849137,2218417.744073276,5364007.182516164,791689.487338362,3538892.0354256467,1685567.4729256465,1332055.273445043,1195172.0582573274,902247.7767045259,7548992.808728448,158636435.01077586,854688.4336273706,824322.442887931,917859.0484248708,2307795.6299838363,3031826.3741918104,1518073.5231681035,663796.9135237068,1292458.062030173,1685282.2521551724,2032582.614224138,1393802.0856681035,7604277.051724138,5819709.064116379,18726490.376077585,937377.7918462643,12200941.078663792,1399268.5895743535,960761.8620530184,910069.7109375,2695974.7184806033,1654095.368427298,983101.6834590519,4827877.699757543,3154724.7042025863,588276.6212348064,4329144.401733401,15510287.657327587,1692782.650457974,1427916.702182112,2853478.9948814656,5287705.540813577,21846293.23922414,2911006.0519935344,1606981.51490194,1112071.0327316811,3511304.0534752156,1433281.0280707977,11480174.836206896,1981972.3879310344,2081394.9865301724,1400253.853516164,7816457.409213362,1356102.9698275863,1344253.9288254308,1222934.7390894396,1982933.080145474,7959593.4251077585,599345.3126127872,4730838.4616109915,1686928.144869827,1248584.1348112077,3176468.5476831896,2567311.6842681034,6161268.455818965,12699991.372171337,1302342.083092026,877926.7434545044,2823994.044989224,2246420.2825969825,672536.7830023707,2972126.4719827585,1948238.319369612,3765033.0478866384,720946.809536638,1692835.963372628,4968775.522495689,4396089.5657327585,2110114.5928071123,967958.1419270833,3719013.4993265085,1044549.9244342672,6817599.509294181,3612686.8903556033,2329202.9872372844,5697752.679498922,499190.0983118976,1671632.4423765517,1214332.3255657328,1690548.7753245686,9696464.68049569,3403717.481950431,5380327.453125,3125532.4104256467,7150786.325835129,9623961.940732758,2002620.1174217667,1131522.3581627156,2651422.7002963363,942219.6035883622,11109947.509159483,2159248.2308056033,1959604.1217672413,2424745.5358297415,2724813.0673491377,3254493.40625,6515231.171875,4453670.016971983,2311803.5672144396,3200436.1349676726,540214.7647515087,2058669.8892780172,1808376.354660991,1597862.4113364206,1852848.384698276,998134.1128771552,1429445.045932112,5909312.674568965,2310750.495689655,2526249.8542564656,3905949.6338900863,4005960.406788793,2128921.5539803663,1203115.356767734,7801846.070581896,6816904.789331896,2209888.757408405,1836658.4143049568,1642348.1131465517,2590938.922279095,1239355.2786907328,38572063.5862069,1012860.0061290949,3645049.3401131467,1492923.9248575429,2235349.8981681033,4441397.102303448,1369986.5172424568,5196885.334859914,3683517.2047413792,1074660.0682954742,5191611.011988146,3984508.9154094825,626759.2274005751,1042548.1151131466,635485.8089978448,10523169.105603449,1488837.0290948276,1214318.0915275863,14410781.044719828,2291544.6643318967,3255306.5167025863,3345863.2556573274,6548902.831896552,2423981.9797952585,3135026.2505724137,6500852.023979526,862887.3184267242,1422167.9418103448,1870539.6949084052,1852116.411099138,3407862.9762931033,1461334.6366851295,944186.2601023706,1026309.6034033763,703100.6129781897,8426761.842672413,4205234.623383621,1229557.6423771551,6277699.8534482755,3181613.424299569,8113274.5280172415,11953838.375,5953752.091594827,1131093.902764009,1175925.5203450432,1147518.026739009,1379972.6686696121,1098481.6605603448,5936342.897306034,2192065.771112931,855600.5400730604,2592794.111260776,3968272.812634698,1812094.9137931035,4051768.057112069,746663.3879310344,1989067.6935614224,1665805.6999259698,1383846.0152209052,907534.9025469829,2347694.433054957,10899901.530172413,86825665.59267241,1654927.8477640084,8875328.77101293,6404588.930259483,10724103.184267242,1168022.0363685344,1296616.4236260776,54805448.27155172,8353335.059806035,1216183.2407058189,885598.6825159483,1727216.4686153017,728802.789500431,2313949.638200431,9784944.209051725,949909.2047749999,722952.1463556037,4144484.8331088363,1170239.0262661637,5495239.304956896,1005056.2147090518,3980947.619073276,810951.2263454313,13719215.828663792,2311614.862742457,553182.9861331901,853142.8084590518,1264142.9027394396,544626.1914195053,3102241.1632543104,4066319.0959051726,1287532.000952371,2904009.152478448,900214.0750838364,1460238.235994612,3108232.4537984915,765116.3185710133,11934455.409482758,2984678.7380812494,2140554.6926185344,635362.6120689656,2487021.5177801726,933404.6070237786,754251.236799569,1989373.1278286637,2048428.1994881465,31072496.44827586,2174090.1013327553,2134512.9754870688,9842067.27101293,1109066.6963900863,13918416.223060345,798291.2709691093,3012397.8563443967,963595.1775951867,1364758.521554957,2339383.9796605604,2276609.2648168104,4132749.7168879295,689096.232051832,653018.2028557112,1112324.5045797413,715459.0762398707,784767.0047144396,2399436.9411368533,1752088.2098601682,3669115.3604525863,3184389.1505926726,3758176.6640625,1115091.0165060344,1251267.8623534488,7008318.5657327585,1071571.6553780173,4015073.9510379317,3857527.1473599137,1133414.2219155172,17393197.525862068,1959394.328125,2335195.9396551726,1293828.0001665957,1801763.614224138,694387.9967672414,1158456.1531521552,902011.6938379313,809458.78515625,1514992.277752155,5155067.879310345,4267876.9915140085,14575294.87338362,1934828.7012655602,1873750.73828125,3595049.3030711208,1017800.3764144396,1782690.8302801724,6523762.629310345,814899.34375,1421424.513739224,3249200.3368814653,3240462.179552802,1521672.3293372844,739116.3555368535,6642759.495150862,1119777.9754849137,5982404.971443965,1029760.5736323994,1130661.66015625,5338899.564116379,6170109.016837285,3212783.9820851292,2612248.8054956896,2457899.1858836208,5678532.166056035,2706333.183054957,1277689.5517722631,2354210.596821595,2700946.9304956896,67246085.62284483,1235597.8403903025,653549.9773713363,1110652.046952371,21286197.549568966,2723253.423370689,1935433.3630793102,746138.4885506466,885122.5917295258,6319482.96174569,839964.0393318965,13233204.098599138,2554370.6231825436,2389837.3740571123,2629544.33452711,2583171.340382543,6431944.7440732755,6567930.9628232755,949145.3045528018,5420963.093211207,1317275.7974478446,3404654.3191002156,880591.4711081898,30032212.497844826,6339438.224137931,974538.4255825434,2874479.230872845,2370563.3010506467,838097.575700431,2818715.3857758623,3564181.211817457,1982715.8011853448,50770741.50431035,2598121.5134698274,971862.6180765086,4864170.8483297415,4862999.095366379,5014623.36018319,2113044.0382881467,685693.4792568967,2325762.407131034,1438477.7162581896,1229083.0721650864,2580386.889143319,4149349.3393715513,2443370.847521552,2702127.6189385774,1908054.726225862,111355186.34806034,1045918.0126625001,15500608.293103449,1506183.0318282326,1456212.3944299568,2326991.3683327585,2046659.8475217673,1774281.3907596983,1531786.7172346984,2769187.0915278015,3584893.798760776,6833517.566810345,813900.2525928881,14311752.359590517,2388272.1425109915,2190847.8543743533,1968098.7257543104,43237825.47198276,4908049.852101293,5007295.306842673,40489395.77586207,1704713.873114224,1283205.3196390087,3496509.431573276,3315869.3727101292,1256601.0406788792,42778082.90948276,6427851.290039439,8223744.407327586,1410143.7361573274,9204558.752155172,1317448.9205215084,3135224.666487069,68004848.7025862,16194146.375,1164234.6100835348,2630241.875538793,3474940.344288793,689408.1029094828,1584008.7689273707,844632.7752210564,4096323.2332974137,952179.0924030172,1219763.9294857758,670308.4499599139,2640391.606950431,2772415.9485452585,1179391.1394127156,1339406.8128142958,849545.2368221263,4853985.7623922415,14408626.282327587,1653142.0465732322,1077039.7526939656,1277371.4078663792,8650421.968211208,2249957.956896552,853058.3100081896,2998463.546336207,1870838.2932381465,13615690.877693966,6486614.050646552,2319813.9931303877,647826.8473868534,1141311.9127155172,940826.0218211206,872073.9634967672,3222188.12394612,1565496.5323275863,1836790.4918519394,6106884.866654096,787485.1666922413,1773376.5158943965,1583097.238551724,2788182.8077855604,1839622.4381734913,6700000.880738147,2395610.7141702585,1052007.6866918104,609503.48046875,2064566.947198276,797287.9649784482,1411234.9388469828,2653882.8828125,11061700.13900862,865782.1767400863,4211253.393049569,1924850.125,1608542.6985512928,1395240.8099407328,1602612.2916721976,2198908.566002155,2210851.552262931,3062540.824757543,1289524.0080176725,15526355.75700431,2719894.6505926726,996617.1795528018,1906621.7255926724,2408253.7101293104,1002789.9748959483,2263930.2180765085,1411153.5082215797,2532832.196794181,1047313.2575452586,1177325.1198846987,1028021.7615842673,1391785.7053983617,3904457.0711206896,798230.8341864224,789002.4755523707,7297647.310344827,3370632.544586207,3360038.3538023694,681364.3287318966,1057284.2964717674,651479.1239469112,1946866.9605334052,2878910.167160776,1909312.5339439656,923050.3319991789,8243452.716594827,905874.3205909481,29166846.747844826,72810300.32327586,1202166.631061638,965354.5622982332,850427.8246228448,6096581.994612069,3751286.5646553882,688546.6059405173,2439425.971174569,3979312.4023101293,1677195.075802543,788276.1651403018,1561043.045149784,1220455.0141433189,1286506.0075431035,3788564.9405980604,1273389.5131346988,1085064.7897988507,1115078.1026400863,2762472.4528556033,18320755.413793102,1632540.9539331896,8152187.062769396,820174.1557801726,832375.6003502156,2283599.638739224,728230.1632547415,1407203.0949644395,4640164.988146552,2048676.1148978448,15639311.075431034,1387809.537176724,3285401.3330077585,1216925.0053879311,3956958.1858836208,856352.6088362068,5515618.427633405,1348850.1911368535,794205.2165965517,995617.4967672414,8029123.424003233,5631217.109375,1463305.4222790948,981854.0425915949,1668072.5408135776,1157245.1538254311,2437793.906115302,3860355.611260776,108018477.78448276,3806715.0556314653,1276426.36975,1620731.0924062491,1666865.9760269397,1658403.654229526,53434196.6637931,3552319.8154633623,666332.9415409482,6224398.868534483,1394304.3587015087,1549705.035156293,2157932.6468548276,2625680.3688038792,3766605.777478448,17054708.867456898,914773.7175089594,68949846.13793103,1126643.7821743551,1642088.0482292816,1924811.7036637932,1123214.368130388,1296770.595635776,10896947.134159483,1127716.1518049568,3188878.503502155,10018395.935344828,1767001.838900862,2651586.703663793,8894379.387931034,5517179.751414439,3882164.667588361,5586416.155320045,4606456.432415086,53971049.969827585,18924200.804956898,7088813.735452586,1265496.0307112068,2492870.0592672415,3302229.3591056033,2222601.751617457,4072433.2677801726,1525301.3655711208,4843863.400592673,6210637.296336207,722881.6869612068,1543255.8439967656,1035037.7897359914,1365866.2222297057,1380014.8254310344,1243881.9091109922,4318594.027620689,1197138.6318657328,1740978.8521012932,39497314.84051724,568602.5593629531,5022021.583647629,3553919.2370689656,3767681.0148168104,2476549.9874060345,1064873.6122712502,970476.4672015087,3781192.4719827585,16685877.644396551,31805730.516163792,1318062.004344138,949146.4089439656,2918992.473195043,13653023.08512931,3202117.7396282325,1551682.283007543,605353.1788793104,7618718.744612069,1809389.3449620688,1493413.307650862,1215423.0639838362,2083377.8774245689,4158608.420292457,3599611.8917025863,1174720.5845905172,1836915.2081088363,3941046.301929741,3378674.2860991377,8243476.89924569,2052371.0251885776,1669810.2799724133,2526759.9342672415,1585401.005926724,1709460.8733353445,1185095.173020043,53349314.06896552,2003331.4227460492,1213956.8195043104,945442.0716931034,2990106.8925821115,36868853.73275862,1049632.4030250004,9328993.414601292,680533.2956405173,15333853.294181034,3426433.9989224137,2902938.2419202584,1084268.1855875012,1351746.2532813936,2917620.2380118533,3229499.0420258623,3661920.4606681033,4257735.932381465,4951296.571390086,15811190.342672413,3955456.6819101293,1702486.1793777586,2330258.835398707,1932880.2442607756,3612889.716595905,967821.1678400863,855914.9711778017,5956867.02653556,2713005.2844827585,2055359.0854661637,2690449.6747036637,1962036.2437036636,1549783.3558728448,1978302.4661234908,1589750.6296478442,962306.2954607758,2791905.5164331896,1730240.1913396548,2982433.1425997834,1520708.21566056,2328975.9822198274,961984.4507127157,4852369.999881465,16684217.36637931,1393564.9195851292,14653820.813577587,768104.4963637933,15047939.085668104,3223301.093884698,662199.6986801724,1288220.1652747844,4206143.7180765085,7137663.114495689,775956.8286637932,598395.8629759339,9344097.700431034,1751450.4480064656,2677499.7844827585,1615258.9109749994,1464319.6475032326,2573139.740301724,1420535.6282629312,2811062.487980389,1026806.7481620692,1463736.6489762932,3832254.6126077585,1814881.0293349123,881494.5463362068,2456240.4961161995,4475762.442618535,782808.8802801723,1561369.4104256465,2376921.6298829746,3290467.113146552,682872.755387931,3458228.4186422415,21736839.01185345,2364524.662345043,2037553.4760237068,1145610.152007112,5693982.9471982755,1031368.138874138,66205882.8987069,974030.3367389548,26078040.98922414,1057031.0915948276,1113881.7812504312,7866603.32475862,777637.1371228448,4364169.328663793,4158016.421336207,3471222.2087823274,11249789.654094828,5142665.78987069,15584673.690193966,3389413.2106681033,1833377.991538575,977431.9791314657,2435408.6165140085,1036681.7161233836,1200694.2840862072,1084683.0730086206,1997546.5845991352,15401912.306034483,714964.994612069,568602.5593629531,934463.1094424569,2955361.6515859915,4353001.93049569,4503606.244477371,580268.2589581897,734192.435142888,13125636.203663792,1466522.5148168104,3133298.353987069,30892831.622844826,1062082.7812508622,5660196.640894396,18945282.332435343,1303462.7074364224,1564051.0311153017,3741818.0588631467,1602574.3921071119,4324545.9537984915,1077934.2939958621,1066037.1418475576,539495.4714439656,2267983.7812006464,5723192.515625,11024906.60075431,2748240.1848060344,5738246.3925107755,4025256.913681179,2423930.0549568967,32344276.01724138,8473880.520474138,1356642.5649935347,142869319.51293105,1842367.4516702585,804861.1955887936,1157012.2257212503,2570065.1584051726,1987337.9582435344,2655881.58996306,2576690.359509698,2745204.9827586208,589004.6951778018,1279848.3888469827,841264.8038793104,70384136.90948276,2217065.3952047415,3970959.651939655,932682.7299316812,797012.9088490354,971170.0920002155,2804791.4597588363,819447.7424629316,16511894.114224138,3254056.433189655,6951831.42887931,1320555.9622152094,2274082.2023168104,3115404.5549568967,2600759.509667457,12647056.003502155,5103302.056842673,6093498.191810345,1950464.0051185344,3192033.7683189656,2492138.880136853,2410935.0480872844,8335213.790409483,1465921.2113517239,5092720.444234914,9054680.417834051,4385080.910695043,1525179.7704747845,7792943.957435345,1178753.4292834052,2758271.2413799567,5314770.093211207,11275233.98599138,900743.4753727729,4847532.763000216,61907939.68965517,3473289.8820043104,3053117.0270743533,1904329.692395115,7300759.784350216,62494485.24137931,4068583.4865301726,2156523.741123491,2345009.1767241377,3186695.5909549566,6283561.560084052,4971997.11637931,57689580.073275864,1447970.3782327587,7416297.911907327,1510979.0638469828,5243644.576643535,856358.6193433191,8786234.215517242,7491885.200969827,35640837.61853448,1134695.415275862,3928438.4050377156,986613.9362877156,528380.1146620689,1530005.8332564652,16884390.137931034,2232258.8836206896,12157491.743534483,1997125.2215808188,649847.3144534483,18671084.040409483,964865.7867075433,32363645.330818966,1349372.6243525867,4402237.880118535,1776080.2386853448,738859.9413725862,9620173.85708513,10044663.143318966,822365.7296605604,4171455.8817349137,2717900.7610452585,1017103.4758890086,15340070.56788793,1558797.1633890087,602901.9715448273,14318983.467672413,1825632.658270474,1359411.4801993535,1578480.8389267242,6617307.387055389,2422718.827721336,2650558.991783405,6587545.1015625,1028614.643678161,10996189.04525862,1611705.0973265066,5882338.366109914,821831.7761359918,1184838.202014871,1398863.856411638,889373.7900006474,3353618.3502155175,1080117.3678609913,45388792.58728448,2580819.3464439656,1219440.7506745688,750726.6823814656,882628.1085668104,1890408.2886584052,12846984.440653015,2480975.2219827585,1858331.3818706896,4279173.940126725,1646267.9059829742,32830073.904094826,1156793.8287984913,5839518.4485452585,25295356.245689657,1246308.6901939656,54648721.97737069,2192030.3546269396,649516.8064549577,1521378.5189924568,2224579.8902288764,5117794.582570043,1143492.2813128594,33563084.515086204,5052060.977909483,943509.0821785565,6461419.316136854,855208.442349138,1728142.606479095,1210298.5331368535,2663313.4795258623,17607585.640625,3956924.413523707,1759346.7408405172,1733926.7462284483,17922796.799568966,993275.3696795258,1237213.6547056753,1414166.2972566453,5108716.852909483,9913379.96174569,1820874.5075465513,6035721.107758621,23834062.603448275,2809597.544450431,1173027.2995096997,2382838.9252760774,1090868.9622844828,9151845.197198275,1507265.0232691378,3242388.594288793,951372.23828125,865261.9011314656,906797.9712643677,1591657.448838362,1031634.5374461206,1024637.4529903018,566989.6776762936,13699773.74137931,1278809.9943426724,3793117.4445043104,1629481.4544719828,3845379.762122845,2179341.761280819,7275730.946659483,1342844.4445043104,7661299.4218077585,5095840.101427802,3247336.752424569,1989104.2489224137,743151.3254310344,1066560.4194504311,1241343.3671539333,7411108.924030173,18885595.740840517,870903.428744612,12078358.25538793,851400.3489652305,2728120.025323276,15908725.993534483,20977789.165948275,3280467.8594435346,4980504.326373923,4581480.8626077585,2189385.9348060344,2105051.4810447544,741392.2564232043,965791.423356681,1122090.770698277,11855492.47575431,2087189.0571120689,3346770.400727586,3072447.339439655,1984628.0006060344,4673692.276959051,1995431.7673771551,759533.4075969828,2125187.6599331885,2387777.636559694,1451955.0703125,4373971.09455819],"xaxis":"x","y":[2450488,1188444,1205617,579888.3125,3637973,2228675.5,5662234,783104.125,620104,21069722,3537977,2918553.25,5448975,770333,5907068,6388260,369291.5,1784385.25,873923712,1889990,1275236.125,646505.1875,1058986,533370.625,1384693.5,2196718,1295685.75,5702077,378714.4063,727374.6875,2989919.75,1066901,4155007.75,5253241,533329,1271021,1057256,9341427,2659770,1054995,3874632,13951571,1344512,6508595,1286817.375,398357,729535,5117308.5,1860901,827618,2017423,242805,1212883.75,1079477,4547129,8819864,499887,620483,4735249,478982,7046345.5,1191803,811703.6875,792734.8125,4420650.5,1847401,1073156.375,727703.5,729096.875,858897,4897760,1030150,1784796.5,870633.5,1614322,552015.625,6345009.5,2147013,4235405,2788860,1691963,4946798.5,7482832,4024810,1093653,630040,1582655.875,1835804,722062.875,994194.875,1964681.25,969682.8125,6695413,764906,3291518,13158205,3344833,44984468,12704014,524144.6875,25377650,6962255,872114.5,466672.6875,1381407,2919980.75,3481727.5,2992693,464155.1875,2372126.25,5965171,1811506.375,670779,15552193,5337334,896403,4039667,4784731,749207,8381834.5,25970248,900973,1436773.875,1200381,2225265.5,5348309,966812.125,9791557,777094,5471735,662424,939413.625,17924416,454296,452744,11441,4188660.5,1686262.25,1767480,342726.0938,1987334.75,666385,8611054,410433.0938,1501111.75,1210229,4268054,914670,1509492,1185469,10750010,572654.3125,9290214,4145920.5,1332591.5,549509,1134195.5,1682454.875,2364351,7475577.5,3418139,1035592.813,567608.625,6015128,7380088,102673696,2177075,875789,1972234,618876.1875,6010407,1189402,2916511,3757006,974305.625,505601,1325597.625,4189569.25,3229864.5,1508561.875,15878689,5528223,1605522,1747090.375,19490284,0,14585813,5696695,541950.875,5968153.5,1346952,612158.1875,1076084,731092,2804534,690529.5,2190460,1829122.375,7240274,947884,9162048,2470566,690557.6875,25959642,703847.375,207572,3155391,948975,666687.5,1247361,986353.125,53166156,1419980,1171380,888308,15590743,830945,1074350.375,3805877,2108012,659450,3693616,4696639,2433777,804620.5,5258024.5,2420605.5,13171370,761504.875,4054994.25,858932,3267564,776237.375,12095818,3556291,887403,6797528,2055534,1983256,566183,1989287,1045413,1169932.75,2964206,3782378.25,802312.5,10062436,1034941.313,12086616,1138520,14168729,2173143.75,1441631,1742351.25,1222949.25,2478960,1913558.375,473874.0938,3518459.5,2920918,19645206,36667044,98960776,13010578,2280352.5,1228041,4961405,3157579,809291.8125,2288635,6293359,5454482,7739699,2077039.375,3577120,1574750.875,708967,2458760.5,748335.3125,686949,1465795.75,1244150,2557533,1686405,2186148.5,1194835.5,3744483,4476997,8355988.5,6914839.5,1738724,90558.70313,3087318,965425.1875,913899,2292169,490743,601590,1117658.75,1082531,326778.6875,1821047,230365,809870.8125,387810,678608,1297872,684234.875,1114384.625,742630,2454812,10420096,694241,1966424,3663534.5,515025,592534,600060,2001643.75,1866800,2360288.5,6859198,955641,4358944.5,1034827.313,6596057.5,1784233,1886382,1100575,1632731,835085,738990,4385512,1227772,77164568,549834,6609358,1789710,1375366,5104947,1883864,2137686,2208196,3004271,866096.6875,1679127,498745,3717064,1446455,710825.1875,8259071,16644664,924438.125,542882,711118.375,219483,1041652.813,0,318205,905750.375,1024033,1696448,694479,5457975,5885427.5,13093350,1270139,10213132,1200811,1076644.75,581589,1967637,2912264,967697,4700396,7369591,489557.0938,1529510.25,8320719,1130960,807358,3837228,4988493.5,12719042,2996958,5238803,958725,1529309,1256575.875,11319836,5767765,6036588,683274,16371764,2126288,920068.875,942089,1497496.5,1358022,529807,2912140,1017787.125,685888.1875,5252131,1817550,7338511,17109052,961476.1875,688641.8125,2211671,3864699,647906.1875,1938887,4547379,1750419,648634.875,1739693.25,2962476,3893073.5,2338238,809257,719471.1875,934357,408513,3258819.5,1911591,2890253,468396.9063,1868497.125,1072487.875,2151376.5,8168547,2472350.25,1417778,2337651.25,5917000,5653656,3059758,2266983,2676594.75,1379739,7637986,1128179.125,1472548,3371455,3091559,7003615,4059424.75,1318096,2156407,1765813.625,455798.9063,641713,1027394.313,648879.3125,1947717,896485,2248776,3404992,2158629,1380293.625,7067404,32579658,1489363.875,1359579,3017709.25,10154608,3546459,892227,2439484.75,3040924.25,317581,32381836,1401315.875,1921222,1494727,1703504.875,2563985.5,1762174,4078339,6454983,684155,10403123,3240830,508014.5938,1022957.688,654625.5,20311228,1481698,1093757.75,18661606,293217,922041,8381744,2127534,2694853,362874,9586556,609494,1133028.875,502667.6875,1457118,2265836,962238,815800,1259456,999924.3125,10317852,4022735,877788,6578370,2477499,51168308,11788719,12783255,1677877,989079.5,1431784,928409,1521202,4685346,2148565,538740,1889761.125,3605749.75,1443367.75,4206839.5,578854.125,739662,3740373.75,1385697,608755.375,1634178.25,8696206,137635696,1364344,3590731,3605855.5,5357833,1176342.25,716940.625,47148456,6320060,1045640,1275231.75,783345.6875,1929011,2307938,22055140,759060.8125,924121,2361088,1493131.75,5261096,755618.1875,2735128,688375.1875,8493420,1858224.625,551219,562617,10303305,657385.8125,1645748.25,4760734,1612105.75,3366528,1245923.75,1093743,2468322.5,816510.875,7682562.5,1607972.25,2512443,2069880,1634334,1334125.125,625938,3158151,405062,40062888,287132.5,1192100,4934559,956481,7520554,818317,2003474,827211,1967129,2429021,1006989.688,2616226,454124.3125,872555,268269,1085439,1055624,3191228.25,53401,1012341,2140476,3265995,811900,713326.875,6935723,204499.0938,12525174,2189735,1585441,14361382,1750362.25,1824054.5,1137513.625,1150926,593824,555864,1761137.5,497991,2470642,4321141.5,2274873.5,12731388,1898509.75,1245753,4486477,990361.5,4560492,5707529.5,3643112,1198630,3151107.75,966376,1104074.625,895310.5,6750112,2225134,3775187.5,431390.4063,976537,5415438,306721,2188452,2234448,3928695.25,4983934,2023197,2028125.625,1593610.125,3436032,85357952,591249,505949.0938,833747,25476332,1186682,1307883,965786,876871,1888344.25,758875,8651835,2442220,3863772,3947209.25,2014500.375,4424263,6127771.5,522791.0938,2093011,422133,2646130,680395.875,35383012,6313678,681354.1875,841242,1507168,784584,2025533,7439009,2090543.75,37980712,3239754,730211.1875,599390,3895403,861972.875,162960,580688,930439.375,1453543,642052,1482997,819354.625,4294550,2024367.75,1525624,157606480,869699,5903033,8926736,3289053,1820292,2277316,1844825,94186,869835,2491238.25,4407932,637685,9155568,4912520,3999505.25,6983736,18848780,3303036,6361236.5,25397086,862541,752441,1551204,2056762,802755,136241424,13962749,3312125,2114860.25,3568237,1109924,3542542,46510960,17044842,10960107,2471623,3132453,597761,1107290.625,658297.5,9029489,1015172,2701872,493969.6875,1886007,3226884,1216543,1469556.25,789209.8125,2081072.625,6884914,1623657,393493,645927,5672838.5,2401890,624993,3485865.5,3005926,8024064.5,9373179,1872413,145573,1834570,714898,876237,2015117.25,958242.875,1976856,10077532,462465.5,694189,1056099,1418915.875,2485521,2756494,1271753.25,1176615,1096646.25,1943933.625,534039,849504,2322314,0,2466100,3629065.75,1188994.625,1822834.25,1169948.875,2122581.25,2400507.25,7175646,3996541.5,350264.5938,14709151,2067352.5,415586,1559199.25,832168,1207869.875,1041304,577033.875,1677881,796042.375,813641,1085183,871355.5,5169726,661383,828531,6102883,2227640,2890379,507603.3125,602191.1875,466904,2767381,847419.375,1618695,436848,4425832.5,439065.9063,28450436,72587024,781682.5,5876439,642133,2407904.5,6058494,611536,1926134.875,2556131,1639849.25,696522,1054581.375,872326.375,677536.3125,3751417,818836.125,1334876,1287159.25,2334260,16163194,1172785,2887392,474468.5938,1153690.125,1228963.75,447947,1332912.625,0,443365,8504904,929688,3573328.25,1293336,2075833,1059605.625,1507270,1419453,586827.5,1360340,13659926,1971236,1364214.75,609508,1790148.625,2245568.25,2855545,3944933.5,286685536,4767227,815115.5,1261634.5,1845150.625,1698074.625,61576184,1226435,716071.625,5249190,1128208.125,863703,2301043.75,1080203,2376794,11325513,941564.8125,41655852,387279,1734533,1540235,570982,1689631,8909924,700899,686019.1875,8580279,938908,4167253.75,4222682,6118300.5,7323351,3185005.25,7256842,53079164,9569715,3644982.25,238255,1614026.25,1655612,1570758.25,11594121,1929118,3021669,6814269,841841,1258619.125,803938,938096.1875,961749,2058532.875,2220053.25,1313366,1297633.75,65336980,541542.6875,3608955.75,1983509,3218824,363808.1875,770175.8125,592221,3791402,23364636,49102164,1625308.625,497528,4202335,9986032,709990.8125,1423463,567521,8748273,3373582,846389.3125,1000804.625,2998208.25,2153603.5,2660396,1289073,1247972,4253369.5,2105451,13586070,935580.3125,1082920.125,3321035.75,3288836,3560614.25,512166,51625548,2615973.5,1251906,555153.125,810636.875,48729588,548292.6875,24473814,785052.875,13300859,571309,3570161.75,522972.1875,2473302,2098850,6881405.5,2306723.5,2876857,5089903,10564729,5039523,1730886,1633399.375,1696978.75,1739286.75,395346.3125,349139,6040837,945020.1875,1764767.25,914023,1232822.875,1625695.75,2545405,5807879,1392015,6283226,2662054.25,4133811,1582794.5,1259959,964015,6424029,12448381,873711.125,13911761,1475475.5,10498344,1788148,618366,776991,1331122.125,5903142,634751,520405,11026945,5981430,2203026.5,2166711,1427974.625,2654755,1050982,2581741.75,1639136.25,1402048.5,3165109,729397.1875,937635,868859.8125,4409881.5,705262.125,3542708.5,2729167.75,2628864,813038,2773779,21781324,2161828,1199392,832986.875,602245,1074989,35685224,342493,94178648,1136676,741287.1875,882896.5,1505709,0,1803753.25,3220635.25,8201974,2035595,12265516,3306123,313528.0938,241478.5938,2465266,1270582.125,1982967,824516,533078,19344782,1172065,511180.6875,255971.2031,758008.875,1911611,2503948,636641,677279,7838600,499928,2921368,22591288,964923.8125,2278160,17186624,591622.8125,919746.3125,1717130.875,1815350,3024478.5,5975230,860364.1875,600543,3275779.25,7261732,8921778,1976400,5572503,6159684,2960377.25,13270687,7370743,1323038.75,291614432,1425599,689803,439266.9063,3020001.5,2168701.5,1234095.5,2219744,3045139,638201,1943772,855393,41213584,3674705,2712983.75,1201630.125,648022,556700.6875,816846,196410,9977330,7419765,25453690,1120197,2483320,1358226,1441665.375,33268410,5535626,6471427,1437569,2128963.5,1581425.75,1231546,5037781,1581036.75,5392617.5,4180298,4817062,687953.6875,3743228.5,578789,1519952.125,2355732,7969653,431357,5094197,65047284,10513817,2599171.75,842593,10105379,45074876,2391566.25,1004253,1827424.125,652991.6875,3534690.75,7657568,64715652,1073507,9671619,1540258.5,8163413,520206.6875,6744317,6577130,63668488,695346,2666634,676032.625,571644,1103594.75,16614404,2885740.5,9851937,1743198,572082,40613740,861007.8125,47102428,529033.5,41680640,1398253,987989,4590246.5,7976729,786346,9146727,1688612.5,801264,18628834,1108310.875,500237,17673018,978211,1921890,2926089.5,8839812,1634669.375,1115020,5121586,804002,16246106,3612526.75,10340282,881386.1875,920430.625,1183623,323739.4063,2993032.25,788055,19150024,1929828,2248134.5,669117,495152.6875,941257.3125,21795830,1513700.625,445943.8125,2843033.5,460010,16020642,812013,5838129,18967014,1038351.875,20657852,1808401.375,588492.5,1380283,2345752.5,14833476,1541879,19827916,4396566,899266.125,8354235,466985,1280263.75,1011296,1325972.75,11821841,5481549,537141,1037696,17750994,792015,697397.875,1736775,3616057,8070420,9159185,3730953.5,18576900,1356972.25,656980,1487769.5,706961,1916309,4038090,4027134.75,868163,3767135,517029,1208168.75,864765,783208.625,535649.875,14751117,1896858,1757601.25,1116705,1920352.125,2755578,4828599,873648,13440924,1730431,3441643.25,2178074.5,690750,573395,936616.5,2762846,6821833,897514,18074716,228212,2549892,22530168,19991412,3215953,3245111.5,6057734,3234787.75,1928391.25,416566.1875,741850,1045337,11963428,3035962.25,1855556,5040382,2096835.5,3002337,797170.3125,892455.875,1751483.875,8054334,686055,1381590.125],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","SiteEnergyUse_predRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train.ravel(),\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","print(ScoresRF)\n","figRF.show()\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[11800362.100072604,11710542.12376512,11012697.372418003,10049162.831608992,9808730.528398992,9551753.238072116,9571579.788772048,9542329.21898698,9513529.776652798,9523532.033538114]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[15496185.623419976,15517804.577616317,14757938.113749348,14373193.493618263,14083304.90258823,14166067.023332015,13998270.982017554,14023861.276196241,13932615.663700985,13942428.182765953]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[8104538.5767252315,7903279.669913923,7267456.631086659,5725132.169599721,5534156.154209754,4937439.452812217,5144888.595526543,5060797.161777721,5094443.889604611,5104635.884310274]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[10749596.671605939,9708352.614919398,8131881.928952679,8071176.2869982,8027899.159801702,6743197.119152374,7300781.052568851,7006328.698124266,7124148.556896352,7224028.533155321],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[17204689.140435703,17145861.657223452,14714065.201249994,16159435.879562693,15812863.387115749,15810933.494092342,15757334.152509635,15919838.805825243,15952149.632236525,15833752.800795866],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[14610030.001671068,15324718.837478258,16385421.186880626,13967700.23195764,13842237.604287505,14404010.1599539,13939450.44279892,13839730.173829004,13539889.326510353,13693147.444661113],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[6729697.883937806,7469171.452167802,8128689.701699527,4527836.407489805,4736309.4354863735,4569403.74305896,4663032.045273037,4543321.198583083,4565396.164401934,4516305.344708116],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[9707796.802712504,8904606.057036687,7703428.843307189,7519665.352036615,6624343.055303627,6231221.674103,6197301.250709793,6402427.218573309,6386065.203218824,6350426.044370154],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=sqrt<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF = visuRMSEGrid(RandomForestRegressor(), 'RF', n_estimatorsRF,\n","                             'n estimators', GridRF, BestParametresRF,\n","                             'randomforestregressor__max_features')\n","FigRMSEGRidRF.show()\n","if write_data is True:\n","    FigRMSEGRidRF.write_image('./Figures/ConsoGraphRMSERF.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                   6\n","1          adaboostregressor__loss              linear\n","                           R²          RMSE           MAE\n","AdaBoostRegressor()  0.279939  2.453079e+07  4.238622e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predAB=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,10032879.898373984,2654921.321812808,2654921.321812808,97366625,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,33155230.79139073,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,10032879.898373984,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,10032879.898373984,17450414.36375839,5310821.426722885,46835558.210191086,2868189.654849948,2654921.321812808,17450414.36375839,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,17450414.36375839,5310821.426722885,2654921.321812808,5310821.426722885,10032879.898373984,2654921.321812808,5310821.426722885,46835558.210191086,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,46835558.210191086,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2868189.654849948,5310821.426722885,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,22190563.32150655,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,5310821.426722885,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,22190563.32150655,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,46835558.210191086,26438853.312340155,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2868189.654849948,17450414.36375839,17450414.36375839,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,59312477.12,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,130221230.98706897,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,22190563.32150655,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2868189.654849948,2868189.654849948,2654921.321812808,5310821.426722885,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,46835558.210191086,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,5310821.426722885,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2868189.654849948,2654921.321812808,5310821.426722885,5310821.426722885,10032879.898373984,17450414.36375839,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2868189.654849948,2868189.654849948,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,46835558.210191086,2654921.321812808,17450414.36375839,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,59925572.07339449,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,46835558.210191086,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,5310821.426722885,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2868189.654849948,17450414.36375839,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,5310821.426722885,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,59925572.07339449,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,48258102.791366905,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,130221230.98706897,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,46835558.210191086,5310821.426722885,5310821.426722885,46835558.210191086,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,26438853.312340155,2654921.321812808,10032879.898373984,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,46835558.210191086,17450414.36375839,2654921.321812808,2868189.654849948,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,46835558.210191086,67893683.2031496,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,17450414.36375839,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,69343109.6226415,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,59312477.12,5310821.426722885,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,59925572.07339449,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2868189.654849948,10032879.898373984,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,5310821.426722885,5310821.426722885,2654921.321812808,48258102.791366905,33155230.79139073,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,5310821.426722885,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,10032879.898373984,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,5310821.426722885,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,48258102.791366905,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,22190563.32150655,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,5310821.426722885,22190563.32150655,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,17450414.36375839,2654921.321812808,17450414.36375839,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2868189.654849948,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,59925572.07339449,2654921.321812808,26438853.312340155,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,5310821.426722885,26438853.312340155,2868189.654849948,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2868189.654849948,26438853.312340155,2654921.321812808,2868189.654849948,17450414.36375839,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,10032879.898373984,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,33155230.79139073,17450414.36375839,2654921.321812808,69343109.6226415,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,59925572.07339449,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,22190563.32150655,2868189.654849948,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,5310821.426722885,10032879.898373984,5310821.426722885,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,5310821.426722885,17450414.36375839,2654921.321812808,2654921.321812808,46835558.210191086,2868189.654849948,5310821.426722885,2654921.321812808,5310821.426722885,48258102.791366905,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,5310821.426722885,59312477.12,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,26438853.312340155,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,17450414.36375839,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,5310821.426722885,2868189.654849948,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,17450414.36375839,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,46835558.210191086,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2868189.654849948,2654921.321812808,2868189.654849948,2654921.321812808,46835558.210191086,2654921.321812808,5310821.426722885,22190563.32150655,2654921.321812808,67893683.2031496,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,46835558.210191086,5310821.426722885,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2868189.654849948,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,10032879.898373984,2654921.321812808,5310821.426722885,26438853.312340155,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,10032879.898373984,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948,2654921.321812808,10032879.898373984,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,17450414.36375839,2654921.321812808,10032879.898373984,2654921.321812808,2868189.654849948,10032879.898373984,17450414.36375839,2654921.321812808,5310821.426722885,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,17450414.36375839,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,5310821.426722885,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2654921.321812808,2868189.654849948],"xaxis":"x","y":[2450488,1188444,1205617,579888.3125,3637973,2228675.5,5662234,783104.125,620104,21069722,3537977,2918553.25,5448975,770333,5907068,6388260,369291.5,1784385.25,873923712,1889990,1275236.125,646505.1875,1058986,533370.625,1384693.5,2196718,1295685.75,5702077,378714.4063,727374.6875,2989919.75,1066901,4155007.75,5253241,533329,1271021,1057256,9341427,2659770,1054995,3874632,13951571,1344512,6508595,1286817.375,398357,729535,5117308.5,1860901,827618,2017423,242805,1212883.75,1079477,4547129,8819864,499887,620483,4735249,478982,7046345.5,1191803,811703.6875,792734.8125,4420650.5,1847401,1073156.375,727703.5,729096.875,858897,4897760,1030150,1784796.5,870633.5,1614322,552015.625,6345009.5,2147013,4235405,2788860,1691963,4946798.5,7482832,4024810,1093653,630040,1582655.875,1835804,722062.875,994194.875,1964681.25,969682.8125,6695413,764906,3291518,13158205,3344833,44984468,12704014,524144.6875,25377650,6962255,872114.5,466672.6875,1381407,2919980.75,3481727.5,2992693,464155.1875,2372126.25,5965171,1811506.375,670779,15552193,5337334,896403,4039667,4784731,749207,8381834.5,25970248,900973,1436773.875,1200381,2225265.5,5348309,966812.125,9791557,777094,5471735,662424,939413.625,17924416,454296,452744,11441,4188660.5,1686262.25,1767480,342726.0938,1987334.75,666385,8611054,410433.0938,1501111.75,1210229,4268054,914670,1509492,1185469,10750010,572654.3125,9290214,4145920.5,1332591.5,549509,1134195.5,1682454.875,2364351,7475577.5,3418139,1035592.813,567608.625,6015128,7380088,102673696,2177075,875789,1972234,618876.1875,6010407,1189402,2916511,3757006,974305.625,505601,1325597.625,4189569.25,3229864.5,1508561.875,15878689,5528223,1605522,1747090.375,19490284,0,14585813,5696695,541950.875,5968153.5,1346952,612158.1875,1076084,731092,2804534,690529.5,2190460,1829122.375,7240274,947884,9162048,2470566,690557.6875,25959642,703847.375,207572,3155391,948975,666687.5,1247361,986353.125,53166156,1419980,1171380,888308,15590743,830945,1074350.375,3805877,2108012,659450,3693616,4696639,2433777,804620.5,5258024.5,2420605.5,13171370,761504.875,4054994.25,858932,3267564,776237.375,12095818,3556291,887403,6797528,2055534,1983256,566183,1989287,1045413,1169932.75,2964206,3782378.25,802312.5,10062436,1034941.313,12086616,1138520,14168729,2173143.75,1441631,1742351.25,1222949.25,2478960,1913558.375,473874.0938,3518459.5,2920918,19645206,36667044,98960776,13010578,2280352.5,1228041,4961405,3157579,809291.8125,2288635,6293359,5454482,7739699,2077039.375,3577120,1574750.875,708967,2458760.5,748335.3125,686949,1465795.75,1244150,2557533,1686405,2186148.5,1194835.5,3744483,4476997,8355988.5,6914839.5,1738724,90558.70313,3087318,965425.1875,913899,2292169,490743,601590,1117658.75,1082531,326778.6875,1821047,230365,809870.8125,387810,678608,1297872,684234.875,1114384.625,742630,2454812,10420096,694241,1966424,3663534.5,515025,592534,600060,2001643.75,1866800,2360288.5,6859198,955641,4358944.5,1034827.313,6596057.5,1784233,1886382,1100575,1632731,835085,738990,4385512,1227772,77164568,549834,6609358,1789710,1375366,5104947,1883864,2137686,2208196,3004271,866096.6875,1679127,498745,3717064,1446455,710825.1875,8259071,16644664,924438.125,542882,711118.375,219483,1041652.813,0,318205,905750.375,1024033,1696448,694479,5457975,5885427.5,13093350,1270139,10213132,1200811,1076644.75,581589,1967637,2912264,967697,4700396,7369591,489557.0938,1529510.25,8320719,1130960,807358,3837228,4988493.5,12719042,2996958,5238803,958725,1529309,1256575.875,11319836,5767765,6036588,683274,16371764,2126288,920068.875,942089,1497496.5,1358022,529807,2912140,1017787.125,685888.1875,5252131,1817550,7338511,17109052,961476.1875,688641.8125,2211671,3864699,647906.1875,1938887,4547379,1750419,648634.875,1739693.25,2962476,3893073.5,2338238,809257,719471.1875,934357,408513,3258819.5,1911591,2890253,468396.9063,1868497.125,1072487.875,2151376.5,8168547,2472350.25,1417778,2337651.25,5917000,5653656,3059758,2266983,2676594.75,1379739,7637986,1128179.125,1472548,3371455,3091559,7003615,4059424.75,1318096,2156407,1765813.625,455798.9063,641713,1027394.313,648879.3125,1947717,896485,2248776,3404992,2158629,1380293.625,7067404,32579658,1489363.875,1359579,3017709.25,10154608,3546459,892227,2439484.75,3040924.25,317581,32381836,1401315.875,1921222,1494727,1703504.875,2563985.5,1762174,4078339,6454983,684155,10403123,3240830,508014.5938,1022957.688,654625.5,20311228,1481698,1093757.75,18661606,293217,922041,8381744,2127534,2694853,362874,9586556,609494,1133028.875,502667.6875,1457118,2265836,962238,815800,1259456,999924.3125,10317852,4022735,877788,6578370,2477499,51168308,11788719,12783255,1677877,989079.5,1431784,928409,1521202,4685346,2148565,538740,1889761.125,3605749.75,1443367.75,4206839.5,578854.125,739662,3740373.75,1385697,608755.375,1634178.25,8696206,137635696,1364344,3590731,3605855.5,5357833,1176342.25,716940.625,47148456,6320060,1045640,1275231.75,783345.6875,1929011,2307938,22055140,759060.8125,924121,2361088,1493131.75,5261096,755618.1875,2735128,688375.1875,8493420,1858224.625,551219,562617,10303305,657385.8125,1645748.25,4760734,1612105.75,3366528,1245923.75,1093743,2468322.5,816510.875,7682562.5,1607972.25,2512443,2069880,1634334,1334125.125,625938,3158151,405062,40062888,287132.5,1192100,4934559,956481,7520554,818317,2003474,827211,1967129,2429021,1006989.688,2616226,454124.3125,872555,268269,1085439,1055624,3191228.25,53401,1012341,2140476,3265995,811900,713326.875,6935723,204499.0938,12525174,2189735,1585441,14361382,1750362.25,1824054.5,1137513.625,1150926,593824,555864,1761137.5,497991,2470642,4321141.5,2274873.5,12731388,1898509.75,1245753,4486477,990361.5,4560492,5707529.5,3643112,1198630,3151107.75,966376,1104074.625,895310.5,6750112,2225134,3775187.5,431390.4063,976537,5415438,306721,2188452,2234448,3928695.25,4983934,2023197,2028125.625,1593610.125,3436032,85357952,591249,505949.0938,833747,25476332,1186682,1307883,965786,876871,1888344.25,758875,8651835,2442220,3863772,3947209.25,2014500.375,4424263,6127771.5,522791.0938,2093011,422133,2646130,680395.875,35383012,6313678,681354.1875,841242,1507168,784584,2025533,7439009,2090543.75,37980712,3239754,730211.1875,599390,3895403,861972.875,162960,580688,930439.375,1453543,642052,1482997,819354.625,4294550,2024367.75,1525624,157606480,869699,5903033,8926736,3289053,1820292,2277316,1844825,94186,869835,2491238.25,4407932,637685,9155568,4912520,3999505.25,6983736,18848780,3303036,6361236.5,25397086,862541,752441,1551204,2056762,802755,136241424,13962749,3312125,2114860.25,3568237,1109924,3542542,46510960,17044842,10960107,2471623,3132453,597761,1107290.625,658297.5,9029489,1015172,2701872,493969.6875,1886007,3226884,1216543,1469556.25,789209.8125,2081072.625,6884914,1623657,393493,645927,5672838.5,2401890,624993,3485865.5,3005926,8024064.5,9373179,1872413,145573,1834570,714898,876237,2015117.25,958242.875,1976856,10077532,462465.5,694189,1056099,1418915.875,2485521,2756494,1271753.25,1176615,1096646.25,1943933.625,534039,849504,2322314,0,2466100,3629065.75,1188994.625,1822834.25,1169948.875,2122581.25,2400507.25,7175646,3996541.5,350264.5938,14709151,2067352.5,415586,1559199.25,832168,1207869.875,1041304,577033.875,1677881,796042.375,813641,1085183,871355.5,5169726,661383,828531,6102883,2227640,2890379,507603.3125,602191.1875,466904,2767381,847419.375,1618695,436848,4425832.5,439065.9063,28450436,72587024,781682.5,5876439,642133,2407904.5,6058494,611536,1926134.875,2556131,1639849.25,696522,1054581.375,872326.375,677536.3125,3751417,818836.125,1334876,1287159.25,2334260,16163194,1172785,2887392,474468.5938,1153690.125,1228963.75,447947,1332912.625,0,443365,8504904,929688,3573328.25,1293336,2075833,1059605.625,1507270,1419453,586827.5,1360340,13659926,1971236,1364214.75,609508,1790148.625,2245568.25,2855545,3944933.5,286685536,4767227,815115.5,1261634.5,1845150.625,1698074.625,61576184,1226435,716071.625,5249190,1128208.125,863703,2301043.75,1080203,2376794,11325513,941564.8125,41655852,387279,1734533,1540235,570982,1689631,8909924,700899,686019.1875,8580279,938908,4167253.75,4222682,6118300.5,7323351,3185005.25,7256842,53079164,9569715,3644982.25,238255,1614026.25,1655612,1570758.25,11594121,1929118,3021669,6814269,841841,1258619.125,803938,938096.1875,961749,2058532.875,2220053.25,1313366,1297633.75,65336980,541542.6875,3608955.75,1983509,3218824,363808.1875,770175.8125,592221,3791402,23364636,49102164,1625308.625,497528,4202335,9986032,709990.8125,1423463,567521,8748273,3373582,846389.3125,1000804.625,2998208.25,2153603.5,2660396,1289073,1247972,4253369.5,2105451,13586070,935580.3125,1082920.125,3321035.75,3288836,3560614.25,512166,51625548,2615973.5,1251906,555153.125,810636.875,48729588,548292.6875,24473814,785052.875,13300859,571309,3570161.75,522972.1875,2473302,2098850,6881405.5,2306723.5,2876857,5089903,10564729,5039523,1730886,1633399.375,1696978.75,1739286.75,395346.3125,349139,6040837,945020.1875,1764767.25,914023,1232822.875,1625695.75,2545405,5807879,1392015,6283226,2662054.25,4133811,1582794.5,1259959,964015,6424029,12448381,873711.125,13911761,1475475.5,10498344,1788148,618366,776991,1331122.125,5903142,634751,520405,11026945,5981430,2203026.5,2166711,1427974.625,2654755,1050982,2581741.75,1639136.25,1402048.5,3165109,729397.1875,937635,868859.8125,4409881.5,705262.125,3542708.5,2729167.75,2628864,813038,2773779,21781324,2161828,1199392,832986.875,602245,1074989,35685224,342493,94178648,1136676,741287.1875,882896.5,1505709,0,1803753.25,3220635.25,8201974,2035595,12265516,3306123,313528.0938,241478.5938,2465266,1270582.125,1982967,824516,533078,19344782,1172065,511180.6875,255971.2031,758008.875,1911611,2503948,636641,677279,7838600,499928,2921368,22591288,964923.8125,2278160,17186624,591622.8125,919746.3125,1717130.875,1815350,3024478.5,5975230,860364.1875,600543,3275779.25,7261732,8921778,1976400,5572503,6159684,2960377.25,13270687,7370743,1323038.75,291614432,1425599,689803,439266.9063,3020001.5,2168701.5,1234095.5,2219744,3045139,638201,1943772,855393,41213584,3674705,2712983.75,1201630.125,648022,556700.6875,816846,196410,9977330,7419765,25453690,1120197,2483320,1358226,1441665.375,33268410,5535626,6471427,1437569,2128963.5,1581425.75,1231546,5037781,1581036.75,5392617.5,4180298,4817062,687953.6875,3743228.5,578789,1519952.125,2355732,7969653,431357,5094197,65047284,10513817,2599171.75,842593,10105379,45074876,2391566.25,1004253,1827424.125,652991.6875,3534690.75,7657568,64715652,1073507,9671619,1540258.5,8163413,520206.6875,6744317,6577130,63668488,695346,2666634,676032.625,571644,1103594.75,16614404,2885740.5,9851937,1743198,572082,40613740,861007.8125,47102428,529033.5,41680640,1398253,987989,4590246.5,7976729,786346,9146727,1688612.5,801264,18628834,1108310.875,500237,17673018,978211,1921890,2926089.5,8839812,1634669.375,1115020,5121586,804002,16246106,3612526.75,10340282,881386.1875,920430.625,1183623,323739.4063,2993032.25,788055,19150024,1929828,2248134.5,669117,495152.6875,941257.3125,21795830,1513700.625,445943.8125,2843033.5,460010,16020642,812013,5838129,18967014,1038351.875,20657852,1808401.375,588492.5,1380283,2345752.5,14833476,1541879,19827916,4396566,899266.125,8354235,466985,1280263.75,1011296,1325972.75,11821841,5481549,537141,1037696,17750994,792015,697397.875,1736775,3616057,8070420,9159185,3730953.5,18576900,1356972.25,656980,1487769.5,706961,1916309,4038090,4027134.75,868163,3767135,517029,1208168.75,864765,783208.625,535649.875,14751117,1896858,1757601.25,1116705,1920352.125,2755578,4828599,873648,13440924,1730431,3441643.25,2178074.5,690750,573395,936616.5,2762846,6821833,897514,18074716,228212,2549892,22530168,19991412,3215953,3245111.5,6057734,3234787.75,1928391.25,416566.1875,741850,1045337,11963428,3035962.25,1855556,5040382,2096835.5,3002337,797170.3125,892455.875,1751483.875,8054334,686055,1381590.125],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predAB"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB, \\\n","BestParametresAB, \\\n","ScoresAB, \\\n","SiteEnergyUse_predAB, \\\n","figAB = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train.ravel(),\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predAB',\n","                         score=score,\n","                         param_grid=param_gridAB)\n","\n","print(BestParametresAB)\n","print(ScoresAB)\n","figAB.show()\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[15012106.226164613,12537828.497672828,12706926.821613032,12061435.772477183,15053131.079900011,12567741.072301408,11284319.828928888,11516635.862168208,12460267.588464253,10651124.082718892,11352390.812817376,10725647.90951997,9883514.09705178,10659910.864300031,10974855.216534302,10822454.095229864,11319721.924120054,11408138.435555484,10721783.001743466,11422664.225801865,10747790.457280135,11311975.920186559,12172185.112634722,12976587.893086005,12803316.04631779,11908224.20901752,14053707.488490501,15422682.62441869,14501911.740205133,15276990.157323051]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[20809135.85493984,16398905.979537502,17069182.506111097,16893200.920600425,20521426.500788365,17069058.209287766,16485939.596708803,16970674.224931285,16344006.293896679,15028767.144606758,14963448.205568079,15453786.733287552,13833526.228430346,14872531.321648613,15995471.792650469,16087170.440666072,15652262.204823224,16287570.685996072,15475837.668210518,16024800.226466447,15229184.241790794,15705346.302770117,16723758.398749476,17321586.051423334,18541848.398994412,16099467.666253667,19365112.215514675,21386407.569881916,20352461.88487009,19065950.670618918]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[9215076.597389385,8676751.015808154,8344671.137114967,7229670.624353942,9584835.659011658,8066423.935315051,6082700.061148973,6062597.499405133,8576528.883031826,6273481.020831027,7741333.420066674,5997509.085752388,5933501.965673216,6447290.406951451,5954238.640418136,5557737.749793655,6987181.643416883,6528706.185114896,5967728.335276414,6820528.225137283,6266396.672769477,6918605.537603,7620611.826519968,8631589.734748676,7064783.693641167,7716980.751781373,8742302.761466328,9458957.678955466,8651361.595540177,11488029.644027187]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[9678069.265657611,15144290.216391418,9808066.49438925,9389373.081288176,9889209.478010466,9830711.613418916,6497845.49288103,8164645.273835861,9179711.393382898,7925405.565636848,10210872.16034365,8390787.986560501,7752561.270408254,6779512.767756911,7079680.6877224175,7188932.619624827,9262087.730156979,8041259.482357261,7410744.271037537,8230225.136753798,7851461.133765049,7422706.01101786,9409063.17944846,9845204.8641978,7637288.316443593,9769197.457117174,10573715.68523936,11615515.798472296,7584241.755005211,14764242.507694172],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[17740237.626952015,17218214.361669466,17867385.03170469,17372744.93651777,17298871.9068341,18416215.56995337,18553090.902082156,16363359.788097445,16195853.696979424,15841807.016405387,16505056.315907564,16872551.274215274,15711348.149962364,15777108.697325671,16257029.862410903,16046060.733172726,16175420.199432436,16005743.097264644,16158078.704904659,15994336.013629144,16229581.831998399,15645515.50506327,16753862.32275844,15785127.731978288,20219516.655187014,17004414.668537684,22160348.33744395,22782987.567585655,16215596.729304405,20749325.921208736],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[24428194.46596565,14002877.74033556,17794174.443918984,18069815.83855797,24345454.05581797,13580384.007095817,16159937.154503804,19599492.788261596,18107766.750917826,15902071.327367987,14009459.980142327,15958020.05044954,13466806.632500738,15762948.268005496,17822456.769027088,18341504.70733942,16797678.765357964,18466729.677907415,16877959.408951428,17917856.533828318,16143637.355507912,17332340.597787373,18539919.16254493,18914563.252649546,19260544.074351117,16495567.651692552,16973927.91917792,22380421.317364383,24532489.301943142,17105800.348933954],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[14791437.874728912,6519363.148494279,7131215.078668566,5751609.565675797,14226115.613350328,5501508.8841264015,5519963.845721411,5542850.25003434,9713799.52869264,5360344.155103148,6011658.541920405,5841468.425792386,5953669.474665105,6668440.618195759,5817068.660427668,6076382.9681528695,6236039.772300634,6170238.257708487,6194211.660409461,6586668.973894759,6046544.323868445,6427086.306321053,8750525.600621361,6585923.758577672,7153335.385270958,6163345.224346569,6659373.327774889,8507112.500141572,10076983.459378276,14638332.466169477],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[8422591.89751887,9804397.021473417,10933793.059383672,9723635.440346198,9506004.345487189,15509885.286912542,9690761.749456039,7912831.2106118,9104206.572348475,8225992.349081094,10024907.065772926,6565411.810582146,6533184.957722443,8311543.970216329,7898040.1030834345,6459389.447859481,8127383.15335226,8356721.6625396125,6967920.963414242,8384234.470903299,7467727.641260872,9732231.180743234,7407555.297800424,13752119.858026713,9745895.800336272,10108596.04339363,13901172.172816392,11827375.938529555,14100247.455394648,9127249.542608919],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=linear<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB,\n","                             'n estimators', GridAB, BestParametresAB,\n","                             'adaboostregressor__loss')\n","FigRMSEGRidAB.show()\n","if write_data is True:\n","    FigRMSEGRidAB.write_image('./Figures/ConsoGraphRMSEAB.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.8 Modèle GradientBoostRegressor"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 paramètre GradientBoostingRegressor()\n","0  gradientboostingregressor__n_estimators                         599\n","1          gradientboostingregressor__loss              absolute_error\n","                                   R²          RMSE           MAE\n","GradientBoostingRegressor()  0.348033  2.334208e+07  3.212301e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predGB=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[2150882.3357131938,1490617.8809581997,859893.1208193115,792295.4715988751,3761421.6351100365,1176629.797374388,3607120.1150080385,491269.56750988186,1252528.3011609504,4012912.392458764,1457332.1452377595,2344615.8840128323,1661034.6504707828,883231.7370264683,8372917.741326731,7192462.523191694,1356908.8637845123,1422782.0141529187,117598136.50858723,1572594.2472320942,1731189.1634856386,687720.2115480459,955138.3681504441,649078.2015753306,2148554.530865632,2031264.1581376565,1052419.1531606151,3899980.6020654645,1560138.8769420346,615230.8111953518,1517173.2539045454,1540931.577843189,3588308.2848270936,6115354.915737862,667934.6729433137,1679670.1968566899,1467506.9158282469,13821387.006263249,1273921.5933805734,959713.5950871472,1699078.4131344433,5327388.766882226,706286.1802328619,9648767.182465062,1237845.341291568,995938.9000359554,658103.7238607501,1303274.9777130708,694182.255032529,785091.1552399086,1724136.924068344,1050336.6373694215,1041340.9827222222,1678413.7457244245,3011614.387363296,6907076.732548581,777646.6493426071,2893580.697478516,1628093.4063292367,614191.58355618,7598114.00828502,956322.2160695527,1409652.3418462158,1305400.584163448,1048488.6227594268,2018984.5970400951,1781885.1434711243,863282.2265193484,905305.5142112604,1246647.5996727792,3859060.163739573,1562466.6817895959,2344615.8840128323,971638.2644946034,1906261.5426933442,612419.4744596275,2405649.2877908004,1557393.1914323734,2262778.422493348,818670.6798022812,1368492.6583896324,3718576.2593845204,1384746.4410319023,2028420.1621431306,1790199.4305071624,929000.7127667813,1967686.497040095,1002367.2815674193,796018.5202634841,854361.2655130319,1484454.5058479977,736474.7002421181,5465579.612495225,1273157.4788853128,5644838.562093062,15413313.343871344,3718254.7864921093,43164235.10857778,3365914.3253223887,748827.834898949,8879706.345885824,10945851.92079439,1224143.2543636777,951359.0494959996,2006225.1369141147,3557809.6540487474,1927455.2652670543,3822688.58021448,485244.1336684676,2148492.288572984,6685841.8230685815,1887547.5455590698,1107640.8703505334,17125080.883936945,3437271.086044281,1174097.064380734,5089422.686906092,6554486.701912995,573114.7955595247,4353646.649939235,27297711.92645628,991021.7743795096,1947021.8296300762,1226879.6083698699,787695.0498733543,2268360.544257031,1689875.0636415419,4511769.330065025,714929.2571872283,3360813.407491018,573123.3958879051,688133.6137792821,3651234.0438666334,674914.5828502806,538027.6647238497,1970014.3018876563,2213239.8034182945,1010710.2502313132,1699078.4131344433,1678011.0197442344,1098247.0513484336,707024.3856690585,2021312.4018876564,793659.0837798117,1505266.44538409,1290726.7094743534,6315001.633613272,976165.6923292271,888997.0051539902,656265.0851181165,4540314.259945061,611863.7787086187,2332066.9351747828,2977185.7862320384,1717263.8402686452,744577.0089474613,1366164.853542071,1092167.4049813824,2334394.7400223436,2405649.2877908004,3428043.577174656,1142712.8041717664,696815.602792276,1395470.3644570464,5690576.987112869,42508628.95041739,1787615.4510610818,768114.7859664572,1301742.2447161449,825661.9220133234,3640298.031134028,3074491.8094788156,1448281.8423343024,2344615.8840128323,1538166.7869781891,687984.9050823859,1494143.3191495931,1678011.0197442344,3810096.718070182,1774327.7946457446,17200351.884022873,1785576.5604996223,1329342.909596509,2614514.7267347993,5043089.10278588,1958514.3194880986,1947021.8296300762,10339231.010205321,765786.9811188962,2332066.9351747828,1741113.2933417957,739094.0774008356,1381581.9048534331,789743.7399017412,1224551.8035223086,2142449.137846032,1392675.9725727641,765786.9811188962,1583061.9664996907,898560.5365520673,9738470.443895705,987305.4107597586,1969050.1092210317,9141514.356542384,729232.0884667801,1246647.5996727792,3525427.4956414877,895431.426806471,865422.2511841686,1416826.0421454015,1224232.4008329741,7993629.264640557,1997022.7826708185,826794.5838255697,2218359.2953096107,17817644.117205855,820083.5348644437,825717.6756968304,3662186.766514525,861143.6551193363,695604.7577999668,3133644.972124937,4783792.815132016,1172429.3856464315,1328863.8829736484,8979587.725135282,1451850.3014922722,9405628.773868643,1361405.0488178527,2148492.288572984,930100.2803023836,2891252.892630955,1010026.1428212301,2029768.8672565545,3794599.551236026,986929.1833238906,5319199.325654287,665343.1778066879,2409244.9995578043,604377.4110636556,758734.0023659241,2032627.7703185931,1588780.1776755007,2378520.5872662505,3380616.134334416,712221.216451022,19434649.66713887,922281.8834847118,4527219.586961912,1171843.0590874434,13930367.959258448,1613873.1675830085,959289.9011834183,2327639.5147393793,1415428.58449348,1224551.8035223086,1113761.696557949,712221.216451022,2059065.8742995316,1852411.6831971726,8845238.519531753,36890602.542472295,19367756.68134935,1949349.6344776375,687720.2115480459,684898.5020955404,2526590.842139448,5412871.45873587,803128.735105947,1349188.48426862,1117975.417020518,5770379.969036097,9040176.298164893,1696750.608286882,713262.2860115516,1721809.1192207828,656265.0851181165,1993759.4112792148,864438.8839353587,671933.5369018851,956962.0963358573,688406.2112578767,3734394.10842467,994974.7073693307,1510251.258000123,1748048.7082868817,3681253.843335307,2803149.622890802,10014778.542873953,11943112.308684282,3647612.024496994,967345.8181405008,861278.8719094506,1816729.2982844005,905632.0733126936,1964455.4929356116,869113.6682126663,760156.6512389338,861233.8719661908,3348375.6204012027,833149.6296220889,1329342.909596509,1353691.7150951612,935725.0402777699,887593.233647107,690734.016105438,1160485.5305234652,764585.7325253034,916224.8870650289,1014300.7495783273,1572594.2472320942,2070663.4918630982,590099.7651613565,1386340.445591172,3060867.073868092,528876.0294090206,614191.58355618,1008574.806786246,1920972.3494118818,2021312.4018876564,2189324.5034182947,3881168.7718845196,753181.592036513,3651740.911499799,1224143.2543636777,2966057.7565752417,1724136.924068344,4890329.807269489,1421600.500446352,1057049.3441746605,998308.1535474061,867402.2217227557,2376334.2136016283,1029913.846224683,42202347.24037529,584986.3341187726,6756724.928039413,2446028.1852034256,1904075.0807654986,5001350.363698812,1337061.0789680483,2032627.7703185931,1923237.9119667953,3869171.320091184,703958.3753853006,2254902.192858738,1875236.183647143,1424576.0378688404,1002970.1357729418,744310.1907552889,6932586.277455946,54650831.75640341,670913.6495440068,706897.6560509925,933695.1745518009,956322.2160695527,2260125.7337540668,2148554.530865632,789743.7399017412,1224551.8035223086,1398558.2741268843,1268827.4574136722,1156789.5135548299,2032627.7703185931,6082933.805674728,14157093.158090252,921029.1864454661,6943934.804929061,1171843.0590874434,1020467.7331166182,706286.1802328619,2985596.073782162,1613873.1675830085,882622.9069888903,1708060.5127871793,2512400.5207022205,609280.8829183223,2018984.5970400951,12989632.576555118,1465198.919054055,1419384.6715423043,1930176.842693344,5294210.850756237,13431340.103469621,1724136.924068344,1783248.755652061,918622.6550841845,1353936.8701138573,1224551.8035223086,3794335.958856564,1406591.4595232883,1971377.914068593,1019813.6391684582,3622625.058199198,1412346.1556028554,1385434.99655333,1166736.431706324,1676085.9408768632,3945081.4775547627,486154.6594912931,4329670.023964472,1200698.607314871,1236329.7073477784,2132074.422600299,1337061.0789680483,2064283.125868235,6169971.155353846,1243081.6474959922,977910.1706659367,2567393.434912782,1663362.455318344,649602.4044925732,2133843.7241470935,2090403.8772626852,1351609.065266296,712601.452339667,1360463.162283778,2708898.7921427432,3761421.6351100365,2148492.288572984,921029.1864454661,4246166.777226434,1051863.5378754695,2399142.2548676534,3503692.013157023,1910951.769199399,3592999.910955183,1822714.7089520136,1648642.7146730486,1300265.913118907,1241528.1727957602,8967864.188207153,2966485.2548626736,2702717.895595285,2287992.4364481177,3879831.5641064057,4981667.893202649,1944887.6494118816,762021.7833979147,1587242.8116579845,1090252.0975303699,9599199.212665211,1385458.2233122436,2022676.014068593,2382711.5295770103,1807012.1814734351,1135077.8497787712,6907076.732548581,2106210.1229346097,1246647.5996727792,2632988.6346791647,570795.5910403441,736513.3185150967,1470426.2394947035,1177175.499175125,1536477.4280255584,993349.5792270709,896457.8025387243,5561979.366247345,2047413.66089222,1549064.5448606384,1292788.0169443928,3070813.011168357,1850083.8783496113,801651.8572819129,3127171.8732815133,5294210.850756237,1037217.0808638182,1351516.2891161812,2030299.9654710318,2124576.988572984,1614837.3602496332,23284059.834590252,668664.752889281,2022676.014068593,1322806.7806747511,2516715.339033167,1705732.707939618,1443048.9450569309,5122526.243891612,1826406.1259805115,1152760.572184757,1446351.4473704298,3883496.5767320814,567256.304308131,1385434.99655333,653937.2802705554,9060614.397184849,1381581.9048534331,796389.2508180352,11058714.423455358,1255643.0208148302,3401119.830391272,1572594.2472320942,2565065.6300652204,1680338.8245917957,1402900.0424947904,3376997.116651291,614747.2793071886,1366164.853542071,1069946.5051033855,1750376.513134443,2512321.420075151,1825042.513799575,856186.6708527178,916890.095394187,768465.884501207,6940254.64867457,3794599.551236026,1163079.0002987876,6394709.332660018,2891384.5028514843,6813043.624404881,10814496.799638804,5395219.681786228,982057.1158591822,1288398.904626792,815443.6516820072,1186050.042888981,830040.286734145,5395219.681786228,2513926.30796824,938052.8451253312,2349750.750909848,3251978.284256867,2059065.8742995316,4698297.209882176,765786.9811188962,2073373.0653729045,1848720.2661686747,1366164.853542071,788779.5472351167,2148492.288572984,9582268.28566178,44771471.15818467,1447227.657488012,6107231.011195017,1975083.777193261,7232585.862226147,1244672.1390129023,1201244.5382363342,57180649.28238756,8284919.77725992,1229705.3179318022,1556428.9987657487,1600398.551599738,895431.426806471,2372610.691121216,9027436.481058862,766356.6591758969,626383.1261843379,2674235.5613873126,827712.481886584,6306711.368593207,946625.8092968297,4007235.928798914,993360.9715411275,11614007.490638196,1511614.8701810597,592525.9199491631,789163.2725560431,1467506.9158282469,660523.2536890848,2324679.8186960323,4264353.42191622,971396.761844695,2635316.4395267256,682570.6972479793,1186050.042888981,2930622.4727894273,729232.0884667801,9467159.909832101,2947210.790338784,1741113.2933417957,1558756.80361331,2427227.858454081,713614.8468874319,668906.5786135859,1582348.5953199991,1905094.8325637013,31125232.770772837,1048008.8325218604,1727735.0610909483,6496322.958689017,986929.1833238906,9637982.890937155,827095.6725823215,2032627.7703185931,651930.2093401345,1226879.6083698699,1549064.5448606384,1949900.8195881096,3267276.971838883,649078.2015753306,959385.3423024792,1096442.623573596,568656.8396683851,656265.0851181165,1804684.3766258738,1507967.8590818646,1947198.951057162,3291277.7955750646,3401119.830391272,1128651.0055867073,1183722.2380414198,6815371.429252442,995378.3669459539,2029768.8672565545,3585433.3464552276,965025.4282492426,15531283.867425017,2107600.619299532,2344615.8840128323,1027002.5120946815,1536477.4280255584,651406.0064228916,781208.1012603597,1149069.1551562597,959113.8454969792,1250435.9788150748,5537008.6694938,2817670.5400149375,11767522.703393865,1485394.2004095993,1196890.585132288,2978436.4712373065,921194.8380925661,2088076.072415124,4462150.486064756,1420739.104055383,1207220.1000093322,3881168.7718845196,1683036.7524640907,1124253.4026296341,746625.0231448995,8258066.381447135,1209866.1376901262,6457872.57761647,866785.863365105,1221397.5688540165,3429467.3308756724,3413652.6940423725,3093716.882972761,2150882.3357131938,2094341.244232198,4056897.8702582866,1720736.3648755604,918854.7369007285,1614958.783505018,1741113.2933417957,68029370.10676001,1049452.8154260518,611863.7787086187,647698.1315112052,13999255.959994849,1248363.5560414002,1640842.9174662943,866766.68878292,806601.6339378705,1866054.523958979,765534.2183034929,9272920.568360891,1146861.480158869,1381388.5164816335,927013.0989491154,1944825.4071192339,4336742.264122622,6560346.703418307,1010147.6251150862,1921906.4082738233,1825042.513799575,3256309.3954727612,704696.5808214972,15862483.794100702,6775721.611392995,804145.2514859672,1338424.691148985,1513942.675028621,768114.7859664572,1676085.9408768632,2051241.835884012,1534149.623177997,45852879.22462743,2138559.724462742,997508.1454192108,2484403.563573502,1791281.4487608818,2891252.892630955,1242956.1826442813,704228.7105211638,1423510.6975674422,1247531.421616149,938052.8451253312,2249408.818610102,1967686.497040095,1431953.053477491,2148554.530865632,1904558.3329713687,92159671.99938321,809532.072458885,8597691.466046192,960189.4529717857,2021312.4018876564,1781076.492480228,1149189.2850064302,1957645.0060876624,935775.1655440523,1069672.0358668964,2189386.745710943,8074983.819066912,669605.7320543238,8033648.13098529,973376.6171727435,1580020.7904724379,1672411.6038085015,21679977.582778726,3622625.058199198,3153297.855403697,44911832.94151325,1263754.4504806323,1240666.6931035018,2345670.4009449854,2544411.7521180827,1003905.552597342,27225670.53105817,2446481.5026361104,4793111.5131574925,1224551.8035223086,5805943.397724332,918552.69191259,1909970.2180755755,41416465.80994309,11593510.607849162,1465179.1109806856,2353882.619586941,3180530.902460709,615041.7818734852,1465179.1109806856,752530.8290836307,2170606.312607959,838657.7190776713,804273.8290903094,649602.4044925732,2032627.7703185931,2346943.688860394,1005611.6510663357,857565.3159717502,724881.8419466639,3491406.0272142887,10814496.799638804,1006144.4050726547,764349.588245476,690734.016105438,5341439.287664795,1906261.5426933442,650025.9363587663,2020348.2092210317,1246681.6872052543,13562503.383221898,4509729.524520611,1944825.4071192339,789743.7399017412,863968.9870901637,997302.5122168917,748952.8279924606,3264949.1669913214,1396230.469279323,1805986.8034604539,3374669.3118037293,675201.4150257433,1275789.125876867,1243855.9776433215,1721809.1192207828,1824078.3211329503,3051900.873195583,1546609.8196862212,768684.464023458,1612509.555402072,1507777.2274064908,791107.3520826778,1247250.4685727826,2331634.6569304783,10343938.698762782,1170101.5807988702,4204354.795772623,1534149.623177997,1173288.8033602934,1169515.254239882,1198290.9594998558,1627978.0472630297,1655514.3351621178,2039726.493021542,999753.9885310711,15440519.20340151,2667827.482548154,1562466.6817895959,1384976.683723016,1370701.2780278858,741155.1373463691,2085481.536527733,989015.8692597394,2171026.9017365538,880258.5957927998,1328863.8829736484,798883.1686879342,1224551.8035223086,4214316.488087935,683991.0650159213,573738.2497718075,3696207.4108152157,3945081.4775547627,1888911.1577400065,649602.4044925732,687720.2115480459,607589.6229100976,1749926.2585345877,1505953.52702228,1621728.3908285738,745560.9732996757,6336459.602976553,792295.4715988751,25738074.992944885,63217548.017313235,753410.5617837604,991021.7743795096,771704.6128493118,8224888.465321146,1393142.559609485,622627.952622566,1658286.5085335623,2817670.5400149375,1238197.627643314,660776.9151177745,1183722.2380414198,1027586.0413771219,911942.9133879085,3604695.5843845196,1224551.8035223086,979058.1402615757,655451.5012508043,2565065.6300652204,15661183.133492928,1429375.0780969781,3620297.253351637,684432.6519741545,762021.7833979147,1721809.1192207828,487571.93851602875,1385434.99655333,6501355.5360325,960013.0687696921,12007916.367210701,1293517.9191026941,1418411.2992078217,1368492.6583896324,2334394.7400223436,895312.2772607954,5944215.629166979,1204110.8988280704,652866.0450819692,954673.9792422206,1782072.6900678666,3786407.1742885206,1679882.122154408,1021174.6294373417,1416062.1020378761,712601.452339667,2407977.0926383613,4264353.42191622,113481390.9646883,3155985.6734164963,1344428.6795444011,1245669.9176749566,1465179.1109806856,1661034.6504707828,48753992.3983776,3696207.4108152157,886669.200306429,6572344.1552116405,1169515.254239882,974960.0953455359,1374817.6959386186,1410650.2034538167,3413794.0504604056,10593906.866219949,603516.3628092824,65330819.81050036,1058483.787410014,1243855.9776433215,976671.2278537069,976924.5743329093,1425109.81900048,9178746.71666374,783052.0739799767,2512321.420075151,9121469.58920256,1888026.3377458702,1969050.1092210317,9768798.303389952,2125100.547834483,3945081.4775547627,3992910.6721580178,2415325.8981703436,47131835.211262584,13854564.922389235,6685841.8230685815,760156.6512389338,1948385.4418110128,2032627.7703185931,2183143.6068708366,1808823.1770368817,1410123.200323669,3538751.3127474515,6671192.180313494,714549.021298583,1246035.751193839,871341.3948974676,1276513.932823715,1071575.6960631527,1191777.6463085546,3398792.02554371,1241528.1727957602,1721809.1192207828,12937124.108906236,526548.2245614595,4268396.82773892,2514649.224922713,3389878.750460406,1571679.9899211826,1020467.7331166182,611863.7787086187,3883496.5767320814,6503683.340880061,11284116.520962562,952810.5633028828,950025.7175814916,2119188.37022938,11439245.218927678,1327015.1047489478,1279381.8989140007,651930.2093401345,6757209.138181886,2021312.4018876564,917209.3336159514,990148.4479884246,1061814.4618212895,2706570.9872951824,2514649.224922713,795986.888627373,1209246.5548253765,2161413.2700124476,2635316.4395267256,5745564.638857227,1623526.265629199,1216736.3222663703,2380383.7247294486,1368091.7401257365,1276462.0817871357,694182.255032529,117421290.1812689,1272563.199601574,743482.9421939302,766356.6591758969,1067344.231019335,12985080.301094167,1286414.3482065059,2262515.7808942767,649078.2015753306,13275459.57814268,2168692.2426935937,2325835.4509098483,935725.0402777699,1106538.36050152,1678413.7457244245,1570266.442384533,3522451.1702327635,2457201.329878155,4023719.954132297,13048559.390362125,4456895.628838643,1008472.2099202158,1920972.3494118818,1234845.7890508433,3803793.1967707146,841924.908934411,956802.0063071194,5051345.789042832,1661034.6504707828,1481529.3196209117,1465953.5669144925,1158157.725675904,1285817.3524394955,942322.1363658373,867750.0560317299,650025.9363587663,4644644.566728337,1712203.1664142597,1211728.4788594625,1433038.4713113278,1678413.7457244245,882640.6392153165,3267276.971838883,13004420.27800195,1224143.2543636777,11161864.29477309,693250.5825463441,13210022.55197499,1475400.0761939466,604815.8895361925,934558.504863853,1501330.2969938067,3224443.510479917,768114.7859664572,614191.58355618,9405600.560214855,1969050.1092210317,2068335.687015537,1200618.764347417,1224551.8035223086,2567393.434912782,1254617.4192595605,2517282.7329718824,1211518.867291298,1673701.2256069498,2994859.054915486,1139352.9237327336,761061.8072134854,984601.3784763293,3425715.7723270943,736474.7002421181,751061.3048133441,2243149.3644806147,3135560.5260136677,865610.0313669097,2531625.594196166,15561695.083594307,2083060.2749266007,1696858.491427859,784833.9576209142,6754881.333334325,938115.7246444061,71349556.74512321,959385.3423024792,19001582.043901104,1057049.3441746605,1146861.480158869,1967686.497040095,787333.9617556221,2030299.9654710318,3253981.5906251995,1676085.9408768632,9553776.023000985,3463657.8765208647,15329846.853372153,2531625.594196166,1244319.794825218,1385434.99655333,1741005.2975282709,857565.3159717502,1194105.4511561159,1256548.4747184266,2106210.1229346097,12282078.65022578,634228.5923456982,526548.2245614595,878390.6135394553,1510251.258000123,1970014.3018876563,3625474.591106969,768114.7859664572,852078.9395660375,10970923.018745497,1309085.4805178682,2291867.9701300077,23249149.55213038,1171769.2595331727,2565065.6300652204,9193819.369965304,1042698.6201263274,1885698.532898309,2330173.192944669,1616200.9724305698,3698028.4296395015,993349.5792270709,866785.863365105,659836.446342751,2298795.3391717756,4493793.422554772,8328361.841977499,2032627.7703185931,4727400.728421634,1391940.5625091556,1721809.1192207828,15629804.524694864,10211640.977637533,1174950.0665053565,125599279.86679521,1190300.8631685835,1047125.0105784909,953709.7865755957,1738785.4884942344,2148554.530865632,1918080.5338176878,1717070.5382105124,1680338.8245917957,561037.6736503775,1013522.3422658335,828266.7889236851,65467730.59378666,1578026.8232816139,4028468.86968112,765786.9811188962,866766.68878292,707784.6773612913,1970014.3018876563,795986.888627373,11711477.769516159,2514649.224922713,4223895.031523004,1387762.8014008913,1970014.3018876563,1724136.924068344,1677342.3920091286,1612926.7568569498,3881168.7718845196,3183848.099441571,1751136.7313931307,1920910.107119234,1064274.3730903843,1323824.13929436,8981915.529982843,788199.0798894184,3542209.141640979,5396919.7581448015,3078288.9481184985,1183722.2380414198,5488142.4588218015,1195359.248805296,1612509.555402072,4732363.642433338,10016926.96438485,631167.9528296314,2103882.318087049,72254376.81969097,2514649.224922713,3522451.1702327635,1226879.6083698699,3671607.6506950743,47464948.78351535,3482691.707149432,1368537.7279069463,1661034.6504707828,1429051.8265814467,3869171.320091184,5046547.638866691,123512722.13543408,1546675.4883732086,4616425.734712698,1054090.2632222602,2655657.1398148723,710232.8639024879,10209313.172789972,2407977.0926383613,10104924.928451657,882586.4006403611,2021312.4018876564,703958.3753853006,581763.1296460406,1433038.4713113278,17891342.14711227,2213239.8034182945,9284674.409177467,1650970.5195206099,592986.2493842078,5732651.357989244,807204.267611324,8748748.755613688,925647.980564264,3146084.0112542873,2160327.617758927,651930.2093401345,2103882.318087049,6775721.611392995,727209.6467942252,2635316.4395267256,2351554.814739379,1226471.059211239,11899842.025074672,1881762.950882015,657508.6414951897,5707116.808032498,1387786.0281598049,1417756.3893410412,1271593.7885330121,2020359.9915194425,1327015.1047489478,1875236.183647143,4857620.955693301,786339.8382913932,10419209.698848713,1254678.8281482055,3951327.4916360816,656624.6711251544,955358.0234029278,1404739.1706743424,733013.6856583644,2512321.420075151,897640.0821083566,21959731.241810378,2266887.1420851685,1465179.1109806856,785006.156908061,766410.957196714,1467122.1135522362,4066493.0247740354,2329818.3099300433,957685.263922131,2891252.892630955,1140398.3826203023,30181201.7416029,788220.4549491872,4519740.54764385,14792448.32974429,792076.3964587153,21612449.066639256,2196263.4341448424,629860.9461239785,1226523.6868167857,2103882.318087049,2396814.4500200925,941851.7423237859,25547426.45500589,4264353.42191622,993360.9715411275,5236550.442021884,712560.6687500492,1651825.6177673338,992476.2528359859,1676085.9408768632,13273018.195894402,3441185.325408319,1245283.9874918426,1386441.023718815,17286307.950475175,1085021.7935345077,687720.2115480459,859893.1208193115,3995238.4770055786,9523928.212579284,962517.257819347,4993160.922470873,18859943.374778952,1384094.611131307,1088087.460210448,1727389.1676651638,993349.5792270709,6318537.1722749565,1377145.50078618,4659823.891544765,907157.2323094798,710934.4811639903,677533.6158652966,1224551.8035223086,787161.7624684755,1078652.1696237766,640752.5561758269,11848017.850095943,1003905.552597342,2025756.5652672334,1572594.2472320942,2529297.789348604,1947215.4542594429,8196074.791630909,1092101.0196243452,2218359.2953096107,3264386.461201641,2096997.0334214414,1976783.0420057632,707024.3856690585,680001.9886117192,1068544.3013008493,5020989.917589596,9829653.495407663,993349.5792270709,8990114.468046974,833312.1640050985,2370858.988860394,5425669.2264567,9437472.66044715,1826406.1259805115,3677781.7098046355,3428043.577174656,1808823.1770368817,1355941.1573395783,764518.1023455929,753181.592036513,1243855.9776433215,7253450.5688917115,1426292.5480713474,1815799.0795174746,2301538.4603830203,1748048.7082868817,3383344.020503506,1504428.4399213113,782763.3503923476,1307864.3569274347,1271554.6192940145,1294453.5146828361,2529297.789348604],"xaxis":"x","y":[2450488,1188444,1205617,579888.3125,3637973,2228675.5,5662234,783104.125,620104,21069722,3537977,2918553.25,5448975,770333,5907068,6388260,369291.5,1784385.25,873923712,1889990,1275236.125,646505.1875,1058986,533370.625,1384693.5,2196718,1295685.75,5702077,378714.4063,727374.6875,2989919.75,1066901,4155007.75,5253241,533329,1271021,1057256,9341427,2659770,1054995,3874632,13951571,1344512,6508595,1286817.375,398357,729535,5117308.5,1860901,827618,2017423,242805,1212883.75,1079477,4547129,8819864,499887,620483,4735249,478982,7046345.5,1191803,811703.6875,792734.8125,4420650.5,1847401,1073156.375,727703.5,729096.875,858897,4897760,1030150,1784796.5,870633.5,1614322,552015.625,6345009.5,2147013,4235405,2788860,1691963,4946798.5,7482832,4024810,1093653,630040,1582655.875,1835804,722062.875,994194.875,1964681.25,969682.8125,6695413,764906,3291518,13158205,3344833,44984468,12704014,524144.6875,25377650,6962255,872114.5,466672.6875,1381407,2919980.75,3481727.5,2992693,464155.1875,2372126.25,5965171,1811506.375,670779,15552193,5337334,896403,4039667,4784731,749207,8381834.5,25970248,900973,1436773.875,1200381,2225265.5,5348309,966812.125,9791557,777094,5471735,662424,939413.625,17924416,454296,452744,11441,4188660.5,1686262.25,1767480,342726.0938,1987334.75,666385,8611054,410433.0938,1501111.75,1210229,4268054,914670,1509492,1185469,10750010,572654.3125,9290214,4145920.5,1332591.5,549509,1134195.5,1682454.875,2364351,7475577.5,3418139,1035592.813,567608.625,6015128,7380088,102673696,2177075,875789,1972234,618876.1875,6010407,1189402,2916511,3757006,974305.625,505601,1325597.625,4189569.25,3229864.5,1508561.875,15878689,5528223,1605522,1747090.375,19490284,0,14585813,5696695,541950.875,5968153.5,1346952,612158.1875,1076084,731092,2804534,690529.5,2190460,1829122.375,7240274,947884,9162048,2470566,690557.6875,25959642,703847.375,207572,3155391,948975,666687.5,1247361,986353.125,53166156,1419980,1171380,888308,15590743,830945,1074350.375,3805877,2108012,659450,3693616,4696639,2433777,804620.5,5258024.5,2420605.5,13171370,761504.875,4054994.25,858932,3267564,776237.375,12095818,3556291,887403,6797528,2055534,1983256,566183,1989287,1045413,1169932.75,2964206,3782378.25,802312.5,10062436,1034941.313,12086616,1138520,14168729,2173143.75,1441631,1742351.25,1222949.25,2478960,1913558.375,473874.0938,3518459.5,2920918,19645206,36667044,98960776,13010578,2280352.5,1228041,4961405,3157579,809291.8125,2288635,6293359,5454482,7739699,2077039.375,3577120,1574750.875,708967,2458760.5,748335.3125,686949,1465795.75,1244150,2557533,1686405,2186148.5,1194835.5,3744483,4476997,8355988.5,6914839.5,1738724,90558.70313,3087318,965425.1875,913899,2292169,490743,601590,1117658.75,1082531,326778.6875,1821047,230365,809870.8125,387810,678608,1297872,684234.875,1114384.625,742630,2454812,10420096,694241,1966424,3663534.5,515025,592534,600060,2001643.75,1866800,2360288.5,6859198,955641,4358944.5,1034827.313,6596057.5,1784233,1886382,1100575,1632731,835085,738990,4385512,1227772,77164568,549834,6609358,1789710,1375366,5104947,1883864,2137686,2208196,3004271,866096.6875,1679127,498745,3717064,1446455,710825.1875,8259071,16644664,924438.125,542882,711118.375,219483,1041652.813,0,318205,905750.375,1024033,1696448,694479,5457975,5885427.5,13093350,1270139,10213132,1200811,1076644.75,581589,1967637,2912264,967697,4700396,7369591,489557.0938,1529510.25,8320719,1130960,807358,3837228,4988493.5,12719042,2996958,5238803,958725,1529309,1256575.875,11319836,5767765,6036588,683274,16371764,2126288,920068.875,942089,1497496.5,1358022,529807,2912140,1017787.125,685888.1875,5252131,1817550,7338511,17109052,961476.1875,688641.8125,2211671,3864699,647906.1875,1938887,4547379,1750419,648634.875,1739693.25,2962476,3893073.5,2338238,809257,719471.1875,934357,408513,3258819.5,1911591,2890253,468396.9063,1868497.125,1072487.875,2151376.5,8168547,2472350.25,1417778,2337651.25,5917000,5653656,3059758,2266983,2676594.75,1379739,7637986,1128179.125,1472548,3371455,3091559,7003615,4059424.75,1318096,2156407,1765813.625,455798.9063,641713,1027394.313,648879.3125,1947717,896485,2248776,3404992,2158629,1380293.625,7067404,32579658,1489363.875,1359579,3017709.25,10154608,3546459,892227,2439484.75,3040924.25,317581,32381836,1401315.875,1921222,1494727,1703504.875,2563985.5,1762174,4078339,6454983,684155,10403123,3240830,508014.5938,1022957.688,654625.5,20311228,1481698,1093757.75,18661606,293217,922041,8381744,2127534,2694853,362874,9586556,609494,1133028.875,502667.6875,1457118,2265836,962238,815800,1259456,999924.3125,10317852,4022735,877788,6578370,2477499,51168308,11788719,12783255,1677877,989079.5,1431784,928409,1521202,4685346,2148565,538740,1889761.125,3605749.75,1443367.75,4206839.5,578854.125,739662,3740373.75,1385697,608755.375,1634178.25,8696206,137635696,1364344,3590731,3605855.5,5357833,1176342.25,716940.625,47148456,6320060,1045640,1275231.75,783345.6875,1929011,2307938,22055140,759060.8125,924121,2361088,1493131.75,5261096,755618.1875,2735128,688375.1875,8493420,1858224.625,551219,562617,10303305,657385.8125,1645748.25,4760734,1612105.75,3366528,1245923.75,1093743,2468322.5,816510.875,7682562.5,1607972.25,2512443,2069880,1634334,1334125.125,625938,3158151,405062,40062888,287132.5,1192100,4934559,956481,7520554,818317,2003474,827211,1967129,2429021,1006989.688,2616226,454124.3125,872555,268269,1085439,1055624,3191228.25,53401,1012341,2140476,3265995,811900,713326.875,6935723,204499.0938,12525174,2189735,1585441,14361382,1750362.25,1824054.5,1137513.625,1150926,593824,555864,1761137.5,497991,2470642,4321141.5,2274873.5,12731388,1898509.75,1245753,4486477,990361.5,4560492,5707529.5,3643112,1198630,3151107.75,966376,1104074.625,895310.5,6750112,2225134,3775187.5,431390.4063,976537,5415438,306721,2188452,2234448,3928695.25,4983934,2023197,2028125.625,1593610.125,3436032,85357952,591249,505949.0938,833747,25476332,1186682,1307883,965786,876871,1888344.25,758875,8651835,2442220,3863772,3947209.25,2014500.375,4424263,6127771.5,522791.0938,2093011,422133,2646130,680395.875,35383012,6313678,681354.1875,841242,1507168,784584,2025533,7439009,2090543.75,37980712,3239754,730211.1875,599390,3895403,861972.875,162960,580688,930439.375,1453543,642052,1482997,819354.625,4294550,2024367.75,1525624,157606480,869699,5903033,8926736,3289053,1820292,2277316,1844825,94186,869835,2491238.25,4407932,637685,9155568,4912520,3999505.25,6983736,18848780,3303036,6361236.5,25397086,862541,752441,1551204,2056762,802755,136241424,13962749,3312125,2114860.25,3568237,1109924,3542542,46510960,17044842,10960107,2471623,3132453,597761,1107290.625,658297.5,9029489,1015172,2701872,493969.6875,1886007,3226884,1216543,1469556.25,789209.8125,2081072.625,6884914,1623657,393493,645927,5672838.5,2401890,624993,3485865.5,3005926,8024064.5,9373179,1872413,145573,1834570,714898,876237,2015117.25,958242.875,1976856,10077532,462465.5,694189,1056099,1418915.875,2485521,2756494,1271753.25,1176615,1096646.25,1943933.625,534039,849504,2322314,0,2466100,3629065.75,1188994.625,1822834.25,1169948.875,2122581.25,2400507.25,7175646,3996541.5,350264.5938,14709151,2067352.5,415586,1559199.25,832168,1207869.875,1041304,577033.875,1677881,796042.375,813641,1085183,871355.5,5169726,661383,828531,6102883,2227640,2890379,507603.3125,602191.1875,466904,2767381,847419.375,1618695,436848,4425832.5,439065.9063,28450436,72587024,781682.5,5876439,642133,2407904.5,6058494,611536,1926134.875,2556131,1639849.25,696522,1054581.375,872326.375,677536.3125,3751417,818836.125,1334876,1287159.25,2334260,16163194,1172785,2887392,474468.5938,1153690.125,1228963.75,447947,1332912.625,0,443365,8504904,929688,3573328.25,1293336,2075833,1059605.625,1507270,1419453,586827.5,1360340,13659926,1971236,1364214.75,609508,1790148.625,2245568.25,2855545,3944933.5,286685536,4767227,815115.5,1261634.5,1845150.625,1698074.625,61576184,1226435,716071.625,5249190,1128208.125,863703,2301043.75,1080203,2376794,11325513,941564.8125,41655852,387279,1734533,1540235,570982,1689631,8909924,700899,686019.1875,8580279,938908,4167253.75,4222682,6118300.5,7323351,3185005.25,7256842,53079164,9569715,3644982.25,238255,1614026.25,1655612,1570758.25,11594121,1929118,3021669,6814269,841841,1258619.125,803938,938096.1875,961749,2058532.875,2220053.25,1313366,1297633.75,65336980,541542.6875,3608955.75,1983509,3218824,363808.1875,770175.8125,592221,3791402,23364636,49102164,1625308.625,497528,4202335,9986032,709990.8125,1423463,567521,8748273,3373582,846389.3125,1000804.625,2998208.25,2153603.5,2660396,1289073,1247972,4253369.5,2105451,13586070,935580.3125,1082920.125,3321035.75,3288836,3560614.25,512166,51625548,2615973.5,1251906,555153.125,810636.875,48729588,548292.6875,24473814,785052.875,13300859,571309,3570161.75,522972.1875,2473302,2098850,6881405.5,2306723.5,2876857,5089903,10564729,5039523,1730886,1633399.375,1696978.75,1739286.75,395346.3125,349139,6040837,945020.1875,1764767.25,914023,1232822.875,1625695.75,2545405,5807879,1392015,6283226,2662054.25,4133811,1582794.5,1259959,964015,6424029,12448381,873711.125,13911761,1475475.5,10498344,1788148,618366,776991,1331122.125,5903142,634751,520405,11026945,5981430,2203026.5,2166711,1427974.625,2654755,1050982,2581741.75,1639136.25,1402048.5,3165109,729397.1875,937635,868859.8125,4409881.5,705262.125,3542708.5,2729167.75,2628864,813038,2773779,21781324,2161828,1199392,832986.875,602245,1074989,35685224,342493,94178648,1136676,741287.1875,882896.5,1505709,0,1803753.25,3220635.25,8201974,2035595,12265516,3306123,313528.0938,241478.5938,2465266,1270582.125,1982967,824516,533078,19344782,1172065,511180.6875,255971.2031,758008.875,1911611,2503948,636641,677279,7838600,499928,2921368,22591288,964923.8125,2278160,17186624,591622.8125,919746.3125,1717130.875,1815350,3024478.5,5975230,860364.1875,600543,3275779.25,7261732,8921778,1976400,5572503,6159684,2960377.25,13270687,7370743,1323038.75,291614432,1425599,689803,439266.9063,3020001.5,2168701.5,1234095.5,2219744,3045139,638201,1943772,855393,41213584,3674705,2712983.75,1201630.125,648022,556700.6875,816846,196410,9977330,7419765,25453690,1120197,2483320,1358226,1441665.375,33268410,5535626,6471427,1437569,2128963.5,1581425.75,1231546,5037781,1581036.75,5392617.5,4180298,4817062,687953.6875,3743228.5,578789,1519952.125,2355732,7969653,431357,5094197,65047284,10513817,2599171.75,842593,10105379,45074876,2391566.25,1004253,1827424.125,652991.6875,3534690.75,7657568,64715652,1073507,9671619,1540258.5,8163413,520206.6875,6744317,6577130,63668488,695346,2666634,676032.625,571644,1103594.75,16614404,2885740.5,9851937,1743198,572082,40613740,861007.8125,47102428,529033.5,41680640,1398253,987989,4590246.5,7976729,786346,9146727,1688612.5,801264,18628834,1108310.875,500237,17673018,978211,1921890,2926089.5,8839812,1634669.375,1115020,5121586,804002,16246106,3612526.75,10340282,881386.1875,920430.625,1183623,323739.4063,2993032.25,788055,19150024,1929828,2248134.5,669117,495152.6875,941257.3125,21795830,1513700.625,445943.8125,2843033.5,460010,16020642,812013,5838129,18967014,1038351.875,20657852,1808401.375,588492.5,1380283,2345752.5,14833476,1541879,19827916,4396566,899266.125,8354235,466985,1280263.75,1011296,1325972.75,11821841,5481549,537141,1037696,17750994,792015,697397.875,1736775,3616057,8070420,9159185,3730953.5,18576900,1356972.25,656980,1487769.5,706961,1916309,4038090,4027134.75,868163,3767135,517029,1208168.75,864765,783208.625,535649.875,14751117,1896858,1757601.25,1116705,1920352.125,2755578,4828599,873648,13440924,1730431,3441643.25,2178074.5,690750,573395,936616.5,2762846,6821833,897514,18074716,228212,2549892,22530168,19991412,3215953,3245111.5,6057734,3234787.75,1928391.25,416566.1875,741850,1045337,11963428,3035962.25,1855556,5040382,2096835.5,3002337,797170.3125,892455.875,1751483.875,8054334,686055,1381590.125],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle GradientBoostingRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predGB"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle GradientBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsGB = np.logspace(1, 3, 10, dtype=int)\n","param_gridGB = {\n","    'gradientboostingregressor__n_estimators':\n","    n_estimatorsGB,\n","    'gradientboostingregressor__loss':\n","    ['squared_error', 'absolute_error', 'huber', 'quantile']\n","}\n","\n","GridGB, \\\n","BestParametresGB, \\\n","ScoresGB, \\\n","SiteEnergyUse_predGB, \\\n","figGB = reg_modelGrid(model=GradientBoostingRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train.ravel(),\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predGB',\n","                         score=score,\n","                         param_grid=param_gridGB)\n","\n","print(BestParametresGB)\n","print(ScoresGB)\n","figGB.show()\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[10,16,27,46,77,129,215,359,599,1000],"y":[12636279.439223845,11722649.224670844,10945869.868637083,10657951.159286048,10534702.55679464,10485498.269038957,10267772.191547388,10217283.402632616,9918766.736961719,10383778.388199735]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[10,16,27,46,77,129,215,359,599,1000],"y":[17651791.145179544,16659782.441393163,15801253.815414118,15468396.891255412,15301293.359268293,15263555.925190693,15075875.147782914,15079123.428918604,14783637.477484599,15107703.77477623]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[10,16,27,46,77,129,215,359,599,1000],"y":[7620767.733268146,6785516.007948526,6090485.921860049,5847505.427316684,5768111.754320986,5707440.61288722,5459669.235311862,5355443.376346629,5053895.996438839,5659853.001623241]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[10,16,27,46,77,129,215,359,599,1000],"xaxis":"x","y":[11963922.788010737,11259158.155787772,10225338.260809088,9784020.645779846,9400743.006372645,9368217.139336197,8111349.096495616,7817145.657024889,6721725.813236033,7138228.944257913],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[10,16,27,46,77,129,215,359,599,1000],"xaxis":"x","y":[20682802.839825567,19566728.82713601,18577440.145167377,18196243.63050611,18026747.030633062,17974297.907715604,17915279.310678337,17922001.032529097,17605002.800951667,17829096.223291855],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[10,16,27,46,77,129,215,359,599,1000],"xaxis":"x","y":[15247021.723502852,14355068.482281521,13894449.092672238,13721886.430166686,13656229.653382517,13649316.111486912,13633410.119743489,13739569.174810309,13667174.566918533,13632539.866216166],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[10,16,27,46,77,129,215,359,599,1000],"xaxis":"x","y":[6245074.304910167,5476746.370609766,5063804.724127655,4955724.401575235,4926020.464854237,4896954.137731723,4863075.219683057,4881670.675433026,5073353.765758847,4746890.034837917],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[10,16,27,46,77,129,215,359,599,1000],"xaxis":"x","y":[9042575.539869906,7955544.287539147,6968317.1204090575,6631880.688402371,6663772.628730743,6538706.0489243455,6815747.21113644,6726030.4733657595,6526576.737943519,8572136.872394832],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle GB pour le paramètre<br>gradientboostingregressor__loss=absolute_error<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["FigRMSEGRidGB = visuRMSEGrid(GradientBoostingRegressor(), 'GB', n_estimatorsGB,\n","                             'n estimators', GridGB, BestParametresGB,\n","                             'gradientboostingregressor__loss')\n","FigRMSEGRidGB.show()\n","if write_data is True:\n","    FigRMSEGRidGB.write_image('./Figures/ConsoGraphRMSEGB.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Consommation énergétique au log"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["SiteEnergyUse_train_log = np.log2(1 + SiteEnergyUse_train)\n","SiteEnergyUse_test_log = np.log2(1 + SiteEnergyUse_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.8836309836052285\n","rmse : 9861558.367917802\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logLR=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[20.812339782714844,20.696609497070312,20.540557861328125,20.368507385253906,21.040634155273438,20.572738647460938,21.039207458496094,20.46546173095703,20.525253295898438,20.986923217773438,20.418678283691406,20.7603759765625,20.69872283935547,20.36516571044922,21.860321044921875,21.727813720703125,20.402122497558594,20.403175354003906,86.21753692626953,20.727073669433594,20.664405822753906,20.48999786376953,20.484725952148438,20.429672241210938,20.719215393066406,20.651588439941406,20.356773376464844,21.19684600830078,20.45147705078125,20.484115600585938,20.610084533691406,20.549819946289062,20.960914611816406,21.55682373046875,20.592185974121094,20.630165100097656,20.61528778076172,22.84478759765625,20.73284912109375,20.59178924560547,20.69164276123047,21.408065795898438,20.589027404785156,22.19402313232422,20.560928344726562,20.46031951904297,20.52033233642578,20.301109313964844,20.561492919921875,20.61150360107422,20.77777862548828,20.46587371826172,20.45648193359375,20.661773681640625,21.035049438476562,21.60173797607422,20.450477600097656,20.819580078125,20.799240112304688,20.548423767089844,21.730010986328125,20.463592529296875,20.68804168701172,20.562454223632812,20.420181274414062,20.551971435546875,20.458290100097656,20.533859252929688,20.55292510986328,20.508926391601562,20.900115966796875,20.5269775390625,20.80504608154297,20.541030883789062,20.796478271484375,20.431442260742188,20.63885498046875,20.52391815185547,20.886581420898438,20.476242065429688,20.7376708984375,21.03246307373047,20.538421630859375,20.888710021972656,20.556854248046875,20.455810546875,20.523399353027344,20.370849609375,20.52923583984375,20.52703857421875,20.419944763183594,20.45184326171875,21.275177001953125,20.47063446044922,21.556671142578125,23.461624145507812,20.98358154296875,24.041099548339844,20.889862060546875,20.533157348632812,21.925254821777344,21.979110717773438,20.519287109375,20.373435974121094,20.880538940429688,20.949661254882812,20.51154327392578,21.264495849609375,20.421722412109375,20.819252014160156,21.487289428710938,20.537673950195312,20.53974151611328,23.37183380126953,21.077354431152344,20.478675842285156,21.347244262695312,21.73291778564453,20.56292724609375,21.232162475585938,25.49291229248047,20.492874145507812,20.51264190673828,20.657875061035156,20.43218231201172,20.8033447265625,20.609451293945312,21.32952880859375,20.590103149414062,20.96253204345703,20.550071716308594,20.46147918701172,21.062950134277344,20.558502197265625,20.498756408691406,20.61199951171875,20.846336364746094,20.484397888183594,20.727584838867188,20.7205810546875,20.373428344726562,20.60875701904297,20.632247924804688,20.402359008789062,20.656654357910156,20.46251678466797,21.45977783203125,20.570648193359375,20.55284881591797,20.56322479248047,21.13690948486328,20.46576690673828,20.690818786621094,20.72479248046875,20.60529327392578,20.531951904296875,20.65850067138672,20.648193359375,20.777931213378906,20.665374755859375,21.04505157470703,20.502395629882812,20.448265075683594,20.499366760253906,21.62835693359375,23.940750122070312,20.47119903564453,20.579360961914062,20.73847198486328,20.581100463867188,21.12493133544922,20.988265991210938,20.642654418945312,20.77362823486328,20.560203552246094,20.416343688964844,20.656524658203125,20.72686004638672,20.987869262695312,20.48889923095703,23.28784942626953,20.574195861816406,20.47608184814453,20.961952209472656,21.343215942382812,20.64746856689453,20.514968872070312,22.18090057373047,20.497573852539062,20.67339324951172,20.841232299804688,20.448455810546875,20.677520751953125,20.44922637939453,20.576065063476562,20.64905548095703,20.667030334472656,20.496742248535156,20.589874267578125,20.461578369140625,22.30406951904297,20.367042541503906,20.56072235107422,22.858978271484375,20.500106811523438,20.50806427001953,21.170387268066406,20.45299530029297,20.36438751220703,20.70177459716797,20.563209533691406,21.632102966308594,20.838165283203125,20.622116088867188,20.67723846435547,24.08258056640625,20.614273071289062,20.39990234375,21.121795654296875,20.64623260498047,20.6011962890625,20.992507934570312,21.285873413085938,20.564430236816406,20.50609588623047,21.87610626220703,20.701675415039062,21.771774291992188,20.66008758544922,20.77515411376953,20.494461059570312,20.72423553466797,20.53961181640625,20.520065307617188,21.174545288085938,20.646514892578125,21.50726318359375,20.569427490234375,20.98639678955078,20.4971923828125,20.54035186767578,20.70697784423828,20.51111602783203,20.9832763671875,20.98870849609375,20.425140380859375,23.333053588867188,20.5853271484375,20.934539794921875,20.63355255126953,22.8873291015625,20.492919921875,20.564971923828125,20.776161193847656,20.52655029296875,20.584190368652344,20.484031677246094,20.425193786621094,20.719337463378906,20.604774475097656,21.96839141845703,26.390472412109375,23.4625244140625,20.602584838867188,20.491668701171875,20.507568359375,20.996315002441406,21.474388122558594,20.39006805419922,20.590965270996094,20.41399383544922,21.65624237060547,21.972557067871094,20.588890075683594,20.52124786376953,20.76708221435547,20.563514709472656,20.75566864013672,20.460304260253906,20.51795196533203,20.482452392578125,20.437606811523438,21.195510864257812,20.4105224609375,20.432052612304688,20.62781524658203,21.226394653320312,20.84954833984375,21.80632781982422,22.00391387939453,21.152748107910156,20.54882049560547,20.439117431640625,20.456024169921875,20.688819885253906,20.855987548828125,20.48174285888672,20.537322998046875,20.42010498046875,21.04938507080078,20.376510620117188,20.477066040039062,20.484664916992188,20.368728637695312,20.32373809814453,20.519615173339844,20.47344970703125,20.531326293945312,20.40985107421875,20.46595001220703,20.708030700683594,20.662315368652344,20.583717346191406,20.662551879882812,20.93688201904297,20.54564666748047,20.54851531982422,20.574447631835938,20.66217041015625,20.63573455810547,20.790611267089844,21.04491424560547,20.638580322265625,21.00371551513672,20.51831817626953,20.91828155517578,20.76024627685547,21.377334594726562,20.550254821777344,20.47625732421875,20.548171997070312,20.551124572753906,20.94501495361328,20.570968627929688,28.633155822753906,20.565208435058594,21.83100128173828,20.7979736328125,20.760215759277344,21.163330078125,20.493637084960938,20.677818298339844,20.827529907226562,21.028305053710938,20.504837036132812,20.87146759033203,20.568359375,20.392372131347656,20.57318878173828,20.430404663085938,21.72478485107422,26.245849609375,20.514244079589844,20.544189453125,20.365432739257812,20.46435546875,20.85570526123047,20.725135803222656,20.448951721191406,20.579010009765625,20.712188720703125,20.500732421875,20.64592742919922,20.692825317382812,21.371421813964844,23.152542114257812,20.537391662597656,21.720474243164062,20.63361358642578,20.49340057373047,20.586639404296875,20.995628356933594,20.49695587158203,20.541900634765625,20.5645751953125,21.008384704589844,20.482688903808594,20.549232482910156,22.60699462890625,20.820335388183594,20.66070556640625,20.810401916503906,21.358779907226562,23.1431884765625,20.750938415527344,20.49170684814453,20.647232055664062,20.49744415283203,20.57696533203125,20.935882568359375,20.54193115234375,20.648902893066406,20.359527587890625,20.94837188720703,20.50617218017578,20.607040405273438,20.589248657226562,20.576324462890625,20.996429443359375,20.504661560058594,21.22968292236328,20.597564697265625,20.600082397460938,20.717025756835938,20.490463256835938,20.640106201171875,21.37548828125,20.52228546142578,20.471031188964844,20.932212829589844,20.74256134033203,20.47563934326172,20.82257080078125,20.630516052246094,20.41498565673828,20.50878143310547,20.535247802734375,20.738754272460938,21.077049255371094,20.812652587890625,20.538406372070312,20.29327392578125,20.631576538085938,20.793731689453125,21.070228576660156,20.830642700195312,21.256317138671875,20.481475830078125,20.579368591308594,20.669029235839844,20.63214111328125,22.0106201171875,20.837112426757812,20.72821807861328,20.873252868652344,21.063758850097656,22.404273986816406,20.708251953125,20.44080352783203,20.662200927734375,20.644813537597656,21.693115234375,20.45777130126953,20.686485290527344,20.826866149902344,20.65666961669922,20.481002807617188,21.563858032226562,20.679412841796875,20.505752563476562,20.665184020996094,20.467315673828125,20.602745056152344,20.398330688476562,20.530685424804688,20.728225708007812,20.575057983398438,20.559471130371094,21.512283325195312,20.65729522705078,20.624618530273438,20.47553253173828,21.074966430664062,20.52149200439453,20.573562622070312,21.101211547851562,21.208648681640625,20.709999084472656,20.675186157226562,20.627243041992188,20.773216247558594,20.53924560546875,24.09991455078125,20.423988342285156,20.678932189941406,20.652084350585938,20.90967559814453,20.479515075683594,20.474510192871094,21.333221435546875,20.59294891357422,20.476890563964844,20.7607421875,21.201576232910156,20.471412658691406,20.606796264648438,20.480995178222656,21.906837463378906,20.67767333984375,20.559417724609375,21.986236572265625,20.487930297851562,20.989105224609375,20.707260131835938,20.792640686035156,20.79804229736328,20.427330017089844,20.937896728515625,20.513092041015625,20.624343872070312,20.31231689453125,20.68828582763672,20.82842254638672,20.54686737060547,20.643264770507812,20.537109375,20.441757202148438,21.71135711669922,21.154449462890625,20.67889404296875,21.604454040527344,20.957656860351562,21.716209411621094,22.187965393066406,21.27416229248047,20.505088806152344,20.37999725341797,20.39525604248047,20.645599365234375,20.508506774902344,21.28913116455078,20.716819763183594,20.44806671142578,20.895057678222656,20.915695190429688,20.694107055664062,21.174346923828125,20.49816131591797,20.56652069091797,20.487350463867188,20.622390747070312,20.40021514892578,20.798377990722656,22.29039764404297,24.348594665527344,20.74139404296875,21.522689819335938,20.57056427001953,21.500228881835938,20.403396606445312,20.500595092773438,29.013084411621094,21.60730743408203,20.70026397705078,20.476036071777344,20.54107666015625,20.45301055908203,20.546470642089844,21.823829650878906,20.44641876220703,20.568092346191406,20.625511169433594,20.425987243652344,21.518264770507812,20.48968505859375,21.08911895751953,20.467018127441406,22.598907470703125,20.462928771972656,20.542984008789062,20.435455322265625,20.623123168945312,20.39013671875,20.785369873046875,21.225112915039062,20.34514617919922,20.762237548828125,20.425048828125,20.63935089111328,20.835220336914062,20.49170684814453,22.085205078125,20.351791381835938,20.80847930908203,20.558555603027344,21.090248107910156,20.431121826171875,20.590576171875,20.50616455078125,20.504547119140625,24.07946014404297,20.38335418701172,20.735877990722656,21.69268035888672,20.6519775390625,22.803787231445312,20.439430236816406,20.70207977294922,20.55633544921875,20.64520263671875,20.628494262695312,20.727706909179688,20.977813720703125,20.429428100585938,20.495742797851562,20.419815063476562,20.58765411376953,20.56444549560547,20.57415008544922,20.634071350097656,20.591964721679688,21.000930786132812,21.014862060546875,20.696273803710938,20.55889129638672,21.685882568359375,20.39397430419922,20.54436492919922,20.52349090576172,20.580909729003906,22.876007080078125,20.70429229736328,20.824783325195312,20.574234008789062,20.73271942138672,20.512496948242188,20.549148559570312,20.36042022705078,20.474838256835938,20.38568878173828,21.51886749267578,20.750022888183594,22.118865966796875,20.745452880859375,20.66936492919922,20.952529907226562,20.547683715820312,20.547996520996094,21.259986877441406,20.51006317138672,20.441604614257812,21.104957580566406,20.672271728515625,20.47985076904297,20.45203399658203,22.05474090576172,20.60204315185547,21.411231994628906,20.39922332763672,20.593597412109375,21.136573791503906,20.870994567871094,20.97931671142578,20.80626678466797,20.587379455566406,21.162437438964844,20.64112091064453,20.455490112304688,20.60877227783203,20.81366729736328,28.171493530273438,20.47069549560547,20.465904235839844,20.428688049316406,23.057151794433594,20.517494201660156,20.555572509765625,20.54248046875,20.525672912597656,21.05811309814453,20.5523681640625,21.79521942138672,20.389053344726562,20.637420654296875,20.361175537109375,20.76611328125,21.29723358154297,21.386329650878906,20.475479125976562,20.598648071289062,20.557296752929688,20.99951171875,20.52623748779297,23.188941955566406,21.772789001464844,20.453582763671875,20.525978088378906,20.547088623046875,20.57811737060547,20.584884643554688,20.485984802246094,20.645721435546875,29.351707458496094,20.80742645263672,20.468467712402344,20.615684509277344,20.544998168945312,20.737060546875,20.39208221435547,20.65033721923828,20.539642333984375,20.503089904785156,20.451248168945312,20.915618896484375,20.53374481201172,20.401512145996094,20.71002197265625,20.52838134765625,26.652366638183594,20.56341552734375,22.888961791992188,20.546661376953125,20.629684448242188,20.54491424560547,20.471572875976562,20.49115753173828,20.460861206054688,20.413314819335938,20.716629028320312,21.714500427246094,20.43543243408203,21.629249572753906,20.594070434570312,20.42364501953125,20.646743774414062,24.173385620117188,20.92321014404297,20.95673370361328,25.324180603027344,20.599380493164062,20.471908569335938,20.880722045898438,20.949745178222656,20.67723846435547,24.385238647460938,20.646224975585938,21.427703857421875,20.56268310546875,21.233543395996094,20.49237060546875,20.931686401367188,25.23583221435547,22.33129119873047,20.540603637695312,20.901206970214844,21.005264282226562,20.588661193847656,20.53276824951172,20.49317169189453,20.705665588378906,20.630569458007812,20.443862915039062,20.47466278076172,20.723663330078125,20.87718963623047,20.57281494140625,20.455711364746094,20.51251983642578,21.017166137695312,22.183212280273438,20.430091857910156,20.52276611328125,20.519287109375,21.289291381835938,20.788742065429688,20.511032104492188,20.61290740966797,20.71363067626953,22.190826416015625,21.221145629882812,20.801048278808594,20.44879913330078,20.521766662597656,20.493698120117188,20.53455352783203,20.925888061523438,20.662086486816406,20.529510498046875,20.855377197265625,20.418777465820312,20.62969970703125,20.688003540039062,20.65837860107422,20.499656677246094,21.183731079101562,20.728652954101562,20.52985382080078,20.455276489257812,20.507286071777344,20.484024047851562,20.719223022460938,20.953636169433594,22.524696350097656,20.481910705566406,21.193260192871094,20.65020751953125,20.489608764648438,20.55109405517578,20.568206787109375,20.57074737548828,20.743911743164062,20.773468017578125,20.329673767089844,22.36658477783203,20.684051513671875,20.536598205566406,20.689598083496094,20.595748901367188,20.507469177246094,20.75092315673828,20.453018188476562,20.932846069335938,20.56908416748047,20.50609588623047,20.66297149658203,20.56634521484375,21.387283325195312,20.595184326171875,20.593582153320312,20.938438415527344,21.02721405029297,20.586746215820312,20.475509643554688,20.491905212402344,20.568138122558594,20.53173828125,20.41429901123047,20.51837921142578,20.52617645263672,21.447341918945312,20.368865966796875,24.063255310058594,27.586288452148438,20.438095092773438,20.491195678710938,20.536102294921875,22.01190185546875,20.41674041748047,20.52051544189453,20.703170776367188,20.738067626953125,20.506942749023438,20.589340209960938,20.55683135986328,20.49018096923828,20.460952758789062,20.966644287109375,20.57904052734375,20.561622619628906,20.43907928466797,20.781227111816406,23.594268798828125,20.76506805419922,20.864837646484375,20.480674743652344,20.440284729003906,20.67571258544922,20.504241943359375,20.60411834716797,21.268028259277344,20.47046661376953,23.170974731445312,20.765594482421875,20.42754364013672,20.705825805664062,20.77893829345703,20.473609924316406,21.362281799316406,20.764785766601562,20.42383575439453,20.44647979736328,20.87512969970703,20.999557495117188,20.651763916015625,20.47962188720703,20.59471893310547,20.5072021484375,20.718772888183594,21.253005981445312,31.88458251953125,20.982406616210938,20.59978485107422,20.367324829101562,20.54077911376953,20.665512084960938,30.96556854248047,20.912109375,20.465675354003906,21.532493591308594,20.55103302001953,20.374267578125,20.45928955078125,20.771148681640625,21.071922302246094,21.956497192382812,20.47241973876953,27.840431213378906,20.438011169433594,20.689369201660156,20.45685577392578,20.574127197265625,20.485694885253906,22.204429626464844,20.600425720214844,20.8487548828125,21.808250427246094,20.553131103515625,20.55681610107422,22.127357482910156,20.489295959472656,21.00444793701172,20.90740203857422,20.85596466064453,30.51105499267578,22.92877197265625,21.533485412597656,20.535987854003906,20.552215576171875,20.698074340820312,20.76732635498047,20.650039672851562,20.827972412109375,21.2081298828125,21.57575225830078,20.507659912109375,20.434974670410156,20.584190368652344,20.524391174316406,20.511398315429688,20.58245086669922,20.924636840820312,20.648414611816406,20.673324584960938,22.49310302734375,20.46312713623047,21.127487182617188,20.896217346191406,20.995872497558594,20.444900512695312,20.495140075683594,20.46599578857422,21.13787841796875,21.338783264160156,22.033828735351562,20.402206420898438,20.565505981445312,20.957176208496094,22.44652557373047,20.39344024658203,20.682266235351562,20.55803680419922,21.37804412841797,20.6318359375,20.593055725097656,20.604217529296875,20.62232208251953,20.69202423095703,20.90705108642578,20.485450744628906,20.79877471923828,20.495567321777344,20.802978515625,21.41553497314453,20.412620544433594,20.3858642578125,20.744346618652344,20.50908660888672,20.55963897705078,20.561317443847656,30.67829132080078,20.430633544921875,20.589988708496094,20.4464111328125,20.330795288085938,22.43505859375,20.5806884765625,20.838333129882812,20.42980194091797,22.34406280517578,20.743118286132812,20.822601318359375,20.36554718017578,20.438514709472656,20.667396545410156,20.628517150878906,21.006996154785156,20.88323974609375,21.080162048339844,23.20092010498047,20.449249267578125,20.512611389160156,20.68201446533203,20.498428344726562,21.04962158203125,20.41613006591797,20.500038146972656,21.409774780273438,20.647979736328125,20.701339721679688,20.633621215820312,20.39093017578125,20.719635009765625,20.371566772460938,20.44660186767578,20.51123046875,21.129859924316406,20.36719512939453,20.586708068847656,20.667816162109375,20.670555114746094,20.60619354248047,21.018539428710938,22.837234497070312,20.519187927246094,21.911270141601562,20.44725799560547,22.046554565429688,20.4288330078125,20.527687072753906,20.63733673095703,20.425743103027344,21.054588317871094,20.57866668701172,20.54828643798828,21.67591094970703,20.566383361816406,20.59644317626953,20.65069580078125,20.571998596191406,20.863510131835938,20.573219299316406,20.69727325439453,20.463592529296875,20.648780822753906,20.983062744140625,20.36725616455078,20.629974365234375,20.549339294433594,20.930511474609375,20.451194763183594,20.521392822265625,20.794296264648438,21.08844757080078,20.618927001953125,20.94660186767578,22.65003204345703,20.866256713867188,20.713966369628906,20.50981903076172,21.318389892578125,20.475936889648438,26.3702392578125,20.49620819091797,23.2105712890625,20.476226806640625,20.389450073242188,20.529495239257812,20.524879455566406,20.64067840576172,20.92351531982422,20.574668884277344,22.392974853515625,21.008209228515625,22.653945922851562,20.981964111328125,20.42578125,20.60205078125,20.52649688720703,20.45648193359375,20.664939880371094,20.63916015625,20.668914794921875,22.178298950195312,20.580833435058594,20.46312713623047,20.516212463378906,20.432594299316406,20.610885620117188,21.113174438476562,20.578964233398438,20.563194274902344,22.780319213867188,20.460952758789062,20.845703125,25.005508422851562,20.396156311035156,20.81391143798828,21.845779418945312,20.536903381347656,20.470611572265625,20.78894805908203,20.575439453125,21.04059600830078,20.573715209960938,20.399009704589844,20.51805877685547,20.410079956054688,21.09410858154297,21.65813446044922,20.704559326171875,21.068092346191406,20.7318115234375,20.668418884277344,23.58161163330078,21.83055877685547,20.36163330078125,32.08770751953125,20.678939819335938,20.385963439941406,20.399932861328125,20.710983276367188,20.72374725341797,20.547531127929688,20.88288116455078,20.761550903320312,20.540313720703125,20.459495544433594,20.471961975097656,27.764511108398438,20.8306884765625,21.082130432128906,20.496986389160156,20.54175567626953,20.421051025390625,20.616256713867188,20.485321044921875,22.41649627685547,20.941299438476562,21.12195587158203,20.688690185546875,20.618751525878906,20.74089813232422,20.547645568847656,20.620750427246094,21.065475463867188,20.721450805664062,20.560882568359375,20.741744995117188,20.36951446533203,20.460403442382812,21.95911407470703,20.385879516601562,21.045852661132812,21.56812286376953,20.899505615234375,20.5662841796875,21.391868591308594,20.606109619140625,20.458480834960938,21.154029846191406,22.09221649169922,20.583038330078125,20.559356689453125,24.999588012695312,20.92205810546875,21.01500701904297,20.665542602539062,20.913108825683594,29.58031463623047,20.999488830566406,20.6986083984375,20.64037322998047,20.427764892578125,20.9705810546875,21.24657440185547,30.46796417236328,20.654983520507812,21.27210235595703,20.639541625976562,20.55541229248047,20.43781280517578,21.74852752685547,20.747894287109375,22.20343017578125,20.65289306640625,20.61700439453125,20.50080108642578,20.471046447753906,20.632171630859375,24.055992126464844,20.845443725585938,21.808746337890625,20.66187286376953,20.611038208007812,21.16681671142578,20.48089599609375,21.803207397460938,20.42742919921875,20.992691040039062,20.87877655029297,20.55768585205078,20.572418212890625,21.68907928466797,20.59503936767578,20.8148193359375,20.813499450683594,20.601707458496094,22.528594970703125,20.49273681640625,20.435707092285156,21.23932647705078,20.54029083251953,20.595428466796875,20.650344848632812,20.443817138671875,20.394546508789062,20.562820434570312,21.16358184814453,20.49713897705078,22.441688537597656,20.440467834472656,21.112030029296875,20.46105194091797,20.41551971435547,20.717369079589844,20.43182373046875,20.833213806152344,20.554237365722656,24.297866821289062,20.94489288330078,20.538429260253906,20.442359924316406,20.510841369628906,20.53527069091797,20.88201141357422,20.857421875,20.38794708251953,20.721466064453125,20.64838409423828,23.980140686035156,20.610488891601562,21.17108154296875,22.892364501953125,20.540756225585938,24.275291442871094,20.778358459472656,20.461151123046875,20.670730590820312,20.583114624023438,20.709671020507812,20.49604034423828,24.75176239013672,21.17858123779297,20.46869659423828,21.289947509765625,20.52033233642578,20.37366485595703,20.686737060546875,20.586585998535156,22.509613037109375,20.807342529296875,20.47454833984375,20.697303771972656,23.058090209960938,20.66254425048828,20.491104125976562,20.540069580078125,20.97992706298828,21.782516479492188,20.629180908203125,21.310989379882812,23.2330322265625,20.42278289794922,20.505722045898438,20.70738983154297,20.57464599609375,21.517044067382812,20.54180908203125,21.198814392089844,20.543434143066406,20.43872833251953,20.50518035888672,20.575355529785156,20.59233856201172,20.623619079589844,20.46063995361328,22.357833862304688,20.67877960205078,20.891578674316406,20.727462768554688,20.884902954101562,20.790771484375,21.68470001220703,20.697128295898438,20.68456268310547,21.021705627441406,20.540184020996094,20.677467346191406,20.606834411621094,20.570144653320312,20.309112548828125,21.13494110107422,21.895599365234375,20.575393676757812,22.008201599121094,20.442062377929688,20.954727172851562,21.212509155273438,21.949371337890625,20.58924102783203,21.15087890625,21.043968200683594,20.656692504882812,20.4698486328125,20.460296630859375,20.638198852539062,20.69599151611328,21.765289306640625,20.533607482910156,20.571006774902344,20.8858642578125,20.61534881591797,21.03826904296875,20.71991729736328,20.52898406982422,20.430328369140625,20.440147399902344,20.469772338867188,20.91387939453125],"xaxis":"x","y":[21.224638239972652,20.180643707721647,20.201341431117804,19.1454180237343,21.79470380164697,21.087755789009996,22.432940196049124,19.578846464119135,19.242152996635603,24.328668011629535,21.754493646354923,21.476822454566275,22.377553705906216,19.555124576954142,22.494011033388475,22.606991826298977,18.49440443743341,20.76699650557159,29.702932107840674,20.84994793377778,20.282334104450392,19.302304652102425,20.014253448408297,19.02478155099295,20.401136284262034,21.066918904158555,20.30528553843669,22.443056343683086,18.530754586286314,19.472341179018663,21.511675814584603,20.024996233202344,21.986420084483047,22.32477661567823,19.024668956843474,20.277557571355647,20.011894681671833,23.15521167718674,21.342870607762556,20.008806098312462,21.88562823605827,23.73392435177354,20.358652273917528,22.63391493571178,20.2953770114005,18.603706022480708,19.476619646192038,22.286954060950823,20.827570650601533,19.658607240576167,20.944082894927185,17.889444547032436,20.210011039401028,20.041902410540708,22.116524819871177,23.07232514287102,18.931245369435278,19.243034483473725,22.175009164867692,18.8696149273692,22.748443989592825,20.18471556395119,19.630595419370202,19.596480627252774,22.075827573908075,20.81706640440522,20.033430227416382,19.472993206549905,19.47575297115503,19.712127286089668,22.22369094458836,19.974424393125233,20.76732896720974,19.73170766432226,20.622497836834853,19.074352191519743,22.597191120827183,21.03390016791011,22.014068840206768,21.41124459992273,20.690267441816314,22.238063995762765,22.835153147091482,21.940489607019817,20.06072495330695,19.26508618967162,20.593917077867186,20.807981392523768,19.461766940454588,19.92317059187003,20.905864572180928,19.887154874828134,22.674741834325705,19.54492482512454,21.650322094933124,23.6494594685329,21.673503182786,25.42292365756086,23.598781185238046,18.99960834234589,24.597055201180662,22.731123431811554,19.734159687698988,18.832054600441634,20.397708053035622,21.477527921557282,21.731372277489818,21.51301334156099,18.824250825082274,21.177749972976986,22.50813230656899,20.78875924862035,19.355480148534408,23.890614784737277,22.347688134832723,19.773789560999628,21.94580529932221,22.19000668879013,19.515006779114817,22.99883477706531,24.630356510701596,19.78112594827553,20.45440259621003,20.195062159996276,21.08554669425927,22.350651659007692,19.88287753302219,23.223107004233785,19.56773145357299,22.383567194160783,19.337397595787667,19.84140252838032,24.09542285957458,18.7932762528682,18.78833918294514,13.482051629033913,21.99805786914009,20.68539834867811,20.75326327581395,18.386700720994313,20.922404198298533,19.34599856610255,23.03775857233843,18.64679105166152,20.517600912543127,20.206849822052806,22.025147336797662,19.802893385023037,20.525632635954643,20.1770277231615,23.357834800271604,19.12730750072142,23.147280554074683,21.983261369651657,20.345804248394614,19.067786210059246,20.11323917875491,20.682137237753555,21.173013406383724,22.83375379636568,21.704780057376663,19.98202682053532,19.114539527933296,22.520164246985647,22.815206783859768,26.613491397737352,21.053960341071367,19.740225451250925,20.911400034394557,19.239293618022067,22.519031496965763,20.18180618902739,21.47581257946244,21.841152373314472,19.894016350184707,18.947642644803835,20.33821258053668,21.998370834730604,21.623042657920415,20.52474339677355,23.920588558527324,22.398384643288747,20.614611901935827,20.73652363425427,24.216251851014647,0,23.798062566113977,22.44169398922513,19.04780522125321,22.508853453023672,20.361268080220963,19.223547338390134,20.037360610262493,19.47969541290554,21.419330156983328,19.397345612637686,21.062803097791257,20.802720957968873,22.787613064091445,19.854352512470033,23.127238847948263,21.236410750491928,19.39740450234132,24.629767207576666,19.424905147339715,17.66325927218184,21.589387814254906,19.856012075783497,19.346653316091935,20.250448783340936,19.91174617647474,25.664004852328446,20.437440195258947,20.159778968313912,19.760702082570422,23.894186440194584,19.664395199250066,20.03503448449456,21.85979788510952,21.007452333347548,19.330905939549268,21.816602846893197,22.163197583801455,21.21476614620658,19.61795076264764,22.32608970797742,21.206937139516242,23.650902186871754,19.538495641550785,21.951268793802367,19.712186074557728,21.6397845044252,19.566140232030243,23.528005120775013,21.76194235515012,19.759231530600534,22.69657897049395,20.97108250648827,20.91944021038121,19.11091145461158,20.92382072739302,19.99564295418165,20.157995405754583,21.499214768467855,21.850862595399537,19.613806547176576,23.262476414885043,19.981118924422468,23.526907160221576,20.11872947152901,23.756207113729698,21.05135284285491,20.459271509540073,20.732604891283092,20.22193428509613,21.2413041438754,20.86782723578085,18.85414731140336,21.746512886013605,21.477990920672163,24.227674033652324,25.12798066804062,26.560353492028305,23.633181830866906,21.120826057383237,20.22792847217847,22.242317589489687,21.590387855717424,19.62630225708032,21.126056594579662,22.585399041291762,22.379011027342607,22.883846216197703,20.986097829919892,21.770367489208184,20.58669309831316,19.43536098584635,21.22950036954701,19.51332725702657,19.38984557001998,20.48325363998835,20.246730162293353,21.286321988690805,20.685520474331085,21.05996063251775,20.18838178436931,21.836335494435165,22.094100243073623,22.994379248863297,22.721264543402345,20.729598340663756,16.46658160832902,21.557923127735883,19.880806435323297,19.801676786988352,21.12828261562749,18.90461110336481,19.19842345841963,20.092049623606254,20.0459782414438,18.31795878333722,20.7963375193431,17.813568277801206,19.627334048970262,18.56499419954315,19.372221038245304,20.307717788382128,19.384134222108464,20.087817122473812,19.502286013993587,21.227181698040095,23.312865371844044,19.405079121883418,20.90714373186156,21.80480516491849,18.974285739983618,19.176540848422555,19.19474964169837,20.93275451807793,20.832136714555805,21.170532392724574,22.709608681261756,19.86611073536812,22.055547735078193,19.980960001271505,22.653172763321756,20.76687340459434,20.847191191309296,20.06982734293826,20.638856572715298,19.67156525332246,19.49519726866692,22.064314182636632,20.227612418267608,26.201435233706484,19.068639219627578,22.656078929958284,20.77129521126714,20.391385205003306,22.283464834927894,20.845264152741507,21.027619198526104,21.07443745441354,21.518584006439504,19.724170230709305,20.679280780496406,18.927945745238855,21.825732486030095,20.464091007664432,19.439137307013926,22.977548256996624,23.988556498480502,19.81821879465883,19.050281782029014,19.439732238591404,17.74375624808358,19.990444456103596,0,18.279601513401296,19.78875556523867,19.965832185850637,20.69408662857846,19.405573621751774,22.379934620081865,22.48871602661339,23.642331039751582,20.276556094805027,23.283922162181995,20.195578868662224,20.03811220566055,19.14964293785794,20.90803339180857,21.473710208099476,19.884197354704103,22.164351182655523,22.813153319462682,18.901120542977083,20.544639287700853,22.988276940898057,20.109117749689304,19.622850798933502,21.871633434683428,22.25017305320405,23.60048678836413,21.515067916471864,22.320806056618878,19.870759032200375,20.544449448312626,20.261067504600565,23.43234984844275,22.4595812045188,22.52530214820436,19.382106816183008,23.96470652783692,21.019906267101412,19.811383905702034,19.845505364796534,20.51412216383031,20.373076483151994,19.01511010272945,21.47364877893911,19.957005833266606,19.38761598733965,22.32447174476143,20.79356441567801,22.807056132685855,24.028256571904787,19.874893102286897,19.393396349595932,21.076706013187607,21.881924997624267,19.305427636738685,20.88679803779653,22.116604136681058,20.73926969664862,19.30704929356398,20.730402345071546,21.49837252262123,21.892478522473827,21.156990969891915,19.626240196846272,19.456579392491523,19.83361590003151,18.640025995609424,21.63591845810156,20.866343204364526,21.462764853897205,18.837375101562536,20.833447685328323,20.03253125195289,21.036829264514445,22.96164822427184,21.237452292596537,20.43520123557409,21.156628899315663,22.496434708234126,22.4307529287208,21.544986593746263,21.11234277813463,21.351967832505505,20.39596499860976,22.86476103379213,20.105565995835796,20.48988421076927,21.684940338873297,21.559903574554543,22.73966855399758,21.952844226236508,20.330025112864092,21.040198736231254,20.75190246631361,18.798041102461394,19.29157093266521,19.97055996653021,19.307592868706436,20.89335338152254,19.773921527816817,21.100709172660746,21.699220401665585,21.041684548715036,20.396544813616845,22.752749154816065,24.957468167964976,20.506265808043914,20.374729613814893,21.525022859298808,23.275631354399913,21.75794824506378,19.767052898077655,21.218145625261048,21.536078922759724,18.276769619768288,24.94868149530031,20.41835179368666,20.87359355439842,20.51145154622031,20.700075492290427,21.289957235167595,20.748925773432315,21.959550623213953,22.62198208884591,19.38396579774315,23.31051349089806,21.627932359551377,18.954513256380405,19.964316452607815,19.32031247988204,24.27577420165758,20.498820970198462,20.060863127783303,24.1535698900103,18.16161414446258,19.814472942891484,22.998819199975294,21.020751435153667,21.36177568305088,18.469113141288748,23.192581335464276,19.21725486016369,20.111754471035223,18.939248297157278,20.474687273562058,21.111612649480875,19.87603574815164,19.637857750008234,20.264370436179377,19.93146081401427,23.29863946166844,21.939745630681145,19.743514665635868,22.649298943378753,21.24045362673594,25.608747220451313,23.49090374555705,23.60775201354553,20.678206389277292,19.915728419308504,20.449383439914303,19.824402536620187,20.53678125858074,22.159724468145836,21.034942664057727,19.03923233773122,20.84977321505041,21.781868242254394,20.46100849412487,22.004305689493815,19.142842792258786,19.496508584409785,21.834751391009725,20.402181439081684,19.21550544985804,20.640134807950194,23.051954851262433,27.036279453018476,20.37977707208339,21.775846548503456,21.781910553210967,22.353218452653266,20.165877660805254,19.451496130670126,25.490707221646936,22.59150705240579,19.995956185326282,20.28232915493825,19.579291420064532,20.879430687458026,21.138173662648782,24.394611647917593,19.533857847503334,19.817723799212757,21.171020993541568,20.50991100623185,22.326932218927173,19.527299811101727,21.383177447413818,19.392837666571104,23.017914332247763,20.82549425312562,19.072268708954628,19.10179618438648,23.29660399040893,19.32638298997934,20.650313109616484,22.182752894747274,20.620515848299537,21.682830461889793,20.248785506000544,20.060843672034636,21.235100056704788,19.639114342316898,22.873156355744623,20.616811975583957,21.260660009666424,20.98111639712373,20.640272301462904,20.347463631132083,19.255662542640188,21.590649178257912,18.627786784209636,25.25576312419618,18.13136213470814,20.18507504172347,22.234490018926184,19.867378293238115,22.842407702796528,19.642302060721956,20.934073076796377,19.657897588782742,20.907660872216642,21.21194412605265,19.94161891161789,21.31905629271608,18.792730927848385,19.734888199833236,18.03332620487895,20.049848549450562,20.009665992350836,21.60568082130027,15.704606153972572,19.9492653280832,21.029500902796773,21.639091593296747,19.630944296038688,19.444205825873514,22.72561505846151,17.641741979338622,23.57832742490694,21.06232551474456,20.596453669823063,23.77569135147608,20.739222922576804,20.798718195952173,20.117453663452505,20.134364899685817,19.1796783064222,19.08437502019272,20.74807693982553,18.925763040720064,21.23645513023823,22.042981377352834,21.11735552640613,23.60188649068525,20.85643673732399,20.248587775712384,22.0971519060863,19.917597162486278,22.120758360865494,22.44443523329341,21.796740313790345,20.192956161016543,21.587428114342384,19.88222659302695,20.074407563972667,19.772030191805687,22.686480223069132,21.085461437053635,21.84811725117268,18.71863791145474,19.897316659199046,22.368646863289158,18.2265721237695,21.061479969798718,21.091487685756913,21.905619198419114,22.248853822319084,20.948206085285577,20.95171629836988,20.60386819333271,21.712312461652626,26.34702224299619,19.173408752888843,18.94863556098072,19.669251869992156,24.602654298813697,20.178503166977418,20.318803159054653,19.8813455186455,19.742006737354636,20.848691129006262,19.533504643608662,23.044574887856943,21.2197623270523,21.881578906408237,21.91240193642795,20.941991359524017,22.077006044437276,22.54693130601029,18.995877797816178,20.997149152437558,18.687341507608156,21.335453055117675,19.376016989288264,25.07655356851684,22.59004948235383,19.378047539426834,19.682163069515127,20.52340976581651,19.581570228112746,20.9498708711936,22.82667920636651,20.99544749522815,25.17876365321419,21.627453285628658,19.477956222501692,19.193137896037967,21.893341530106955,19.71728461871742,17.31416721288844,19.147406179743545,19.827554176093653,20.471143313052202,19.29233286786097,20.500085221471384,19.644130235297485,22.034075873229305,20.949040678200554,20.540968959330602,27.231751620760654,19.73015830855728,22.493025220696325,23.089701491122522,21.64924126399572,20.795739259102447,21.118903696326274,20.815053320149143,16.52324032760632,19.730383893455734,21.24843214899758,22.071670864625634,19.282486682508637,23.126218118696794,22.228032143928214,21.93139047566312,22.735567798975932,24.167967887858172,21.655361699836966,22.600876020595507,24.59815969618305,19.718235182480434,19.521220854266264,20.564957928128795,20.971944131576173,19.6146020174933,27.021590188993713,23.73507977629211,21.659326126254854,21.01213158467734,21.766780414954713,20.08203076321153,21.756353934255824,25.471067413053014,24.02283197509071,23.385758678777925,21.23702785682451,21.578861893090256,19.189211661593234,20.07860380095908,19.328382384869762,23.106213073613993,19.95329417355663,21.365528432523575,18.914065908446993,20.846904364921407,21.621710733177533,20.214357069981805,20.48695013292876,19.590051196125845,20.988896575504317,22.71500741252458,20.63081635106876,18.585982112110514,19.30101383466248,22.43563961615873,21.19573925103901,19.253482814272665,21.733085888220945,21.51937854264472,22.935901951961643,23.1601071574333,20.83646802652741,17.151393182854086,20.807011308942396,19.447379908647196,19.74096325611752,20.942433069954838,19.870033345928174,20.91477708386503,23.264639171673256,18.81898933836999,19.40497105732979,20.010315016400824,20.436358643191994,21.24511744232927,21.394403553854325,20.278388484789648,20.16621212904916,20.06466810918053,20.8905482708943,19.02658827934665,19.696262913257698,21.147132242697182,0,21.233800456337935,21.791167162104042,20.18131197591118,20.79775274411387,20.158015290040524,21.01738902857623,21.1949084635434,22.774677489141574,21.93032100053686,18.418089755206072,23.810210740127232,20.9793536678564,18.664791000361163,20.57237479635235,19.66651702047476,20.204034803826428,19.989961267682528,19.13829898996108,20.67820982860559,19.602487517256634,19.634034627117707,20.049508251055094,19.732903567032626,22.301656666857568,19.3351286191496,19.66019789348857,22.54105973810428,21.087085320291497,21.46282774650559,18.953344800503015,19.199864465913354,18.832769512656196,21.40009037173508,19.692718291075924,20.626400634350908,18.73677516240749,22.077517747249683,18.74408127304125,24.761947476774708,26.113208352362694,19.576225059708314,22.48651099143561,19.29251486362711,21.199347342918216,22.53052802544868,19.222080263625543,20.87727804782974,21.285530909063,20.6451326442611,19.40981146751902,20.008240359915625,19.734510138857544,19.369940873414674,21.839004592901738,19.643216988179674,20.34827538264879,20.295760247970698,21.15453445100085,23.946209069915817,20.161508356002734,21.461336053955172,18.855956110093064,20.13782559566735,20.22901210527111,18.772971741326142,20.346151863763343,0,18.758138614258336,23.01986369036341,19.826388660339887,21.76883742068305,20.30266681112443,20.98525964839946,20.015097338364022,20.523507399137607,20.436904665975636,19.162579413291002,20.375536910022955,23.70344643795525,20.910669810022966,20.379640393188353,19.217287998251134,20.77164874576569,21.098649782686852,21.445335194484834,21.911569915872246,28.094893882984294,22.184719195487645,19.636646744775323,20.26686372980413,20.815307943684378,20.695469281090816,25.87586915107988,20.22604051998145,19.449746389320087,22.323663662536706,20.105603080001295,19.720177445507503,21.133857610241805,20.04287236504247,21.180586044654405,23.433073191606535,19.844702412663505,25.31201588491192,18.563017473748218,20.726116689654802,20.55471999163649,19.12308826699376,20.688277632644148,23.08698185712261,19.4188490987079,19.38789150561276,23.032593297237835,19.840625811644227,21.990665865987562,22.00972851632279,22.544699771649725,22.804072710576072,21.602864772837197,22.7909106273511,25.661642339745672,23.190044679879648,21.797480754192584,17.862153020413583,20.62223350550368,20.658934051201378,20.583030645279486,23.466890235880964,20.879510709815698,21.526914680071894,22.700127681411185,19.683189962828273,20.26341148584,19.616726513482636,19.839377869097518,19.875302398958702,20.973185759004682,21.082163500516533,20.324838680602316,20.30745292915028,25.9613964584893,19.046718202293015,21.78315042330069,20.91962424027046,21.618102712310883,18.472822451162664,19.554830163405484,19.175778559517227,21.854300382519792,24.477823287446007,25.5492833010917,20.6322831501737,18.924421094438486,22.00276008739277,23.251480243552532,19.437442862246026,20.440974576504654,19.114316794022223,23.060566976295764,21.685850226111516,19.690963589379074,19.932730372590424,21.5156696442052,21.03832189892855,21.34321011850724,20.297903654101066,20.251155291034372,22.02017509916011,21.00569855470944,23.695624962968736,19.83550351971754,20.046496736817677,21.663202256858984,21.64914607700524,21.763695119906945,18.966254774482525,25.621581882833034,21.318917047123914,20.25569596256737,19.082528830592434,19.62869805766532,25.538294718669484,19.064589338432878,24.544735671741783,19.58243213803125,23.66501619396984,19.12391425698277,21.767558412000408,18.99637745675535,21.23800756191085,21.00116832090369,22.714272038785044,21.137414277628565,21.45606258158953,22.279207015403138,23.33275256138136,22.26485604208308,20.723080111555596,20.639447033358472,20.694537918678005,20.730065203035867,18.59276109193853,18.41344612635984,22.526317266984485,19.84998714954673,20.751047310028625,19.801872521805933,20.233535275947663,20.632626738131346,21.279464358226686,22.469580214601123,20.408744363117133,22.583074271938578,21.34410908350179,21.979041347822633,20.59404343806881,20.264946502479802,19.878697565839243,22.615047199531546,23.56945490190484,19.736798486144902,23.729801820804443,20.49274951238928,23.323658577961712,20.770035525423065,19.238103804361465,19.567540218990334,20.344212591822245,22.493051859930933,19.27583351001347,18.98927807008273,23.394529944648035,22.51205924599309,21.071056073380674,21.047075972047644,20.445539922646542,21.340147837597844,20.003307902740296,21.299913824117656,20.644505230109406,20.419105854581318,21.593824209166353,19.476347089125902,19.838668436714478,19.728765556885957,22.07230878544129,19.427802084161115,21.756421739524402,21.380030171433972,21.32600862652118,19.632965031850407,21.403421935393933,24.37658838285707,21.043820980181792,20.1938730272849,19.66793597026211,19.199993380834425,20.035891808657503,25.0888235331049,18.385719188798834,26.48889669142553,20.116390923202314,19.499674996520806,19.751886432327307,20.522012502780594,0,20.782571363318098,21.618914296187317,22.967539915594646,20.95701983082078,23.548104709979565,21.65670940491333,18.258239797953646,17.88154175390193,21.233312475041235,20.277059332832177,20.919229965693855,19.653189714082398,19.02398982442439,24.205441209659515,20.16062238071552,18.963476628745678,17.96562762596234,19.531857117715408,20.86635829845881,21.255773747330974,19.280122810790118,19.369392868205782,22.9021647605954,18.93136369208174,21.47821316674072,24.42926325370543,19.880057005564264,21.11943827672747,24.0347829295736,19.174320595643714,19.810878030088894,20.711569411286565,20.79181709109951,21.52825545142037,22.510563057499144,19.714589625374927,19.195910426040165,21.643407148870626,22.79188245530678,23.088899981103793,20.914444260183114,22.409894316390734,22.55442514417979,21.497350091305258,23.661739831337005,22.81337882106461,20.335424976569172,28.119486884464127,20.443137811296648,19.395826969085725,18.744741571834194,21.52611831315801,21.048400728737306,20.235023779555554,21.0819625214105,21.53807712673746,19.283653604184654,20.890428315584586,19.706229562174084,25.296616627387504,21.809197399053033,21.371449507411626,20.19656265688086,19.305685493406397,19.08654493081137,19.639706353282477,17.583516204502104,23.250222506325386,22.82294225806205,24.601371538637977,20.0953223269386,21.243839329097405,20.37329318617189,20.45930590947763,24.987649624596244,22.400315304318365,22.625652665035147,20.455200776646638,21.021720462702472,20.59279530263181,20.232040256335257,22.264357262987502,20.592440384261195,22.36255454353493,21.99517470550352,22.199722361671988,19.391954018620176,21.83585207290849,19.14268047046844,20.53559540130039,21.167744601954155,22.926085660752005,18.718526187000297,22.28042360433217,25.95498550595432,23.32578333060166,21.309621092817217,19.68447811713372,23.268620237666717,25.42582021905868,21.189524929277866,19.937692777054252,20.801380865239704,19.316707310363867,21.75315297668226,22.868455030888445,25.947611368302585,20.033901511817852,23.20532613123076,20.554742003246933,22.96074119343453,18.988728193664233,22.68524113143557,22.64902697475969,25.92407619036082,19.407373582124833,21.346588936497685,19.366735480432887,19.124759964560997,20.07378037525111,23.985931291943867,21.460510640680972,23.23197611805285,20.73330584327736,19.12586494836624,25.275464586661446,19.715668478243582,25.489298123537854,19.013002282515146,25.312874128977917,20.415195026786527,19.91413691399634,22.130140513245646,22.92736601506673,19.584806561357016,23.12482431901705,20.687407722542194,19.611919934246885,24.151034118830534,20.079932477068777,18.93225512877545,24.075045174066695,19.899787637164348,20.874095085342866,21.48054296008632,23.07558442006914,20.640568320462897,20.08863945104873,22.28815948936714,19.616841359021425,23.953590715604236,21.784577239189012,23.301772335078063,19.749416399115965,19.811951028236585,20.17477942437346,18.304477915148002,21.513176875338157,19.58793862702669,24.19084293977443,20.88004158687887,21.10029756186111,19.35190112925433,18.917516857772046,19.84423117509742,24.377548873947223,20.529649424214696,18.766505656199122,21.439000175436664,18.811308834423215,23.933428716720645,19.631145075685264,22.477074905595526,24.176989311672042,19.985865383441666,24.300186984861302,20.78628428680642,19.166666955002903,20.396533708238668,21.16161998720379,23.822353472570573,20.556259058220718,24.241029788998915,22.067946023704817,19.778390202711652,22.99407646829777,18.833019773765052,20.288010749874967,19.947775322749504,20.338620784352592,23.494951508290605,22.38615246636496,19.034944006994706,19.984953818018486,24.081396558763124,19.595170049801663,19.41162451128841,20.72798026434645,21.785986387464803,22.944212504137,23.12678795739032,21.831113335529626,24.147006515510686,20.371960850528456,19.32549212260673,20.504720566168093,19.431273145079818,20.869899532948505,21.94524199207118,21.941322673977073,19.72760807429288,21.84503668728796,18.979888467927786,20.204391739449186,19.721950276191546,19.579038969018928,19.030933465764726,23.8143209659356,20.855181011615457,20.74517719106974,20.090817980596317,20.872940195293037,21.393924057871644,22.20317352529318,19.736694248820182,23.68012909153893,20.722700819009958,21.714666548374087,21.05462253329071,19.397806220065235,19.129172314836154,19.83710046840733,21.397724240612124,22.701728218692,19.775576525065844,24.10746972295679,17.80002145055656,21.28200527852275,24.425354799557958,24.25287711008343,21.61681534013004,21.629837063073623,22.53034703709958,21.62524006847279,20.878967106147208,18.668189678924886,19.500769926583363,19.995538068644624,23.512127623334848,21.53372289651904,20.823420888524133,22.265101932257092,20.999782941566835,21.517654972490075,19.604530267831706,19.76742293183983,20.740147099081785,22.94133404816206,19.387966817051925,20.39789928981696],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données de consommation prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBNumM_train, SiteEnergyUse_train_log)\n","\n","SiteEnergyUse_pred_logLR = pipeLR.predict(BEBNumM_test)\n","\n","LRr2_log = metrics.r2_score(SiteEnergyUse_test_log, SiteEnergyUse_pred_logLR)\n","print(\"r2 :\", LRr2)\n","LRrmse_log = metrics.mean_squared_error(SiteEnergyUse_test_log,\n","                                        SiteEnergyUse_pred_logLR,\n","                                        squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=SiteEnergyUse_pred_logLR.squeeze(),\n","    y=SiteEnergyUse_test_log.squeeze(),\n","    labels={\n","        'x': f'{SiteEnergyUse_pred_logLR=}'.partition('=')[0],\n","        'y': f'{SiteEnergyUse_test_log=}'.partition('=')[0]\n","    },\n","    title=\n","    'Visualisation des données de consommation prédites par le modèle de régression linéaire<br>vs les données test'\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.2 Modèle Ridge"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre      Ridge()\n","0  ridge__alpha  1175.087131\n","               R²      RMSE       MAE\n","Ridge() -0.159119  2.306572  1.110623\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logRidge=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[20.788364274115793,20.69198915453076,20.545649553104,20.42056086406208,21.041154083220267,20.609791173998516,21.03292478465875,20.513875612652292,20.574445871226278,20.937695481112343,20.436522084334122,20.789375236667563,20.734353057063338,20.41663204528268,21.795913012235097,21.794065374155373,20.452720441480736,20.460253305020633,80.5150666831989,20.714175389002847,20.700405643003652,20.53575386046483,20.49452684472585,20.479585937332487,20.738220574837662,20.64035239851555,20.413644913906715,21.17913792366147,20.4945628735514,20.530604326072783,20.643003824209693,20.554876379440394,20.95839407182302,21.552840002418645,20.596913919716922,20.609599682638702,20.613220449345963,22.709104958218106,20.72575172412036,20.599000758803896,20.680544079443905,21.344539321853034,20.59121583593995,22.06660217027446,20.618875985501422,20.469540826754436,20.52773964704895,20.361064101889212,20.566649356614274,20.613577211255578,20.759736530925768,20.474838844645024,20.506139492359026,20.65388869294677,20.998588353752783,21.546040412412694,20.460638241873273,20.790196805598395,20.806676202460974,20.55496544129311,21.77591920252668,20.472415256121298,20.741805921646534,20.606324974462183,20.468861927874592,20.584269508265727,20.503927240458452,20.57497178340539,20.591973974923537,20.515147840370886,20.957683564229292,20.529946296619368,20.828009058916948,20.58781604416921,20.776436150931616,20.481401366406295,20.66378709027346,20.526307897408454,20.88789395889505,20.486184286495607,20.727996370082614,21.036819806249195,20.540488525736972,20.88203264001608,20.566074710388197,20.46543283660666,20.558780037950385,20.427465228949195,20.570863793257608,20.569666954237636,20.46782266097639,20.499427646119923,21.199116562610318,20.47953824053847,21.551482075144676,23.399559934692775,21.003223156703182,23.72082645385334,20.88744962675186,20.57839239735506,21.818411173647373,21.892593447629384,20.559651014433687,20.425029612551448,20.89035319579679,20.968339436691046,20.542683789785617,21.21289907437809,20.47249878152513,20.83427592308737,21.45638731459608,20.56248142855496,20.549293815720503,23.248736858276924,21.132928357020425,20.494477930714734,21.329693684123647,21.709868132473513,20.57424530617431,21.228916561023677,25.26925493942377,20.536038799108347,20.548474128927513,20.652713495815135,20.483524994389185,20.8336559074815,20.65044721540344,21.25991752689731,20.592141006525637,20.977096604100495,20.55654812304976,20.510314578699827,21.091061992265068,20.564094954808503,20.516100080492606,20.60490666641158,20.8585514682666,20.52940329257273,20.70608093236964,20.747630190163665,20.430795547015652,20.61115082047424,20.62298006192839,20.452930374015356,20.691125316320935,20.479687462718108,21.445053183489883,20.572508593503645,20.55662940779077,20.56815639549285,21.169960994931834,20.514236370993885,20.71037800020304,20.76039275882691,20.65005165920934,20.53879841858325,20.690787202061532,20.68560486893999,20.7553663464062,20.685070230290613,21.00642795633546,20.544551532233278,20.496346266815614,20.505020932537374,21.647876771603975,23.64034874755298,20.52198864469224,20.58254017969301,20.74228208964805,20.622472112746067,21.076733088667456,20.95597842947587,20.64551249061599,20.799433443610933,20.596427542149698,20.468972422702343,20.717023656301695,20.752432999343267,21.000972496820644,20.53394131194501,23.208042700075268,20.57619499133528,20.483558033930077,20.963811619269787,21.269609476160923,20.647403019705266,20.551231026595623,22.074236767912414,20.54257335685746,20.69545410156636,20.82100761580672,20.49666527994137,20.670245187479807,20.45961415367779,20.612632221628054,20.67328389653152,20.72118310553636,20.541892570954783,20.62987321800938,20.475069394859986,22.350439759692115,20.41991246000892,20.594370918650235,22.948182966710267,20.546336550740815,20.514391411590132,21.13060351614288,20.462988989776534,20.416732170694882,20.69903168695573,20.60638757984537,21.563212261697377,20.852715264753275,20.624760172466935,20.666046777258035,24.070996457281197,20.616357921900963,20.450707637136528,21.134368181459823,20.647364724502403,20.605205397788215,20.98616655787942,21.25237198922938,20.569142244421602,20.5477809012588,21.778380503027623,20.731617076993384,21.71575770265561,20.694749746497646,20.79267216332598,20.508752221576337,20.737476141247377,20.580103861748647,20.56272042265507,21.13235476288447,20.642548256738404,21.453823460225617,20.57370160032406,20.98352456856528,20.545760973801062,20.582167689819034,20.6919717958962,20.56107101275947,20.949978899745037,20.999505933779247,20.475658326355504,23.131625371402617,20.623156473559327,20.97772263066062,20.63101562717339,22.761624004561202,20.533362806088054,20.568072603916963,20.797168418762737,20.570151131830713,20.619892015616234,20.53089974339634,20.475763062648223,20.743102681997723,20.600717391522792,21.946269232892192,26.258133472806332,23.244273888292707,20.59648121353058,20.53724926197755,20.516900755118506,20.950670525235335,21.496592661872054,20.442066892987164,20.625943445862255,20.432413854657966,21.68101810181653,21.949529910203374,20.62179454811398,20.528583356073636,20.790166354852484,20.568488060419796,20.79038278558093,20.50703848339088,20.52542381124326,20.527378445715307,20.486638181042274,21.171514999034287,20.46019209031058,20.477196079077572,20.6567592000801,21.219466739113525,20.856234516648183,21.705071367567022,21.903183262242703,21.094293343286786,20.594095766616807,20.45065338196734,20.504267133593107,20.70064198820952,20.859446934259708,20.490790833630655,20.542856585298143,20.473139635955842,21.002230141986267,20.431395462017274,20.484419199003547,20.492854432946196,20.422213798349294,20.380505050779764,20.527029767731626,20.486744235064982,20.58002934797845,20.457964154849932,20.47895230915818,20.6974766235869,20.661229438947295,20.588754709881034,20.662812581866646,20.988895082507316,20.552591418658135,20.555151639146835,20.590743811365304,20.689378919306318,20.6267497194769,20.810953742024704,21.037986529962858,20.63771310804451,21.047505496165233,20.55875493726264,20.9295282930929,20.746353773877544,21.398351497376954,20.577663448331123,20.48593805206798,20.554323076477136,20.570766370292368,20.9125576184889,20.573209851880428,27.908594217109275,20.57737471787498,21.896098373336006,20.77315406011974,20.753905751363156,21.1527333238842,20.49918119759408,20.666539441652983,20.806402544835272,21.01568608290432,20.549020457542646,20.90419142469574,20.568545531103457,20.44891393154189,20.59325903502504,20.482699266151787,21.649505775377886,25.636482907274406,20.559692098854182,20.559096853350354,20.41873632587091,20.47296803099954,20.866755811196303,20.74345157079071,20.45931158216549,20.615231250590213,20.7034642251407,20.508886634089155,20.649688818965785,20.67934525394056,21.322794732106196,23.000978054116047,20.54484727216477,21.661647854308676,20.63102726453925,20.536475200328013,20.589132747451412,21.00708374445747,20.53659913103605,20.556035340846062,20.564841898990803,20.97882919874979,20.536680559673325,20.58189332227532,22.509374464331202,20.823446077051184,20.680396136362738,20.78884158293596,21.327020813055377,23.074429364009593,20.737548790720123,20.53550083313363,20.64547523107162,20.50263749525383,20.61504855229905,21.00616365034817,20.54464274413731,20.639950420910143,20.412403070595808,20.907594729191192,20.510411255646794,20.640266074009567,20.59215855246978,20.610576127427123,20.948385751060368,20.513818266975345,21.208416156891435,20.637195030680537,20.66089139640419,20.755019022149302,20.496388229788227,20.635616371826494,21.31994280197927,20.57026728209272,20.51830923470333,20.89532330371421,20.728257456280602,20.522958576704255,20.79971088264708,20.634412536577695,20.461595147788806,20.55248839256825,20.58498161570505,20.733294266384682,21.075271969348645,20.829672489821196,20.54546077732092,20.356223035375784,20.629211835465444,20.76918637198501,21.11446185275368,20.814332465181685,21.249733598841093,20.52132881502418,20.616086129431512,20.704857131023523,20.670112804074645,22.00217773765246,20.877416453987234,20.725320163885193,20.888056691481438,21.07257753519042,22.49135981290997,20.72781902495932,20.48959407197014,20.691811113437016,20.656924447480186,21.682076881111037,20.50298556194998,20.67228368894526,20.808746894730113,20.659476911361317,20.4933541923898,21.518193946604534,20.665071847896055,20.51271564410719,20.699191091258903,20.515627036213882,20.604279611384513,20.45472953716357,20.57727598904712,20.71778982251157,20.57643620448063,20.560372495531713,21.480436513563014,20.694983205321183,20.66485985909037,20.483098425981083,21.087118060777428,20.559354084784317,20.581045318200292,21.136076075212177,21.190996806004428,20.70282307103372,20.668176619817697,20.653791308719818,20.80005716337678,20.53994107151079,23.82652983531389,20.476248585741555,20.665827540024587,20.69358892124152,20.92642728873181,20.52192831618585,20.488829936044997,21.278357508575574,20.59017393805569,20.486467552214506,20.760967469527962,21.144454997442498,20.519158976751704,20.64007120801354,20.527793902218143,21.78097463269105,20.67037901718717,20.599119400149707,21.938817339098147,20.494136399494753,20.946382246415013,20.696778381635436,20.806156774380277,20.782406473256625,20.47303374671459,20.92753178980666,20.521339095827198,20.65802784035202,20.370397998532322,20.680869423351446,20.83801116388771,20.54932674820079,20.643743488530095,20.54354321324486,20.490525061238756,21.599378826704076,21.11205902000859,20.69370020331887,21.525217048141098,20.95556037098095,21.634996518294503,22.103267475543493,21.2162947291763,20.518969374623797,20.438993304516455,20.44709967210743,20.644947661699618,20.51706818255741,21.23546351259011,20.787082966972733,20.46015738302365,20.925461054757626,20.981923851700255,20.723805299234904,21.192236379531106,20.543131950418633,20.58087221511209,20.526376396100503,20.656276416790433,20.451091670209834,20.812916739523438,22.281576296602324,23.947692657608613,20.729819729912524,21.505899899599864,20.606125822228343,21.512767750803985,20.451416740812917,20.546388290452033,28.79087894918912,21.55789080416732,20.711795091865415,20.518821139501835,20.587419592876486,20.46300644582532,20.57272078255136,21.740280474489392,20.494697056898755,20.572491314274856,20.65257583095438,20.476374024355756,21.434035663057404,20.53805394258426,21.039993571014605,20.51423351880098,22.58205097540002,20.507381948875633,20.5502057586573,20.448296013593513,20.620821924770617,20.44165500574234,20.837141575776464,21.20982048332945,20.403525156576272,20.751053075582295,20.476206596916853,20.63827982403581,20.85228783139475,20.537919467591426,22.10530350017083,20.413760713879906,20.78948181487813,20.55951529770349,21.07701312389355,20.48068283822729,20.5948759643222,20.518145238108062,20.542127428704116,23.877195130272316,20.43414468644337,20.721020485194163,21.671773082919852,20.64663133401708,22.655086115236927,20.45079303035763,20.687632962977705,20.561959498173607,20.64236695601579,20.664707140163085,20.765128658242617,20.959981484260847,20.479580118649558,20.503347551391148,20.436176240592253,20.605303371359938,20.569372500224983,20.618782753159664,20.629159290499086,20.593467813879148,20.97627804054123,20.968999466959517,20.712801826647443,20.599960162965843,21.573031476762175,20.44547082250055,20.58415650536683,20.57526068545781,20.596879055170152,22.70386328137975,20.733683696948184,20.846408616849068,20.613871379369826,20.72179889504956,20.520646672558662,20.56557205126016,20.41320604883999,20.484815046262707,20.43780008752744,21.512804213093286,20.763374564319154,22.243485863822634,20.794534229529116,20.676273859350612,20.957894195551127,20.605059694658177,20.59371837837604,21.238313225571375,20.51391410276997,20.45269573967537,21.089157510103426,20.676097559186932,20.532192041046702,20.50399876075382,21.956120126103333,20.61131523194799,21.369413507956768,20.450096675429,20.592990357413253,21.182563388533175,20.87608092838699,20.939510558696018,20.78283652533337,20.624922357025863,21.110097416581578,20.642094847473864,20.50494767831395,20.64870818550981,20.800367274847964,27.612741439271424,20.478482480438064,20.514271283091457,20.478841145917595,22.98212176214794,20.52284013920285,20.553151797839657,20.547418432714377,20.532534241782336,21.13790505537613,20.56246651504772,21.868128996097212,20.438738429531856,20.643041283455652,20.408096117334086,20.784636562201218,21.2631763523249,21.331161644517913,20.522053550567772,20.595793957403156,20.556088774268318,20.9553135123095,20.570456662272584,23.02019450231634,21.75281541058502,20.507661927159685,20.530375333595728,20.54921041023647,20.581521910180456,20.618227695478584,20.534190316536943,20.677095664309913,28.879780586849204,20.83059322725339,20.516835948362143,20.62423044066099,20.53610007531166,20.74950264739674,20.441496485240137,20.665415460712655,20.590363862933515,20.510672097397812,20.46290795655095,20.932614544623842,20.56648521083085,20.4527522569905,20.732515918046314,20.540822843399404,26.544543017137322,20.570830900928062,22.93956870676812,20.586358945897178,20.620675297148807,20.544679369413796,20.47943258773351,20.515203126975504,20.47070470950553,20.431694381566032,20.740930339023365,21.69194960612453,20.484729653041608,21.615743030573785,20.59943352293621,20.47745107990641,20.689300654927393,23.913565200606342,20.885204437280915,20.956846478983937,24.843384444403917,20.598186441268734,20.482103182072258,20.907733293996134,20.944049377211087,20.672287722007837,24.251859526593098,20.67298028609064,21.422969413527376,20.601672797814132,21.23479615435844,20.498658313051585,20.920152946488294,24.91242045459272,22.315094738517384,20.580127766568964,20.874818672382567,20.998255203039836,20.600278589719064,20.572526291144307,20.540387536626675,20.747414494465215,20.637242718729098,20.492282303483275,20.521992675338062,20.70554328056666,20.863081621637466,20.573867872028842,20.502988680072388,20.55592141549628,21.042030231005377,22.12375105689935,20.473636901602365,20.529910015781418,20.526628278609536,21.2488885718665,20.76954101166091,20.51943056782653,20.636469709180762,20.722899729945254,22.07347030507301,21.228475267764317,20.818058104844525,20.459241757970343,20.528885927585936,20.50146811680512,20.544692918955477,20.942858585714852,20.693999115038267,20.54026586644102,20.886837631605008,20.4712477317347,20.636737322923953,20.68186527410875,20.689273318517586,20.542733248967018,21.17677023341383,20.775928664078346,20.536240742808022,20.49793189096721,20.54615599466864,20.49290301553383,20.712041140597847,20.92698304519665,22.40044884917722,20.528448086219946,21.247963664789495,20.681104736847903,20.542731730623135,20.590333106337596,20.614134866129195,20.60715314436973,20.753018500172665,20.81875277605805,20.38987613816141,22.224627581258932,20.74924187849082,20.5375845049246,20.720765388616442,20.59491052277978,20.553958789929748,20.75313615298052,20.503281674611593,20.917455912765313,20.608737777261595,20.5477809012588,20.677317921339498,20.60643064342635,21.357082164148355,20.60225674980268,20.599611923212173,20.89340008997279,20.975667323956003,20.607636174798117,20.522941120655467,20.537476190611773,20.576926020655986,20.53553477368014,20.464995682385076,20.521234005894485,20.533582574503583,21.385016719960863,20.420781974013373,23.76811122990799,27.276702478457626,20.4887627716987,20.535042082987612,20.548356085361338,21.95156501507835,20.464315496262422,20.530242633075574,20.73991137891576,20.752691462461755,20.549785758779358,20.597512242469076,20.597585665834156,20.533711325936803,20.507741684875334,20.979056353233286,20.61529525610243,20.564507904476454,20.488104489140348,20.79364352973511,23.601859744926717,20.763140117570074,20.865859026745273,20.528458490116233,20.489047115774824,20.702383061804326,20.513192939726785,20.63767391064685,21.22346928405046,20.481376043286303,23.109009230654156,20.755710532798982,20.473219944568317,20.697785190602126,20.756157687284524,20.518827135004763,21.183453368240386,20.76969115136978,20.475880570026778,20.45862022309453,20.825616372664566,21.004177652640124,20.68601851500587,20.488495565182138,20.629091424369186,20.551173370226326,20.701836634243072,21.234786303142364,30.74772530805425,20.96997148366995,20.636056392956,20.426037827808596,20.57883521205016,20.692439354373338,30.286546552072757,20.872857015285867,20.511774891294976,21.47770261284944,20.590321468971737,20.425655250370795,20.50383221227708,20.76774580789783,21.038388675446217,22.03481292737577,20.520188883630112,27.455038632967902,20.45539846768006,20.68472591851537,20.46653838636314,20.582435612829016,20.500947463222285,22.152356582407712,20.601398531065463,20.856130542528202,21.747131327368667,20.56642747391834,20.590879708892913,22.004989809838367,20.522056757209448,20.960325317059123,20.90395394388056,20.8486167489767,30.1089912890491,22.74979911641976,21.493177833931647,20.541652117931868,20.586314039183236,20.684093299210517,20.799821300272587,20.676474375094585,20.835228387730773,21.176073190320754,21.513805260476907,20.51635248455716,20.482145981001192,20.624454216672458,20.564230317898705,20.52509958241739,20.624037508286857,20.921635158053732,20.681791518253505,20.7028446636637,22.44399506976337,20.51189726045648,21.11540190089986,20.865427087645692,20.97524988504441,20.490430386709836,20.538121887596883,20.51445748094518,21.089200984837582,21.253369785419068,21.93997029747588,20.453832686524194,20.57454159245752,20.943819484584697,22.42791288249531,20.442741683386917,20.728996993168124,20.56364691622298,21.28852896640929,20.622583825149476,20.632379262821576,20.641351237482418,20.658446962342897,20.718597985427376,20.875086101307613,20.494276224705043,20.79419601487749,20.544859764878844,20.78862331865975,21.391963143749013,20.468275695431466,20.459622241501755,20.768052736528457,20.550573869064657,20.59629272762121,20.566532982955696,30.211770747216775,20.486645522260282,20.5946529481314,20.494685419532896,20.391000223364376,22.28210182184276,20.62641856537129,20.811446989294605,20.47984195938136,22.180699359506235,20.72420729932212,20.864986415410417,20.419463224821996,20.4861936429784,20.658921853680237,20.659459059532526,21.00798281642109,20.87019580329413,21.06940325837992,23.04432838262037,20.47018897728233,20.51433105980402,20.70485624692354,20.549912691902495,21.053749319246293,20.46914764849352,20.50728679973399,21.361519888457543,20.677977510731193,20.733578149960408,20.64278606903769,20.44605007686333,20.763870825749954,20.42377148299386,20.45730413655503,20.51957021621682,21.066799535987904,20.42799382586543,20.64080820887108,20.699107918649812,20.661743914900736,20.611466734701384,20.99066781221677,22.743351763099835,20.559511366043395,21.97101348935479,20.499212856483375,21.92295096861824,20.474319675641873,20.54446306302115,20.63435642656509,20.472376346187744,21.047412031841827,20.581999042180623,20.554930529195538,21.638852306083663,20.599256262708487,20.634596816666974,20.654829024330848,20.60898198512461,20.83410494061961,20.613731016990133,20.7140377749453,20.514375875126746,20.683613627374562,20.954561238824322,20.41853774158458,20.630067358675984,20.590139306059157,20.938371517303956,20.498857415192894,20.564452661747055,20.80748635324288,21.092015453623585,20.6179352279493,20.912733156677373,22.46937357725651,20.848554046460507,20.74519740732051,20.5562514352467,21.27076258590375,20.529642141435577,25.92437559586645,20.50379558997667,22.901516371974303,20.48587404655576,20.439169012068593,20.564212508209923,20.53168471407472,20.661149680224458,20.918964794296336,20.609098662539953,22.40283143235631,21.03993125368901,22.464053392085223,20.948045406940416,20.47380780836413,20.635835206841328,20.531532164327768,20.503675284657994,20.66473166648851,20.66180636588411,20.65570376838058,22.077397632123507,20.58972669051646,20.51189726045648,20.561214573857907,20.477688693057686,20.603900034264885,21.09795657023604,20.58223178949778,20.576728983597295,22.75346708103134,20.47218522889971,20.837643492220465,24.80206879158839,20.45378377251308,20.821797041839645,21.942515833936085,20.58494474643182,20.525733315716685,20.863497266494083,20.57405696428971,21.02555529550991,20.575736241189265,20.449904658892347,20.5256332838287,20.468969939391336,21.043763694206643,21.6024406307675,20.68898414426039,21.07981690207277,20.688372739742544,20.696854632518466,23.48407436143303,21.694012824939172,20.420245948054237,31.03570401372693,20.666477950494322,20.436114203530938,20.45202806814749,20.735381411208365,20.742142367131716,20.589290495104773,20.882584621825963,20.74891854101837,20.547808461290607,20.47376497464796,20.482243188408148,27.41901024396654,20.821631450132788,21.079570014433003,20.54200894461336,20.546813289689773,20.47241065685184,20.607179369032504,20.494020202656174,22.310807402362503,20.905616730635373,21.10021847271121,20.680206772654024,20.61090572851124,20.729967476719242,20.568905524437046,20.655770782423446,21.06331632055163,20.723361587980385,20.565847827354336,20.762845594632616,20.426301131550847,20.472684955128155,21.819074661229276,20.439387710749983,21.05641330086139,21.530361412484257,20.935860760520793,20.60721775199734,21.4558985036704,20.634904454635805,20.500794682968213,21.15892599954336,21.96495300633431,20.588132110807646,20.59515003436973,24.715692180182735,20.88844579731229,21.014940989541074,20.659544629573624,20.9494444911656,28.831062780970008,21.005113588935536,20.695160091195373,20.6724996424888,20.47341196110497,20.96543688308403,21.17385337186648,29.74758335999647,20.647837262700747,21.377400641482396,20.679126737332997,20.62319167694279,20.48833958861998,21.65331866673752,20.72576438849227,22.06842737403148,20.650543303902662,20.61195527463541,20.545471060956036,20.481393660700533,20.66500444118373,23.987286383735615,20.85774849002242,21.73673922928424,20.65678028763317,20.621221437058015,21.168168036007913,20.53013674272641,21.903448349581836,20.47778214562454,21.046423902575775,20.85901560633807,20.56319305895453,20.602499441881864,21.65102702098212,20.596615573697935,20.798478168591284,20.83238258295128,20.600205524245048,22.387635541341695,20.531324111614076,20.485038043236838,21.197430677943785,20.543679720151633,20.598066147954544,20.685057565918708,20.486843396491317,20.443725040801894,20.564657402543773,21.143926108301347,20.50969263509314,22.359754690975564,20.487010399929726,21.067908224644682,20.511532671741836,20.464794668507313,20.708090078069148,20.481680663186882,20.842200615596497,20.557634806774384,23.986676525207734,20.91584068700138,20.57678121697627,20.490990555873065,20.55689967945443,20.57718028145484,20.87918363383238,20.905283771366776,20.44068188508465,20.735483226623884,20.67518617904583,23.758119538104534,20.617847037979015,21.170452997348924,22.75275428212906,20.585746073817592,24.071292422783884,20.796293607692448,20.511975335179972,20.677794007175983,20.612135180812068,20.72726963674554,20.50861820458194,24.330971622241854,21.165429497629166,20.51625240641066,21.25908864253624,20.529033746821632,20.43063232074987,20.68204539568407,20.619462272214847,22.324808296030252,20.846755468550207,20.482196462124723,20.69759574607755,23.04018675746944,20.682216699345418,20.536859410221314,20.54527715739655,20.937014626982165,21.72840298644877,20.627053104098835,21.301936635719635,23.00960958347274,20.469544932241018,20.519293712655156,20.747062402925373,20.576098720870757,21.54052001961233,20.54452637047873,21.163014738421097,20.5482097735927,20.48788919787198,20.51543378579761,20.61201933761348,20.596945593448357,20.664254361545897,20.509616336748362,22.378845163731533,20.67365511249612,20.923287774956993,20.71460461405228,20.89065878384809,20.768513183160973,21.600363994268548,20.692292530737273,20.672081820692423,21.051612630443888,20.584103816743575,20.71630731390209,20.60938775954679,20.574399842275525,20.37052628212724,21.200005238972587,21.840582262432715,20.576732957310004,21.95837598526416,20.45286311231245,20.929774586860223,21.28338969770252,22.008003310905565,20.586419691444707,21.244630964023624,21.003206525666105,20.682262301110754,20.520096428553344,20.512123743076064,20.637439629946858,20.68899897937954,21.833523616267687,20.576043900703223,20.564148006469,20.874350065683156,20.64544167553689,21.012622646554732,20.754493265076032,20.57288305305392,20.47799726007291,20.485977325900638,20.478941437324778,20.91636948691894],"xaxis":"x","y":[21.224638239972652,20.180643707721647,20.201341431117804,19.1454180237343,21.79470380164697,21.087755789009996,22.432940196049124,19.578846464119135,19.242152996635603,24.328668011629535,21.754493646354923,21.476822454566275,22.377553705906216,19.555124576954142,22.494011033388475,22.606991826298977,18.49440443743341,20.76699650557159,29.702932107840674,20.84994793377778,20.282334104450392,19.302304652102425,20.014253448408297,19.02478155099295,20.401136284262034,21.066918904158555,20.30528553843669,22.443056343683086,18.530754586286314,19.472341179018663,21.511675814584603,20.024996233202344,21.986420084483047,22.32477661567823,19.024668956843474,20.277557571355647,20.011894681671833,23.15521167718674,21.342870607762556,20.008806098312462,21.88562823605827,23.73392435177354,20.358652273917528,22.63391493571178,20.2953770114005,18.603706022480708,19.476619646192038,22.286954060950823,20.827570650601533,19.658607240576167,20.944082894927185,17.889444547032436,20.210011039401028,20.041902410540708,22.116524819871177,23.07232514287102,18.931245369435278,19.243034483473725,22.175009164867692,18.8696149273692,22.748443989592825,20.18471556395119,19.630595419370202,19.596480627252774,22.075827573908075,20.81706640440522,20.033430227416382,19.472993206549905,19.47575297115503,19.712127286089668,22.22369094458836,19.974424393125233,20.76732896720974,19.73170766432226,20.622497836834853,19.074352191519743,22.597191120827183,21.03390016791011,22.014068840206768,21.41124459992273,20.690267441816314,22.238063995762765,22.835153147091482,21.940489607019817,20.06072495330695,19.26508618967162,20.593917077867186,20.807981392523768,19.461766940454588,19.92317059187003,20.905864572180928,19.887154874828134,22.674741834325705,19.54492482512454,21.650322094933124,23.6494594685329,21.673503182786,25.42292365756086,23.598781185238046,18.99960834234589,24.597055201180662,22.731123431811554,19.734159687698988,18.832054600441634,20.397708053035622,21.477527921557282,21.731372277489818,21.51301334156099,18.824250825082274,21.177749972976986,22.50813230656899,20.78875924862035,19.355480148534408,23.890614784737277,22.347688134832723,19.773789560999628,21.94580529932221,22.19000668879013,19.515006779114817,22.99883477706531,24.630356510701596,19.78112594827553,20.45440259621003,20.195062159996276,21.08554669425927,22.350651659007692,19.88287753302219,23.223107004233785,19.56773145357299,22.383567194160783,19.337397595787667,19.84140252838032,24.09542285957458,18.7932762528682,18.78833918294514,13.482051629033913,21.99805786914009,20.68539834867811,20.75326327581395,18.386700720994313,20.922404198298533,19.34599856610255,23.03775857233843,18.64679105166152,20.517600912543127,20.206849822052806,22.025147336797662,19.802893385023037,20.525632635954643,20.1770277231615,23.357834800271604,19.12730750072142,23.147280554074683,21.983261369651657,20.345804248394614,19.067786210059246,20.11323917875491,20.682137237753555,21.173013406383724,22.83375379636568,21.704780057376663,19.98202682053532,19.114539527933296,22.520164246985647,22.815206783859768,26.613491397737352,21.053960341071367,19.740225451250925,20.911400034394557,19.239293618022067,22.519031496965763,20.18180618902739,21.47581257946244,21.841152373314472,19.894016350184707,18.947642644803835,20.33821258053668,21.998370834730604,21.623042657920415,20.52474339677355,23.920588558527324,22.398384643288747,20.614611901935827,20.73652363425427,24.216251851014647,0,23.798062566113977,22.44169398922513,19.04780522125321,22.508853453023672,20.361268080220963,19.223547338390134,20.037360610262493,19.47969541290554,21.419330156983328,19.397345612637686,21.062803097791257,20.802720957968873,22.787613064091445,19.854352512470033,23.127238847948263,21.236410750491928,19.39740450234132,24.629767207576666,19.424905147339715,17.66325927218184,21.589387814254906,19.856012075783497,19.346653316091935,20.250448783340936,19.91174617647474,25.664004852328446,20.437440195258947,20.159778968313912,19.760702082570422,23.894186440194584,19.664395199250066,20.03503448449456,21.85979788510952,21.007452333347548,19.330905939549268,21.816602846893197,22.163197583801455,21.21476614620658,19.61795076264764,22.32608970797742,21.206937139516242,23.650902186871754,19.538495641550785,21.951268793802367,19.712186074557728,21.6397845044252,19.566140232030243,23.528005120775013,21.76194235515012,19.759231530600534,22.69657897049395,20.97108250648827,20.91944021038121,19.11091145461158,20.92382072739302,19.99564295418165,20.157995405754583,21.499214768467855,21.850862595399537,19.613806547176576,23.262476414885043,19.981118924422468,23.526907160221576,20.11872947152901,23.756207113729698,21.05135284285491,20.459271509540073,20.732604891283092,20.22193428509613,21.2413041438754,20.86782723578085,18.85414731140336,21.746512886013605,21.477990920672163,24.227674033652324,25.12798066804062,26.560353492028305,23.633181830866906,21.120826057383237,20.22792847217847,22.242317589489687,21.590387855717424,19.62630225708032,21.126056594579662,22.585399041291762,22.379011027342607,22.883846216197703,20.986097829919892,21.770367489208184,20.58669309831316,19.43536098584635,21.22950036954701,19.51332725702657,19.38984557001998,20.48325363998835,20.246730162293353,21.286321988690805,20.685520474331085,21.05996063251775,20.18838178436931,21.836335494435165,22.094100243073623,22.994379248863297,22.721264543402345,20.729598340663756,16.46658160832902,21.557923127735883,19.880806435323297,19.801676786988352,21.12828261562749,18.90461110336481,19.19842345841963,20.092049623606254,20.0459782414438,18.31795878333722,20.7963375193431,17.813568277801206,19.627334048970262,18.56499419954315,19.372221038245304,20.307717788382128,19.384134222108464,20.087817122473812,19.502286013993587,21.227181698040095,23.312865371844044,19.405079121883418,20.90714373186156,21.80480516491849,18.974285739983618,19.176540848422555,19.19474964169837,20.93275451807793,20.832136714555805,21.170532392724574,22.709608681261756,19.86611073536812,22.055547735078193,19.980960001271505,22.653172763321756,20.76687340459434,20.847191191309296,20.06982734293826,20.638856572715298,19.67156525332246,19.49519726866692,22.064314182636632,20.227612418267608,26.201435233706484,19.068639219627578,22.656078929958284,20.77129521126714,20.391385205003306,22.283464834927894,20.845264152741507,21.027619198526104,21.07443745441354,21.518584006439504,19.724170230709305,20.679280780496406,18.927945745238855,21.825732486030095,20.464091007664432,19.439137307013926,22.977548256996624,23.988556498480502,19.81821879465883,19.050281782029014,19.439732238591404,17.74375624808358,19.990444456103596,0,18.279601513401296,19.78875556523867,19.965832185850637,20.69408662857846,19.405573621751774,22.379934620081865,22.48871602661339,23.642331039751582,20.276556094805027,23.283922162181995,20.195578868662224,20.03811220566055,19.14964293785794,20.90803339180857,21.473710208099476,19.884197354704103,22.164351182655523,22.813153319462682,18.901120542977083,20.544639287700853,22.988276940898057,20.109117749689304,19.622850798933502,21.871633434683428,22.25017305320405,23.60048678836413,21.515067916471864,22.320806056618878,19.870759032200375,20.544449448312626,20.261067504600565,23.43234984844275,22.4595812045188,22.52530214820436,19.382106816183008,23.96470652783692,21.019906267101412,19.811383905702034,19.845505364796534,20.51412216383031,20.373076483151994,19.01511010272945,21.47364877893911,19.957005833266606,19.38761598733965,22.32447174476143,20.79356441567801,22.807056132685855,24.028256571904787,19.874893102286897,19.393396349595932,21.076706013187607,21.881924997624267,19.305427636738685,20.88679803779653,22.116604136681058,20.73926969664862,19.30704929356398,20.730402345071546,21.49837252262123,21.892478522473827,21.156990969891915,19.626240196846272,19.456579392491523,19.83361590003151,18.640025995609424,21.63591845810156,20.866343204364526,21.462764853897205,18.837375101562536,20.833447685328323,20.03253125195289,21.036829264514445,22.96164822427184,21.237452292596537,20.43520123557409,21.156628899315663,22.496434708234126,22.4307529287208,21.544986593746263,21.11234277813463,21.351967832505505,20.39596499860976,22.86476103379213,20.105565995835796,20.48988421076927,21.684940338873297,21.559903574554543,22.73966855399758,21.952844226236508,20.330025112864092,21.040198736231254,20.75190246631361,18.798041102461394,19.29157093266521,19.97055996653021,19.307592868706436,20.89335338152254,19.773921527816817,21.100709172660746,21.699220401665585,21.041684548715036,20.396544813616845,22.752749154816065,24.957468167964976,20.506265808043914,20.374729613814893,21.525022859298808,23.275631354399913,21.75794824506378,19.767052898077655,21.218145625261048,21.536078922759724,18.276769619768288,24.94868149530031,20.41835179368666,20.87359355439842,20.51145154622031,20.700075492290427,21.289957235167595,20.748925773432315,21.959550623213953,22.62198208884591,19.38396579774315,23.31051349089806,21.627932359551377,18.954513256380405,19.964316452607815,19.32031247988204,24.27577420165758,20.498820970198462,20.060863127783303,24.1535698900103,18.16161414446258,19.814472942891484,22.998819199975294,21.020751435153667,21.36177568305088,18.469113141288748,23.192581335464276,19.21725486016369,20.111754471035223,18.939248297157278,20.474687273562058,21.111612649480875,19.87603574815164,19.637857750008234,20.264370436179377,19.93146081401427,23.29863946166844,21.939745630681145,19.743514665635868,22.649298943378753,21.24045362673594,25.608747220451313,23.49090374555705,23.60775201354553,20.678206389277292,19.915728419308504,20.449383439914303,19.824402536620187,20.53678125858074,22.159724468145836,21.034942664057727,19.03923233773122,20.84977321505041,21.781868242254394,20.46100849412487,22.004305689493815,19.142842792258786,19.496508584409785,21.834751391009725,20.402181439081684,19.21550544985804,20.640134807950194,23.051954851262433,27.036279453018476,20.37977707208339,21.775846548503456,21.781910553210967,22.353218452653266,20.165877660805254,19.451496130670126,25.490707221646936,22.59150705240579,19.995956185326282,20.28232915493825,19.579291420064532,20.879430687458026,21.138173662648782,24.394611647917593,19.533857847503334,19.817723799212757,21.171020993541568,20.50991100623185,22.326932218927173,19.527299811101727,21.383177447413818,19.392837666571104,23.017914332247763,20.82549425312562,19.072268708954628,19.10179618438648,23.29660399040893,19.32638298997934,20.650313109616484,22.182752894747274,20.620515848299537,21.682830461889793,20.248785506000544,20.060843672034636,21.235100056704788,19.639114342316898,22.873156355744623,20.616811975583957,21.260660009666424,20.98111639712373,20.640272301462904,20.347463631132083,19.255662542640188,21.590649178257912,18.627786784209636,25.25576312419618,18.13136213470814,20.18507504172347,22.234490018926184,19.867378293238115,22.842407702796528,19.642302060721956,20.934073076796377,19.657897588782742,20.907660872216642,21.21194412605265,19.94161891161789,21.31905629271608,18.792730927848385,19.734888199833236,18.03332620487895,20.049848549450562,20.009665992350836,21.60568082130027,15.704606153972572,19.9492653280832,21.029500902796773,21.639091593296747,19.630944296038688,19.444205825873514,22.72561505846151,17.641741979338622,23.57832742490694,21.06232551474456,20.596453669823063,23.77569135147608,20.739222922576804,20.798718195952173,20.117453663452505,20.134364899685817,19.1796783064222,19.08437502019272,20.74807693982553,18.925763040720064,21.23645513023823,22.042981377352834,21.11735552640613,23.60188649068525,20.85643673732399,20.248587775712384,22.0971519060863,19.917597162486278,22.120758360865494,22.44443523329341,21.796740313790345,20.192956161016543,21.587428114342384,19.88222659302695,20.074407563972667,19.772030191805687,22.686480223069132,21.085461437053635,21.84811725117268,18.71863791145474,19.897316659199046,22.368646863289158,18.2265721237695,21.061479969798718,21.091487685756913,21.905619198419114,22.248853822319084,20.948206085285577,20.95171629836988,20.60386819333271,21.712312461652626,26.34702224299619,19.173408752888843,18.94863556098072,19.669251869992156,24.602654298813697,20.178503166977418,20.318803159054653,19.8813455186455,19.742006737354636,20.848691129006262,19.533504643608662,23.044574887856943,21.2197623270523,21.881578906408237,21.91240193642795,20.941991359524017,22.077006044437276,22.54693130601029,18.995877797816178,20.997149152437558,18.687341507608156,21.335453055117675,19.376016989288264,25.07655356851684,22.59004948235383,19.378047539426834,19.682163069515127,20.52340976581651,19.581570228112746,20.9498708711936,22.82667920636651,20.99544749522815,25.17876365321419,21.627453285628658,19.477956222501692,19.193137896037967,21.893341530106955,19.71728461871742,17.31416721288844,19.147406179743545,19.827554176093653,20.471143313052202,19.29233286786097,20.500085221471384,19.644130235297485,22.034075873229305,20.949040678200554,20.540968959330602,27.231751620760654,19.73015830855728,22.493025220696325,23.089701491122522,21.64924126399572,20.795739259102447,21.118903696326274,20.815053320149143,16.52324032760632,19.730383893455734,21.24843214899758,22.071670864625634,19.282486682508637,23.126218118696794,22.228032143928214,21.93139047566312,22.735567798975932,24.167967887858172,21.655361699836966,22.600876020595507,24.59815969618305,19.718235182480434,19.521220854266264,20.564957928128795,20.971944131576173,19.6146020174933,27.021590188993713,23.73507977629211,21.659326126254854,21.01213158467734,21.766780414954713,20.08203076321153,21.756353934255824,25.471067413053014,24.02283197509071,23.385758678777925,21.23702785682451,21.578861893090256,19.189211661593234,20.07860380095908,19.328382384869762,23.106213073613993,19.95329417355663,21.365528432523575,18.914065908446993,20.846904364921407,21.621710733177533,20.214357069981805,20.48695013292876,19.590051196125845,20.988896575504317,22.71500741252458,20.63081635106876,18.585982112110514,19.30101383466248,22.43563961615873,21.19573925103901,19.253482814272665,21.733085888220945,21.51937854264472,22.935901951961643,23.1601071574333,20.83646802652741,17.151393182854086,20.807011308942396,19.447379908647196,19.74096325611752,20.942433069954838,19.870033345928174,20.91477708386503,23.264639171673256,18.81898933836999,19.40497105732979,20.010315016400824,20.436358643191994,21.24511744232927,21.394403553854325,20.278388484789648,20.16621212904916,20.06466810918053,20.8905482708943,19.02658827934665,19.696262913257698,21.147132242697182,0,21.233800456337935,21.791167162104042,20.18131197591118,20.79775274411387,20.158015290040524,21.01738902857623,21.1949084635434,22.774677489141574,21.93032100053686,18.418089755206072,23.810210740127232,20.9793536678564,18.664791000361163,20.57237479635235,19.66651702047476,20.204034803826428,19.989961267682528,19.13829898996108,20.67820982860559,19.602487517256634,19.634034627117707,20.049508251055094,19.732903567032626,22.301656666857568,19.3351286191496,19.66019789348857,22.54105973810428,21.087085320291497,21.46282774650559,18.953344800503015,19.199864465913354,18.832769512656196,21.40009037173508,19.692718291075924,20.626400634350908,18.73677516240749,22.077517747249683,18.74408127304125,24.761947476774708,26.113208352362694,19.576225059708314,22.48651099143561,19.29251486362711,21.199347342918216,22.53052802544868,19.222080263625543,20.87727804782974,21.285530909063,20.6451326442611,19.40981146751902,20.008240359915625,19.734510138857544,19.369940873414674,21.839004592901738,19.643216988179674,20.34827538264879,20.295760247970698,21.15453445100085,23.946209069915817,20.161508356002734,21.461336053955172,18.855956110093064,20.13782559566735,20.22901210527111,18.772971741326142,20.346151863763343,0,18.758138614258336,23.01986369036341,19.826388660339887,21.76883742068305,20.30266681112443,20.98525964839946,20.015097338364022,20.523507399137607,20.436904665975636,19.162579413291002,20.375536910022955,23.70344643795525,20.910669810022966,20.379640393188353,19.217287998251134,20.77164874576569,21.098649782686852,21.445335194484834,21.911569915872246,28.094893882984294,22.184719195487645,19.636646744775323,20.26686372980413,20.815307943684378,20.695469281090816,25.87586915107988,20.22604051998145,19.449746389320087,22.323663662536706,20.105603080001295,19.720177445507503,21.133857610241805,20.04287236504247,21.180586044654405,23.433073191606535,19.844702412663505,25.31201588491192,18.563017473748218,20.726116689654802,20.55471999163649,19.12308826699376,20.688277632644148,23.08698185712261,19.4188490987079,19.38789150561276,23.032593297237835,19.840625811644227,21.990665865987562,22.00972851632279,22.544699771649725,22.804072710576072,21.602864772837197,22.7909106273511,25.661642339745672,23.190044679879648,21.797480754192584,17.862153020413583,20.62223350550368,20.658934051201378,20.583030645279486,23.466890235880964,20.879510709815698,21.526914680071894,22.700127681411185,19.683189962828273,20.26341148584,19.616726513482636,19.839377869097518,19.875302398958702,20.973185759004682,21.082163500516533,20.324838680602316,20.30745292915028,25.9613964584893,19.046718202293015,21.78315042330069,20.91962424027046,21.618102712310883,18.472822451162664,19.554830163405484,19.175778559517227,21.854300382519792,24.477823287446007,25.5492833010917,20.6322831501737,18.924421094438486,22.00276008739277,23.251480243552532,19.437442862246026,20.440974576504654,19.114316794022223,23.060566976295764,21.685850226111516,19.690963589379074,19.932730372590424,21.5156696442052,21.03832189892855,21.34321011850724,20.297903654101066,20.251155291034372,22.02017509916011,21.00569855470944,23.695624962968736,19.83550351971754,20.046496736817677,21.663202256858984,21.64914607700524,21.763695119906945,18.966254774482525,25.621581882833034,21.318917047123914,20.25569596256737,19.082528830592434,19.62869805766532,25.538294718669484,19.064589338432878,24.544735671741783,19.58243213803125,23.66501619396984,19.12391425698277,21.767558412000408,18.99637745675535,21.23800756191085,21.00116832090369,22.714272038785044,21.137414277628565,21.45606258158953,22.279207015403138,23.33275256138136,22.26485604208308,20.723080111555596,20.639447033358472,20.694537918678005,20.730065203035867,18.59276109193853,18.41344612635984,22.526317266984485,19.84998714954673,20.751047310028625,19.801872521805933,20.233535275947663,20.632626738131346,21.279464358226686,22.469580214601123,20.408744363117133,22.583074271938578,21.34410908350179,21.979041347822633,20.59404343806881,20.264946502479802,19.878697565839243,22.615047199531546,23.56945490190484,19.736798486144902,23.729801820804443,20.49274951238928,23.323658577961712,20.770035525423065,19.238103804361465,19.567540218990334,20.344212591822245,22.493051859930933,19.27583351001347,18.98927807008273,23.394529944648035,22.51205924599309,21.071056073380674,21.047075972047644,20.445539922646542,21.340147837597844,20.003307902740296,21.299913824117656,20.644505230109406,20.419105854581318,21.593824209166353,19.476347089125902,19.838668436714478,19.728765556885957,22.07230878544129,19.427802084161115,21.756421739524402,21.380030171433972,21.32600862652118,19.632965031850407,21.403421935393933,24.37658838285707,21.043820980181792,20.1938730272849,19.66793597026211,19.199993380834425,20.035891808657503,25.0888235331049,18.385719188798834,26.48889669142553,20.116390923202314,19.499674996520806,19.751886432327307,20.522012502780594,0,20.782571363318098,21.618914296187317,22.967539915594646,20.95701983082078,23.548104709979565,21.65670940491333,18.258239797953646,17.88154175390193,21.233312475041235,20.277059332832177,20.919229965693855,19.653189714082398,19.02398982442439,24.205441209659515,20.16062238071552,18.963476628745678,17.96562762596234,19.531857117715408,20.86635829845881,21.255773747330974,19.280122810790118,19.369392868205782,22.9021647605954,18.93136369208174,21.47821316674072,24.42926325370543,19.880057005564264,21.11943827672747,24.0347829295736,19.174320595643714,19.810878030088894,20.711569411286565,20.79181709109951,21.52825545142037,22.510563057499144,19.714589625374927,19.195910426040165,21.643407148870626,22.79188245530678,23.088899981103793,20.914444260183114,22.409894316390734,22.55442514417979,21.497350091305258,23.661739831337005,22.81337882106461,20.335424976569172,28.119486884464127,20.443137811296648,19.395826969085725,18.744741571834194,21.52611831315801,21.048400728737306,20.235023779555554,21.0819625214105,21.53807712673746,19.283653604184654,20.890428315584586,19.706229562174084,25.296616627387504,21.809197399053033,21.371449507411626,20.19656265688086,19.305685493406397,19.08654493081137,19.639706353282477,17.583516204502104,23.250222506325386,22.82294225806205,24.601371538637977,20.0953223269386,21.243839329097405,20.37329318617189,20.45930590947763,24.987649624596244,22.400315304318365,22.625652665035147,20.455200776646638,21.021720462702472,20.59279530263181,20.232040256335257,22.264357262987502,20.592440384261195,22.36255454353493,21.99517470550352,22.199722361671988,19.391954018620176,21.83585207290849,19.14268047046844,20.53559540130039,21.167744601954155,22.926085660752005,18.718526187000297,22.28042360433217,25.95498550595432,23.32578333060166,21.309621092817217,19.68447811713372,23.268620237666717,25.42582021905868,21.189524929277866,19.937692777054252,20.801380865239704,19.316707310363867,21.75315297668226,22.868455030888445,25.947611368302585,20.033901511817852,23.20532613123076,20.554742003246933,22.96074119343453,18.988728193664233,22.68524113143557,22.64902697475969,25.92407619036082,19.407373582124833,21.346588936497685,19.366735480432887,19.124759964560997,20.07378037525111,23.985931291943867,21.460510640680972,23.23197611805285,20.73330584327736,19.12586494836624,25.275464586661446,19.715668478243582,25.489298123537854,19.013002282515146,25.312874128977917,20.415195026786527,19.91413691399634,22.130140513245646,22.92736601506673,19.584806561357016,23.12482431901705,20.687407722542194,19.611919934246885,24.151034118830534,20.079932477068777,18.93225512877545,24.075045174066695,19.899787637164348,20.874095085342866,21.48054296008632,23.07558442006914,20.640568320462897,20.08863945104873,22.28815948936714,19.616841359021425,23.953590715604236,21.784577239189012,23.301772335078063,19.749416399115965,19.811951028236585,20.17477942437346,18.304477915148002,21.513176875338157,19.58793862702669,24.19084293977443,20.88004158687887,21.10029756186111,19.35190112925433,18.917516857772046,19.84423117509742,24.377548873947223,20.529649424214696,18.766505656199122,21.439000175436664,18.811308834423215,23.933428716720645,19.631145075685264,22.477074905595526,24.176989311672042,19.985865383441666,24.300186984861302,20.78628428680642,19.166666955002903,20.396533708238668,21.16161998720379,23.822353472570573,20.556259058220718,24.241029788998915,22.067946023704817,19.778390202711652,22.99407646829777,18.833019773765052,20.288010749874967,19.947775322749504,20.338620784352592,23.494951508290605,22.38615246636496,19.034944006994706,19.984953818018486,24.081396558763124,19.595170049801663,19.41162451128841,20.72798026434645,21.785986387464803,22.944212504137,23.12678795739032,21.831113335529626,24.147006515510686,20.371960850528456,19.32549212260673,20.504720566168093,19.431273145079818,20.869899532948505,21.94524199207118,21.941322673977073,19.72760807429288,21.84503668728796,18.979888467927786,20.204391739449186,19.721950276191546,19.579038969018928,19.030933465764726,23.8143209659356,20.855181011615457,20.74517719106974,20.090817980596317,20.872940195293037,21.393924057871644,22.20317352529318,19.736694248820182,23.68012909153893,20.722700819009958,21.714666548374087,21.05462253329071,19.397806220065235,19.129172314836154,19.83710046840733,21.397724240612124,22.701728218692,19.775576525065844,24.10746972295679,17.80002145055656,21.28200527852275,24.425354799557958,24.25287711008343,21.61681534013004,21.629837063073623,22.53034703709958,21.62524006847279,20.878967106147208,18.668189678924886,19.500769926583363,19.995538068644624,23.512127623334848,21.53372289651904,20.823420888524133,22.265101932257092,20.999782941566835,21.517654972490075,19.604530267831706,19.76742293183983,20.740147099081785,22.94133404816206,19.387966817051925,20.39789928981696],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge_log = np.logspace(-3, 5, 1000)\n","param_gridRidge_log = {'ridge__alpha': alphasridge_log}\n","\n","GridRidge_log, \\\n","BestParametresRidge_log, \\\n","ScoresRidge_log, \\\n","SiteEnergyUse_pred_logRidge_log, \\\n","figRidge_log = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=SiteEnergyUse_train_log,\n","                            y_test=SiteEnergyUse_test_log,\n","                            y_test_name='SiteEnergyUse_test_log',\n","                            y_pred_name='SiteEnergyUse_pred_logRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge_log)\n","\n","print(BestParametresRidge_log)\n","print(ScoresRidge_log)\n","figRidge_log.show()\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.722590553549784,1.7225905531804593,1.7225905528042613,1.722590552421062,1.7225905520307319,1.7225905516331372,1.7225905512281436,1.7225905508156127,1.7225905503954049,1.7225905499673768,1.7225905495313831,1.722590549087276,1.7225905486349031,1.7225905481741122,1.7225905477047463,1.7225905472266452,1.7225905467396463,1.722590546243584,1.7225905457382908,1.7225905452235935,1.7225905446993177,1.7225905441652856,1.7225905436213147,1.7225905430672206,1.722590542502815,1.7225905419279055,1.722590541342297,1.7225905407457902,1.722590540138183,1.7225905395192675,1.7225905388888343,1.7225905382466689,1.7225905375925525,1.722590536926263,1.722590536247574,1.7225905355562545,1.7225905348520698,1.72259053413478,1.7225905334041414,1.7225905326599058,1.72259053190182,1.7225905311296263,1.7225905303430618,1.72259052954186,1.7225905287257472,1.722590527894447,1.7225905270476758,1.7225905261851466,1.722590525306566,1.722590524411635,1.722590523500049,1.7225905225714988,1.7225905216256685,1.7225905206622365,1.7225905196808742,1.7225905186812498,1.722590517663022,1.7225905166258453,1.7225905155693666,1.7225905144932274,1.722590513397061,1.722590512280495,1.72259051114315,1.7225905099846393,1.722590508804569,1.7225905076025374,1.7225905063781366,1.7225905051309494,1.7225905038605525,1.7225905025665136,1.722590501248393,1.7225904999057424,1.7225904985381058,1.722590497145017,1.7225904957260034,1.7225904942805823,1.722590492808262,1.7225904913085426,1.722590489780913,1.7225904882248553,1.7225904866398398,1.7225904850253273,1.722590483380769,1.7225904817056061,1.722590479999269,1.7225904782611774,1.7225904764907405,1.722590474687356,1.7225904728504116,1.7225904709792819,1.7225904690733311,1.7225904671319114,1.7225904651543626,1.722590463140012,1.7225904610881755,1.7225904589981549,1.7225904568692396,1.7225904547007065,1.7225904524918174,1.7225904502418221,1.7225904479499554,1.7225904456154386,1.7225904432374768,1.7225904408152624,1.722590438347972,1.7225904358347663,1.7225904332747912,1.7225904306671762,1.7225904280110353,1.7225904253054645,1.7225904225495448,1.722590419742339,1.722590416882893,1.722590413970234,1.7225904110033725,1.7225904079812995,1.7225904049029874,1.7225904017673905,1.7225903985734419,1.7225903953200565,1.7225903920061278,1.722590388630529,1.7225903851921132,1.7225903816897108,1.7225903781221317,1.7225903744881623,1.722590370786568,1.72259036701609,1.7225903631754467,1.7225903592633323,1.7225903552784172,1.7225903512193461,1.7225903470847395,1.7225903428731921,1.7225903385832715,1.7225903342135198,1.7225903297624516,1.7225903252285537,1.7225903206102848,1.7225903159060745,1.7225903111143244,1.7225903062334047,1.7225903012616572,1.7225902961973916,1.7225902910388853,1.722590285784386,1.722590280432107,1.722590274980229,1.7225902694268984,1.722590263770228,1.7225902580082946,1.7225902521391396,1.7225902461607678,1.7225902400711477,1.722590233868208,1.7225902275498413,1.7225902211138995,1.722590214558195,1.7225902078804993,1.7225902010785425,1.722590194150013,1.7225901870925546,1.72259017990377,1.7225901725812147,1.7225901651223996,1.7225901575247904,1.7225901497858032,1.7225901419028091,1.7225901338731273,1.7225901256940293,1.7225901173627354,1.7225901088764133,1.7225901002321788,1.7225900914270937,1.7225900824581657,1.7225900733223458,1.7225900640165297,1.7225900545375539,1.7225900448821974,1.722590035047178,1.7225900250291537,1.722590014824719,1.7225900044304066,1.7225899938426834,1.7225899830579514,1.7225899720725448,1.7225899608827304,1.7225899494847063,1.7225899378745975,1.7225899260484596,1.7225899140022727,1.7225899017319435,1.7225898892333025,1.7225898765021015,1.7225898635340144,1.7225898503246353,1.7225898368694739,1.722589823163959,1.7225898092034329,1.7225897949831517,1.7225897804982835,1.722589765743906,1.7225897507150059,1.722589735406477,1.7225897198131164,1.7225897039296274,1.7225896877506117,1.7225896712705726,1.722589654483911,1.7225896373849228,1.7225896199677986,1.722589602226621,1.722589584155362,1.7225895657478822,1.7225895469979278,1.7225895278991288,1.7225895084449967,1.7225894886289221,1.722589468444173,1.7225894478838928,1.7225894269410973,1.7225894056086706,1.722589383879367,1.7225893617458055,1.7225893392004668,1.7225893162356936,1.7225892928436846,1.7225892690164943,1.7225892447460294,1.7225892200240467,1.7225891948421483,1.722589169191783,1.7225891430642377,1.7225891164506393,1.7225890893419487,1.7225890617289596,1.7225890336022942,1.722589004952401,1.7225889757695505,1.722588946043833,1.7225889157651533,1.722588884923231,1.7225888535075922,1.72258882150757,1.722588788912299,1.7225887557107122,1.7225887218915361,1.7225886874432892,1.7225886523542755,1.7225886166125826,1.7225885802060759,1.7225885431223968,1.7225885053489562,1.7225884668729314,1.7225884276812617,1.7225883877606443,1.7225883470975283,1.7225883056781122,1.722588263488338,1.7225882205138863,1.7225881767401723,1.7225881321523406,1.7225880867352596,1.7225880404735172,1.7225879933514148,1.722587945352963,1.7225878964618748,1.7225878466615618,1.7225877959351272,1.7225877442653617,1.722587691634735,1.7225876380253933,1.7225875834191495,1.7225875277974811,1.7225874711415208,1.722587413432052,1.722587354649501,1.7225872947739316,1.7225872337850372,1.7225871716621355,1.72258710838416,1.7225870439296536,1.7225869782767618,1.7225869114032242,1.7225868432863691,1.722586773903102,1.7225867032299031,1.7225866312428153,1.7225865579174369,1.7225864832289148,1.7225864071519357,1.722586329660717,1.7225862507289968,1.7225861703300296,1.7225860884365722,1.7225860050208777,1.722585920054685,1.7225858335092092,1.7225857453551334,1.7225856555625971,1.722585564101187,1.7225854709399269,1.7225853760472671,1.7225852793910736,1.7225851809386192,1.7225850806565688,1.7225849785109724,1.72258487446725,1.7225847684901843,1.7225846605439041,1.7225845505918769,1.7225844385968927,1.7225843245210544,1.7225842083257643,1.7225840899717109,1.722583969418856,1.722583846626422,1.722583721552877,1.7225835941559218,1.7225834643924753,1.7225833322186623,1.7225831975897954,1.722583060460363,1.7225829207840124,1.722582778513536,1.7225826336008538,1.7225824859969994,1.722582335652102,1.7225821825153709,1.7225820265350786,1.722581867658544,1.7225817058321133,1.7225815410011442,1.7225813731099866,1.7225812021019657,1.7225810279193605,1.722580850503388,1.7225806697941821,1.722580485730773,1.7225802982510694,1.7225801072918359,1.7225799127886734,1.7225797146759987,1.7225795128870196,1.7225793073537177,1.7225790980068212,1.7225788847757861,1.722578667588771,1.7225784463726146,1.7225782210528109,1.7225779915534845,1.7225777577973669,1.7225775197057716,1.7225772771985661,1.722577030194148,1.7225767786094182,1.7225765223597516,1.7225762613589737,1.7225759955193276,1.7225757247514495,1.722575448964338,1.7225751680653243,1.722574881960043,1.7225745905524004,1.7225742937445456,1.7225739914368365,1.7225736835278085,1.7225733699141415,1.7225730504906287,1.7225727251501393,1.722572393783586,1.72257205627989,1.7225717125259454,1.722571362406581,1.7225710058045276,1.722570642600375,1.7225702726725391,1.7225698958972189,1.7225695121483606,1.7225691212976142,1.7225687232142946,1.7225683177653413,1.722567904815272,1.7225674842261445,1.7225670558575097,1.7225666195663685,1.722566175207128,1.7225657226315523,1.7225652616887186,1.7225647922249696,1.7225643140838645,1.7225638271061297,1.72256333112961,1.7225628259892183,1.7225623115168816,1.7225617875414923,1.7225612538888526,1.722560710381621,1.7225601568392577,1.7225595930779694,1.7225590189106517,1.7225584341468299,1.7225578385926053,1.722557232050591,1.7225566143198539,1.7225559851958518,1.7225553444703732,1.7225546919314723,1.7225540273634041,1.722553350546562,1.7225526612574076,1.7225519592684067,1.722551244347958,1.7225505162603258,1.7225497747655691,1.7225490196194677,1.7225482505734528,1.7225474673745313,1.7225466697652099,1.7225458574834203,1.7225450302624434,1.7225441878308267,1.722543329912309,1.722542456225737,1.7225415664849848,1.7225406603988698,1.722539737671068,1.7225387980000302,1.7225378410788927,1.722536866595392,1.722535874231772,1.7225348636646978,1.7225338345651586,1.7225327865983797,1.7225317194237246,1.7225306326945997,1.7225295260583586,1.7225283991562026,1.7225272516230814,1.7225260830875908,1.722524893171872,1.722523681491508,1.7225224476554153,1.722521191265741,1.7225199119177534,1.7225186091997329,1.7225172826928605,1.7225159319711074,1.7225145566011204,1.7225131561421065,1.7225117301457185,1.7225102781559358,1.7225087997089463,1.7225072943330257,1.7225057615484147,1.7225042008671974,1.7225026117931754,1.7225009938217437,1.722499346439759,1.7224976691254164,1.7224959613481157,1.7224942225683306,1.7224924522374756,1.722490649797772,1.7224888146821133,1.7224869463139263,1.722485044107034,1.7224831074655154,1.7224811357835648,1.7224791284453513,1.722477084824872,1.722475004285809,1.7224728861813863,1.7224707298542161,1.7224685346361575,1.7224662998481626,1.7224640248001282,1.722461708790742,1.7224593511073316,1.7224569510257108,1.7224545078100242,1.72245202071259,1.7224494889737467,1.7224469118216927,1.7224442884723292,1.7224416181290993,1.7224388999828322,1.7224361332115765,1.7224333169804436,1.722430450441444,1.7224275327333245,1.7224245629814057,1.7224215402974186,1.72241846377934,1.7224153325112297,1.7224121455630648,1.7224089019905757,1.722405600835082,1.7224022411233268,1.722398821867312,1.7223953420641351,1.7223918006958239,1.7223881967291734,1.7223845291155808,1.7223807967908829,1.7223769986751958,1.722373133672749,1.722369200671728,1.7223651985441122,1.722361126145517,1.7223569823150346,1.7223527658750775,1.722348475631224,1.7223441103720625,1.7223396688690407,1.722335149876314,1.7223305521305978,1.722325874351018,1.7223211152389695,1.7223162734779716,1.7223113477335281,1.7223063366529914,1.7223012388654246,1.7222960529814735,1.722290777593233,1.7222854112741257,1.7222799525787753,1.722274400042893,1.7222687521831546,1.7222630074970962,1.7222571644630023,1.7222512215398051,1.7222451771669829,1.7222390297644672,1.7222327777325543,1.7222264194518178,1.7222199532830302,1.7222133775670891,1.7222066906249478,1.7221998907575518,1.7221929762457822,1.7221859453504043,1.722178796312022,1.72217152735104,1.7221641366676337,1.7221566224417206,1.7221489828329464,1.7221412159806728,1.722133320003976,1.7221252930016497,1.7221171330522211,1.7221088382139684,1.7221004065249548,1.7220918360030637,1.7220831246460466,1.7220742704315808,1.722065271317332,1.7220561252410334,1.7220468301205678,1.722037383854063,1.7220277843199991,1.7220180293773208,1.7220081168655663,1.7219980446050045,1.7219878103967823,1.7219774120230853,1.7219668472473089,1.7219561138142434,1.721945209450268,1.7219341318635593,1.7219228787443115,1.7219114477649726,1.7218998365804876,1.7218880428285623,1.7218760641299347,1.7218638980886656,1.7218515422924376,1.7218389943128762,1.7218262517058787,1.721813312011962,1.7218001727566263,1.7217868314507332,1.7217732855908991,1.721759532659909,1.7217455701271427,1.7217313954490208,1.7217170060694702,1.7217023994204026,1.7216875729222163,1.7216725239843165,1.7216572500056508,1.7216417483752722,1.7216260164729142,1.7216100516695967,1.7215938513282432,1.7215774128043286,1.7215607334465464,1.721543810597499,1.721526641594415,1.7215092237698904,1.7214915544526552,1.7214736309683658,1.7214554506404305,1.7214370107908556,1.7214183087411281,1.7213993418131266,1.7213801073300612,1.7213606026174517,1.7213408250041358,1.7213207718233143,1.7213004404136332,1.7212798281203034,1.72125893229626,1.7212377503033625,1.721216279513639,1.721194517310574,1.7211724610904409,1.7211501082636862,1.7211274562563585,1.7211045025115954,1.7210812444911585,1.7210576796770283,1.7210338055730563,1.721009619706678,1.7209851196306882,1.7209603029250826,1.720935167198969,1.7209097100925455,1.7208839292791562,1.7208578224674216,1.720831387403448,1.7208046218731212,1.7207775237044836,1.7207500907702014,1.7207223209901241,1.7206942123339377,1.72066576282392,1.7206369705377926,1.7206078336116881,1.7205783502432155,1.7205485186946508,1.7205183372962345,1.7204878044495995,1.7204569186313154,1.7204256783965703,1.7203940823829786,1.7203621293145315,1.7203298180056865,1.7202971473656017,1.720264116402523,1.720230724228323,1.7201969700631985,1.7201628532405313,1.720128373211918,1.7200935295523645,1.720058321965666,1.7200227502899572,1.7199868145034507,1.719950514730367,1.7199138512470502,1.7198768244882832,1.7198394350538067,1.7198016837150363,1.719763571421994,1.7197250993104496,1.719686268709275,1.719647081148025,1.7196075383647365,1.719567642313954,1.719527395174989,1.7194867993604075,1.7194458575247578,1.7194045725735339,1.719362947672384,1.7193209862565582,1.7192786920406082,1.7192360690283306,1.7191931215229617,1.7191498541376269,1.7191062718060415,1.7190623797934634,1.7190181837079088,1.7189736895116152,1.718928903532768,1.7188838324774824,1.7188384834420383,1.718792863925378,1.7187469818418557,1.7187008455342396,1.7186544637869763,1.7186078458396943,1.718561001400969,1.71851394066233,1.7184666743125114,1.7184192135519485,1.7183715701075113,1.7183237562474722,1.7182757847967047,1.7182276691521108,1.7181794232982663,1.7181310618232863,1.7180825999348965,1.7180340534767171,1.7179854389447364,1.7179367735039826,1.7178880750053769,1.717839362002763,1.7177906537701104,1.7177419703188708,1.7176933324154924,1.717644761599073,1.71759628019915,1.717547911353607,1.7174996790266994,1.7174516080271793,1.7174037240265072,1.7173560535771486,1.7173086241309339,1.7172614640574717,1.717214602662607,1.7171680702069057,1.717121897924152,1.7170761180398517,1.7170307637897184,1.716985869438132,1.7169414702965564,1.7168976027418963,1.7168543042347832,1.7168116133377693,1.716769569733416,1.7167282142422615,1.7166875888406483,1.7166477366783925,1.7166087020962855,1.7165705306433972,1.7165332690941775,1.716496965465328,1.716461669032428,1.7164274303463025,1.7163943012491039,1.7163623348900992,1.7163315857411323,1.7163021096117586,1.7162739636640147,1.7162472064268215,1.7162218978099866,1.7161980991178023,1.716175873062205,1.7161552837754903,1.7161363968225565,1.7161192792126587,1.7161039994106606,1.7160906273477567,1.7160792344316533,1.7160698935561847,1.71606267911035,1.7160576669867489,1.716054934589399,1.7160545608409195,1.7160566261890569,1.7160612126125396,1.7160684036262441,1.7160782842856481,1.7160909411905614,1.7161064624881093,1.7161249378749592,1.716146458598758,1.7161711174587844,1.7161990088057741,1.7162302285409197,1.7162648741140152,1.7163030445207341,1.7163448402990145,1.7163903635245468,1.7164397178053263,1.7164930082752683,1.7165503415868568,1.7166118259028118,1.7166775708867483,1.7167476876928163,1.7168222889542846,1.7169014887710694,1.716985402696156,1.717074147720916,1.717167842259278,1.717266606130733,1.7173705605421472,1.7174798280683565,1.7175945326315052,1.717714799479109,1.7178407551608064,1.7179725275037625,1.7181102455866983,1.7182540397125021,1.718404041379389,1.7185603832505734,1.718723199122406,1.7188926238909423,1.719068793516891,1.7192518449889025,1.7194419162851502,1.719639146333152,1.7198436749677875,1.720055642887457,1.7202751916083296,1.7205024634166253,1.7207376013188764,1.7209807489901148,1.7212320507199226,1.7214916513562888,1.7217596962472146,1.7220363311800093,1.7223217023182116,1.7226159561360803,1.7229192393505943,1.723231698850904,1.7235534816251683,1.7238847346847315,1.7242256049855762,1.7245762393469992,1.7249367843674612,1.7253073863375579,1.725688191150072,1.7260793442070554,1.726480990323907,1.7268932736304154,1.7273163374687328,1.7277503242882535,1.7281953755373856,1.7286516315522,1.7291192314419548,1.729598312971491,1.7300890124405233,1.7305914645598306,1.7311058023243846,1.731632156883452,1.732170657407717,1.7327214309534802,1.7332846023240147,1.7338602939281427,1.7344486256361424,1.7350497146330803,1.7356636752696883,1.7362906189109197,1.7369306537823248,1.7375838848144025,1.7382504134851022,1.7389303376606577,1.7396237514349526,1.740330744967629,1.7410514043211645,1.7417858112971605,1.7425340432720884,1.7432961730327687,1.744072268611853,1.7448623931236085,1.7456666046003015,1.746484955829504,1.7473174941926406,1.7481642615051194,1.749025293858388,1.749900621464274,1.7507902685019687,1.7516942529680246,1.7526125865297453,1.7535452743823412,1.7544923151102427,1.7554537005529494,1.7564294156758073,1.7574194384460995,1.7584237397148357,1.7594422831046237,1.7604750249039998,1.7615219139685911,1.762582891629478,1.7636578916091046,1.7647468399451003,1.765849654922326,1.766966247013483,1.768096518828591,1.7692403650736115,1.7703976725185158,1.7715683199750298,1.772752178284307,1.773949110314741,1.7751589709701037,1.7763816072081926,1.7776168580701206,1.7788645547203732,1.7801245204977316,1.7813965709771238,1.7826805140424475,1.7839761499703788,1.7852832715251499,1.7866016640642528,1.7879311056549974,1.7892713672018228,1.7906222125842342,1.7919833988052054,1.7933546761498615,1.7947357883542303,1.7961264727838184,1.7975264606217511,1.7989354770661716,1.8003532415365975,1.8017794678888748,1.8032138646383835,1.8046561351910904,1.8061059780820572,1.807563087220967,1.8090271521442307,1.8104978582732116,1.8119748871780956,1.8134579168469116,1.8149466219592107,1.8164406741638905,1.8179397423606432,1.8194434929845087,1.820951590292997,1.8224636966552517,1.8239794728427152,1.8254985783207673,1.8270206715407948,1.8285454102321772,1.8300724516936469,1.8316014530835207,1.8331320717082875,1.8346639653090502,1.8361967923453353,1.8377302122757935,1.8392638858353276,1.840797475308196,1.8423306447966659,1.8438630604847923,1.8453943908969284,1.8469243071505876,1.8484524832032978,1.8499785960931017,1.8515023261723909,1.8530233573347676,1.8545413772346664,1.8560560774994659,1.8575671539338732,1.8590743067163573,1.8605772405874454,1.8620756650297232,1.863569294439379,1.865057848289186,1.8665410512828067,1.868018633500347,1.8694903305351045,1.8709558836214621,1.872415039753923,1.8738675517972703,1.8753131785878925,1.8767516850262969,1.8781828421608737,1.8796064272629867,1.8810222238934755,1.8824300219606738,1.883829617770068,1.885220814065726,1.8866034200636417,1.8879772514771578,1.889342130534629,1.8906978859895198,1.8920443531231073,1.8933813737400071,1.8947087961567182,1.8960264751833993,1.8973342720991062,1.8986320546207078,1.899919696865713,1.9011970793092465,1.9024640887354074,1.903720618183253,1.9049665668876496,1.9062018402152348,1.9074263495957318,1.908640012448867,1.9098427521071237,1.9110344977345854,1.9122151842420991,1.9133847521990077,1.9145431477416721,1.9156903224790338,1.9168262333954296,1.9179508427508967,1.9190641179791808,1.920166031583668,1.9212565610314478,1.9223356886457215,1.9234034014967443,1.9244596912915095,1.9255045542623592,1.9265379910547,1.9275600066140128,1.9285706100723208,1.929569814634278,1.930557637463054]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.8836093558923157,1.883609355303637,1.8836093547040031,1.8836093540932095,1.8836093534710496,1.8836093528373106,1.883609352191778,1.8836093515342318,1.8836093508644487,1.8836093501822009,1.8836093494872561,1.883609348779379,1.8836093480583274,1.8836093473238573,1.883609346575719,1.8836093458136578,1.883609345037414,1.8836093442467243,1.8836093434413201,1.883609342620927,1.8836093417852662,1.8836093409340542,1.8836093400670006,1.8836093391838111,1.8836093382841856,1.883609337367818,1.8836093364343967,1.8836093354836039,1.8836093345151175,1.8836093335286068,1.8836093325237373,1.8836093315001674,1.8836093304575485,1.8836093293955263,1.8836093283137398,1.8836093272118213,1.8836093260893962,1.8836093249460824,1.8836093237814915,1.8836093225952277,1.8836093213868872,1.8836093201560598,1.883609318902326,1.883609317625261,1.883609316324429,1.883609314999389,1.8836093136496892,1.8836093122748718,1.8836093108744696,1.8836093094480053,1.883609307994994,1.8836093065149428,1.883609305007348,1.8836093034716967,1.883609301907466,1.8836093003141259,1.8836092986911332,1.8836092970379368,1.8836092953539743,1.8836092936386737,1.8836092918914504,1.8836092901117119,1.8836092882988522,1.8836092864522553,1.8836092845712935,1.8836092826553266,1.8836092807037041,1.8836092787157612,1.883609276690823,1.883609274628201,1.8836092725271933,1.8836092703870861,1.883609268207152,1.8836092659866486,1.883609263724822,1.8836092614209032,1.883609259074108,1.8836092566836395,1.883609254248684,1.8836092517684146,1.8836092492419878,1.8836092466685441,1.8836092440472085,1.8836092413770902,1.8836092386572814,1.883609235886857,1.8836092330648755,1.8836092301903768,1.8836092272623843,1.883609224279902,1.883609221241916,1.8836092181473938,1.8836092149952828,1.8836092117845107,1.8836092085139873,1.8836092051825997,1.8836092017892152,1.8836091983326804,1.8836091948118199,1.8836091912254371,1.8836091875723122,1.8836091838512037,1.8836091800608454,1.8836091761999494,1.883609172267203,1.883609168261269,1.8836091641807853,1.8836091600243643,1.883609155790594,1.8836091514780333,1.8836091470852174,1.883609142610652,1.8836091380528164,1.8836091334101603,1.8836091286811059,1.8836091238640447,1.8836091189573394,1.8836091139593223,1.883609108868293,1.8836091036825215,1.8836090984002447,1.8836090930196656,1.8836090875389564,1.883609081956253,1.8836090762696576,1.8836090704772361,1.8836090645770203,1.8836090585670033,1.8836090524451425,1.8836090462093564,1.8836090398575251,1.8836090333874886,1.8836090267970476,1.8836090200839617,1.883609013245948,1.883609006280682,1.883608999185796,1.8836089919588779,1.8836089845974704,1.8836089770990705,1.8836089694611298,1.8836089616810507,1.883608953756189,1.8836089456838505,1.8836089374612899,1.8836089290857125,1.883608920554271,1.8836089118640649,1.8836089030121392,1.8836088939954851,1.8836088848110375,1.8836088754556735,1.8836088659262125,1.8836088562194155,1.8836088463319818,1.8836088362605505,1.8836088260016979,1.8836088155519362,1.8836088049077127,1.883608794065409,1.8836087830213393,1.8836087717717482,1.883608760312813,1.8836087486406374,1.883608736751253,1.883608724640619,1.8836087123046168,1.8836086997390549,1.8836086869396595,1.8836086739020808,1.883608660621887,1.8836086470945625,1.8836086333155098,1.8836086192800439,1.8836086049833942,1.8836085904207003,1.8836085755870124,1.8836085604772874,1.8836085450863893,1.883608529409086,1.883608513440049,1.883608497173849,1.8836084806049571,1.883608463727741,1.8836084465364638,1.8836084290252815,1.8836084111882414,1.8836083930192817,1.8836083745122247,1.8836083556607808,1.8836083364585412,1.8836083168989795,1.8836082969754468,1.8836082766811706,1.883608256009253,1.8836082349526682,1.8836082135042576,1.883608191656732,1.8836081694026647,1.8836081467344918,1.8836081236445086,1.8836081001248666,1.8836080761675718,1.8836080517644818,1.883608026907301,1.883608001587583,1.88360797579672,1.8836079495259468,1.8836079227663352,1.8836078955087894,1.8836078677440455,1.8836078394626674,1.8836078106550427,1.883607781311381,1.8836077514217093,1.8836077209758697,1.8836076899635146,1.8836076583741042,1.8836076261969024,1.8836075934209748,1.8836075600351831,1.8836075260281793,1.8836074913884078,1.883607456104097,1.883607420163255,1.8836073835536697,1.883607346262899,1.883607308278271,1.883607269586877,1.8836072301755706,1.8836071900309574,1.8836071491393982,1.8836071074869962,1.8836070650595984,1.8836070218427878,1.8836069778218802,1.8836069329819172,1.8836068873076628,1.8836068407835984,1.8836067933939162,1.8836067451225138,1.883606695952991,1.883606645868641,1.8836065948524474,1.8836065428870776,1.8836064899548763,1.8836064360378597,1.8836063811177108,1.8836063251757709,1.883606268193035,1.8836062101501443,1.8836061510273812,1.8836060908046603,1.8836060294615242,1.883605966977134,1.8836059033302641,1.8836058384992944,1.8836057724622028,1.8836057051965582,1.8836056366795115,1.8836055668877905,1.8836054957976898,1.8836054233850632,1.8836053496253162,1.883605274493397,1.8836051979637884,1.883605120010499,1.8836050406070544,1.883604959726488,1.883604877341333,1.8836047934236113,1.8836047079448264,1.88360462087595,1.8836045321874169,1.883604441849111,1.8836043498303576,1.8836042560999124,1.8836041606259497,1.8836040633760534,1.8836039643172051,1.8836038634157732,1.883603760637501,1.8836036559474967,1.8836035493102188,1.883603440689468,1.8836033300483697,1.8836032173493678,1.8836031025542073,1.8836029856239218,1.8836028665188234,1.883602745198486,1.8836026216217345,1.8836024957466269,1.883602367530445,1.8836022369296765,1.8836021039000013,1.8836019683962772,1.8836018303725237,1.883601689781907,1.8836015465767244,1.8836014007083874,1.883601252127407,1.8836011007833746,1.8836009466249464,1.8836007895998275,1.8836006296547505,1.8836004667354622,1.8836003007867006,1.883600131752181,1.8835999595745727,1.8835997841954835,1.8835996055554363,1.8835994235938522,1.8835992382490294,1.8835990494581205,1.8835988571571147,1.8835986612808133,1.8835984617628092,1.8835982585354638,1.8835980515298854,1.8835978406759055,1.883597625902055,1.8835974071355404,1.8835971843022188,1.8835969573265747,1.8835967261316926,1.8835964906392324,1.8835962507694026,1.883596006440934,1.8835957575710525,1.883595504075451,1.883595245868261,1.8835949828620242,1.8835947149676628,1.8835944420944515,1.883594164149983,1.883593881040142,1.8835935926690714,1.883593298939138,1.883592999750905,1.8835926950030941,1.883592384592554,1.8835920684142262,1.8835917463611078,1.883591418324219,1.883591084192563,1.8835907438530919,1.883590397190668,1.8835900440880258,1.8835896844257327,1.883589318082148,1.8835889449333842,1.8835885648532664,1.8835881777132872,1.8835877833825667,1.8835873817278082,1.8835869726132537,1.88358655590064,1.8835861314491504,1.88358569911537,1.8835852587532387,1.8835848102140003,1.883584353346155,1.8835838879954085,1.883583414004623,1.883582931213762,1.8835824394598388,1.8835819385768628,1.8835814283957872,1.8835809087444473,1.8835803794475086,1.8835798403264077,1.8835792911992932,1.8835787318809647,1.8835781621828154,1.8835775819127654,1.8835769908752023,1.8835763888709143,1.883575775697028,1.8835751511469376,1.8835745150102405,1.8835738670726687,1.8835732071160143,1.8835725349180634,1.8835718502525205,1.8835711528889343,1.8835704425926254,1.883569719124606,1.8835689822415038,1.8835682316954836,1.8835674672341656,1.8835666886005424,1.8835658955328978,1.8835650877647203,1.8835642650246158,1.8835634270362227,1.8835625735181194,1.8835617041837347,1.8835608187412545,1.883559916893529,1.8835589983379757,1.8835580627664799,1.883557109865301,1.8835561393149662,1.8835551507901704,1.8835541439596715,1.8835531184861838,1.8835520740262712,1.8835510102302349,1.8835499267420046,1.8835488231990227,1.8835476992321305,1.883546554465448,1.8835453885162574,1.8835442009948806,1.8835429915045536,1.8835417596413044,1.8835405049938228,1.8835392271433293,1.8835379256634464,1.883536600120063,1.883535250071195,1.883533875066851,1.883532474648888,1.8835310483508694,1.8835295956979183,1.8835281162065691,1.8835266093846181,1.883525074730969,1.8835235117354778,1.8835219198787934,1.8835202986321995,1.8835186474574481,1.8835169658065951,1.8835152531218318,1.8835135088353112,1.883511732368977,1.8835099231343837,1.8835080805325186,1.8835062039536172,1.8835042927769794,1.8835023463707807,1.883500364091879,1.8834983452856213,1.8834962892856455,1.883494195413681,1.8834920629793432,1.883489891279927,1.8834876796001967,1.8834854272121722,1.8834831333749131,1.883480797334298,1.8834784183228006,1.883475995559265,1.8834735282486716,1.8834710155819079,1.8834684567355284,1.8834658508715156,1.8834631971370328,1.8834604946641804,1.883457742569742,1.8834549399549292,1.883452085905122,1.8834491794896084,1.8834462197613169,1.8834432057565453,1.8834401364946882,1.8834370109779577,1.8834338281911018,1.8834305871011208,1.883427286656974,1.8834239257892866,1.883420503410057,1.883417018412346,1.883413469669979,1.8834098560372317,1.883406176348517,1.8834024294180665,1.883398614039608,1.8833947289860395,1.8833907730090973,1.8833867448390202,1.8833826431842131,1.8833784667308993,1.8833742141427758,1.8833698840606579,1.8833654751021267,1.8833609858611637,1.883356414907788,1.8833517607876866,1.8833470220218398,1.8833421971061428,1.883337284511024,1.8833322826810561,1.8833271900345672,1.8833220049632426,1.8833167258317274,1.8833113509772192,1.883305878709062,1.8833003073083314,1.8832946350274187,1.883288860089609,1.883282980688656,1.8832769949883506,1.883270901122088,1.8832646971924316,1.8832583812706656,1.8832519513963537,1.8832454055768872,1.8832387417870302,1.883231957968462,1.8832250520293152,1.8832180218437102,1.8832108652512858,1.8832035800567257,1.8831961640292825,1.8831886149022985,1.8831809303727196,1.883173108100612,1.88316514570867,1.8831570407817235,1.883148790866243,1.883140393469839,1.8831318460607624,1.8831231460673985,1.8831142908777616,1.8831052778389838,1.8830961042568086,1.8830867673950693,1.8830772644751823,1.8830675926756237,1.8830577491314153,1.8830477309336002,1.8830375351287243,1.8830271587183143,1.8830165986583522,1.8830058518587536,1.8829949151828425,1.8829837854468283,1.8829724594192798,1.8829609338206026,1.8829492053225156,1.882937270547528,1.8829251260684172,1.8829127684077127,1.8829001940371717,1.882887399377268,1.8828743807966752,1.8828611346117579,1.8828476570860593,1.8828339444298012,1.8828199927993778,1.8828057982968633,1.8827913569695156,1.8827766648092896,1.8827617177523563,1.8827465116786226,1.8827310424112642,1.88271530571626,1.882699297301935,1.8826830128185126,1.882666447857672,1.882649597952117,1.8826324585751533,1.8826150251402736,1.8825972930007562,1.8825792574492723,1.8825609137175074,1.8825422569757917,1.8825232823327447,1.8825039848349356,1.8824843594665572,1.88246440114911,1.8824441047411118,1.882423465037814,1.8824024767709433,1.882381134608456,1.8823594331543165,1.8823373669482915,1.8823149304657685,1.882292118117596,1.8822689242499453,1.8822453431441977,1.8822213690168574,1.8821969960194898,1.8821722182386866,1.882147029696065,1.8821214243482873,1.8820953960871223,1.882068938739533,1.8820420460677985,1.8820147117696755,1.8819869294785923,1.881958692763886,1.8819299951310753,1.881900830022177,1.8818711908160677,1.8818410708288882,1.8818104633144968,1.8817793614649712,1.8817477584111617,1.881715647223295,1.8816830209116389,1.8816498724272166,1.8816161946625876,1.8815819804526865,1.8815472225757266,1.8815119137541707,1.8814760466557718,1.8814396138946852,1.8814026080326536,1.881365021580273,1.8813268469983355,1.8812880766992572,1.8812487030485916,1.881208718366635,1.881168114930119,1.8811268849740055,1.8810850206933767,1.8810425142454303,1.8809993577515796,1.8809555432996652,1.8809110629462804,1.8808659087192152,1.8808200726200197,1.8807735466266968,1.8807263226965247,1.88067839276901,1.8806297487689854,1.8805803826098462,1.8805302861969362,1.8804794514310874,1.8804278702123143,1.880375534443672,1.880322436035281,1.8802685669085217,1.8802139190004112,1.880158484268152,1.8801022546938813,1.880045222289598,1.8799873791023016,1.8799287172193202,1.87986922877386,1.879808905950756,1.8797477409924521,1.8796857262051987,1.8796228539654856,1.879559116726707,1.8794945070260667,1.879429017491736,1.8793626408502568,1.8792953699342043,1.8792271976901156,1.8791581171866838,1.8790881216232251,1.879017204338429,1.8789453588193872,1.8788725787109146,1.8787988578251675,1.8787241901515552,1.8786485698669586,1.8785719913462584,1.878494449173174,1.8784159381514216,1.8783364533161941,1.8782559899459623,1.8781745435746127,1.8780921100039119,1.8780086853163092,1.8779242658880781,1.8778388484028008,1.8777524298651946,1.8776650076152852,1.8775765793429322,1.8774871431026976,1.8773966973290714,1.877305240852045,1.8772127729130381,1.877119293181178,1.8770248017699311,1.8769292992540827,1.8768327866870695,1.8767352656186578,1.876636738112967,1.8765372067668393,1.876436674728542,1.8763351457168131,1.8762326240402343,1.8761291146169263,1.8760246229945772,1.875919155370769,1.875812718613626,1.8757053202827587,1.8755969686504965,1.875487672723411,1.8753774422641114,1.8752662878133028,1.8751542207121015,1.8750412531245924,1.8749273980606165,1.8748126693987777,1.8746970819096507,1.8745806512791878,1.8744633941322904,1.8743453280565496,1.8742264716261259,1.8741068444257543,1.8739864670748656,1.8738653612517877,1.8737435497180257,1.873621056342589,1.8734979061263517,1.8733741252264182,1.8732497409804778,1.8731247819311245,1.8729992778501128,1.872873259762533,1.872746759970879,1.872619812078976,1.8724924510157552,1.8723647130588381,1.8722366358579023,1.8721082584578133,1.8719796213214805,1.8718507663524144,1.871721736916956,1.8715925778661464,1.8714633355572095,1.8713340578746138,1.8712047942506833,1.87107559568573,1.8709465147676703,1.8708176056910972,1.87068892427578,1.8705605279845487,1.870432475940546,1.8703048289438007,1.8701776494870983,1.8700510017711198,1.8699249517188037,1.8697995669889185,1.8696749169887947,1.8695510728862017,1.8694281076203285,1.8693060959118428,1.8691851142719957,1.8690652410107471,1.8689465562438774,1.8688291418990628,1.8687130817208844,1.8685984612747417,1.8684853679496536,1.8683738909599101,1.8682641213455624,1.8681561519717196,1.8680500775266353,1.8679459945185615,1.8678440012713484,1.8677441979187774,1.8676466863976025,1.8675515704392862,1.8674589555604233,1.8673689490518228,1.8672816599662527,1.867197199104823,1.8671156790020098,1.867037213909296,1.8669619197774447,1.866889914237373,1.8668213165796441,1.866756247732563,1.8666948302388786,1.8666371882310848,1.8665834474053378,1.866533734993968,1.8664881797366102,1.8664469118499463,1.8664100629960663,1.8663777662494547,1.8663501560626115,1.8663273682303052,1.8663095398524876,1.8662968092958485,1.866289316154049,1.8662872012066254,1.8662906063765743,1.8662996746866405,1.8663145502143041,1.8663353780454826,1.8663623042269581,1.8663954757175378,1.866435040337954,1.8664811467195155,1.8665339442515152,1.8665935830273974,1.8666602137896988,1.8667339878737546,1.866815057150186,1.8669035739661584,1.8669996910854196,1.8671035616271117,1.867215339003352,1.867335176855586,1.867463228989696,1.8675996493098657,1.8677445917511815,1.867898210210969,1.8680606584788437,1.8682320901654639,1.8684126586299659,1.8686025169060627,1.8688018176267953,1.8690107129478972,1.8692293544697658,1.8694578931580066,1.8696964792625312,1.8699452622351762,1.8702043906458221,1.870474012096988,1.8707542731368652,1.8710453191707743,1.8713472943710125,1.871660341585075,1.8719846022422135,1.8723202162583243,1.8726673219391397,1.8730260558817062,1.8733965528741372,1.8737789457936271,1.8741733655027262,1.8745799407438688,1.874998798032158,1.8754300615464237,1.875873853018554,1.8763302916211335,1.8767994938534134,1.877281573425641,1.8777766411418004,1.8782848047808156,1.878806168976266,1.8793408350946958,1.87988890111259,1.8804504614921027,1.881025607055645,1.8816144248594342,1.882216998066128,1.8828334058166756,1.8834637231015239,1.8841080206313408,1.8847663647074113,1.8854388170918879,1.8861254348780863,1.886826270361015,1.887541370908363,1.8882707788321542,1.8890145312613091,1.8897726600153462,1.890545191479487,1.891332146481415,1.8921335401699613,1.8929493818959957,1.8937796750958105,1.8946244171772806,1.895483599409106,1.8963572068134376,1.8972452180621855,1.898147605377332,1.8990643344355482,1.8999953642774365,1.9009406472217045,1.9019001287845878,1.902873747604825,1.9038614353744905,1.9048631167759897,1.9058787094255045,1.9069081238231753,1.9079512633103086,1.909008024033858,1.9100782949184512,1.9111619576462047,1.9122588866445445,1.9133689490822698,1.9144920048740355,1.9156279066934556,1.916776499994985,1.9179376230447183,1.919111106960249,1.9202967757596763,1.9214944464198545,1.9227039289439445,1.9239250264383043,1.9251575351987351,1.9264012448060779,1.927655938231124,1.9289213919487904,1.9301973760614688,1.9314836544314569,1.9327799848223324,1.934086119049124,1.9354018031370963,1.9367267774889534,1.9380607770602365,1.9394035315426703,1.9407547655551871,1.9421141988423505,1.9434815464798543,1.9448565190867906,1.946238823044317,1.947628160720385,1.9490242307001282,1.9504267280215355,1.9518353444159884,1.9532497685532557,1.9546696862905035,1.9560947809248896,1.9575247334492962,1.9589592228107362,1.9603979261709872,1.9618405191689812,1.9632866761844912,1.9647360706026427,1.9661883750787923,1.9676432618033028,1.9691004027657653,1.9705594700181999,1.9720201359368001,1.9734820734817744,1.9749449564548494,1.9764084597540175,1.9778722596251166,1.9793360339098414,1.980799462289797,1.9822622265262329,1.983724010695088,1.985184501417011,1.9866433880820324,1.988100363068577,1.9895551219565253,1.9910073637340564,1.9924567909980107,1.9939031101475468,1.9953460315708667,1.996785269824821,1.9982205438072107,1.9996515769216288,2.0010780972347106,2.0024998376256584,2.0039165359279614,2.005327935063206,2.006733783166935,2.0081338337065033,2.0095278455908994,2.010915583272537,2.012296816841011,2.0136713221088534,2.015038880689332,2.0163992800663353,2.017752313656433,2.0190977808631856,2.0204354871238026,2.021765243948271,2.0230868689510713,2.02440018587561,2.0257050246115345,2.0270012212050608,2.0282886178625072,2.0295670629471787,2.0308364109698105,2.032096522572739,2.0333472645080075,2.0345885096095975,2.0358201367600017,2.037042030851335,2.0382540827412057,2.0394561892035625,2.0406482528747265,2.0418301821948375,2.0430018913449293,2.044163300179857,2.045314334157297,2.046454924263035,2.0475850069327737,2.0487045239706587,2.049813422464761,2.050911654699705,2.0519991780666715,2.0530759549709745,2.054141952737413,2.0551971435136043,2.0562415041714908,2.0572750162072073,2.058297665639508,2.05930944290692,2.0603103427638096,2.061300364175535,2.0622795102128415,2.063247787945669,2.0642052083365314,2.0651517861335975,2.066087539763648]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.5615717512072524,1.5615717510572815,1.5615717509045195,1.5615717507489146,1.561571750590414,1.561571750428964,1.5615717502645092,1.5615717500969937,1.561571749926361,1.5615717497525528,1.5615717495755101,1.5615717493951728,1.561571749211479,1.561571749024367,1.5615717488337735,1.5615717486396326,1.5615717484418785,1.5615717482404436,1.5615717480352616,1.5615717478262598,1.5615717476133693,1.561571747396517,1.5615717471756287,1.56157174695063,1.5615717467214443,1.561571746487993,1.5615717462501975,1.5615717460079765,1.5615717457612484,1.5615717455099283,1.5615717452539313,1.5615717449931703,1.5615717447275566,1.5615717444569999,1.561571744181408,1.5615717439006878,1.5615717436147434,1.5615717433234777,1.5615717430267912,1.5615717427245839,1.5615717424167528,1.561571742103193,1.5615717417837975,1.561571741458459,1.5615717411270655,1.5615717407895051,1.5615717404456624,1.5615717400954214,1.5615717397386626,1.5615717393752648,1.5615717390051038,1.5615717386280548,1.561571738243989,1.5615717378527763,1.5615717374542823,1.5615717370483737,1.561571736634911,1.5615717362137538,1.561571735784759,1.5615717353477812,1.5615717349026714,1.561571734449278,1.561571733987448,1.5615717335170232,1.5615717330378445,1.5615717325497482,1.561571732052569,1.5615717315461375,1.561571731030282,1.5615717305048262,1.5615717299695928,1.5615717294243987,1.5615717288690596,1.5615717283033854,1.5615717277271848,1.5615717271402614,1.561571726542416,1.5615717259334456,1.561571725313142,1.561571724681296,1.5615717240376918,1.5615717233821105,1.5615717227143295,1.561571722034122,1.5615717213412568,1.5615717206354978,1.5615717199166055,1.561571719184335,1.5615717184384388,1.5615717176786617,1.5615717169047463,1.561571716116429,1.5615717153134425,1.5615717144955132,1.5615717136623637,1.56157171281371,1.561571711949264,1.5615717110687326,1.561571710171815,1.561571709258207,1.5615717083275986,1.5615717073796735,1.5615717064141081,1.5615717054305753,1.561571704428741,1.5615717034082637,1.5615717023687972,1.5615717013099881,1.5615717002314766,1.5615716991328956,1.561571698013872,1.5615716968740259,1.5615716957129695,1.5615716945303075,1.5615716933256392,1.5615716920985543,1.5615716908486355,1.5615716895754588,1.5615716882785908,1.5615716869575915,1.561571685612011,1.5615716842413923,1.56157168284527,1.5615716814231686,1.5615716799746058,1.5615716784990885,1.5615716769961157,1.5615716754651767,1.5615716739057508,1.561571672317308,1.5615716706993092,1.5615716690512036,1.5615716673724314,1.5615716656624226,1.561571663920595,1.5615716621463576,1.5615716603391072,1.5615716584982295,1.5615716566230993,1.5615716547130785,1.561571652767519,1.5615716507857587,1.5615716487671254,1.5615716467109326,1.5615716446164807,1.5615716424830595,1.5615716403099429,1.5615716380963933,1.5615716358416576,1.5615716335449707,1.5615716312055516,1.5615716288226058,1.5615716263953232,1.5615716239228798,1.5615716214044342,1.561571618839132,1.561571616226101,1.561571613564454,1.561571610853286,1.5615716080916762,1.5615716052786868,1.561571602413361,1.5615715994947268,1.561571596521792,1.5615715934935461,1.5615715904089618,1.5615715872669895,1.5615715840665634,1.561571580806595,1.5615715774859777,1.5615715741035838,1.561571570658264,1.561571567148848,1.5615715635741436,1.5615715599329372,1.5615715562239914,1.561571552446047,1.5615715485978203,1.5615715446780054,1.5615715406852697,1.5615715366182583,1.5615715324755892,1.561571528255856,1.5615715239576258,1.561571519579439,1.5615715151198082,1.5615715105772194,1.561571505950131,1.5615715012369702,1.5615714964361385,1.5615714915460042,1.5615714865649075,1.561571481491158,1.5615714763230324,1.5615714710587758,1.5615714656966024,1.5615714602346902,1.5615714546711859,1.561571449004201,1.5615714432318115,1.5615714373520584,1.5615714313629454,1.56157142526244,1.5615714190484722,1.5615714127189317,1.561571406271672,1.5615713997045035,1.5615713930151984,1.5615713862014868,1.5615713792610562,1.5615713721915516,1.5615713649905745,1.5615713576556813,1.5615713501843835,1.5615713425741462,1.5615713348223879,1.5615713269264788,1.5615713188837401,1.5615713106914435,1.5615713023468107,1.5615712938470114,1.5615712851891619,1.5615712763703264,1.561571267387514,1.5615712582376784,1.5615712489177174,1.56157123942447,1.5615712297547177,1.5615712199051817,1.5615712098725227,1.5615711996533392,1.5615711892441677,1.5615711786414792,1.5615711678416802,1.5615711568411095,1.561571145636039,1.5615711342226712,1.5615711225971394,1.5615711107555026,1.5615710986937499,1.5615710864077927,1.561571073893471,1.5615710611465434,1.5615710481626925,1.5615710349375205,1.5615710214665481,1.5615710077452125,1.5615709937688675,1.5615709795327801,1.56157096503213,1.5615709502620074,1.5615709352174125,1.5615709198932521,1.5615709042843386,1.5615708883853896,1.5615708721910244,1.5615708556957622,1.5615708388940215,1.5615708217801179,1.561570804348261,1.561570786592554,1.5615707685069915,1.561570750085456,1.5615707313217182,1.5615707122094324,1.5615706927421373,1.5615706729132506,1.5615706527160693,1.5615706321437666,1.5615706111893903,1.5615705898458585,1.5615705681059602,1.561570545962349,1.5615705234075454,1.5615705004339306,1.5615704770337464,1.5615704531990895,1.5615704289219134,1.561570404194021,1.5615703790070659,1.5615703533525467,1.5615703272218062,1.5615703006060269,1.5615702734962296,1.5615702458832703,1.5615702177578343,1.5615701891104385,1.5615701599314233,1.561570130210952,1.5615700999390063,1.5615700691053853,1.5615700376996993,1.5615700057113668,1.5615699731296142,1.561569939943468,1.561569906141754,1.5615698717130926,1.5615698366458948,1.5615698009283598,1.5615697645484699,1.5615697274939864,1.5615696897524467,1.5615696513111597,1.561569612157201,1.561569572277411,1.561569531658387,1.5615694902864825,1.5615694481477995,1.5615694052281874,1.5615693615132356,1.5615693169882703,1.5615692716383491,1.5615692254482565,1.5615691784024992,1.5615691304853012,1.5615690816805974,1.5615690319720308,1.5615689813429448,1.5615689297763797,1.5615688772550653,1.5615688237614191,1.5615687692775357,1.5615687137851855,1.561568657265806,1.561568599700497,1.561568541070015,1.5615684813547663,1.5615684205348013,1.5615683585898077,1.5615682954991048,1.561568231241637,1.5615681657959657,1.5615680991402643,1.5615680312523104,1.56156796210948,1.561567891688738,1.5615678199666339,1.5615677469192928,1.561567672522408,1.5615675967512337,1.5615675195805776,1.5615674409847928,1.5615673609377712,1.5615672794129314,1.5615671963832163,1.5615671118210794,1.5615670256984804,1.5615669379868742,1.5615668486572034,1.561566757679889,1.561566665024821,1.5615665706613495,1.5615664745582767,1.561566376683845,1.5615662770057295,1.5615661754910282,1.5615660721062494,1.5615659668173074,1.5615658595895048,1.5615657503875289,1.5615656391754371,1.5615655259166483,1.5615654105739312,1.5615652931093922,1.5615651734844682,1.561565051659911,1.5615649275957781,1.56156480125142,1.5615646725854702,1.5615645415558312,1.5615644081196633,1.5615642722333725,1.5615641338525976,1.5615639929321974,1.5615638494262398,1.5615637032879848,1.561563554469876,1.5615634029235235,1.5615632485996933,1.5615630914482908,1.5615629314183486,1.5615627684580138,1.5615626025145297,1.5615624335342255,1.5615622614624989,1.5615620862438027,1.5615619078216305,1.5615617261384986,1.5615615411359334,1.5615613527544556,1.5615611609335633,1.5615609656117169,1.5615607667263223,1.5615605642137163,1.5615603580091473,1.5615601480467618,1.5615599342595858,1.5615597165795072,1.5615594949372609,1.5615592692624096,1.5615590394833276,1.5615588055271798,1.5615585673199095,1.561558324786216,1.5615580778495373,1.5615578264320322,1.5615575704545626,1.5615573098366733,1.5615570444965734,1.5615567743511194,1.5615564993157924,1.5615562193046828,1.5615559342304681,1.5615556440043943,1.5615553485362577,1.5615550477343818,1.5615547415056013,1.56155442975524,1.5615541123870904,1.5615537893033942,1.5615534604048238,1.5615531255904584,1.5615527847577668,1.561552437802586,1.5615520846191002,1.5615517250998212,1.561551359135567,1.5615509866154422,1.5615506074268164,1.561550221455306,1.5615498285847507,1.561549428697196,1.5615490216728691,1.5615486073901643,1.5615481857256175,1.5615477565538882,1.5615473197477403,1.5615468751780215,1.5615464227136442,1.5615459622215644,1.5615454935667645,1.561545016612235,1.5615445312189515,1.5615440372458607,1.5615435345498614,1.5615430229857847,1.5615425024063778,1.561541972662288,1.5615414336020441,1.561540885072041,1.5615403269165238,1.5615397589775737,1.561539181095092,1.5615385931067864,1.5615379948481578,1.5615373861524868,1.5615367668508224,1.5615361367719718,1.5615354957424854,1.5615348435866523,1.5615341801264893,1.561533505181732,1.5615328185698292,1.5615321201059358,1.5615314096029098,1.5615306868713072,1.56152995171938,1.561529203953073,1.5615284433760277,1.561527669789582,1.5615268829927702,1.5615260827823314,1.5615252689527155,1.5615244412960863,1.561523599602336,1.5615227436590935,1.5615218732517393,1.5615209881634173,1.5615200881750553,1.5615191730653821,1.561518242610951,1.5615172965861597,1.5615163347632803,1.5615153569124862,1.5615143628018826,1.5615133521975408,1.5615123248635376,1.5615112805619893,1.5615102190530992,1.5615091400952015,1.5615080434448092,1.5615069288566685,1.5615057960838132,1.561504644877624,1.5615034749878922,1.5615022861628869,1.561501078149424,1.561499850692945,1.5614986035375917,1.5614973364262927,1.5614960491008516,1.5614947413020388,1.561493412769691,1.561492063242811,1.5614906924596776,1.56148930015796,1.5614878860748325,1.5614864499471022,1.5614849915113371,1.5614835105040037,1.5614820066616073,1.5614804797208397,1.5614789294187377,1.5614773554928392,1.5614757576813556,1.5614741357233455,1.5614724893588972,1.5614708183293162,1.561469122377327,1.5614674012472731,1.5614656546853327,1.5614638824397398,1.5614620842610103,1.5614602599021845,1.5614584091190675,1.5614565316704898,1.5614546273185668,1.5614526958289772,1.5614507369712398,1.56144875051901,1.5614467362503808,1.561444693948195,1.5614426234003655,1.56144052440021,1.5614383967467944,1.5614362402452833,1.561434054707307,1.5614318399513358,1.5614295958030673,1.5614273220958237,1.5614250186709617,1.5614226853782929,1.5614203220765162,1.561417928633663,1.5614155049275547,1.5614130508462696,1.5614105662886248,1.5614080511646704,1.561405505396194,1.5614029289172402,1.561400321674641,1.561397683628559,1.5613950147530462,1.5613923150366118,1.5613895844828036,1.5613868231108052,1.5613840309560412,1.5613812080708025,1.5613783545248756,1.561375470406191,1.5613725558214857,1.5613696108969697,1.5613666357790157,1.5613636306348557,1.561360595653291,1.5613575310454144,1.5613544370453454,1.5613513139109794,1.5613481619247445,1.5613449813943738,1.5613417726536873,1.561338536063388,1.5613352720118652,1.5613319809160129,1.5613286632220553,1.5613253194063879,1.5613219499764193,1.561318555471436,1.5613151364634659,1.5613116935581557,1.5613082273956567,1.561304738651521,1.5613012280376006,1.5612976963029608,1.5612941442347956,1.561290572659355,1.5612869824428754,1.5612833744925179,1.5612797497573103,1.5612761092290999,1.5612724539435032,1.5612687849808689,1.5612651034672362,1.5612614105753073,1.561257707525411,1.5612539955864804,1.561250276077025,1.5612465503661097,1.5612428198743333,1.5612390860748095,1.5612353504941487,1.5612316147134366,1.561227880369222,1.5612241491544947,1.5612204228196687,1.5612167031735666,1.5612129920843958,1.5612092914807327,1.5612056033524997,1.5612019297519435,1.5611982727946128,1.5611946346603338,1.5611910175941843,1.5611874239074677,1.5611838559786864,1.561180316254513,1.5611768072507628,1.561173331553367,1.5611698918193404,1.5611664907777605,1.5611631312307375,1.5611598160543914,1.5611565481998322,1.5611533306941408,1.5611501666413568,1.5611470592234684,1.5611440117014133,1.5611410274160809,1.561138109789327,1.561135262324997,1.5611324886099598,1.561129792315155,1.5611271771966528,1.5611246470967308,1.5611222059449672,1.5611198577593537,1.5611176066474286,1.5611154568074332,1.561113412529495,1.561111478196833,1.5611096582869999,1.5611079573731488,1.5611063801253389,1.561104931311875,1.5611036158006886,1.5611024385607586,1.5611014046635774,1.561100519284666,1.5610997877051367,1.5610992153133099,1.5610988076063892,1.5610985701921927,1.561098508790947,1.561098629237152,1.5610989374815039,1.561099439592903,1.5611001417605272,1.5611010502959868,1.5611021716355666,1.5611035123425452,1.5611050791096077,1.561106878761355,1.5611089182568985,1.5611112046925666,1.561113745304705,1.5611165474725879,1.5611196187214371,1.561122966725561,1.561126599311599,1.5611305244618998,1.5611347503180142,1.5611392851843209,1.5611441375317825,1.5611493160018357,1.5611548294104187,1.561160686752145,1.5611668972046162,1.5611734701328852,1.5611804150940758,1.5611877418421518,1.5611954603328442,1.561203580728748,1.5612121134045727,1.561221068952569,1.5612304581881256,1.5612402921555346,1.5612505821339429,1.5612613396434771,1.561272576451553,1.5612843045793754,1.5612965363086195,1.5613092841883118,1.5613225610419013,1.5613363799745263,1.5613507543804859,1.5613656979509112,1.5613812246816416,1.561397348881308,1.5614140851796292,1.5614314485359162,1.561449454247795,1.5614681179601424,1.5614874556742464,1.5615074837571825,1.5615282189514155,1.5615496783846279,1.5615718795797717,1.5615948404653552,1.5616185793859538,1.561643115112959,1.5616684668555572,1.5616946542719483,1.5617216974807957,1.561749617072921,1.5617784341232341,1.5618081702029016,1.5618388473917642,1.5618704882909888,1.5619031160359673,1.5619367543094587,1.5619714273549732,1.5620071599904017,1.5620439776218902,1.5620819062579563,1.5621209725238494,1.5621612036761567,1.5622026276176462,1.562245272912357,1.5622891688009248,1.5623343452161487,1.5623808327987931,1.5624286629136264,1.5624778676656879,1.562528479916791,1.5625805333022456,1.562634062247809,1.5626891019868552,1.5627456885777575,1.5628038589214852,1.562863650779404,1.5629251027912798,1.56298825449347,1.5630531463373154,1.563119819707701,1.5631883169418002,1.5632586813479774,1.5633309572248575,1.5634051898805326,1.5634814256519178,1.5635597119242286,1.5636400971505757,1.5637226308716676,1.5638073637356034,1.5638943475177443,1.5639836351406498,1.5640752806940645,1.5641693394549363,1.5642658679074495,1.5643649237630615,1.5644665659805113,1.564570854785793,1.564677851692065,1.5647876195194734,1.56490022241487,1.5650157258713955,1.5651341967479087,1.5652557032882202,1.565380315140124,1.5655081033741753,1.5656391405021952,1.5657735004954674,1.5659112588025896,1.566052492366944,1.5661972796437558,1.5663457006166848,1.5664978368139264,1.5666537713237674,1.5668135888095573,1.566977375524042,1.567145219323021,1.567317209678264,1.5674934376896512,1.5676739960964634,1.5678589792877828,1.5680484833119306,1.5682426058848915,1.568441446397654,1.568645105922409,1.5688536872175278,1.56906729473126,1.569286034604075,1.569510014669571,1.5697393444538812,1.569974135173489,1.5702144997313807,1.570460552711448,1.5707124103710572,1.5709701906316986,1.5712340130676234,1.5715039988923853,1.5717802709431887,1.572062953662952,1.572352173079989,1.572648056785218,1.5729507339067936,1.573260335082069,1.573576992426784,1.5739008395013858,1.5742320112743813,1.5745706440826117,1.5749168755883665,1.5752708447332233,1.575632691688526,1.5760025578023948,1.576380585543182,1.5767669184392767,1.5771617010151604,1.5775650787236408,1.5779771978741644,1.5783982055571333,1.5788282495641481,1.5792674783041032,1.5797160407150692,1.5801740861718974,1.5806417643894894,1.5811192253216912,1.5816066190557594,1.5821040957023698,1.582611805281144,1.5831298976016739,1.5836585221400408,1.5841978279108238,1.5847479633346229,1.5853090761011073,1.5858813130276357,1.5864648199134908,1.5870597413897929,1.58766622076516,1.5882843998672138,1.5889144188800193,1.589556416177589,1.5902105281535706,1.590876889047274,1.5915556307661944,1.5922468827052154,1.592950771562677,1.5936674211535289,1.5943969522197916,1.5951394822385645,1.5958951252278468,1.596663991550441,1.5974461877162347,1.5982418161831617,1.5990509751571744,1.5998737583915519,1.6007102549859078,1.601560549185257,1.6024247201795208,1.6033028419038662,1.6041949828402775,1.60510120582078,1.6060215678327376,1.6069561198266569,1.6079049065269433,1.608867966246053,1.609845330702497,1.6108370248431534,1.6118430666703505,1.6128634670741782,1.6138982296704945,1.6149473506450835,1.6160108186044224,1.617088614433509,1.6181807111611926,1.6192870738334513,1.6204076593950338,1.621542416579892,1.6226912858107938,1.623854199108515,1.6250310800109775,1.6262218435026785,1.6274263959547617,1.6286446350760242,1.6298764498751583,1.6311217206344968,1.632380318895489,1.6336521074561363,1.634936940380565,1.636234663020892,1.6375451120515188,1.6388681155159432,1.64020349288616,1.6415510551346797,1.6429106048191757,1.6442819361797152,1.645664835248526,1.6470590799721887,1.648464440346136,1.6498806785612867,1.6513075491626268,1.6527447992195072,1.6541921685074004,1.655649389700832,1.6571161885771561,1.6585922842308445,1.6600773892978953,1.6615712101899764,1.6630734473378637,1.6645837954437295,1.6661019437418056,1.667627576266926,1.6691603721304349,1.6707000058029355,1.6722461474033197,1.6737984629935319,1.6753566148784849,1.67692026191055,1.6784890597980302,1.6800626614170127,1.6816407171260122,1.6832228750827878,1.6848087815627424,1.6863980812782868,1.6879904176985892,1.6895854333690938,1.6911827702302413,1.6927820699348006,1.694382974163251,1.695985124936653,1.6975881649264704,1.699191737760814,1.7007954883265948,1.7023990630670989,1.7040021102744967,1.7056042803768456,1.7072052262191428,1.7088046033380186,1.710402070229678,1.7119972886107253,1.7135899236715246,1.715179644321786,1.7167661234280651,1.7183490380429254,1.719928069625504,1.7215029042532621,1.723073232824736,1.7246387512530994,1.7261991606504108,1.7277541675024075,1.729303483833759,1.7308468273637057,1.732383921652025,1.7339144962353086,1.7354382867535296,1.7369550350669318,1.738464489363262,1.7399664042554122,1.74146054086954,1.7429466669237654,1.744424556797545,1.7458939915918648,1.7473547591803809,1.7488066542516734,1.750249478342781,1.7516830398641972,1.7531071541165324,1.7545216432990356,1.7559263365102038,1.7573210697406974,1.758705685858791,1.760080034588615,1.761443972481414,1.7627973628800913,1.7641400758772874,1.7654719882672525,1.7667929834917797,1.7681029515804618,1.76940178908554,1.7706893990116064,1.7719656907404369,1.7732305799512122,1.7744839885363974,1.7757258445135398,1.7769560819332544,1.7781746407836396,1.779381466891396,1.7805765118198846,1.7817597327643804,1.782931092444757,1.7840905589958451,1.7852381058556883,1.7863737116519347,1.7874973600865685,1.7886090398192094,1.789708744349183,1.7907964718965586,1.7918722252823565,1.79293601180811,1.7939878431349585,1.7950277351624597]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.702235582152042,1.7022355820859925,1.7022355820187138,1.702235581950183,1.7022355818803772,1.7022355818092718,1.7022355817368435,1.7022355816630674,1.7022355815879184,1.7022355815113708,1.7022355814333987,1.7022355813539753,1.7022355812730743,1.7022355811906675,1.7022355811067276,1.7022355810212253,1.7022355809341319,1.7022355808454177,1.7022355807550527,1.7022355806630058,1.7022355805692462,1.702235580473742,1.7022355803764602,1.702235580277368,1.7022355801764322,1.7022355800736177,1.70223557996889,1.7022355798622135,1.7022355797535518,1.7022355796428679,1.7022355795301245,1.7022355794152826,1.702235579298304,1.7022355791791484,1.7022355790577754,1.702235578934144,1.7022355788082115,1.702235578679936,1.702235578549273,1.7022355784161787,1.7022355782806078,1.702235578142514,1.7022355780018503,1.702235577858569,1.7022355777126217,1.7022355775639584,1.7022355774125284,1.7022355772582807,1.7022355771011626,1.7022355769411206,1.7022355767781008,1.7022355766120472,1.7022355764429036,1.7022355762706125,1.702235576095115,1.7022355759163523,1.7022355757342629,1.702235575548785,1.7022355753598557,1.702235575167411,1.7022355749713847,1.702235574771711,1.7022355745683218,1.702235574361148,1.7022355741501185,1.7022355739351624,1.7022355737162065,1.7022355734931762,1.7022355732659957,1.702235573034588,1.702235572798874,1.7022355725587741,1.7022355723142064,1.7022355720650877,1.7022355718113333,1.7022355715528577,1.7022355712895718,1.702235571021387,1.7022355707482122,1.702235570469954,1.7022355701865182,1.7022355698978082,1.7022355696037264,1.7022355693041722,1.7022355689990443,1.7022355686882389,1.70223556837165,1.7022355680491708,1.7022355677206906,1.7022355673860985,1.7022355670452807,1.7022355666981215,1.7022355663445026,1.7022355659843038,1.7022355656174033,1.7022355652436758,1.7022355648629945,1.70223556447523,1.7022355640802507,1.702235563677922,1.7022355632681079,1.7022355628506685,1.7022355624254621,1.7022355619923442,1.7022355615511677,1.7022355611017828,1.7022355606440367,1.7022355601777739,1.7022355597028358,1.702235559219061,1.7022355587262858,1.7022355582243418,1.7022355577130597,1.7022355571922645,1.7022355566617797,1.7022355561214255,1.7022355555710178,1.70223555501037,1.7022355544392913,1.7022355538575877,1.7022355532650622,1.7022355526615127,1.7022355520467347,1.7022355514205194,1.7022355507826537,1.7022355501329205,1.7022355494711006,1.7022355487969678,1.7022355481102938,1.7022355474108453,1.7022355466983845,1.7022355459726695,1.7022355452334539,1.7022355444804866,1.702235543713512,1.7022355429322693,1.7022355421364934,1.7022355413259143,1.7022355405002563,1.7022355396592392,1.7022355388025774,1.70223553792998,1.7022355370411508,1.7022355361357877,1.7022355352135836,1.7022355342742257,1.702235533317394,1.7022355323427645,1.702235531350006,1.702235530338782,1.702235529308748,1.7022355282595556,1.702235527190848,1.7022355261022624,1.7022355249934293,1.7022355238639726,1.7022355227135086,1.7022355215416474,1.7022355203479902,1.702235519132133,1.7022355178936623,1.702235516632158,1.7022355153471926,1.7022355140383294,1.7022355127051245,1.7022355113471257,1.7022355099638713,1.7022355085548933,1.7022355071197126,1.7022355056578429,1.702235504168788,1.7022355026520426,1.7022355011070927,1.7022354995334135,1.702235497930472,1.7022354962977237,1.7022354946346157,1.7022354929405834,1.7022354912150521,1.702235489457437,1.702235487667142,1.70223548584356,1.702235483986073,1.7022354820940506,1.7022354801668518,1.702235478203823,1.7022354762042988,1.7022354741676018,1.702235472093041,1.7022354699799136,1.7022354678275033,1.7022354656350807,1.7022354634019032,1.7022354611272132,1.7022354588102406,1.7022354564502005,1.702235454046293,1.7022354515977034,1.7022354491036031,1.7022354465631468,1.7022354439754737,1.702235441339708,1.702235438654957,1.7022354359203113,1.7022354331348453,1.7022354302976157,1.7022354274076619,1.7022354244640057,1.702235421465651,1.7022354184115824,1.7022354153007666,1.7022354121321508,1.702235408904663,1.7022354056172109,1.7022354022686825,1.702235398857945,1.7022353953838452,1.7022353918452073,1.7022353882408352,1.7022353845695093,1.7022353808299893,1.7022353770210101,1.7022353731412845,1.7022353691895016,1.7022353651643247,1.7022353610643945,1.7022353568883253,1.702235352634706,1.7022353483021,1.7022353438890436,1.7022353393940468,1.7022353348155914,1.7022353301521316,1.7022353254020928,1.702235320563872,1.7022353156358356,1.7022353106163213,1.7022353055036352,1.7022353002960522,1.702235294991816,1.702235289589137,1.7022352840861943,1.7022352784811319,1.7022352727720604,1.7022352669570555,1.7022352610341578,1.7022352550013706,1.702235248856663,1.702235242597964,1.7022352362231665,1.702235229730124,1.7022352231166502,1.7022352163805197,1.7022352095194648,1.7022352025311778,1.7022351954133077,1.7022351881634605,1.7022351807791982,1.7022351732580387,1.7022351655974535,1.702235157794869,1.702235149847663,1.7022351417531667,1.7022351335086614,1.7022351251113792,1.7022351165585017,1.7022351078471587,1.7022350989744273,1.7022350899373315,1.702235080732841,1.7022350713578704,1.7022350618092774,1.7022350520838623,1.7022350421783683,1.7022350320894777,1.7022350218138127,1.702235011347935,1.7022350006883424,1.7022349898314695,1.7022349787736863,1.702234967511296,1.702234956040536,1.7022349443575735,1.7022349324585078,1.7022349203393659,1.7022349079961037,1.702234895424604,1.7022348826206741,1.702234869580045,1.7022348562983727,1.7022348427712317,1.7022348289941183,1.7022348149624467,1.7022348006715486,1.702234786116671,1.702234771292975,1.7022347561955353,1.702234740819337,1.7022347251592749,1.7022347092101526,1.7022346929666794,1.7022346764234695,1.702234659575041,1.702234642415813,1.7022346249401048,1.7022346071421337,1.7022345890160135,1.702234570555752,1.7022345517552506,1.7022345326083015,1.7022345131085856,1.7022344932496714,1.7022344730250125,1.7022344524279462,1.7022344314516908,1.702234410089344,1.7022343883338813,1.7022343661781523,1.7022343436148808,1.7022343206366621,1.7022342972359592,1.702234273405102,1.7022342491362852,1.702234224421566,1.702234199252861,1.702234173621944,1.7022341475204457,1.7022341209398477,1.702234093871483,1.7022340663065336,1.7022340382360248,1.7022340096508266,1.702233980541649,1.70223395089904,1.7022339207133828,1.7022338899748926,1.7022338586736157,1.7022338267994248,1.7022337943420172,1.7022337612909122,1.7022337276354471,1.7022336933647768,1.7022336584678681,1.7022336229334978,1.7022335867502507,1.7022335499065155,1.7022335123904822,1.7022334741901393,1.7022334352932695,1.7022333956874485,1.7022333553600402,1.7022333142981945,1.702233272488843,1.7022332299186973,1.7022331865742444,1.702233142441744,1.7022330975072253,1.702233051756483,1.702233005175074,1.7022329577483148,1.7022329094612774,1.7022328602987857,1.7022328102454125,1.7022327592854747,1.7022327074030317,1.702232654581881,1.7022326008055533,1.7022325460573113,1.7022324903201438,1.7022324335767636,1.702232375809603,1.7022323170008098,1.7022322571322457,1.7022321961854794,1.7022321341417859,1.7022320709821404,1.702232006687216,1.7022319412373799,1.7022318746126885,1.7022318067928854,1.7022317377573966,1.702231667485326,1.702231595955454,1.7022315231462315,1.7022314490357782,1.7022313736018766,1.7022312968219708,1.7022312186731614,1.7022311391322025,1.7022310581754976,1.7022309757790965,1.702230891918693,1.7022308065696188,1.7022307197068425,1.702230631304965,1.7022305413382177,1.7022304497804575,1.7022303566051649,1.702230261785441,1.7022301652940042,1.7022300671031871,1.7022299671849348,1.7022298655108015,1.7022297620519478,1.7022296567791393,1.7022295496627435,1.7022294406727279,1.702229329778659,1.7022292169496982,1.702229102154603,1.7022289853617243,1.7022288665390037,1.702228745653976,1.7022286226737646,1.7022284975650828,1.702228370294234,1.7022282408271108,1.702228109129194,1.7022279751655554,1.702227838900857,1.7022277002993518,1.702227559324887,1.7022274159409028,1.7022272701104375,1.7022271217961278,1.7022269709602122,1.7022268175645354,1.7022266615705492,1.7022265029393195,1.7022263416315297,1.7022261776074856,1.7022260108271212,1.7022258412500044,1.7022256688353452,1.7022254935420003,1.7022253153284845,1.7022251341529762,1.7022249499733277,1.7022247627470761,1.7022245724314524,1.702224378983394,1.7022241823595559,1.702223982516325,1.7022237794098327,1.7022235729959716,1.7022233632304082,1.7022231500686031,1.702222933465826,1.7022227133771755,1.7022224897575997,1.7022222625619154,1.7022220317448322,1.702221797260975,1.7022215590649084,1.702221317111163,1.7022210713542638,1.7022208217487573,1.702220568249243,1.7022203108104048,1.7022200493870447,1.7022197839341187,1.7022195144067709,1.7022192407603758,1.7022189629505762,1.702218680933327,1.7022183946649385,1.7022181041021234,1.7022178092020455,1.7022175099223706,1.7022172062213188,1.7022168980577206,1.7022165853910747,1.7022162681816082,1.7022159463903397,1.7022156199791454,1.7022152889108266,1.7022149531491826,1.7022146126590836,1.7022142674065486,1.7022139173588275,1.7022135624844834,1.7022132027534795,1.7022128381372723,1.7022124686089042,1.7022120941431016,1.7022117147163782,1.7022113303071407,1.7022109408957982,1.702210546464877,1.702210146999139,1.7022097424857066,1.7022093329141867,1.702208918276808,1.7022084985685544,1.7022080737873095,1.7022076439340044,1.7022072090127691,1.7022067690310927,1.7022063239999863,1.7022058739341537,1.7022054188521658,1.7022049587766446,1.7022044937344492,1.7022040237568719,1.7022035488798397,1.7022030691441201,1.7022025845955393,1.7022020952852015,1.7022016012697194,1.7022011026114519,1.7022005993787472,1.702200091646197,1.7021995794948952,1.7021990630127082,1.7021985422945505,1.7021980174426725,1.7021974885669544,1.7021969557852084,1.7021964192234937,1.7021958790164378,1.7021953353075685,1.7021947882496555,1.7021942380050623,1.7021936847461088,1.7021931286554413,1.7021925699264189,1.7021920087635038,1.7021914453826688,1.7021908800118108,1.7021903128911804,1.7021897442738185,1.702189174426009,1.7021886036277385,1.7021880321731753,1.702187460371152,1.7021868885456677,1.702186317036399,1.702185746199225,1.7021851764067657,1.7021846080489333,1.7021840415334957,1.7021834772866549,1.7021829157536383,1.7021823573993042,1.7021818027087605,1.702181252187998,1.7021807063645369,1.7021801657880886,1.7021796310312305,1.7021791026900983,1.7021785813850865,1.7021780677615705,1.7021775624906386,1.7021770662698401,1.7021765798239488,1.702176103905739,1.702175639296779,1.702175186808236,1.7021747472816993,1.7021743215900151,1.7021739106381364,1.7021735153639894,1.702173136739353,1.7021727757707508,1.702172433500361,1.702172111006937,1.7021718094067457,1.7021715298545153,1.7021712735444008,1.7021710417109601,1.7021708356301464,1.702170656620309,1.7021705060432137,1.7021703853050694,1.7021702958575706,1.7021702391989542,1.70217021687506,1.7021702304804145,1.7021702816593158,1.702170372106935,1.7021705035704273,1.7021706778500525,1.702170896800307,1.7021711623310642,1.7021714764087252,1.7021718410573783,1.7021722583599666,1.7021727304594647,1.7021732595600636,1.7021738479283615,1.702174497894563,1.702175211853686,1.7021759922667732,1.702176841662112,1.702177762636457,1.7021787578562613,1.702179830058911,1.702180982053966,1.7021822167244027,1.702183537027865,1.702184945997916,1.7021864467452936,1.7021880424591742,1.7021897364084322,1.7021915319429108,1.7021934324946912,1.702195441579365,1.7021975627973118,1.7021997998349807,1.7022021564661691,1.702204636553314,1.7022072440487779,1.7022099829961432,1.702212857531511,1.7022158718848,1.702219030381053,1.7022223374417491,1.7022257975861188,1.7022294154324655,1.7022331956994956,1.7022371432076522,1.702241262880462,1.702245559745884,1.7022500389376758,1.7022547056967616,1.7022595653726214,1.7022646234246825,1.702269885423735,1.7022753570533542,1.7022810441113436,1.7022869525111946,1.7022930882835647,1.7022994575777781,1.702306066663345,1.7023129219315092,1.7023200298968184,1.7023273971987225,1.7023350306032008,1.7023429370044214,1.7023511234264332,1.7023595970248921,1.702368365088826,1.702377435042438,1.7023868144469516,1.7023965110025003,1.7024065325500608,1.7024168870734389,1.7024275827013011,1.7024386277092638,1.702450030522035,1.7024617997156148,1.7024739440195578,1.7024864723192947,1.7024993936585244,1.7025127172416694,1.7025264524364034,1.7025406087762527,1.7025551959632703,1.7025702238707905,1.7025857025462627,1.7026016422141663,1.7026180532790138,1.7026349463284383,1.7026523321363731,1.7026702216663223,1.7026886260747258,1.7027075567144219,1.7027270251382074,1.7027470431024994,1.7027676225711008,1.7027887757190705,1.7028105149366994,1.7028328528335976,1.702855802242891,1.7028793762255305,1.7029035880747179,1.7029284513204432,1.7029539797341449,1.7029801873334849,1.7030070883872468,1.7030346974203547,1.7030630292190168,1.7030920988359888,1.703121921595971,1.7031525131011227,1.7031838892367108,1.703216066176884,1.7032490603905768,1.7032828886475424,1.7033175680245174,1.703353115911519,1.703389550018269,1.7034268883807553,1.7034651493679227,1.7035043516884965,1.7035445143979395,1.7035856569055423,1.7036277989816455,1.7036709607649942,1.7037151627702274,1.7037604258954964,1.703806771430218,1.7038542210629597,1.7039027968894525,1.703952521420737,1.7040034175914398,1.7040555087681766,1.704108818758085,1.7041633718174856,1.7042191926606662,1.7042763064687936,1.7043347388989492,1.704394516093283,1.7044556646882958,1.7045182118242326,1.7045821851545984,1.7046476128557895,1.7047145236368348,1.7047829467492537,1.7048529119970168,1.704924449746623,1.704997590937266,1.70507236709112,1.7051488103237082,1.7052269533543758,1.7053068295168514,1.7053884727698982,1.7054719177080493,1.7055571995724244,1.7056443542616222,1.7057334183426864,1.7058244290621387,1.705917424357075,1.7060124428663188,1.706109523941628,1.7062087076589458,1.7063100348296965,1.7064135470121111,1.7065192865225838,1.706627296447048,1.7067376206523657,1.7068503037977234,1.7069653913460265,1.7070829295752803,1.7072029655899559,1.7073255473323234,1.707450723593751,1.707578544025953,1.70770905915218,1.7078423203783402,1.7079783800040387,1.7081172912335236,1.7082591081865275,1.7084038859089896,1.7085516803836445,1.7087025485404674,1.7088565482669555,1.7090137384182331,1.7091741788269645,1.7093379303130576,1.7095050546931405,1.709675614789792,1.7098496744405103,1.7100272985063947,1.7102085528805273,1.7103935044960275,1.710582221333757,1.7107747724296583,1.7109712278816966,1.711171658856382,1.711376137594849,1.7115847374184618,1.7117975327339199,1.712014599037839,1.7122360129207679,1.7124618520706187,1.712692195275475,1.7129271224257452,1.7131667145156217,1.71341105364382,1.7136602230135505,1.713914306931689,1.7141733908071064,1.714437561148117,1.7147069055589987,1.7149815127355468,1.7152614724596114,1.7155468755925756,1.7158378140677248,1.7161343808814558,1.7164366700832783,1.7167447767645554,1.717058797045928,1.7173788280633697,1.7177049679528178,1.7180373158333184,1.7183759717886302,1.7187210368472299,1.71907261296065,1.7194308029800975,1.7197957106312838,1.7201674404874023,1.720546097940194,1.7209317891690366,1.7213246211079811,1.7217247014106878,1.7221321384131807,1.7225470410943653,1.722969519034238,1.7233996823697262,1.7238376417480932,1.7242835082778483,1.7247373934770924,1.7251994092192515,1.725669667676123,1.7261482812581974,1.7266353625521806,1.727131024255685,1.7276353791090262,1.7281485398240881,1.7286706190102097,1.7292017290970652,1.7297419822544917,1.7302914903092503,1.7308503646586841,1.7314187161812715,1.7319966551440498,1.732584291106913,1.7331817328237846,1.7337890881406752,1.734406463890641,1.7350339657856708,1.7356716983055367,1.7363197645836488,1.736978266289967,1.737647303511037,1.7383269746272123,1.7390173761871566,1.739718602779712,1.7404307469032354,1.7411538988325277,1.741888146483469,1.7426335752755093,1.743390267992156,1.7441583046396243,1.7449377623038198,1.7457287150058405,1.7465312335561929,1.7473453854079333,1.7481712345089484,1.7490088411536127,1.7498582618340603,1.750719549091326,1.7515927513666194,1.7524779128530041,1.7533750733477673,1.7542842681057689,1.7552055276940755,1.756138877848184,1.7570843393301494,1.758041927788947,1.7590116536233862,1.7599935218479155,1.7609875319616528,1.7619936778209806,1.7630119475160466,1.7640423232515128,1.7650847812318924,1.7661392915518188,1.7672058180915784,1.7682843184182482,1.7693747436927618,1.770477038583229,1.7715911411848242,1.772716982946553,1.7738544886051877,1.7750035761266691,1.776164156655236,1.7773361344705532,1.7785194069530805,1.7797138645579125,1.7809193907973042,1.782135862232081,1.7833631484721062,1.7846011121859675,1.7858496091200173,1.7871084881268884,1.7883775912035693,1.7896567535391217,1.790945803572081,1.7922445630575667,1.7935528471441042,1.79487046446013,1.7961972172101395,1.7975329012803947,1.7988773063541004,1.800230216035925,1.8015914079857152,1.8029606540612395,1.8043377204697595,1.8057223679282128,1.8071143518317698,1.808513422430494,1.809919325013827,1.8113318001025924,1.8127505836481892,1.8141754072386391,1.8156059983111197,1.81704208037061,1.818483373214258,1.8199295931610557,1.8213804532864146,1.8228356636611998,1.8242949315947905,1.8257579618817144,1.8272244570514011,1.8286941176205933,1.8301666423479512,1.8316417284903774,1.8331190720605999,1.8345983680855376,1.836079310864983,1.8375615942301398,1.839044911801548,1.840528957245951,1.8420134245316504,1.8434980081819126,1.8449824035259956,1.8464663069473786,1.8479494161287855,1.8494314302936088,1.8509120504433527,1.8523909795907292,1.8538679229880553,1.8553425883506165,1.8568146860746841,1.858283929449876,1.8597500348655873,1.86121272201122,1.862671714069974,1.864126737905965,1.8655775242444677,1.8670238078450945,1.8684653276677428,1.8699018270311585,1.871333053763994,1.872758760348243,1.8741787040549633,1.8755926470722208,1.8770003566251912,1.8784016050883945,1.879796170090037,1.8811838346084646,1.8825643870607405,1.883937621383383,1.8853033371053114,1.8866613394130602,1.888011439208342,1.8893534531580498,1.8906872037368048,1.8920125192621633,1.8933292339226115,1.8946371877984938,1.8959362268760143,1.8972262030544766,1.898506974146932,1.8997784038744037,1.901040361853876,1.902292723580239,1.9035353704023759,1.9047681894936044,1.9059910738166683,1.9072039220834902,1.9084066387098988,1.9095991337655418,1.910781322919202,1.9119531273797346,1.9131144738328383,1.9142652943738856,1.9154055264370209,1.9165351127207537,1.9176540011102479,1.918762144596534,1.919859501192847,1.9209460338483002,1.9220217103591,1.923086503277506,1.9241403898187297,1.9251833517659716,1.9262153753737805,1.9272364512699294,1.9282465743559767,1.9292457437067,1.9302339624685685,1.931211237757417,1.9321775805554902,1.9331330056079983,1.9340775313193528,1.9350111796492064,1.9359339760084502],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.9500528027591324,1.950052802404069,1.9500528020423982,1.9500528016739962,1.9500528012987384,1.950052800916497,1.950052800527142,1.9500528001305413,1.9500527997265595,1.95005279931506,1.9500527988959022,1.950052798468944,1.9500527980340399,1.9500527975910422,1.9500527971398003,1.9500527966801608,1.9500527962119678,1.9500527957350613,1.9500527952492794,1.9500527947544575,1.9500527942504264,1.9500527937370158,1.9500527932140506,1.9500527926813525,1.9500527921387412,1.9500527915860317,1.9500527910230365,1.950052790449564,1.950052789865419,1.9500527892704032,1.950052788664314,1.9500527880469454,1.9500527874180875,1.9500527867775268,1.9500527861250452,1.950052785460421,1.950052784783428,1.9500527840938364,1.950052783391411,1.950052782675914,1.9500527819471014,1.9500527812047257,1.9500527804485344,1.9500527796782703,1.9500527788936715,1.9500527780944716,1.9500527772803986,1.9500527764511755,1.950052775606521,1.9500527747461474,1.9500527738697617,1.9500527729770674,1.9500527720677596,1.9500527711415299,1.950052770198063,1.9500527692370382,1.9500527682581288,1.9500527672610022,1.950052766245319,1.950052765210734,1.9500527641568954,1.9500527630834452,1.950052761990018,1.9500527608762424,1.9500527597417396,1.950052758586124,1.9500527574090023,1.9500527562099745,1.950052754988633,1.9500527537445627,1.9500527524773403,1.9500527511865353,1.9500527498717084,1.9500527485324128,1.9500527471681932,1.9500527457785861,1.9500527443631182,1.950052742921309,1.9500527414526676,1.9500527399566956,1.9500527384328836,1.9500527368807143,1.9500527352996588,1.9500527336891806,1.9500527320487318,1.9500527303777548,1.9500527286756817,1.9500527269419328,1.9500527251759197,1.9500527233770415,1.9500527215446866,1.9500527196782325,1.9500527177770441,1.9500527158404752,1.9500527138678676,1.95005271185855,1.95005270981184,1.9500527077270415,1.9500527056034458,1.9500527034403308,1.950052701236961,1.9500526989925875,1.950052696706447,1.9500526943777627,1.9500526920057426,1.95005268958958,1.950052687128454,1.9500526846215274,1.9500526820679487,1.9500526794668485,1.9500526768173434,1.9500526741185324,1.9500526713694981,1.9500526685693058,1.9500526657170036,1.9500526628116215,1.9500526598521721,1.9500526568376493,1.950052653767028,1.9500526506392646,1.9500526474532953,1.9500526442080373,1.950052640902387,1.9500526375352216,1.9500526341053952,1.9500526306117425,1.9500526270530756,1.950052623428184,1.9500526197358368,1.9500526159747777,1.9500526121437285,1.9500526082413867,1.9500526042664257,1.9500526002174943,1.9500525960932158,1.9500525918921878,1.9500525876129833,1.9500525832541469,1.9500525788141965,1.9500525742916233,1.9500525696848896,1.9500525649924296,1.9500525602126477,1.9500525553439196,1.9500525503845896,1.9500525453329722,1.9500525401873505,1.9500525349459747,1.950052529607063,1.950052524168801,1.95005251862934,1.9500525129867967,1.9500525072392532,1.9500525013847556,1.9500524954213139,1.950052489346901,1.9500524831594523,1.950052476856864,1.950052470436994,1.9500524638976602,1.9500524572366393,1.9500524504516674,1.9500524435404387,1.9500524365006031,1.9500524293297683,1.9500524220254964,1.9500524145853053,1.9500524070066656,1.9500523992870011,1.9500523914236882,1.9500523834140544,1.9500523752553767,1.9500523669448824,1.9500523584797465,1.9500523498570919,1.9500523410739883,1.9500523321274499,1.9500523230144362,1.9500523137318495,1.9500523042765354,1.95005229464528,1.9500522848348099,1.9500522748417908,1.9500522646628264,1.950052254294457,1.9500522437331593,1.9500522329753434,1.9500522220173533,1.9500522108554652,1.9500521994858853,1.95005218790475,1.9500521761081235,1.9500521640919963,1.9500521518522855,1.9500521393848311,1.9500521266853965,1.950052113749666,1.9500521005732439,1.9500520871516525,1.9500520734803304,1.9500520595546325,1.9500520453698262,1.9500520309210916,1.950052016203519,1.9500520012121072,1.9500519859417624,1.9500519703872956,1.9500519545434216,1.9500519384047574,1.9500519219658186,1.95005190522102,1.9500518881646722,1.9500518707909797,1.9500518530940403,1.9500518350678406,1.9500518167062568,1.9500517980030498,1.9500517789518659,1.950051759546232,1.9500517397795563,1.9500517196451228,1.9500516991360917,1.9500516782454953,1.9500516569662372,1.9500516352910884,1.950051613212686,1.95005159072353,1.950051567815981,1.950051544482258,1.9500515207144344,1.9500514965044375,1.950051471844044,1.9500514467248773,1.9500514211384061,1.9500513950759395,1.9500513685286267,1.9500513414874503,1.9500513139432265,1.9500512858866017,1.950051257308047,1.9500512281978568,1.950051198546146,1.9500511683428445,1.9500511375776965,1.9500511062402546,1.950051074319878,1.9500510418057277,1.9500510086867637,1.9500509749517414,1.9500509405892061,1.9500509055874913,1.9500508699347132,1.9500508336187676,1.9500507966273257,1.95005075894783,1.9500507205674884,1.9500506814732725,1.950050641651912,1.9500506010898897,1.950050559773437,1.950050517688531,1.950050474820887,1.9500504311559559,1.9500503866789185,1.9500503413746804,1.9500502952278667,1.9500502482228175,1.9500502003435818,1.9500501515739133,1.9500501018972631,1.9500500512967764,1.9500499997552847,1.950049947255301,1.9500498937790147,1.9500498393082835,1.950049783824629,1.9500497273092303,1.950049669742917,1.9500496111061634,1.950049551379081,1.9500494905414132,1.9500494285725267,1.950049365451407,1.9500493011566484,1.9500492356664492,1.950049168958603,1.9500491010104917,1.9500490317990784,1.9500489613008987,1.9500488894920531,1.9500488163481993,1.9500487418445438,1.950048665955834,1.9500485886563488,1.9500485099198908,1.9500484297197769,1.95004834802883,1.9500482648193693,1.9500481800632017,1.9500480937316111,1.9500480057953502,1.9500479162246298,1.9500478249891098,1.9500477320578868,1.950047637399487,1.950047540981853,1.950047442772334,1.9500473427376754,1.9500472408440068,1.950047137056831,1.9500470313410128,1.9500469236607674,1.9500468139796465,1.9500467022605297,1.9500465884656084,1.9500464725563758,1.950046354493613,1.9500462342373759,1.9500461117469818,1.9500459869809967,1.9500458598972201,1.9500457304526728,1.9500455986035803,1.9500454643053602,1.9500453275126064,1.9500451881790744,1.950045046257666,1.950044901700413,1.9500447544584616,1.950044604482057,1.9500444517205255,1.9500442961222593,1.9500441376346975,1.95004397620431,1.9500438117765804,1.9500436442959856,1.9500434737059802,1.9500432999489756,1.9500431229663222,1.9500429426982897,1.9500427590840481,1.9500425720616454,1.9500423815679904,1.950042187538829,1.950041989908725,1.9500417886110377,1.9500415835779004,1.9500413747401972,1.9500411620275417,1.9500409453682528,1.9500407246893319,1.9500404999164378,1.9500402709738642,1.950040037784514,1.9500398002698722,1.9500395583499845,1.950039311943427,1.9500390609672824,1.9500388053371116,1.950038544966926,1.9500382797691609,1.9500380096546457,1.9500377345325752,1.950037454310481,1.9500371688942,1.9500368781878454,1.9500365820937744,1.950036280512557,1.9500359733429449,1.950035660481837,1.9500353418242475,1.9500350172632714,1.95003468669005,1.950034349993737,1.9500340070614612,1.950033657778292,1.9500333020272007,1.9500329396890246,1.950032570642428,1.950032194763864,1.9500318119275344,1.950031422005351,1.9500310248668917,1.9500306203793638,1.9500302084075571,1.9500297888138036,1.9500293614579343,1.9500289261972321,1.9500284828863907,1.9500280313774647,1.950027571519826,1.9500271031601155,1.9500266261421941,1.9500261403070942,1.9500256454929707,1.9500251415350502,1.9500246282655782,1.9500241055137686,1.950023573105749,1.9500230308645086,1.9500224786098408,1.9500219161582901,1.950021343323094,1.9500207599141246,1.950020165737832,1.9500195605971835,1.9500189442916036,1.9500183166169125,1.9500176773652644,1.950017026325083,1.9500163632809977,1.950015688013779,1.9500150003002699,1.950014299913322,1.9500135866217239,1.9500128601901325,1.9500121203790035,1.9500113669445183,1.9500105996385113,1.9500098182083978,1.9500090223970967,1.950008211942955,1.950007386579671,1.9500065460362161,1.950005690036754,1.9500048183005605,1.9500039305419414,1.9500030264701496,1.9500021057892998,1.9500011681982836,1.9500002133906824,1.9499992410546787,1.949998250872968,1.949997242522666,1.9499962156752182,1.9499951699963067,1.9499941051457546,1.9499930207774305,1.949991916539152,1.9499907920725854,1.949989647013148,1.9499884809899055,1.9499872936254699,1.9499860845358945,1.9499848533305706,1.949983599612119,1.9499823229762834,1.9499810230118195,1.9499796993003844,1.9499783514164266,1.9499769789270691,1.9499755813919968,1.9499741583633379,1.9499727093855495,1.9499712339952944,1.9499697317213234,1.9499682020843525,1.949966644596938,1.9499650587633544,1.9499634440794655,1.9499618000325991,1.9499601261014163,1.9499584217557822,1.9499566864566331,1.949954919655845,1.9499531207960976,1.94995128931074,1.9499494246236522,1.9499475261491073,1.9499455932916319,1.9499436254458655,1.9499416219964172,1.9499395823177232,1.949937505773901,1.949935391718605,1.9499332394948783,1.9499310484350045,1.949928817860358,1.9499265470812561,1.9499242353968032,1.949921882094742,1.9499194864512972,1.9499170477310246,1.9499145651866507,1.9499120380589212,1.9499094655764402,1.949906846955516,1.9499041813999987,1.949901468101123,1.9498987062373485,1.949895894974198,1.9498930334640978,1.9498901208462152,1.949887156246297,1.9498841387765073,1.9498810675352658,1.9498779416070846,1.949874760062405,1.949871521957436,1.9498682263339915,1.9498648722193266,1.9498614586259775,1.9498579845515986,1.9498544489788001,1.9498508508749892,1.9498471891922085,1.9498434628669767,1.9498396708201315,1.9498358119566703,1.9498318851655936,1.9498278893197516,1.9498238232756893,1.9498196858734933,1.9498154759366433,1.9498111922718606,1.9498068336689638,1.949802398900721,1.9497978867227097,1.9497932958731747,1.9497886250728922,1.9497838730250325,1.94977903841503,1.949774119910453,1.9497691161608792,1.9497640257977715,1.9497588474343617,1.9497535796655339,1.949748221067715,1.9497427701987675,1.94973722559789,1.9497315857855144,1.9497258492632215,1.949720014513648,1.9497140800004062,1.94970804416801,1.9497019054418023,1.9496956622278934,1.9496893129131014,1.949682855864904,1.9496762894313913,1.9496696119412324,1.949662821703645,1.9496559170083734,1.9496488961256768,1.9496417573063245,1.9496344987815988,1.9496271187633087,1.9496196154438126,1.9496119869960498,1.9496042315735824,1.9495963473106495,1.9495883323222265,1.9495801847041039,1.9495719025329694,1.9495634838665066,1.9495549267435042,1.9495462291839774,1.9495373891893033,1.9495284047423687,1.9495192738077325,1.9495099943317995,1.9495005642430132,1.9494909814520598,1.9494812438520879,1.9494713493189462,1.949461295711435,1.9494510808715761,1.9494407026248988,1.9494301587807426,1.9494194471325812,1.949408565458361,1.9493975115208626,1.9493862830680768,1.9493748778336075,1.9493632935370888,1.9493515278846263,1.9493395785692607,1.949327443271452,1.949315119659587,1.9493026053905114,1.9492898981100841,1.9492769954537579,1.949263895047185,1.9492505945068477,1.9492370914407169,1.9492233834489379,1.9492094681245438,1.9491953430541975,1.9491810058189651,1.9491664539951177,1.9491516851549657,1.949136696867725,1.9491214867004154,1.9491060522187948,1.9490903909883264,1.9490745005751817,1.949058378547284,1.9490420224753817,1.9490254299341714,1.9490085985034518,1.9489915257693238,1.9489742093254312,1.948956646774247,1.9489388357284014,1.94892077381206,1.948902458662347,1.9488838879308188,1.9488650592849883,1.9488459704099017,1.9488266190097683,1.948807002809646,1.9487871195571866,1.9487669670244365,1.9487465430097008,1.9487258453394705,1.9487048718704132,1.948683620491434,1.9486620891257995,1.9486402757333423,1.9486181783127312,1.9485957949038213,1.9485731235900836,1.9485501625011132,1.948526909815225,1.948503363762131,1.9484795226257112,1.9484553847468784,1.94843094852653,1.948406212428608,1.9483811749832536,1.9483558347900705,1.9483301905214934,1.9483042409262703,1.9482779848330574,1.9482514211541344,1.9482245488892402,1.9481973671295365,1.9481698750616971,1.9481420719721358,1.948113957251366,1.9480855303985065,1.9480567910259294,1.9480277388640592,1.9479983737663233,1.9479686957142632,1.9479387048228047,1.9479084013456982,1.9478777856811245,1.9478468583774804,1.9478156201393402,1.947784071833604,1.94775221449583,1.9477200493367632,1.9476875777490597,1.9476548013142092,1.9476217218096683,1.9475883412161983,1.9475546617254205,1.9475206857475873,1.9474864159195768,1.9474518551131144,1.9474170064432192,1.9473818732768928,1.9473464592420373,1.9473107682366202,1.9472748044380794,1.947238572312979,1.9472020766269142,1.9471653224546708,1.9471283151906376,1.9470910605594831,1.9470535646270886,1.947015833811744,1.946977874895611,1.9469396950364506,1.9469013017796162,1.9468627030703156,1.946823907266144,1.9467849231498786,1.9467457599425477,1.9467064273167647,1.9466669354103263,1.946627294840081,1.9465875167160536,1.9465476126558374,1.9465075947992379,1.9464674758231764,1.9464272689568396,1.9463869879970799,1.9463466473240545,1.9463062619171005,1.9462658473708414,1.9462254199115185,1.946184996413534,1.9461445944162088,1.9461042321407358,1.946063928507327,1.9460237031525414,1.9459835764467852,1.9459435695119698,1.9459037042393212,1.9458640033073236,1.9458244901997872,1.945785189224024,1.9457461255291209,1.945707325124288,1.945668814897275,1.9456306226328313,1.9455927770311972,1.9455553077266057,1.945518245305776,1.945481621326381,1.9454454683354676,1.9454098198878056,1.9453747105641503,1.945340175989389,1.9453062528505545,1.9452729789146757,1.9452403930464484,1.9452085352256925,1.9451774465645804,1.9451471693245976,1.9451177469332221,1.9450892240002853,1.9450616463339907,1.945035060956565,1.9450095161195042,1.9449850613183965,1.9449617473072853,1.9449396261125436,1.9449187510462334,1.9448991767189172,1.94488095905189,1.9448641552888062,1.944848824006665,1.9448350251261324,1.9448228199211581,1.9448122710278672,1.9448034424526932,1.944796399579721,1.944791209177213,1.94478793940329,1.944786659810734,1.9447874413508923,1.9447903563766489,1.9447954786444408,1.9448028833152926,1.9448126469548448,1.9448248475323484,1.9448395644186107,1.944856878382862,1.9448768715885274,1.9448996275878832,1.9449252313155785,1.9449537690810068,1.9449853285595107,1.9450199987824062,1.9450578701258132,1.9450990342982828,1.9451435843272111,1.9451916145440287,1.945243220568167,1.9452984992897884,1.9453575488512829,1.945420468627529,1.945487359204919,1.9455583223591504,1.9456334610317916,1.9457128793056189,1.9457966823787463,1.9458849765375426,1.9459778691283558,1.946075468528058,1.9461778841134167,1.9462852262293182,1.946397606155852,1.9465151360742796,1.946637929031907,1.9467660989058748,1.9468997603659013,1.9470390288359851,1.9471840204551052,1.9473348520369318,1.9474916410285825,1.9476545054684404,1.9478235639430626,1.9479989355432106,1.948180739819019,1.9483690967343332,1.9485641266202418,1.948765950127826,1.9489746881801504,1.9491904619235196,1.9494133926780228,1.949643601887385,1.949881211068148,1.9501263417581942,1.9503791154646393,1.9506396536110941,1.9509080774843257,1.951184508180312,1.9514690665497116,1.951761873142748,1.9520630481535146,1.9523727113637022,1.9526909820857479,1.9530179791054023,1.9533538206237087,1.9536986241983874,1.9540525066846108,1.9544155841751591,1.9547879719399406,1.9551697843648534,1.9555611348899749,1.9559621359470503,1.956372898896258,1.9567935339622258,1.957224150169265,1.9576648552757985,1.9581157557079476,1.9585769564922466,1.9590485611874517,1.9595306718154133,1.960023388790976,1.960526810850873,1.96104103498159,1.9615661563461544,1.962102268209835,1.962649461864715,1.963207826553117,1.9637774493898574,1.9643584152833131,1.9649508068552821,1.9655547043596322,1.9661701855997273,1.9667973258446358,1.9674361977441202,1.9680868712424284,1.9687494134908934,1.9694238887593805,1.9701103583465995,1.9708088804893389,1.9715195102706564,1.9722422995270985,1.9729772967550019,1.9737245470159646,1.9744840918415683,1.9752559691374447,1.9760402130868027,1.9768368540535182,1.9776459184849258,1.978467428814443,1.9793014033641751,1.9801478562476593,1.9810067972729144,1.9818782318459773,1.9827621608751067,1.9836585806758604,1.9845674828772413,1.9854888543291387,1.9864226770112743,1.9873689279438973,1.988327579100453,1.9892985973224793,1.9902819442369741,1.9912775761764903,1.9922854441022182,1.9933054935303138,1.9943376644617432,1.9953818913159012,1.996438102868274,1.9975062221924171,1.998586166606498,1.9996778476246813,2.0007811709136014,2.0018960362541796,2.003022337509037,2.0041599625957294,2.0053087934660505,2.006468706091618,2.0076395704559453,2.0088212505532237,2.0100136043939685,2.0112164840177345,2.012429735513041,2.013653199044654,2.01488670888836,2.0161300934733366,2.017383175432209,2.018645771658876,2.019917693374159,2.0211987461992975,2.022488730237327,2.0237874401623146,2.0250946653164443,2.0264101898148836,2.027733792658393,2.029065247853558,2.0304043245405543,2.0317507871283094,2.033104395436906,2.0344649048470584,2.0358320664564635,2.037205627242824,2.038585330233307,2.039970914680182,2.0413621162423907,2.0427586671727407,2.044160296510446,2.0455667302786806,2.0469776916868327,2.048392901337106,2.0498120774351243,2.051234936004164,2.052661191102656,2.0540905550445623,2.0555227386222445,2.0569574513314293,2.05839440159787,2.0598332970053073,2.061273844524316,2.062715750741642,2.0641587220896196,2.0656024650752713,2.0670466865086863,2.0684910937302843,2.069935394836582,2.071379298904068,2.072822516210819,2.07426475845549,2.07570573897332,2.0771451729488137,2.078582777624749,2.080018272507214,2.0814513795663405,2.0828818234324524,2.0843093315873493,2.0857336345504507,2.087154466059566,2.08857156324604,2.0899846668040722,2.0913935211539987,2.0927978745993583,2.094197479477576,2.0955920923041145,2.096981473909958,2.09836538957232,2.0997436091384745,2.101115907142624,2.102482062915753,2.103841860688404,2.1051950896863607,2.106541544219208,2.10788102376178,2.1092133330285043,2.1105382820406766,2.1118556861867037,2.1131653662753798,2.1144671485822615,2.1157608648892197,2.117046352517274,2.118323454352808,2.119592018867278,2.120851900130553,2.1221029578180044,2.123345057211513,2.1245780691945155,2.1258018702412786,2.1270163424005433,2.128221373273728,2.1294168559878544,2.1306026891633922,2.1317787768771903,2.1329450286206995,2.1341013592536733,2.13524768895354,2.13638394316064,2.137510052519534,2.138625952816571,2.1397315849139225,2.140826894680269,2.1419118329183533,2.1429863552895774,2.1440504222358525,2.145103998898885,2.1461470550370936,2.1471795649403416,2.148201507342671,2.1492128653332156,2.1502136262654776,2.15120378166513,2.1521833271365267,2.1531522622680748,2.154110590536638,2.1550583192111223,2.1559954592553945,2.1569220252306875,2.157838035197625,2.158743510618007,2.159638476256485],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.858757097839295,1.8587570967301497,1.8587570956003632,1.858757094449551,1.8587570932773225,1.858757092083278,1.8587570908670128,1.8587570896281125,1.858757088366156,1.8587570870807144,1.8587570857713502,1.8587570844376191,1.8587570830790665,1.8587570816952315,1.858757080285643,1.858757078849822,1.8587570773872797,1.8587570758975194,1.8587570743800348,1.8587570728343097,1.858757071259818,1.8587570696560252,1.858757068022385,1.858757066358343,1.8587570646633327,1.8587570629367782,1.8587570611780924,1.8587570593866771,1.8587570575619234,1.8587570557032105,1.858757053809907,1.8587570518813687,1.8587570499169404,1.8587570479159534,1.8587570458777278,1.8587570438015708,1.8587570416867762,1.8587570395326247,1.8587570373383846,1.8587570351033094,1.858757032826639,1.8587570305075996,1.8587570281454022,1.8587570257392447,1.8587570232883075,1.8587570207917588,1.8587570182487487,1.8587570156584128,1.8587570130198707,1.8587570103322246,1.8587570075945614,1.8587570048059503,1.8587570019654422,1.8587569990720727,1.8587569961248565,1.8587569931227925,1.8587569900648597,1.858756986950019,1.8587569837772102,1.8587569805453554,1.8587569772533552,1.8587569739000906,1.8587569704844216,1.8587569670051867,1.8587569634612024,1.8587569598512645,1.8587569561741455,1.858756952428595,1.8587569486133388,1.858756944727081,1.8587569407684994,1.8587569367362482,1.8587569326289566,1.858756928445228,1.8587569241836395,1.858756919842743,1.8587569154210615,1.8587569109170923,1.8587569063293041,1.8587569016561365,1.8587568968960013,1.8587568920472797,1.8587568871083227,1.8587568820774514,1.8587568769529554,1.858756871733092,1.8587568664160867,1.858756861000132,1.8587568554833855,1.8587568498639724,1.8587568441399813,1.858756838309467,1.8587568323704464,1.8587568263209,1.8587568201587716,1.8587568138819655,1.858756807488348,1.8587568009757447,1.8587567943419419,1.858756787584684,1.8587567807016727,1.8587567736905688,1.8587567665489881,1.8587567592745022,1.8587567518646382,1.8587567443168767,1.8587567366286508,1.8587567287973468,1.8587567208203026,1.8587567126948052,1.8587567044180928,1.8587566959873503,1.858756687399712,1.8587566786522574,1.858756669742013,1.8587566606659494,1.85875665142098,1.858756642003962,1.8587566324116938,1.8587566226409131,1.8587566126882993,1.8587566025504672,1.8587565922239706,1.858756581705298,1.858756570990874,1.8587565600770548,1.8587565489601299,1.8587565376363198,1.8587565261017744,1.8587565143525717,1.8587565023847172,1.858756490194142,1.8587564777767012,1.858756465128173,1.8587564522442566,1.858756439120572,1.8587564257526565,1.8587564121359659,1.8587563982658701,1.8587563841376533,1.8587563697465121,1.8587563550875539,1.858756340155794,1.8587563249461565,1.8587563094534696,1.8587562936724662,1.8587562775977806,1.8587562612239477,1.8587562445454007,1.8587562275564689,1.8587562102513762,1.8587561926242386,1.8587561746690635,1.8587561563797463,1.8587561377500688,1.8587561187736967,1.8587560994441787,1.858756079754943,1.858756059699295,1.858756039270416,1.8587560184613612,1.8587559972650547,1.8587559756742913,1.8587559536817293,1.858755931279892,1.8587559084611631,1.8587558852177846,1.8587558615418542,1.858755837425322,1.858755812859989,1.8587557878375032,1.8587557623493582,1.8587557363868874,1.8587557099412648,1.858755683003499,1.8587556555644318,1.8587556276147343,1.8587555991449043,1.8587555701452627,1.8587555406059502,1.858755510516924,1.8587554798679546,1.8587554486486222,1.8587554168483127,1.8587553844562148,1.8587553514613162,1.8587553178523992,1.858755283618038,1.8587552487465937,1.858755213226211,1.8587551770448139,1.8587551401901021,1.8587551026495455,1.8587550644103819,1.8587550254596112,1.8587549857839913,1.8587549453700338,1.8587549042039995,1.8587548622718935,1.8587548195594599,1.858754776052178,1.8587547317352575,1.858754686593632,1.858754640611955,1.8587545937745955,1.8587545460656292,1.8587544974688386,1.8587544479677016,1.8587543975453908,1.8587543461847649,1.858754293868363,1.8587542405784008,1.8587541862967618,1.8587541310049929,1.858754074684298,1.8587540173155308,1.8587539588791888,1.8587538993554076,1.858753838723951,1.8587537769642088,1.8587537140551862,1.8587536499754975,1.8587535847033596,1.8587535182165842,1.8587534504925696,1.858753381508294,1.8587533112403078,1.8587532396647242,1.8587531667572121,1.8587530924929891,1.8587530168468098,1.8587529397929603,1.858752861305248,1.8587527813569933,1.8587526999210193,1.8587526169696456,1.858752532474674,1.8587524464073841,1.8587523587385202,1.858752269438282,1.8587521784763164,1.8587520858217035,1.8587519914429493,1.8587518953079747,1.8587517973841023,1.8587516976380478,1.8587515960359076,1.8587514925431472,1.8587513871245898,1.8587512797444046,1.8587511703660935,1.8587510589524805,1.8587509454656972,1.8587508298671713,1.8587507121176128,1.8587505921770011,1.8587504700045707,1.858750345558799,1.8587502187973897,1.8587500896772604,1.8587499581545275,1.8587498241844909,1.8587496877216196,1.8587495487195351,1.858749407130997,1.8587492629078857,1.8587491160011866,1.8587489663609742,1.858748813936394,1.8587486586756459,1.8587485005259659,1.8587483394336097,1.8587481753438326,1.8587480082008723,1.8587478379479296,1.8587476645271486,1.858747487879598,1.85874730794525,1.858747124662961,1.8587469379704509,1.85874674780428,1.8587465540998305,1.8587463567912825,1.858746155811592,1.8587459510924687,1.8587457425643523,1.858745530156389,1.8587453137964085,1.8587450934108978,1.858744868924977,1.8587446402623746,1.8587444073454014,1.8587441700949223,1.8587439284303335,1.8587436822695298,1.8587434315288822,1.8587431761232056,1.8587429159657316,1.858742650968079,1.8587423810402237,1.8587421060904683,1.8587418260254112,1.8587415407499135,1.8587412501670697,1.8587409541781712,1.8587406526826769,1.8587403455781752,1.8587400327603523,1.8587397141229556,1.858739389557757,1.8587390589545183,1.858738722200952,1.8587383791826844,1.8587380297832157,1.858737673883882,1.8587373113638144,1.8587369420998978,1.8587365659667305,1.8587361828365796,1.8587357925793395,1.8587353950624883,1.8587349901510406,1.8587345777075048,1.8587341575918337,1.8587337296613802,1.8587332937708456,1.8587328497722346,1.8587323975148005,1.8587319368449986,1.8587314676064313,1.858730989639797,1.8587305027828358,1.8587300068702735,1.858729501733768,1.8587289872018506,1.858728463099868,1.8587279292499255,1.8587273854708246,1.8587268315780023,1.8587262673834701,1.8587256926957487,1.8587251073198046,1.8587245110569839,1.8587239037049454,1.858723285057593,1.8587226549050038,1.8587220130333595,1.8587213592248748,1.858720693257722,1.858720014905957,1.858719323939444,1.8587186201237762,1.8587179032201986,1.8587171729855267,1.8587164291720648,1.8587156715275215,1.8587148997949272,1.858714113712545,1.858713313013784,1.8587124974271094,1.8587116666759516,1.8587108204786111,1.858709958548167,1.8587090805923776,1.858708186313584,1.8587072754086085,1.858706347568655,1.8587054024792025,1.8587044398199015,1.858703459264466,1.8587024604805629,1.858701443129702,1.858700406867121,1.8586993513416712,1.858698276195698,1.8586971810649233,1.858696065578321,1.8586949293579935,1.8586937720190466,1.858692593169458,1.858691392409948,1.8586901693338445,1.8586889235269481,1.8586876545673927,1.858686362025504,1.8586850454636576,1.8586837044361306,1.858682338488954,1.8586809471597603,1.8586795299776295,1.8586780864629329,1.8586766161271695,1.8586751184728083,1.8586735929931175,1.8586720391719986,1.8586704564838137,1.8586688443932085,1.858667202354938,1.8586655298136807,1.8586638262038573,1.8586620909494398,1.8586603234637615,1.8586585231493236,1.8586566893975927,1.8586548215888017,1.858652919091744,1.858650981263563,1.8586490074495396,1.8586469969828747,1.8586449491844699,1.8586428633626997,1.858640738813186,1.8586385748185619,1.8586363706482378,1.858634125558157,1.8586318387905518,1.858629509573693,1.8586271371216345,1.8586247206339561,1.8586222592954975,1.858619752276091,1.8586171987302893,1.8586145977970847,1.8586119485996284,1.8586092502449423,1.8586065018236249,1.8586037024095527,1.8586008510595788,1.8585979468132212,1.8585949886923494,1.8585919757008653,1.8585889068243766,1.8585857810298652,1.8585825972653507,1.8585793544595473,1.8585760515215135,1.8585726873402988,1.85856926078458,1.858565770702295,1.8585622159202682,1.8585585952438293,1.8585549074564254,1.8585511513192294,1.8585473255707354,1.8585434289263532,1.8585394600779923,1.8585354176936408,1.8585313004169342,1.8585271068667197,1.858522835636611,1.8585184852945371,1.8585140543822816,1.8585095414150148,1.8585049448808195,1.8585002632402046,1.8584954949256154,1.8584906383409312,1.8584856918609574,1.858480653830907,1.8584755225658751,1.8584702963503026,1.8584649734374323,1.8584595520487541,1.8584540303734451,1.858448406567794,1.8584426787546204,1.8584368450226842,1.858430903426082,1.8584248519836377,1.8584186886782796,1.858412411456409,1.8584060182272566,1.8583995068622325,1.8583928751942587,1.8583861210170973,1.858379242084664,1.8583722361103316,1.858365100766221,1.8583578336824829,1.8583504324465665,1.8583428946024751,1.8583352176500125,1.858327399044016,1.8583194361935769,1.8583113264612487,1.8583030671622431,1.8582946555636128,1.8582860888834227,1.8582773642899053,1.8582684789006074,1.858259429781518,1.8582502139461878,1.858240828354831,1.8582312699134171,1.8582215354727443,1.858211621827504,1.8582015257153262,1.8581912438158135,1.8581807727495594,1.8581701090771527,1.8581592492981658,1.8581481898501295,1.8581369271074912,1.8581254573805586,1.8581137769144287,1.8581018818879,1.8580897684123687,1.8580774325307114,1.8580648702161486,1.8580520773710962,1.8580390498259953,1.8580257833381324,1.8580122735904367,1.8579985161902652,1.8579845066681704,1.8579702404766483,1.8579557129888749,1.8579409194974208,1.8579258552129514,1.8579105152629094,1.85789489469018,1.85787898845174,1.857862791417287,1.8578462983678548,1.8578295039944082,1.8578124028964227,1.8577949895804458,1.857777258458641,1.8577592038473136,1.857740819965422,1.8577221009330664,1.8577030407699662,1.8576836333939155,1.8576638726192227,1.8576437521551343,1.857623265604239,1.8576024064608574,1.8575811681094128,1.857559543822786,1.857537526760654,1.8575151099678107,1.857492286372473,1.857469048784569,1.8574453898940135,1.8574213022689625,1.857396778354058,1.8573718104686534,1.8573463908050276,1.8573205114265816,1.8572941642660215,1.85726734112353,1.8572400336649217,1.8572122334197878,1.8571839317796255,1.8571551199959593,1.857125789178446,1.8570959302929748,1.8570655341597497,1.8570345914513695,1.8570030926908934,1.856971028249901,1.8569383883465438,1.8569051630435889,1.8568713422464591,1.8568369157012654,1.8568018729928348,1.8567662035427384,1.8567298966073127,1.8566929412756814,1.856655326467777,1.856617040932364,1.856578073245062,1.8565384118063726,1.856498044839713,1.8564569603894534,1.8564151463189607,1.856372590308653,1.8563292798540656,1.856285202263924,1.8562403446582354,1.856194693966392,1.8561482369252942,1.85610096007749,1.856052849769338,1.8560038921491895,1.8559540731656006,1.8559033785655663,1.8558517938927876,1.855799304485969,1.8557458954771504,1.8556915517900738,1.8556362581385932,1.8555799990251227,1.8555227587391314,1.8554645213556846,1.8554052707340367,1.8553449905162764,1.8552836641260306,1.8552212747672263,1.855157805422917,1.855093238854176,1.855027557599061,1.8549607439716496,1.8548927800611577,1.8548236477311364,1.8547533286187545,1.854681804134176,1.8546090554600239,1.8545350635509508,1.8544598091333084,1.8543832727049259,1.8543054345350007,1.8542262746641083,1.8541457729043325,1.854063908839525,1.8539806618256982,1.8538960109915537,1.8538099352391602,1.8537224132447727,1.8536334234598162,1.8535429441120226,1.8534509532067411,1.8533574285284176,1.8532623476422556,1.853165687896063,1.8530674264222917,1.8529675401402756,1.8528660057586754,1.8527627997781366,1.8526578984941648,1.852551278000232,1.852442914191115,1.8523327827664733,1.8522208592346805,1.8521071189169092,1.8519915369514797,1.851874088298484,1.8517547477446852,1.8516334899087075,1.8515102892465207,1.8513851200572298,1.8512579564891731,1.8511287725463446,1.8509975420951412,1.8508642388714485,1.8507288364880694,1.8505913084425076,1.850451628125113,1.8503097688275938,1.8501657037519104,1.850019406019553,1.8498708486812163,1.8497200047268765,1.8495668470962785,1.849411348689844,1.8492534823800073,1.8490932210229865,1.8489305374709972,1.8487654045849198,1.8485977952474244,1.8484276823765629,1.848255038939835,1.8480798379687344,1.8479020525737875,1.847721655960081,1.8475386214432958,1.8473529224662475,1.8471645326159423,1.8469734256411494,1.846779575470504,1.8465829562311369,1.8463835422678418,1.8461813081627818,1.8459762287557424,1.845768279164928,1.845557434808315,1.8453436714255562,1.8451269651004434,1.8449072922839256,1.8446846298176944,1.8444589549583223,1.8442302454019694,1.843998479309648,1.8437636353330495,1.8435256926409325,1.8432846309460647,1.8430404305327248,1.8427930722847545,1.8425425377141578,1.8422888089902472,1.8420318689693258,1.8417717012249049,1.8415082900784452,1.8412416206306197,1.8409716787930839,1.8406984513207523,1.8404219258445629,1.8401420909047248,1.839858935984438,1.8395724515440677,1.839282629055767,1.838989461038529,1.838692941093657,1.838393063940635,1.8380898254533833,1.837783222696882,1.8374732539641436,1.8371599188135164,1.8368432181062992,1.8365231540446443,1.8361997302097324,1.835872951600189,1.8355428246707282,1.8352093573709916,1.8348725591845638,1.8345324411681352,1.8341890159907828,1.8338422979733522,1.8334923031278993,1.8331390491971704,1.832782555694092,1.8324228439412331,1.8320599371102178,1.8316938602610482,1.8313246403813097,1.8309523064252236,1.8305768893525143,1.8301984221670573,1.8298169399552686,1.829432479924206,1.8290450814393422,1.8286547860619744,1.8282616375862322,1.8278656820756503,1.8274669678992599,1.827065545767172,1.8266614687656024,1.8262547923913086,1.8258455745853945,1.8254338757664454,1.825019758862953,1.8246032893449935,1.8241845352551118,1.8237635672383854,1.8233404585716124,1.8229152851915953,1.8224881257224783,1.822059061502094,1.8216281766072866,1.8211955578781704,1.8207612949412786,1.8203254802315754,1.8198882090132833,1.8194495793994874,1.8190096923704908,1.8185686517908681,1.8181265644251912,1.8176835399523847,1.817239690978682,1.8167951330491396,1.816349984657678,1.8159043672556177,1.815458405258672,1.815012226052367,1.8145659599958546,1.8141197404240907,1.813673703648341,1.8132279889549925,1.8127827386026332,1.8123380978173778,1.8118942147864059,1.811451240649691,1.8110093294898855,1.8105686383203434,1.810129327071251,1.8096915585738427,1.8092554985426712,1.8088213155559194,1.80838918103372,1.8079592692144657,1.807531757129083,1.807106824573255,1.8066846540775598,1.8062654308755128,1.8058493428694868,1.8054365805944874,1.8050273371797634,1.8046218083082333,1.8042201921736984,1.803822689435832,1.8034295031729084,1.8030408388322632,1.8026569041784521,1.802277909239085,1.801904066248321,1.801535589587985,1.801172695726292,1.8008156031541473,1.8004645323189974,1.800119705556202,1.7997813470178992,1.7994496825993367,1.7991249398626328,1.7988073479579392,1.798497137541975,1.798194540693892,1.7978997908284449,1.7976131226064243,1.7973347718423227,1.7970649754091923,1.7968039711406598,1.7965519977300632,1.7963092946266688,1.7960761019289349,1.7958526602747822,1.7956392107288368,1.7954359946666072,1.7952432536555627,1.795061229333078,1.7948901632812129,1.7947302968982957,1.7945818712672839,1.794445127020879,1.7943203042033684,1.7942076421291837,1.7941073792381552,1.7940197529474597,1.7939449995002519,1.7938833538109862,1.7938350493074349,1.7938003177694188,1.7937793891642728,1.7937724914790747,1.7937798505496843,1.7938016898866287,1.7938382304979075,1.7938896907087711,1.7939562859785643,1.7940382287147194,1.7941357280840047,1.7942489898211418,1.794378216034924,1.7945236050119755,1.794685351018307,1.7948636440988401,1.7950586698750854,1.79527060934117,1.795499638658436,1.7957459289488333,1.7960096460873578,1.7962909504937914,1.7965899969240215,1.796906934261233,1.7972419053072737,1.797595046574522,1.7979664880785846,1.7983563531321753,1.7987647581405426,1.7991918123988127,1.7996376178916427,1.8001022690955781,1.8005858527845229,1.801088447838747,1.8016101250578467,1.8021509469781027,1.8027109676946684,1.8032902326890343,1.8038887786622184,1.804506633374128,1.805143815489547,1.805800334431194,1.8064761902402986,1.8071713734451371,1.8078858649379614,1.8086196358607531,1.8093726475002128,1.8101448511924005,1.8109361882374142,1.811746589824487,1.8125759769678684,1.8134242604538287,1.814291340799119,1.8151771082211816,1.8160814426203975,1.817004213574625,1.8179452803462601,1.8189044919020227,1.8198816869456422,1.8208766939635903,1.8218893312839768,1.8229194071486876,1.8239667197988254,1.8250310575734627,1.8261121990217009,1.8272099130279829,1.8283239589505804,1.8294540867731377,1.8306000372691218,1.831761542179001,1.832938324399925,1.8341300981876634,1.8353365693705181,1.8365574355748924,1.8377923864621728,1.8390411039765489,1.8403032626033622,1.8415785296375606,1.8428665654617868,1.8441670238336323,1.8454795521815366,1.8468037919088147,1.848139378705257,1.8494859428657402,1.8508431096152655,1.8522104994398232,1.8535877284224858,1.8549744085840951,1.8563701482279304,1.8577745522877118,1.8591872226783097,1.8606077586485132,1.8620357571352204,1.8634708131184115,1.8649125199762682,1.8663604698398102,1.8678142539464289,1.8692734629917018,1.870737687478893,1.8722065180655385,1.87367954590656,1.8751563629933312,1.8766365624881645,1.878119739053697,1.879605489176666,1.881093411485598,1.8825831070619496,1.8840741797442617,1.8855662364249182,1.887058887339114,1.8885517463456751,1.89004443119939,1.891536563814537,1.8930277705193272,1.8945176823010004,1.8960059350413372,1.8974921697423919,1.898976032742249,1.9004571759206665,1.901935256894463,1.9034099392025559,1.9048808924805691,1.906347792624954,1.9078103219466,1.9092681693139264,1.9107210302854682,1.9121686072320043,1.9136106094482748,1.915046753254378,1.9164767620869425,1.9179003665801888,1.9193173046370224,1.9207273214903027,1.9221301697544606,1.9235256094676465,1.9249134081245978,1.9262933407004421,1.9276651896656523,1.9290287449923826,1.930383804152427,1.93173017210705,1.9330676612889404,1.9343960915765623,1.935715290261156,1.93702509200668,1.9383253388029555,1.9396158799123107,1.940896571809992,1.9421672781186403,1.9434278695371106,1.9446782237639235,1.9459182254156326,1.9471477659403955,1.9483667435270233,1.9495750630097974,1.950772635769324,1.9519593796297,1.953135218752264,1.9543000835261917,1.9554539104561997,1.956596642047609,1.9577282266890237,1.9588486185328566,1.9599577773739545,1.9610556685265366,1.9621422626996885,1.9632175358716115,1.9642814691628492,1.9653340487086937,1.9663752655309583,1.9674051154093213],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.5476304204606286,1.5476304203706073,1.5476304202789106,1.5476304201855076,1.547630420090366,1.5476304199934545,1.5476304198947393,1.547630419794187,1.5476304196917632,1.5476304195874335,1.547630419481162,1.5476304193729133,1.5476304192626498,1.5476304191503343,1.5476304190359287,1.547630418919394,1.5476304188006906,1.547630418679778,1.5476304185566157,1.5476304184311607,1.5476304183033713,1.5476304181732041,1.5476304180406144,1.547630417905557,1.5476304177679865,1.5476304176278555,1.547630417485117,1.5476304173397222,1.5476304171916213,1.5476304170407646,1.5476304168871005,1.5476304167305766,1.5476304165711399,1.5476304164087362,1.54763041624331,1.5476304160748058,1.5476304159031653,1.547630415728331,1.5476304155502425,1.5476304153688403,1.5476304151840623,1.5476304149958455,1.5476304148041262,1.547630414608839,1.5476304144099178,1.5476304142072945,1.5476304140009005,1.547630413790666,1.5476304135765186,1.5476304133583865,1.5476304131361946,1.5476304129098681,1.5476304126793299,1.5476304124445013,1.5476304122053026,1.5476304119616529,1.5476304117134687,1.5476304114606663,1.547630411203159,1.5476304109408598,1.5476304106736796,1.547630410401527,1.5476304101243101,1.5476304098419345,1.5476304095543039,1.5476304092613207,1.5476304089628854,1.5476304086588963,1.5476304083492503,1.547630408033842,1.5476304077125644,1.5476304073853078,1.5476304070519615,1.5476304067124118,1.5476304063665436,1.547630406014239,1.5476304056553782,1.5476304052898395,1.5476304049174983,1.5476304045382283,1.5476304041519007,1.5476304037583837,1.5476304033575439,1.547630402949245,1.547630402533348,1.5476304021097118,1.547630401678192,1.5476304012386422,1.547630400790913,1.5476304003348522,1.5476303998703045,1.5476303993971123,1.5476303989151146,1.5476303984241477,1.5476303979240444,1.547630397414635,1.547630396895746,1.5476303963672018,1.547630395828822,1.5476303952804236,1.5476303947218204,1.5476303941528227,1.5476303935732365,1.5476303929828656,1.5476303923815087,1.5476303917689618,1.5476303911450162,1.5476303905094604,1.5476303898620782,1.547630389202649,1.5476303885309495,1.5476303878467508,1.5476303871498207,1.5476303864399221,1.5476303857168139,1.54763038498025,1.5476303842299803,1.5476303834657499,1.5476303826872988,1.5476303818943626,1.5476303810866718,1.5476303802639517,1.547630379425923,1.5476303785723005,1.5476303777027944,1.5476303768171091,1.5476303759149435,1.5476303749959912,1.5476303740599398,1.5476303731064707,1.5476303721352604,1.547630371145979,1.54763037013829,1.5476303691118514,1.5476303680663133,1.5476303670013214,1.5476303659165136,1.5476303648115208,1.547630363685968,1.5476303625394725,1.547630361371645,1.5476303601820878,1.547630358970398,1.5476303577361628,1.547630356478963,1.547630355198372,1.5476303538939542,1.5476303525652664,1.547630351211857,1.5476303498332662,1.5476303484290261,1.5476303469986588,1.547630345541679,1.5476303440575916,1.5476303425458917,1.5476303410060661,1.547630339437592,1.5476303378399363,1.547630336212556,1.5476303345548985,1.5476303328664005,1.5476303311464885,1.5476303293945783,1.5476303276100742,1.547630325792371,1.54763032394085,1.5476303220548835,1.54763032013383,1.5476303181770372,1.5476303161838405,1.5476303141535628,1.5476303120855148,1.5476303099789936,1.547630307833284,1.5476303056476572,1.5476303034213712,1.5476303011536692,1.5476302988437813,1.5476302964909237,1.547630294094296,1.5476302916530862,1.5476302891664637,1.5476302866335845,1.5476302840535887,1.5476302814256004,1.547630278748727,1.5476302760220595,1.5476302732446723,1.547630270415622,1.5476302675339484,1.5476302645986733,1.5476302616087998,1.5476302585633128,1.547630255461179,1.5476302523013445,1.5476302490827376,1.547630245804265,1.547630242464814,1.5476302390632513,1.5476302355984217,1.5476302320691502,1.5476302284742385,1.547630224812466,1.547630221082591,1.547630217283347,1.5476302134134448,1.547630209471571,1.5476302054563884,1.5476302013665346,1.5476301972006221,1.5476301929572367,1.547630188634939,1.547630184232263,1.547630179747715,1.5476301751797736,1.5476301705268887,1.5476301657874827,1.547630160959948,1.547630156042647,1.547630151033912,1.5476301459320445,1.5476301407353144,1.547630135441959,1.5476301300501834,1.5476301245581596,1.5476301189640256,1.5476301132658845,1.5476301074618044,1.547630101549818,1.5476300955279212,1.5476300893940724,1.5476300831461929,1.5476300767821651,1.547630070299832,1.547630063696997,1.5476300569714223,1.5476300501208293,1.5476300431428969,1.5476300360352606,1.547630028795513,1.5476300214212009,1.547630013909827,1.5476300062588468,1.5476299984656692,1.547629990527655,1.5476299824421156,1.5476299742063138,1.5476299658174608,1.5476299572727164,1.5476299485691882,1.54762993970393,1.5476299306739407,1.547629921476165,1.5476299121074892,1.547629902564744,1.5476298928446999,1.5476298829440687,1.5476298728595006,1.547629862587585,1.547629852124847,1.547629841467749,1.5476298306126863,1.5476298195559888,1.5476298082939184,1.5476297968226678,1.5476297851383594,1.547629773237044,1.5476297611146999,1.5476297487672304,1.547629736190464,1.547629723380152,1.5476297103319667,1.5476296970415016,1.5476296835042682,1.5476296697156955,1.5476296556711282,1.5476296413658253,1.547629626794958,1.5476296119536093,1.547629596836771,1.547629581439343,1.5476295657561308,1.5476295497818446,1.5476295335110977,1.5476295169384036,1.5476295000581748,1.5476294828647215,1.5476294653522489,1.547629447514856,1.547629429346533,1.5476294108411595,1.547629391992503,1.5476293727942168,1.5476293532398366,1.5476293333227806,1.5476293130363452,1.5476292923737052,1.5476292713279076,1.5476292498918747,1.5476292280583974,1.5476292058201349,1.5476291831696116,1.5476291600992151,1.547629136601194,1.5476291126676545,1.5476290882905583,1.5476290634617202,1.5476290381728048,1.5476290124153254,1.547628986180639,1.5476289594599448,1.547628932244282,1.5476289045245253,1.5476288762913832,1.547628847535395,1.5476288182469269,1.547628788416169,1.5476287580331336,1.5476287270876508,1.5476286955693648,1.547628663467732,1.5476286307720166,1.5476285974712873,1.547628563554415,1.5476285290100664,1.5476284938267044,1.5476284579925816,1.547628421495737,1.547628384323993,1.5476283464649516,1.547628307905989,1.5476282686342537,1.547628228636661,1.54762818789989,1.5476281464103785,1.5476281041543194,1.5476280611176552,1.5476280172860761,1.5476279726450135,1.5476279271796352,1.5476278808748434,1.5476278337152662,1.5476277856852572,1.547627736768887,1.5476276869499408,1.5476276362119117,1.5476275845379968,1.5476275319110917,1.5476274783137858,1.5476274237283558,1.5476273681367623,1.5476273115206411,1.5476272538613025,1.5476271951397205,1.547627135336531,1.5476270744320244,1.5476270124061395,1.5476269492384584,1.5476268849082,1.5476268193942133,1.5476267526749727,1.54762668472857,1.5476266155327092,1.5476265450646989,1.547626473301447,1.5476264002194535,1.5476263257948029,1.5476262500031581,1.5476261728197536,1.547626094219388,1.5476260141764169,1.5476259326647455,1.5476258496578215,1.5476257651286274,1.547625679049673,1.547625591392988,1.5476255021301133,1.5476254112320944,1.5476253186694728,1.547625224412277,1.5476251284300162,1.5476250306916697,1.5476249311656811,1.5476248298199478,1.5476247266218128,1.5476246215380565,1.547624514534888,1.5476244055779345,1.547624294632235,1.5476241816622285,1.5476240666317467,1.5476239495040036,1.5476238302415868,1.5476237088064466,1.5476235851598883,1.5476234592625608,1.547623331074448,1.5476232005548576,1.5476230676624116,1.5476229323550377,1.547622794589956,1.5476226543236715,1.5476225115119617,1.547622366109868,1.5476222180716834,1.5476220673509424,1.5476219139004108,1.5476217576720743,1.5476215986171284,1.5476214366859653,1.547621271828166,1.5476211039924863,1.5476209331268478,1.547620759178325,1.5476205820931357,1.5476204018166284,1.5476202182932703,1.5476200314666386,1.5476198412794058,1.5476196476733306,1.5476194505892449,1.5476192499670438,1.547619045745672,1.5476188378631144,1.547618626256383,1.5476184108615063,1.547618191613517,1.5476179684464413,1.547617741293287,1.547617510086032,1.5476172747556138,1.5476170352319167,1.5476167914437615,1.547616543318895,1.5476162907839777,1.5476160337645728,1.5476157721851376,1.5476155059690095,1.5476152350383987,1.5476149593143749,1.5476146787168592,1.547614393164614,1.5476141025752315,1.5476138068651255,1.5476135059495228,1.547613199742452,1.5476128881567361,1.547612571103984,1.5476122484945822,1.5476119202376863,1.547611586241215,1.547611246411841,1.547610900654986,1.547610548874814,1.547610190974225,1.5476098268548508,1.547609456417049,1.5476090795598993,1.5476086961812003,1.5476083061774653,1.5476079094439215,1.547607505874507,1.5476070953618695,1.5476066777973678,1.5476062530710704,1.5476058210717578,1.5476053816869244,1.5476049348027814,1.547604480304261,1.5476040180750217,1.547603547997454,1.5476030699526864,1.5476025838205965,1.547602089479817,1.5476015868077486,1.5476010756805711,1.5476005559732575,1.5476000275595867,1.5475994903121615,1.5475989441024247,1.5475983888006803,1.5475978242761121,1.5475972503968072,1.5475966670297803,1.5475960740410004,1.5475954712954165,1.5475948586569914,1.5475942359887302,1.5475936031527162,1.5475929600101472,1.5475923064213744,1.5475916422459424,1.547590967342635,1.5475902815695186,1.5475895847839938,1.5475888768428445,1.5475881576022936,1.5475874269180612,1.5475866846454223,1.547585930639273,1.547585164754196,1.5475843868445316,1.5475835967644496,1.547582794368029,1.5475819795093362,1.547581152042512,1.5475803118218594,1.5475794587019358,1.5475785925376504,1.547577713184366,1.5475768204980032,1.5475759143351517,1.547574994553186,1.5475740610103823,1.5475731135660469,1.5475721520806438,1.5475711764159308,1.547570186435099,1.5475691820029207,1.5475681629858997,1.5475671292524302,1.5475660806729599,1.5475650171201603,1.547563938469103,1.5475628445974416,1.5475617353856037,1.547560610716984,1.54755947047815,1.5475583145590508,1.5475571428532369,1.5475559552580835,1.5475547516750239,1.5475535320097915,1.5475522961726667,1.547551044078736,1.547549775648155,1.547548490806424,1.5475471894846702,1.5475458716199382,1.547544537155491,1.5475431860411202,1.5475418182334644,1.5475404336963368,1.5475390324010658,1.5475376143268411,1.5475361794610727,1.547534727799759,1.5475332593478657,1.5475317741197159,1.5475302721393878,1.5475287534411286,1.5475272180697737,1.547525666081181,1.5475240975426738,1.5475225125334993,1.5475209111452919,1.547519293482554,1.5475176596631481,1.5475160098187968,1.5475143440955978,1.5475126626545535,1.5475109656721053,1.5475092533406907,1.5475075258693012,1.5475057834840638,1.5475040264288267,1.547502254965763,1.5475004693759848,1.5474986699601698,1.5474968570392027,1.5474950309548268,1.5474931920703134,1.547491340771137,1.5474894774656707,1.5474876025858904,1.5474857165880933,1.5474838199536292,1.5474819131896471,1.5474799968298498,1.5474780714352674,1.5474761375950403,1.5474741959272171,1.5474722470795628,1.5474702917303838,1.547468330589363,1.5474663643984095,1.547464393932519,1.5474624200006508,1.5474604434466128,1.547458465149964,1.5474564860269253,1.5474545070313073,1.5474525291554462,1.5474505534311565,1.547448580930693,1.547446612767727,1.547444650098334,1.5474426941219945,1.547440746082607,1.547438807269512,1.5474368790185316,1.547434962713017,1.5474330597849124,1.5474311717158296,1.5474293000381325,1.5474274463360407,1.5474256122467382,1.547423799461499,1.5474220097268245,1.5474202448455951,1.5474185066782304,1.5474167971438701,1.54741511822156,1.547413471951458,1.5474118604360514,1.547410285841388,1.5474087503983232,1.5474072564037813,1.547405806222032,1.5474044022859827,1.5474030470984885,1.547401743233675,1.5474004933382834,1.5473993001330297,1.547398166413984,1.5473970950539684,1.5473960890039768,1.5473951512946105,1.547394285037541,1.5473934934269906,1.54739277974124,1.5473921473441543,1.5473915996867427,1.5473911403087346,1.5473907728401912,1.54739050100314,1.5473903286132393,1.5473902595814784,1.5473902979159015,1.5473904477233733,1.5473907132113704,1.5473910986898174,1.5473916085729529,1.5473922473812378,1.5473930197433041,1.5473939303979432,1.547394984196139,1.5473961861031447,1.5473975412006062,1.5473990546887344,1.5474007318885246,1.5474025782440295,1.5474045993246837,1.547406800827682,1.5474091885804155,1.5474117685429645,1.5474145468106495,1.5474175296166461,1.5474207233346606,1.547424134481669,1.5474277697207264,1.5474316358638385,1.5474357398749063,1.5474400888727406,1.5474446901341474,1.547449551097091,1.5474546793639268,1.5474600827047191,1.5474657690606275,1.5474717465473828,1.5474780234588372,1.5474846082706004,1.547491509643759,1.5474987364286794,1.5475062976689,1.5475142026051079,1.5475224606792064,1.54753108153847,1.5475400750397923,1.5475494512540233,1.5475592204704014,1.5475693932010766,1.54757998018573,1.5475909923962847,1.5476024410417197,1.5476143375729687,1.5476266936879288,1.5476395213365552,1.5476528327260632,1.5476666403262211,1.5476809568747494,1.547695795382813,1.547711169140619,1.5477270917231092,1.547743576995759,1.5477606391204735,1.5477782925615855,1.5477965520919574,1.5478154327991824,1.5478349500918902,1.547855119706154,1.547875957712002,1.54789748052003,1.5479197048881224,1.5479426479282705,1.5479663271135038,1.5479907602849186,1.5480159656588177,1.5480419618339558,1.548068767798888,1.5480964029394315,1.5481248870462312,1.5481542403224373,1.5481844833914922,1.548215637305029,1.548247723550881,1.5482807640612073,1.54831478122073,1.5483497978750917,1.5483858373393267,1.5484229234064526,1.5484610803561862,1.5485003329637783,1.548540706508975,1.5485822267851064,1.5486249201083044,1.548668813326852,1.5487139338306675,1.5487603095609224,1.5488079690198044,1.5488569412804174,1.5489072559968302,1.5489589434142719,1.54901203437948,1.5490665603512037,1.5491225534108652,1.5491800462733838,1.5492390722981644,1.549299665500257,1.5493618605616908,1.5494256928429806,1.5494911983948176,1.5495584139699476,1.5496273770352331,1.5496981257839104,1.549770699148044,1.5498451368111805,1.5499214792212068,1.5499997676034178,1.5500800439737918,1.550162351152485,1.550246732777539,1.5503332333188145,1.5504218980921411,1.5505127732737,1.5506059059146273,1.5507013439558504,1.5507991362431506,1.5508993325424585,1.5510019835553779,1.551107140934936,1.5512148573015658,1.5513251862593087,1.551438182412241,1.5515539013811162,1.551672399820221,1.5517937354344367,1.5519179669964949,1.5520451543644296,1.5521753584992013,1.5523086414824925,1.5524450665346559,1.5525846980328022,1.5527276015290097,1.552873843768638,1.5530234927087239,1.5531766175364403,1.553333288687587,1.5534935778650925,1.5536575580574925,1.553825303557355,1.5539968899796175,1.554172394279796,1.554351894772028,1.5545354711469033,1.5547232044890371,1.554915177294338,1.5551114734869096,1.5553121784355393,1.555517378969704,1.5557271633950387,1.5559416215081885,1.5561608446109862,1.5563849255238684,1.5566139585984595,1.556848039729234,1.5570872663641766,1.5573317375143443,1.55758155376224,1.5578368172688932,1.5580976317795534,1.5583641026278823,1.5586363367385387,1.558914442628043,1.5591985304038012,1.5594887117611709,1.5597850999784426,1.5600878099096123,1.5603969579748134,1.5607126621482768,1.5610350419436851,1.5613642183967809,1.5617003140450982,1.5620434529046674,1.5623937604435612,1.562751363552137,1.5631163905098335,1.5634889709483826,1.563869235811295,1.5642573173094816,1.5646533488728716,1.5650574650978972,1.5654698016907078,1.5658904954059938,1.5663196839812858,1.5667575060666235,1.5672041011494757,1.5676596094748008,1.5681241719601633,1.5685979301057997,1.5690810258995647,1.569573601716677,1.57007580021421,1.5705877642202701,1.5711096366178279,1.5716415602231744,1.5721836776589904,1.572736131222027,1.5732990627454189,1.5738726134556562,1.574456923824267,1.5750521334142713,1.5756583807214917,1.5762758030108184,1.5769045361475498,1.577544714423941,1.5781964703811222,1.5788599346265617,1.5795352356472656,1.580222499618939,1.5809218502113356,1.5816334083900605,1.5823572922150975,1.5830936166363636,1.5838424932866002,1.5846040302719435,1.5853783319605306,1.5861654987695073,1.5869656269508396,1.587778808376332,1.5886051303222806,1.589444675254202,1.5902975206120964,1.5911637385967108,1.592043395957285,1.5929365537812774,1.5938432672865661,1.594763585616643,1.5956975516393128,1.5966452017494204,1.5976065656761356,1.5985816662953172,1.5995705194474876,1.60057313376194,1.6015895104875013,1.6026196433304618,1.6036635183001884,1.6047211135629047,1.6057923993041372,1.6068773376002925,1.607975882299823,1.6090879789144237,1.6102135645206765,1.6113525676725429,1.6125049083250789,1.6136704977697274,1.6148492385815099,1.6160410245784158,1.6172457407932597,1.6184632634582423,1.619693460002423,1.620936189062278,1.6221913005054815,1.6234586354680212,1.6247380264047095,1.6260292971531358,1.6273322630110485,1.6286467308271355,1.6299724991051265,1.631309358121104,1.6326570900538804,1.6340154691282534,1.635384261770927,1.6367632267788381,1.6381521154996068,1.6395506720237891,1.6409586333885782,1.6423757297925754,1.643801684821214,1.6452362156823996,1.6466790334518988,1.6481298433279863,1.6495883448948352,1.6510542323941153,1.6525271950042455,1.654006917126726,1.6554930786789692,1.6569853553930205,1.6584834191195625,1.6599869381365822,1.6614955774620692,1.6630089991701187,1.6645268627097958,1.666048825226134,1.6675745418826275,1.6691036661845817,1.6706358503027117,1.672170745396352,1.6737080019356783,1.6752472700223389,1.676788199707913,1.678330441309619,1.679873645722719,1.6814174647290816,1.6829615513013794,1.684505559902422,1.686049146779142,1.6875919702507796,1.6891336909908243,1.690673972302306,1.692212480386046,1.6937488846015023,1.6952828577198744,1.6968140761691548,1.6983422202708367,1.6998669744680208,1.701388027544689,1.7029050728359307,1.7044178084289405,1.705925937354637,1.7074291677697624,1.7089272131293627,1.7104197923495628,1.7119066299605825,1.7133874562499554,1.7148620073959404,1.7163300255911413,1.717791259156354,1.7192454626447107,1.7206923969361738,1.7221318293224912,1.7235635335827024,1.7249872900493353,1.7264028856654257,1.7278101140325235,1.7292087754498522,1.730598676944806,1.7319796322949816,1.7333514620419528,1.734713993497,1.7360670607390296,1.7374105046049089,1.7387441726724664,1.7400679192363997,1.7413816052773472,1.7426850984243871,1.7439782729112145,1.7452610095262733,1.7465331955571046,1.7477947247291798,1.749045497139493,1.7502854191851749,1.7515144034874033,1.7527323688108756,1.7539392399791058,1.7551349477858158,1.756319428902675,1.7574926257836483,1.7586544865662048,1.7598049649696346,1.7609440201907192,1.762071616796994,1.7631877246178354,1.764292318633599,1.7653853788630363,1.7664668902491978,1.7675368425440388,1.768595230191926,1.7696420522122431,1.7706773120812846],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.5542768645378222,1.5542768643114775,1.5542768640809204,1.5542768638460729,1.5542768636068547,1.5542768633631847,1.5542768631149801,1.5542768628621564,1.5542768626046277,1.5542768623423064,1.5542768620751035,1.554276861802928,1.5542768615256868,1.554276861243287,1.554276860955631,1.5542768606626223,1.5542768603641606,1.5542768600601442,1.5542768597504706,1.554276859435034,1.554276859113727,1.5542768587864408,1.5542768584530637,1.5542768581134823,1.5542768577675816,1.554276857415244,1.554276857056349,1.5542768566907752,1.5542768563183982,1.5542768559390916,1.5542768555527258,1.5542768551591701,1.5542768547582901,1.55427685434995,1.554276853934011,1.5542768535103317,1.5542768530787674,1.5542768526391721,1.5542768521913957,1.5542768517352867,1.5542768512706897,1.5542768507974467,1.5542768503153965,1.5542768498243758,1.5542768493242172,1.5542768488147511,1.5542768482958038,1.554276847767199,1.5542768472287576,1.5542768466802956,1.5542768461216272,1.5542768455525622,1.554276844972907,1.554276844382465,1.5542768437810348,1.5542768431684124,1.5542768425443898,1.554276841908754,1.5542768412612895,1.554276840601776,1.5542768399299896,1.5542768392457014,1.554276838548679,1.5542768378386855,1.5542768371154796,1.554276836378815,1.554276835628442,1.5542768348641047,1.5542768340855437,1.5542768332924943,1.554276832484687,1.5542768316618465,1.554276830823694,1.554276829969944,1.5542768291003064,1.5542768282144857,1.5542768273121803,1.554276826393084,1.554276825456884,1.554276824503262,1.5542768235318944,1.5542768225424504,1.5542768215345935,1.5542768205079815,1.5542768194622654,1.5542768183970894,1.5542768173120918,1.5542768162069032,1.5542768150811488,1.5542768139344447,1.5542768127664022,1.5542768115766235,1.5542768103647044,1.554276809130233,1.5542768078727898,1.5542768065919472,1.5542768052872695,1.5542768039583137,1.5542768026046274,1.554276801225751,1.5542767998212157,1.5542767983905441,1.5542767969332496,1.554276795448837,1.554276793936802,1.55427679239663,1.554276790827798,1.5542767892297724,1.5542767876020103,1.5542767859439583,1.5542767842550527,1.5542767825347195,1.5542767807823739,1.5542767789974203,1.5542767771792523,1.5542767753272515,1.5542767734407883,1.554276771519222,1.554276769561899,1.5542767675681541,1.55427676553731,1.5542767634686765,1.5542767613615498,1.554276759215214,1.55427675702894,1.5542767548019845,1.5542767525335905,1.5542767502229873,1.554276747869389,1.5542767454719966,1.5542767430299942,1.5542767405425533,1.554276738008827,1.5542767354279552,1.5542767327990605,1.5542767301212492,1.5542767273936113,1.5542767246152207,1.554276721785132,1.554276718902384,1.5542767159659971,1.5542767129749733,1.554276709928296,1.55427670682493,1.554276703663821,1.5542767004438942,1.554276697164056,1.5542766938231916,1.5542766904201655,1.5542766869538216,1.5542766834229824,1.5542766798264476,1.5542766761629951,1.5542766724313806,1.5542766686303355,1.554276664758569,1.5542766608147653,1.5542766567975843,1.5542766527056608,1.554276648537605,1.5542766442920006,1.554276639967405,1.5542766355623496,1.554276631075337,1.554276626504843,1.5542766218493154,1.5542766171071716,1.5542766122768013,1.5542766073565635,1.5542766023447867,1.554276597239768,1.5542765920397739,1.5542765867430373,1.554276581347759,1.554276575852107,1.5542765702542143,1.554276564552179,1.5542765587440646,1.5542765528278986,1.5542765468016713,1.5542765406633356,1.5542765344108072,1.5542765280419624,1.5542765215546384,1.5542765149466318,1.5542765082156982,1.5542765013595523,1.5542764943758653,1.5542764872622659,1.5542764800163387,1.5542764726356226,1.5542764651176117,1.5542764574597536,1.5542764496594477,1.5542764417140456,1.5542764336208494,1.554276425377112,1.5542764169800336,1.5542764084267644,1.5542763997143998,1.5542763908399826,1.5542763818005,1.5542763725928834,1.554276363214008,1.554276353660689,1.5542763439296847,1.554276334017692,1.554276323921346,1.554276313637221,1.5542763031618263,1.554276292491607,1.5542762816229414,1.5542762705521425,1.5542762592754524,1.554276247789045,1.554276236089023,1.5542762241714163,1.5542762120321818,1.5542761996672005,1.554276187072278,1.5542761742431415,1.5542761611754385,1.5542761478647371,1.5542761343065212,1.5542761204961928,1.5542761064290675,1.5542760921003749,1.5542760775052546,1.5542760626387582,1.554276047495844,1.5542760320713764,1.5542760163601266,1.5542760003567666,1.5542759840558715,1.5542759674519149,1.5542759505392676,1.5542759333121967,1.5542759157648631,1.554275897891319,1.554275879685507,1.5542758611412564,1.5542758422522838,1.5542758230121878,1.5542758034144493,1.5542757834524286,1.5542757631193618,1.5542757424083613,1.554275721312411,1.5542756998243656,1.5542756779369467,1.554275655642741,1.5542756329341996,1.5542756098036319,1.5542755862432058,1.554275562244945,1.5542755378007242,1.5542755129022687,1.554275487541151,1.5542754617087864,1.5542754353964328,1.5542754085951858,1.5542753812959764,1.5542753534895675,1.5542753251665522,1.5542752963173494,1.5542752669322004,1.5542752370011665,1.5542752065141257,1.5542751754607682,1.5542751438305946,1.5542751116129112,1.5542750787968271,1.5542750453712495,1.5542750113248818,1.5542749766462183,1.554274941323541,1.5542749053449163,1.5542748686981895,1.5542748313709824,1.5542747933506882,1.554274754624467,1.5542747151792438,1.5542746750017011,1.5542746340782765,1.5542745923951573,1.5542745499382773,1.55427450669331,1.5542744626456657,1.554274417780486,1.5542743720826389,1.554274325536714,1.5542742781270171,1.5542742298375651,1.5542741806520814,1.5542741305539898,1.5542740795264096,1.554274027552149,1.554273974613701,1.554273920693237,1.5542738657726007,1.554273809833302,1.5542737528565118,1.554273694823055,1.5542736357134053,1.5542735755076775,1.554273514185622,1.5542734517266181,1.5542733881096662,1.5542733233133834,1.5542732573159934,1.5542731900953224,1.5542731216287904,1.5542730518934036,1.5542729808657472,1.5542729085219793,1.554272834837821,1.5542727597885502,1.5542726833489926,1.554272605493515,1.554272526196014,1.5542724454299124,1.5542723631681457,1.5542722793831576,1.5542721940468878,1.5542721071307657,1.5542720186056993,1.5542719284420679,1.5542718366097104,1.5542717430779172,1.5542716478154202,1.5542715507903833,1.5542714519703908,1.554271351322439,1.554271248812925,1.5542711444076356,1.554271038071737,1.5542709297697639,1.5542708194656085,1.5542707071225093,1.5542705927030385,1.5542704761690915,1.5542703574818748,1.5542702366018935,1.5542701134889392,1.5542699881020776,1.5542698603996365,1.554269730339191,1.5542695978775531,1.5542694629707554,1.5542693255740403,1.5542691856418447,1.5542690431277861,1.5542688979846495,1.554268750164372,1.5542685996180279,1.5542684462958154,1.5542682901470395,1.5542681311200988,1.5542679691624681,1.5542678042206841,1.5542676362403272,1.5542674651660084,1.5542672909413502,1.5542671135089703,1.5542669328104655,1.5542667487863933,1.5542665613762559,1.5542663705184796,1.5542661761504002,1.554265978208242,1.5542657766271002,1.5542655713409224,1.5542653622824882,1.5542651493833914,1.5542649325740192,1.5542647117835322,1.5542644869398443,1.5542642579696029,1.5542640247981667,1.5542637873495857,1.5542635455465792,1.554263299310515,1.554263048561385,1.5542627932177864,1.554262533196896,1.5542622684144491,1.5542619987847164,1.5542617242204788,1.554261444633005,1.5542611599320286,1.5542608700257206,1.554260574820667,1.5542602742218434,1.5542599681325902,1.5542596564545852,1.554259339087819,1.5542590159305703,1.5542586868793755,1.5542583518290067,1.5542580106724404,1.554257663300833,1.554257309603491,1.5542569494678458,1.5542565827794221,1.5542562094218122,1.554255829276646,1.5542554422235608,1.554255048140175,1.554254646902055,1.5542542383826883,1.5542538224534506,1.5542533989835776,1.5542529678401336,1.5542525288879803,1.5542520819897454,1.5542516270057927,1.5542511637941885,1.5542506922106718,1.5542502121086201,1.5542497233390198,1.5542492257504315,1.5542487191889585,1.5542482034982144,1.5542476785192894,1.554247144090718,1.5542466000484454,1.554246046225794,1.554245482453432,1.5542449085593368,1.5542443243687638,1.5542437297042129,1.554243124385394,1.5542425082291935,1.554241881049642,1.5542412426578787,1.5542405928621201,1.554239931467626,1.5542392582766658,1.5542385730884845,1.5542378756992727,1.5542371659021308,1.5542364434870384,1.5542357082408205,1.5542349599471168,1.55423419838635,1.5542334233356934,1.5542326345690405,1.554231831856975,1.5542310149667398,1.5542301836622086,1.5542293377038552,1.5542284768487278,1.5542276008504183,1.5542267094590365,1.5542258024211855,1.5542248794799327,1.5542239403747882,1.5542229848416793,1.5542220126129287,1.5542210234172318,1.5542200169796376,1.5542189930215267,1.5542179512605956,1.5542168914108383,1.5542158131825308,1.5542147162822173,1.5542136004126974,1.554212465273015,1.5542113105584483,1.5542101359605043,1.55420894116691,1.554207725861611,1.5542064897247698,1.5542052324327638,1.554203953658191,1.5542026530698736,1.5542013303328648,1.5541999851084598,1.5541986170542093,1.554197225823933,1.5541958110677399,1.5541943724320488,1.5541929095596136,1.5541914220895514,1.554189909657373,1.5541883718950193,1.5541868084308992,1.5541852188899326,1.5541836028935965,1.5541819600599776,1.5541802900038253,1.5541785923366136,1.5541768666666034,1.5541751125989134,1.5541733297355937,1.5541715176757056,1.554169676015406,1.5541678043480385,1.5541659022642291,1.5541639693519889,1.5541620051968208,1.5541600093818375,1.5541579814878792,1.5541559210936442,1.554153827775822,1.5541517011092376,1.5541495406669998,1.5541473460206592,1.5541451167403724,1.5541428523950778,1.5541405525526752,1.554138216780217,1.5541358446441076,1.5541334357103127,1.5541309895445743,1.5541285057126424,1.554125983780508,1.5541234233146526,1.5541208238823074,1.5541181850517192,1.5541155063924303,1.5541127874755718,1.5541100278741617,1.5541072271634218,1.5541043849211023,1.5541015007278205,1.5540985741674107,1.5540956048272891,1.5540925922988287,1.5540895361777514,1.5540864360645292,1.5540832915648044,1.5540801022898203,1.554076867856866,1.55407358788974,1.5540702620192246,1.5540668898835768,1.5540634711290358,1.5540600054103446,1.5540564923912892,1.554052931745252,1.5540493231557841,1.5540456663171915,1.5540419609351408,1.554038206727279,1.5540344034238753,1.554030550768474,1.5540266485185703,1.5540226964463018,1.5540186943391587,1.5540146420007097,1.5540105392513492,1.5540063859290631,1.5540021818902094,1.55399792701032,1.5539936211849217,1.5539892643303748,1.5539848563847296,1.5539803973086042,1.5539758870860774,1.5539713257256054,1.5539667132609516,1.55396204975214,1.553957335286424,1.5539525699792756,1.553947753975392,1.5539428874497214,1.5539379706085064,1.5539330036903458,1.553927986967274,1.5539229207458585,1.5539178053683145,1.5539126412136373,1.5539074286987502,1.5539021682796708,1.5538968604526933,1.553891505755585,1.5538861047688017,1.553880658116716,1.5538751664688606,1.5538696305411885,1.5538640510973445,1.5538584289499517,1.553852764961912,1.553847060047718,1.5538413151747785,1.5538355313647556,1.553829709694912,1.5538238512994718,1.5538179573709883,1.553812029161726,1.5538060679850478,1.5538000752168137,1.5537940522967895,1.553788000730056,1.5537819220884375,1.5537758180119265,1.5537696902101215,1.5537635404636678,1.553757370625707,1.5537511826233281,1.5537449784590271,1.5537387602121693,1.553732530040457,1.553726290181402,1.553720042953799,1.5537137907592078,1.5537075360834347,1.5537012814980202,1.5536950296617302,1.5536887833220498,1.5536825453166825,1.553676318575052,1.5536701061198097,1.5536639110683488,1.5536577366343172,1.5536515861291444,1.5536454629635703,1.5536393706491802,1.5536333127999518,1.553627293133808,1.5536213154741798,1.553615383751583,1.5536095020052036,1.5536036743845005,1.5535979051508206,1.5535921986790322,1.553586559459177,1.5535809920981423,1.5535755013213552,1.5535700919745032,1.5535647690252783,1.5535595375651534,1.5535544028111896,1.5535493701078766,1.5535444449290117,1.553539632879617,1.5535349396979028,1.5535303712572737,1.5535259335683849,1.5535216327812547,1.5535174751874279,1.553513467222203,1.5535096154669241,1.5535059266513358,1.5535024076560153,1.5534990655148746,1.553495907417749,1.5534929407130647,1.5534901729105963,1.5534876116843193,1.5534852648753585,1.5534831404950389,1.5534812467280439,1.5534795919356847,1.553478184659287,1.5534770336236978,1.5534761477409207,1.553475536113879,1.5534752080403182,1.553475173016847,1.5534754407431268,1.5534760211262095,1.5534769242850328,1.5534781605550763,1.5534797404931824,1.5534816748825495,1.5534839747378992,1.5534866513108239,1.5534897160953198,1.5534931808335095,1.5534970575215574,1.5535013584157866,1.5535060960389953,1.5535112831869824,1.5535169329352854,1.5535230586461304,1.5535296739756055,1.5535367928810535,1.553544429628694,1.5535525988014744,1.5535613153071548,1.5535705943866283,1.5535804516224816,1.5535909029477994,1.5536019646552086,1.5536136534061764,1.5536259862405508,1.553638980586356,1.55365265426984,1.5536670255257754,1.5536821130080172,1.5536979358003116,1.5537145134273707,1.553731865866194,1.5537500135576552,1.5537689774183425,1.5537887788526543,1.5538094397651543,1.5538309825731778,1.5538534302196922,1.5538768061864106,1.5539011345071516,1.5539264397814487,1.5539527471884014,1.5539800825007695,1.5540084720993022,1.5540379429873015,1.5540685228054159,1.5541002398466564,1.5541331230716318,1.554167202123999,1.5542025073461183,1.5542390697949129,1.5542769212579208,1.5543160942695322,1.5543566221274094,1.554398538909076,1.5544418794886659,1.5544866795538297,1.5545329756227815,1.5545808050614773,1.554630206100921,1.554681217854576,1.5547338803358781,1.5547882344758377,1.5548443221407127,1.5549021861497458,1.5549618702929457,1.555023419348904,1.5550868791026289,1.5551522963633808,1.5552197189824946,1.5552891958711723,1.5553607770182294,1.5554345135077723,1.555510457536798,1.555588662432688,1.5556691826705826,1.555752073890615,1.5558373929149827,1.5559251977648374,1.5560155476769726,1.5561085031202833,1.5562041258119796,1.5563024787335296,1.5564036261463081,1.556507633606926,1.5566145679822196,1.5567244974638699,1.5568374915826302,1.5569536212221375,1.557072958632276,1.5571955774420703,1.5573215526720814,1.557450960746276,1.5575838795033408,1.5577203882074138,1.5578605675582098,1.558004499700498,1.5581522682329156,1.5583039582160783,1.5584596561799617,1.558619450130521,1.5587834295555205,1.5589516854295369,1.5591243102181058,1.5593013978809849,1.5594830438744922,1.5596693451528907,1.5598604001687866,1.5600563088725048,1.5602571727104084,1.5604630946221265,1.560674179036657,1.5608905318673039,1.5611122605054177,1.5613394738129003,1.561572282113434,1.5618107971824005,1.56205513223545,1.5623054019156803,1.5625617222793862,1.5628242107803398,1.5630929862525604,1.5633681688915297,1.5636498802338132,1.5639382431350461,1.5642333817462306,1.5645354214883116,1.5648444890249797,1.5651607122336548,1.565484220174607,1.5658151430581644,1.5661536122099615,1.5664997600341808,1.5668537199747368,1.5672156264743506,1.567585614931473,1.567963821654995,1.568350383816703,1.5687454394014269,1.5691491271548235,1.5695615865287522,1.5699829576241895,1.5704133811316274,1.570852998268911,1.571301950716464,1.5717603805498528,1.572228430169638,1.5727062422284757,1.5731939595554136,1.5736917250773441,1.5741996817375716,1.5747179724114562,1.575246739819092,1.575786126434995,1.576336274394763,1.5768973253986835,1.5774694206122661,1.5780527005636813,1.5786473050380916,1.5792533729688691,1.5798710423256883,1.5805004499995068,1.5811417316844365,1.581795021756525,1.582460453149467,1.5831381572272785,1.5838282636539738,1.5845309002602908,1.5852461929075212,1.5859742653485127,1.5867152390859174,1.5874692332277718,1.5882363643405069,1.5890167462994906,1.5898104901372259,1.5906177038893297,1.5914384924384362,1.5922729573561765,1.5931211967433982,1.5939833050688081,1.5948593730062186,1.5957494872706077,1.5966537304532011,1.5975721808558097,1.5985049123246529,1.5994519940839282,1.6004134905693814,1.6013894612621573,1.602379960523216,1.6033850374286098,1.6044047356059272,1.6054390930722249,1.6064881420737696,1.6075519089279258,1.608630413867534,1.6097236708881255,1.6108316875983337,1.6119544650738602,1.6130919977153622,1.614244273110631,1.6154112719014335,1.6165929676553834,1.6177893267432235,1.619000308221883,1.620225863723679,1.6214659373520326,1.6227204655840497,1.6239893771803262,1.6252725931023246,1.6265700264376484,1.6278815823335502,1.6292071579389806,1.630546642355481,1.6318999165972063,1.633266853560344,1.6346473180021874,1.6360411665300951,1.637448247600548,1.6388684015285082,1.6403014605072406,1.6417472486387545,1.6432055819749924,1.6446762685698597,1.646159108542183,1.6476538941496393,1.6491604098736856,1.650678432515482,1.6522077313027816,1.6537480680077312,1.6552991970754918,1.6568608657635728,1.658432814291733,1.6600147760022854,1.661606477530609,1.6632076389856403,1.6648179741401061,1.6664371906302102,1.668064990164488,1.6697010687414933,1.6713451168759788,1.672996819833197,1.6746558578709323,1.6763219064888515,1.6779946366847478,1.6796737152172285,1.6813588048743837,1.6830495647479606,1.6847456505125518,1.6864467147092954,1.6881524070335792,1.6898623746262222,1.6915762623676147,1.6932937131742771,1.6950143682973136,1.6967378676222118,1.6984638499694626,1.7001919533954581,1.7019218154931395,1.7036530736918707,1.705385365556008,1.707118329081665,1.7088516029911602,1.710584827024659,1.7123176422285298,1.7140496912399428,1.7157806185672624,1.7175100708657942,1.7192376972084642,1.7209631493510305,1.7226860819914362,1.7244061530229453,1.7261230237807068,1.72783635928143,1.7295458284558602,1.7312511043737733,1.732951864461229,1.7346477907098428,1.736338569877857,1.7380238936828234,1.7397034589857119,1.7413769679663114,1.7430441282897795,1.7447046532642494,1.7463582619893931,1.7480046794958959,1.7496436368757848,1.7512748714035982,1.7528981266483914,1.754513152576598,1.7561197056457793,1.7577175488893226,1.7593064519921522,1.7608861913575506,1.7624565501651877,1.7640173184204795,1.7655682929954137,1.7671092776609818,1.7686400831113893,1.7701605269802054,1.7716704338486413,1.7731696352461492,1.7746579696435438,1.7761352824388594,1.7776014259361577,1.779056259317517,1.7804996486084288,1.7819314666368435,1.7833515929861032,1.7847599139420078,1.7861563224342625,1.7875407179725555,1.788913006577519,1.7902731007068242,1.7916209191766665,1.7929563870788867,1.7942794356939875,1.7955900024002884,1.7968880305794699,1.7981734695187543,1.7994462743099582,1.8007064057456657,1.8019538302127467,1.8031885195834598,1.8044104511043617,1.8056196072832429,1.806815975774313,1.807999549261837,1.8091703253424372,1.810328306406253,1.8114734995171557,1.812605916292206,1.8137255727805317,1.8148324893418064,1.815926690524492,1.8170082049440057,1.8180770651609768,1.819133307559728],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","FigRMSEGRidRidge_log = visuRMSEGrid(Ridge(), 'Ridge', alphasridge_log, 'alpha',\n","                                    GridRidge_log)\n","FigRMSEGRidRidge_log.show()\n","if write_data is True:\n","    FigRMSEGRidRidge_log.write_image('./Figures/ConsoGraphRMSERidge_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.3 Modèle Lasso"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      0.1\n","               R²      RMSE       MAE\n","Lasso() -0.104695  2.251772  1.113607\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logLasso=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[20.803373147300025,20.682260054525603,20.548423413997487,20.49147123903428,21.000713248630174,20.628732758530642,21.03958286462613,20.54194499030135,20.599466204367754,20.99362907803233,20.46182814704691,20.80693981626761,20.71599633053716,20.48812234098599,21.676816954797946,21.66933018332629,20.52341334798455,20.49790765273953,77.30171349327227,20.70985570897165,20.730269014261896,20.56492609924143,20.509679962771695,20.52177510682229,20.79344726407697,20.67943902558216,20.48118825984142,21.188549725662863,20.569203617624687,20.559516981844627,20.677582429024287,20.571265499390638,20.99302289040102,21.492359329310812,20.56998337694379,20.657099361886857,20.618709251023937,22.731820333872808,20.688731256992913,20.58410592401018,20.69011784180543,21.255296679462095,20.58043834777598,22.051483389105325,20.635229751832462,20.500174722845628,20.529610687051328,20.428378817664367,20.55463349619486,20.58807863728505,20.75745400658518,20.505418599461503,20.547499038634307,20.6621187832803,20.94495291309338,21.568713949621166,20.490823367346074,20.8369946567472,20.768905931087644,20.54236060610133,21.738581635742914,20.503194049392544,20.742336593471652,20.63406946765119,20.526358811430285,20.663432276276072,20.576213392279183,20.606120959415932,20.623980214820964,20.532232440603533,20.97397115641456,20.56282215054471,20.848675878834978,20.600233333312524,20.77499543814317,20.52368204990455,20.745241077696683,20.559803366148664,20.865312704987605,20.5015226648483,20.69285538269175,21.049223601187716,20.559728389906407,20.8518483112886,20.57879800227339,20.495859652922306,20.636638955287896,20.480647759582016,20.60180588949261,20.599850410211086,20.53986274118956,20.542617016791883,21.250679358842774,20.509934103281175,21.464045307136182,23.13885276312622,20.991416141949014,23.814434207652877,20.8955054442343,20.59232100727345,21.7132410241788,21.925637340982885,20.605876186347686,20.49616525277523,20.833385365943045,20.972489125653308,20.62441582575124,21.18968610934286,20.514330694404997,20.846799765958608,21.478578002068694,20.648228883286013,20.548504153952607,23.01464445097875,21.12941994979151,20.491245299222953,21.252110738898086,21.63001785834437,20.55710698815837,21.253595769071662,25.00074523515177,20.581073702284165,20.626408093652312,20.645036044687604,20.524505510450613,20.820724949163402,20.678625768908667,21.23479204958425,20.58141015530829,20.954442454554027,20.54402306930125,20.538204448101528,21.053484473336045,20.551950329486164,20.510869798256085,20.642364222415313,20.872299017686803,20.573317707455303,20.72254290718019,20.754552541680642,20.483349678156628,20.585529934511644,20.661342898402122,20.509624163484023,20.708646253306785,20.489905901441066,21.357614780615023,20.576636392419672,20.559956752446936,20.55621650343302,21.085049171892226,20.54232393411898,20.780049473338764,20.801859037861462,20.675328063070896,20.54052763074821,20.696077918495213,20.673340606751818,20.78456012716796,20.769636437171144,20.98194459774777,20.5900155539808,20.539344653943235,20.536832406271124,21.580151173772354,23.818807994200426,20.575739410456283,20.571325360161712,20.710015667072504,20.623608296243866,21.04288514877633,20.95663061067828,20.64582184778702,20.819033360925484,20.6443000454385,20.50963564493005,20.727279423594545,20.760308934136212,20.966587610421765,20.591553909632374,23.01464445097875,20.593992819267726,20.514898523759634,20.967016929288775,21.27476503432995,20.66382737171622,20.628728356977728,22.042432121277262,20.572089359794028,20.763887607239845,20.790117105237783,20.539554797835386,20.66345149182496,20.489747655863773,20.645673399403663,20.741085492090622,20.736165035589636,20.57137425613818,20.6734125186183,20.488206424628846,22.19435662347972,20.49028135421197,20.658194587982816,22.813988214513582,20.57476384004231,20.531437880985923,21.004381326423008,20.493292614157724,20.487449544969767,20.673702330559617,20.634661048351045,21.582046824506254,20.821323035438812,20.61226482209415,20.704098260232048,23.676174891688127,20.604624732972827,20.507289380607666,21.078761941228784,20.607392448191863,20.59251217191285,20.951461458426827,21.130010740780662,20.571205214756823,20.59340771234828,21.83849573669363,20.72311768666806,21.73268147170973,20.698081685188644,20.818947102176974,20.50605669634206,20.824915670795768,20.61151174082156,20.634882176299783,21.022069152724495,20.63435838582635,21.269391606691542,20.562041236629803,20.904491066680112,20.5586016422253,20.598537963744374,20.717969919215257,20.599946548329527,20.909741404853275,20.93893730893071,20.517649508807782,22.995034259686697,20.640886291870768,21.018077879736687,20.62224440765634,22.523967236767902,20.594509094170284,20.571427842423834,20.834443847635534,20.613480229640302,20.653305576707798,20.57323858027059,20.517759524754833,20.780545198563235,20.6221154693463,21.883014500702863,26.155508343343755,23.076550031973998,20.63351405067456,20.56649688248547,20.51770377343229,20.905955708244278,21.376712992798076,20.498213064975744,20.65966205364867,20.457568073501115,21.59282667093027,21.79146239694948,20.671152314932808,20.530496926624814,20.779066730119297,20.556564887265356,20.78982558413585,20.550611508944442,20.527178112222032,20.571427842423834,20.52918284725723,21.128481311367924,20.51725193581307,20.550974228352207,20.707696090446383,21.147742303221957,20.84472107667338,21.825459969780987,21.93287290100564,21.108875501459586,20.60737341759702,20.480335180393634,20.57457892213099,20.650087551165253,20.80872426800095,20.506647620916517,20.545489655409394,20.51324330612418,21.012171483999495,20.48571629115727,20.515803099324295,20.523250224754296,20.491922659375224,20.449396251283346,20.528865023410187,20.49958228115266,20.577498041005782,20.516298031153855,20.4922166552916,20.69205563805889,20.67783329926152,20.562004857833355,20.65058388627009,20.966436175519505,20.53986691130145,20.5425561900072,20.582967254912326,20.74029498946521,20.66473371923839,20.834306327072152,21.072855567212354,20.61343120108384,21.014826345909732,20.604934938800675,20.89930881224617,20.741490261281136,21.325167959056635,20.575861568157077,20.501550215369704,20.55598175721739,20.561357605514214,20.887188185356553,20.577051791359303,28.129625418787793,20.55951419198446,21.754225363122117,20.803244502175893,20.756657599557975,21.102068789037713,20.53130923586179,20.690765700795406,20.790624107807737,21.09737000280814,20.57886145253488,20.85691330803973,20.601934541803818,20.501252984893693,20.56873707206864,20.523111374990417,21.611150866944268,25.825722680892056,20.57422273995336,20.553669971258305,20.488690674937843,20.503774689113104,20.880916933539332,20.798941949432592,20.48942983201673,20.648409867063915,20.68249676784525,20.524820469133456,20.6352366231676,20.70470688559824,21.391640933375502,22.97823157207297,20.545890488112793,21.55739569983668,20.62225663165046,20.581532102063555,20.57825025282903,20.94391836135942,20.598223665314705,20.551475614575168,20.584462043109184,20.948352728419206,20.55951419198446,20.66088909131752,22.466799413362306,20.77442842378567,20.66624393228767,20.78802621587196,21.257536258876023,22.70792994058245,20.73265418131928,20.593992819267726,20.621584605159924,20.534939762114558,20.646848965571873,21.010545602056037,20.56321415369323,20.663462390164458,20.48290221915822,20.94446219421327,20.543105390184756,20.67472048350263,20.594689043293993,20.659368384603958,21.003158073828025,20.51492008986065,21.134071073567654,20.653002136921643,20.65930847394929,20.78102652153048,20.528375477273695,20.669803836302915,21.363848493487424,20.610199744862367,20.560992118122208,20.915723877229958,20.724388102498523,20.551485817709718,20.799443426377294,20.64841925498539,20.53492092689291,20.582504202781767,20.60914317329143,20.764336803433313,21.02126512048005,20.8409598569825,20.54675976589676,20.42148113426443,20.620349688568194,20.79917573918305,21.078768229693544,20.808383096643073,21.18969983409266,20.597318804093934,20.662773089469717,20.679160497009008,20.67192882709704,21.898430114582474,20.91024012213947,20.75466626117975,20.884600111097697,21.050223267297707,22.306931232908344,20.782999983906915,20.532287741762964,20.713001745164384,20.635744796301875,21.622893290632955,20.56175738867373,20.698440567729836,20.81845252405227,20.659378364845896,20.50668122643353,21.549530306179346,20.70556227200053,20.529393690777074,20.758330558504852,20.543784701415966,20.593531727805882,20.507010346669563,20.604138824688512,20.697544504603357,20.58076199043418,20.565811926205452,21.4303755275183,20.737704429186916,20.69299178938775,20.514368850880917,21.006101997170884,20.62141258968457,20.566634275691204,21.14825571202737,21.158109396881486,20.666799858059253,20.66126901340949,20.720610301944543,20.805058246116044,20.574123775256805,23.544817246347364,20.516880292856523,20.69146752996486,20.663951571293737,20.864354887440232,20.582100224604094,20.50079600193505,21.143735152543435,20.61104053067624,20.50210640710203,20.73061667377399,21.128419857967376,20.547494683630497,20.674501982506786,20.556564887265356,21.73508191920689,20.663592067757307,20.60250473459463,21.869113043736107,20.526010134412044,20.969356651418575,20.691322198411868,20.86263154770283,20.749570116751485,20.546589228508118,20.926136599803208,20.52288749028694,20.67751549024255,20.438779712392677,20.687672775068872,20.882419898443924,20.581796425020023,20.60431877773091,20.54542521792774,20.53326566129233,21.591876948567776,21.016598622171202,20.668945065835555,21.398913672269952,20.932734248246025,21.687883905177287,22.067281643759348,21.14193705266461,20.516222594308633,20.489905901441066,20.503213022985936,20.63416099151307,20.51840128444598,21.143343532042106,20.626615239272272,20.48893064517982,20.92233002082348,20.76939560006202,20.75752646654238,21.09022747942838,20.572676111511647,20.575182109546095,20.60279042725897,20.67567577912793,20.507692772413527,20.840580935857243,22.19435662347972,24.09157926331789,20.696062559664995,21.52868346454484,20.68184168848557,21.539845667131445,20.524020650379498,20.589054018699485,27.981731025627404,21.50623597010866,20.688794709092527,20.578836418174838,20.627960283102208,20.4933109501489,20.571986213016807,21.73508191920689,20.537647963183296,20.560769941241627,20.661821375761278,20.51840128444598,21.37065409459193,20.579059789618956,21.035990229722497,20.55710089731149,22.32292433593091,20.566586106402312,20.537360992507452,20.476982720715732,20.626169053595845,20.498025176297833,20.84902866342399,21.216541571624393,20.47030526257695,20.77169661982084,20.51770377343229,20.62805417191929,20.89042769053324,20.566691755947872,22.014395519826078,20.46334065199127,20.77285042036153,20.578836418174838,21.016036034100026,20.5230651001626,20.56843467873893,20.530723079804524,20.61899368747345,23.872964732897458,20.505418599461503,20.690091269182446,21.675967822296045,20.639337244382467,22.14611087934798,20.48048186832304,20.71336322495663,20.549707226565683,20.633319311237106,20.695908556067145,20.79112075422975,20.921277524938414,20.52176899482523,20.519837310568832,20.462612871296503,20.5820670599933,20.557493910818256,20.659378364845896,20.636142795781538,20.624823950457007,20.928647357486476,20.993113983985086,20.67199489806593,20.630111199430246,21.59589805626048,20.501788583254985,20.657635883408314,20.626085960732066,20.589112975576537,22.51790137410957,20.767224109633144,20.867284395578586,20.630686036485994,20.701755670576684,20.522160162636975,20.558855245710397,20.483745674752296,20.500370599937405,20.507888950980515,21.319617029497117,20.835717542547872,22.125404569083752,20.78257487367949,20.65830359773638,20.94495291309338,20.589222419147536,20.64841925498539,21.26202724818256,20.546784812413993,20.48248049136118,21.114529685110536,20.66065801813869,20.556559319677536,20.529863671520044,21.70178609253368,20.608119661044878,21.34720914983219,20.506647620916517,20.598150622065702,21.08545135353051,20.8791846103263,20.94629026770558,20.797566750094422,20.68456568013565,21.077931339909952,20.644077461067806,20.546480986694664,20.64445151656876,20.764999366519152,27.322419426689486,20.509778793422598,20.54236060610133,20.5209927711988,22.74917937410165,20.54031250071491,20.589151655140324,20.550281461103282,20.534646972627556,20.75341174033548,20.56064902713296,21.819946708021956,20.510565117845132,20.627542738573492,20.497410925392206,20.81050643423914,21.238737167930633,21.432126215519407,20.56506715575737,20.630383081727988,20.591043710762037,20.964945250385686,20.585529934511644,22.830268436462674,21.66807462974219,20.545541696748842,20.54822753690571,20.568012071384178,20.570255760676467,20.667405660735927,20.603311382728794,20.697544504603357,28.42541442669359,20.79538652625995,20.558760068074786,20.64891433603568,20.577749033687294,20.8369946567472,20.513462204450878,20.613732867715285,20.613556224468535,20.526912752107325,20.491922659375224,20.84902866342399,20.646023478141643,20.522943753695085,20.785281097319498,20.55214858414087,26.083534785866494,20.556948247051462,22.765067468396648,20.61808213765948,20.658927734443935,20.565479247303777,20.510565117845132,20.51869846776891,20.500801753956587,20.45688717421329,20.791998094363585,21.59874392271603,20.527178112222032,21.64613885274476,20.585914400086637,20.530723079804524,20.7007284612575,23.68831115685477,20.92094322953204,20.975548983996582,24.815746921060597,20.603608635438974,20.511461388036132,20.864495178289115,20.868939358355433,20.649748687605637,24.08429080498007,20.752646980767324,21.340911454270877,20.633319311237106,21.241482250850794,20.516298031153855,20.864449235886383,25.10152531726828,22.120698804693024,20.626169053595845,20.87394575450211,20.963445026465035,20.58187135979318,20.618709251023937,20.568301016611553,20.78465894530357,20.621105801101802,20.535111484404005,20.550471226198002,20.733376742916406,20.82585618878656,20.57854632584003,20.546357558991705,20.5861102810463,21.06595461995019,22.05440150532394,20.53490136380684,20.53189046195416,20.528443295613148,21.32805876517057,20.767752721628806,20.520882755251744,20.706220902719373,20.687278517229913,22.192456751016742,21.244397508284628,20.82984215594388,20.489356488052028,20.53081475047186,20.517863135518926,20.529863671520044,20.94960016231681,20.699451740871524,20.56677969175607,20.926136599803208,20.511884960446665,20.634186713600528,20.65980903476398,20.723514753839623,20.601512723922177,21.14825571202737,20.766019495893442,20.538540314753842,20.57274246392158,20.59410546778182,20.508866275848767,20.67565773632266,20.882577585250324,22.091101026658528,20.571205214756823,21.198875923880326,20.701755670576684,20.56604834947311,20.62225663165046,20.626090689862245,20.6544558946403,20.716293787862643,20.80872426800095,20.45584615096859,22.23068824437619,20.768295979698763,20.5716484164481,20.71177679292088,20.600167581095018,20.581888268442796,20.73573340264987,20.54425496935477,20.8509533294419,20.625740763159726,20.59340771234828,20.63977594715988,20.63705528551527,21.29771102611946,20.587196738462847,20.585552597309267,20.94786335005232,21.031814562987904,20.680584043410157,20.551467481718543,20.566735250370755,20.56165063376991,20.567544335644854,20.53506236499522,20.554473704713626,20.535134320020692,21.461657004156013,20.491703494922504,23.659105732667115,26.783235290206726,20.530112996827036,20.579674697406396,20.545541696748842,21.725776361812333,20.536737687710712,20.530211786741532,20.71200265592007,20.824495915948408,20.594550005217187,20.5818499549793,20.62805417191929,20.578628903460753,20.551283025409433,21.001273854056723,20.64847709903156,20.56823239646419,20.53072307051598,20.851664036059486,23.251993675041692,20.748232160822536,20.943368146739793,20.55639539037485,20.531713214039463,20.73925628607156,20.514330694404997,20.67198383971867,21.33485348914697,20.496264862863686,22.737332724533996,20.705680159060073,20.546784812413993,20.676531458716124,20.78539135876792,20.562994414985027,21.313888725156502,20.707675670600644,20.516716323795727,20.48747499706629,20.84457729592271,21.01778061702817,20.717847414543932,20.504564604805516,20.663139601968822,20.581122891446537,20.74267129459499,21.24254361729918,31.049767897660267,20.927049184889693,20.668363311177874,20.477602745930973,20.62602744064967,20.729509956033574,29.64761149843035,20.923821686960704,20.55558667455009,21.517219362566635,20.62224440765634,20.496870609764837,20.56309191375206,20.726122995441287,20.939214627413133,21.99045003001437,20.548576507109857,26.997228019890926,20.480462380606834,20.66143653646711,20.497020932363426,20.581210768136252,20.49790765273953,22.11714163035098,20.591134342628415,20.901452657284192,21.69612710160612,20.548227109228993,20.654527389747695,21.988579720391122,20.603436713457903,21.01122826980667,20.983383391482,20.768091283550866,29.375053547254964,22.731820333872808,21.507142426846052,20.54422447201828,20.65011380801572,20.709694275198004,20.800472884511265,20.728485711705112,20.78257487367949,21.125877616397013,21.584099239325916,20.517649508807782,20.54031250071491,20.62640115202795,20.610686328032752,20.522204096803822,20.6388829533253,20.98607130085323,20.686628771042724,20.737533486082796,22.368933443620662,20.53986691130145,21.164589657794423,20.868472321156354,20.88264353788576,20.563396817420177,20.58326179723112,20.5425561900072,21.068758116886126,21.323515734603394,21.891442710115644,20.509679962771695,20.573040494310852,20.902433489994362,22.231120994439756,20.514770171821404,20.707675670600644,20.55147970571266,21.36072991969084,20.660932469479132,20.63472569202167,20.644999212264455,20.662107438843893,20.796701040921157,20.878618236273518,20.51030870715458,20.696895605553294,20.61272085972843,20.810141224235863,21.39464241610987,20.520507317821025,20.512124359920538,20.81845252405227,20.596341470936377,20.629935773820257,20.55451125625369,29.449205831061533,20.52390795223685,20.581888268442796,20.53763573918918,20.45688717421329,22.264740029780857,20.637999980858655,20.82761943963122,20.522044034692865,22.216698407575855,20.751830382919522,20.855168823192145,20.48893064517982,20.52987482321272,20.667405660735927,20.694867156705815,21.010072750863447,20.804016723360274,21.077931339909952,23.012340759900184,20.491809652863385,20.53490136380684,20.75840107294335,20.58804740957397,21.05165448169327,20.509525797275604,20.523975132577455,21.357360142733388,20.713347125960393,20.70932931448992,20.63835038327506,20.49958228115266,20.69603788658102,20.494361888352444,20.487321193031537,20.52102944318115,21.127540660918626,20.47797806230791,20.645298510962412,20.704818074288912,20.670369979309314,20.597482238209906,20.958547923231208,22.575220582984745,20.60572949841828,21.942931804637258,20.525219717045516,22.037898761170517,20.547939979858054,20.538366063617566,20.62575457711361,20.545217725584013,20.996862176223605,20.570756944435267,20.54232393411898,21.530754350960216,20.663462390164458,20.693365571683426,20.626090689862245,20.641845582223052,20.851419556177145,20.64373498190812,20.79974203467545,20.55450344870696,20.715058870966963,20.924449745009067,20.490153010406146,20.605400036948932,20.620919514865427,20.95185907179464,20.542018041080148,20.594562462534775,20.850356361874866,21.017623158310766,20.608504638268762,20.90231474205535,22.381553433485536,20.82824119996011,20.74903943042685,20.58418341813935,21.38265943547882,20.566897038433527,25.895424416457562,20.520307934342338,23.076550031973998,20.50148298340206,20.511017405627463,20.642364222415313,20.53375462105701,20.732263493215584,20.970719092566558,20.657814836107317,22.060267906846665,20.96004178087119,22.622125930818495,20.92212872417896,20.531554008930037,20.670052448648175,20.562785551601934,20.54707877464461,20.6388829533253,20.632318726254557,20.695721956736296,22.06994516169381,20.573854211246765,20.53986691130145,20.5899729930087,20.551478826154945,20.641306846924188,21.079213424656306,20.571001424317608,20.57173960479765,22.657765727443675,20.501076565066572,20.810604361728377,24.467753305650394,20.491245299222953,20.88210657372277,21.859671612500392,20.59654179333559,20.548227109228993,20.85691330803973,20.594509094170284,21.09436782559284,20.579674697406396,20.506445925013587,20.527398144116138,20.532629045009077,21.09580932211895,21.60923251876185,20.715571728943893,21.043156396845504,20.72232068793558,20.73265418131928,23.474909394523415,21.786371159848947,20.472218817988974,31.049767897660267,20.66378088870916,20.507808607171736,20.50758490443763,20.75876837913867,20.797566750094422,20.64704026586585,20.805965718672574,20.72906034162954,20.534842849719336,20.48645079884122,20.497669097237537,27.002800634376243,20.782042848367315,21.122980136513576,20.57149649607935,20.549645813409196,20.513995935725323,20.646023478141643,20.510039779284003,22.35398222912581,20.910687884839632,21.157844821892542,20.673915230789166,20.64866569138266,20.723514753839623,20.657099361886857,20.68820967953399,21.09328663998863,20.749139896146502,20.58182637972674,20.78761700525494,20.479398963225318,20.50080648926693,21.83849573669363,20.494435200212532,21.063069327386767,21.482326623119384,20.955445583164327,20.63717284157482,21.376712992798076,20.601862584924273,20.575749566474375,21.12179506186293,21.87198328932155,20.561350874148093,20.67124189395804,24.878031776949832,20.892651381519904,21.017538939256333,20.65221152923432,20.954766132669036,28.453979428992657,21.045320666586033,20.684320398686353,20.706450261434103,20.546986508316923,21.04349567861024,21.22435146127267,29.08897419449525,20.655762306339422,21.29751651768826,20.665453443887884,20.65954417110223,20.52978765011084,21.786371159848947,20.769636437171144,21.94882805603997,20.626908154597903,20.647547946197147,20.575133134329178,20.49677674566699,20.684843774715727,23.79116263614267,20.871455562092727,21.750157398099525,20.662773089469717,20.58907614361406,21.205623950658396,20.556948247051462,21.81914062634842,20.519880387734144,21.006101997170884,20.798344439650375,20.551002969942093,20.68258116306045,21.598250641150333,20.5861102810463,20.821156786929052,20.869692490896934,20.60572949841828,22.147994191331442,20.607876413905565,20.527502048066133,21.307502302310038,20.56175738867373,20.600578547198758,20.688731256992913,20.56180809365407,20.515803099324295,20.596952315587746,21.211840712812112,20.508350872953418,22.091101026658528,20.545422130255844,21.044995391876967,20.53805777523171,20.52208652548637,20.687355805506783,20.52397542576336,20.886820536326066,20.561203630869972,23.72702896806602,20.873882318110866,20.623869905688004,20.53375462105701,20.58498864766082,20.621508699888295,20.9732127351907,20.90343495669455,20.496264862863686,20.82231093026349,20.641995818411374,23.59324065084969,20.601932384169686,21.153273024187502,22.582707240159316,20.59962137926427,23.630735375733362,20.835954564488937,20.538326815297356,20.659634171417338,20.69270263018938,20.79779260079968,20.507254402367153,24.113266923129284,21.200166288603086,20.558751626034216,21.14848703493458,20.52978765011084,20.497354630911893,20.644999212264455,20.66893631156969,22.370183551335106,20.8955054442343,20.513468316447934,20.669996292039983,22.861381983733168,20.62653870615872,20.56608737868255,20.54803224618574,20.974266263718118,21.62961987071949,20.61808213765948,21.174660094936733,22.98915516560009,20.542372066497702,20.516723886497605,20.74457521851286,20.580407494604785,21.418736061014886,20.56309191375206,21.24194928424047,20.551112692703242,20.530496926624814,20.515519473616028,20.645036044687604,20.58418341813935,20.63651098153457,20.537471008454503,22.279886858679927,20.651185006914393,20.825218304344933,20.71019832334079,20.92187313264265,20.782999983906915,21.48878507265825,20.654913618306,20.710799527072716,20.93026564376313,20.640536054867525,20.74328414898251,20.58367799940291,20.562774676276824,20.436307850079725,21.213978840806277,21.8660424439555,20.581073702284165,21.8054261287211,20.482940892429966,20.897932129529153,21.275847140500577,21.880185877326504,20.607482165972428,21.22051645747109,20.980397130827704,20.73466818836507,20.560408619083105,20.53766394207108,20.61314393722209,20.66730234315774,21.76355351340915,20.62002915911454,20.589143940910272,20.820724949163402,20.69599147097868,20.96413296905576,20.755311537335064,20.58807863728505,20.53595464681218,20.544927041250762,20.50916267166523,20.94908899435286],"xaxis":"x","y":[21.224638239972652,20.180643707721647,20.201341431117804,19.1454180237343,21.79470380164697,21.087755789009996,22.432940196049124,19.578846464119135,19.242152996635603,24.328668011629535,21.754493646354923,21.476822454566275,22.377553705906216,19.555124576954142,22.494011033388475,22.606991826298977,18.49440443743341,20.76699650557159,29.702932107840674,20.84994793377778,20.282334104450392,19.302304652102425,20.014253448408297,19.02478155099295,20.401136284262034,21.066918904158555,20.30528553843669,22.443056343683086,18.530754586286314,19.472341179018663,21.511675814584603,20.024996233202344,21.986420084483047,22.32477661567823,19.024668956843474,20.277557571355647,20.011894681671833,23.15521167718674,21.342870607762556,20.008806098312462,21.88562823605827,23.73392435177354,20.358652273917528,22.63391493571178,20.2953770114005,18.603706022480708,19.476619646192038,22.286954060950823,20.827570650601533,19.658607240576167,20.944082894927185,17.889444547032436,20.210011039401028,20.041902410540708,22.116524819871177,23.07232514287102,18.931245369435278,19.243034483473725,22.175009164867692,18.8696149273692,22.748443989592825,20.18471556395119,19.630595419370202,19.596480627252774,22.075827573908075,20.81706640440522,20.033430227416382,19.472993206549905,19.47575297115503,19.712127286089668,22.22369094458836,19.974424393125233,20.76732896720974,19.73170766432226,20.622497836834853,19.074352191519743,22.597191120827183,21.03390016791011,22.014068840206768,21.41124459992273,20.690267441816314,22.238063995762765,22.835153147091482,21.940489607019817,20.06072495330695,19.26508618967162,20.593917077867186,20.807981392523768,19.461766940454588,19.92317059187003,20.905864572180928,19.887154874828134,22.674741834325705,19.54492482512454,21.650322094933124,23.6494594685329,21.673503182786,25.42292365756086,23.598781185238046,18.99960834234589,24.597055201180662,22.731123431811554,19.734159687698988,18.832054600441634,20.397708053035622,21.477527921557282,21.731372277489818,21.51301334156099,18.824250825082274,21.177749972976986,22.50813230656899,20.78875924862035,19.355480148534408,23.890614784737277,22.347688134832723,19.773789560999628,21.94580529932221,22.19000668879013,19.515006779114817,22.99883477706531,24.630356510701596,19.78112594827553,20.45440259621003,20.195062159996276,21.08554669425927,22.350651659007692,19.88287753302219,23.223107004233785,19.56773145357299,22.383567194160783,19.337397595787667,19.84140252838032,24.09542285957458,18.7932762528682,18.78833918294514,13.482051629033913,21.99805786914009,20.68539834867811,20.75326327581395,18.386700720994313,20.922404198298533,19.34599856610255,23.03775857233843,18.64679105166152,20.517600912543127,20.206849822052806,22.025147336797662,19.802893385023037,20.525632635954643,20.1770277231615,23.357834800271604,19.12730750072142,23.147280554074683,21.983261369651657,20.345804248394614,19.067786210059246,20.11323917875491,20.682137237753555,21.173013406383724,22.83375379636568,21.704780057376663,19.98202682053532,19.114539527933296,22.520164246985647,22.815206783859768,26.613491397737352,21.053960341071367,19.740225451250925,20.911400034394557,19.239293618022067,22.519031496965763,20.18180618902739,21.47581257946244,21.841152373314472,19.894016350184707,18.947642644803835,20.33821258053668,21.998370834730604,21.623042657920415,20.52474339677355,23.920588558527324,22.398384643288747,20.614611901935827,20.73652363425427,24.216251851014647,0,23.798062566113977,22.44169398922513,19.04780522125321,22.508853453023672,20.361268080220963,19.223547338390134,20.037360610262493,19.47969541290554,21.419330156983328,19.397345612637686,21.062803097791257,20.802720957968873,22.787613064091445,19.854352512470033,23.127238847948263,21.236410750491928,19.39740450234132,24.629767207576666,19.424905147339715,17.66325927218184,21.589387814254906,19.856012075783497,19.346653316091935,20.250448783340936,19.91174617647474,25.664004852328446,20.437440195258947,20.159778968313912,19.760702082570422,23.894186440194584,19.664395199250066,20.03503448449456,21.85979788510952,21.007452333347548,19.330905939549268,21.816602846893197,22.163197583801455,21.21476614620658,19.61795076264764,22.32608970797742,21.206937139516242,23.650902186871754,19.538495641550785,21.951268793802367,19.712186074557728,21.6397845044252,19.566140232030243,23.528005120775013,21.76194235515012,19.759231530600534,22.69657897049395,20.97108250648827,20.91944021038121,19.11091145461158,20.92382072739302,19.99564295418165,20.157995405754583,21.499214768467855,21.850862595399537,19.613806547176576,23.262476414885043,19.981118924422468,23.526907160221576,20.11872947152901,23.756207113729698,21.05135284285491,20.459271509540073,20.732604891283092,20.22193428509613,21.2413041438754,20.86782723578085,18.85414731140336,21.746512886013605,21.477990920672163,24.227674033652324,25.12798066804062,26.560353492028305,23.633181830866906,21.120826057383237,20.22792847217847,22.242317589489687,21.590387855717424,19.62630225708032,21.126056594579662,22.585399041291762,22.379011027342607,22.883846216197703,20.986097829919892,21.770367489208184,20.58669309831316,19.43536098584635,21.22950036954701,19.51332725702657,19.38984557001998,20.48325363998835,20.246730162293353,21.286321988690805,20.685520474331085,21.05996063251775,20.18838178436931,21.836335494435165,22.094100243073623,22.994379248863297,22.721264543402345,20.729598340663756,16.46658160832902,21.557923127735883,19.880806435323297,19.801676786988352,21.12828261562749,18.90461110336481,19.19842345841963,20.092049623606254,20.0459782414438,18.31795878333722,20.7963375193431,17.813568277801206,19.627334048970262,18.56499419954315,19.372221038245304,20.307717788382128,19.384134222108464,20.087817122473812,19.502286013993587,21.227181698040095,23.312865371844044,19.405079121883418,20.90714373186156,21.80480516491849,18.974285739983618,19.176540848422555,19.19474964169837,20.93275451807793,20.832136714555805,21.170532392724574,22.709608681261756,19.86611073536812,22.055547735078193,19.980960001271505,22.653172763321756,20.76687340459434,20.847191191309296,20.06982734293826,20.638856572715298,19.67156525332246,19.49519726866692,22.064314182636632,20.227612418267608,26.201435233706484,19.068639219627578,22.656078929958284,20.77129521126714,20.391385205003306,22.283464834927894,20.845264152741507,21.027619198526104,21.07443745441354,21.518584006439504,19.724170230709305,20.679280780496406,18.927945745238855,21.825732486030095,20.464091007664432,19.439137307013926,22.977548256996624,23.988556498480502,19.81821879465883,19.050281782029014,19.439732238591404,17.74375624808358,19.990444456103596,0,18.279601513401296,19.78875556523867,19.965832185850637,20.69408662857846,19.405573621751774,22.379934620081865,22.48871602661339,23.642331039751582,20.276556094805027,23.283922162181995,20.195578868662224,20.03811220566055,19.14964293785794,20.90803339180857,21.473710208099476,19.884197354704103,22.164351182655523,22.813153319462682,18.901120542977083,20.544639287700853,22.988276940898057,20.109117749689304,19.622850798933502,21.871633434683428,22.25017305320405,23.60048678836413,21.515067916471864,22.320806056618878,19.870759032200375,20.544449448312626,20.261067504600565,23.43234984844275,22.4595812045188,22.52530214820436,19.382106816183008,23.96470652783692,21.019906267101412,19.811383905702034,19.845505364796534,20.51412216383031,20.373076483151994,19.01511010272945,21.47364877893911,19.957005833266606,19.38761598733965,22.32447174476143,20.79356441567801,22.807056132685855,24.028256571904787,19.874893102286897,19.393396349595932,21.076706013187607,21.881924997624267,19.305427636738685,20.88679803779653,22.116604136681058,20.73926969664862,19.30704929356398,20.730402345071546,21.49837252262123,21.892478522473827,21.156990969891915,19.626240196846272,19.456579392491523,19.83361590003151,18.640025995609424,21.63591845810156,20.866343204364526,21.462764853897205,18.837375101562536,20.833447685328323,20.03253125195289,21.036829264514445,22.96164822427184,21.237452292596537,20.43520123557409,21.156628899315663,22.496434708234126,22.4307529287208,21.544986593746263,21.11234277813463,21.351967832505505,20.39596499860976,22.86476103379213,20.105565995835796,20.48988421076927,21.684940338873297,21.559903574554543,22.73966855399758,21.952844226236508,20.330025112864092,21.040198736231254,20.75190246631361,18.798041102461394,19.29157093266521,19.97055996653021,19.307592868706436,20.89335338152254,19.773921527816817,21.100709172660746,21.699220401665585,21.041684548715036,20.396544813616845,22.752749154816065,24.957468167964976,20.506265808043914,20.374729613814893,21.525022859298808,23.275631354399913,21.75794824506378,19.767052898077655,21.218145625261048,21.536078922759724,18.276769619768288,24.94868149530031,20.41835179368666,20.87359355439842,20.51145154622031,20.700075492290427,21.289957235167595,20.748925773432315,21.959550623213953,22.62198208884591,19.38396579774315,23.31051349089806,21.627932359551377,18.954513256380405,19.964316452607815,19.32031247988204,24.27577420165758,20.498820970198462,20.060863127783303,24.1535698900103,18.16161414446258,19.814472942891484,22.998819199975294,21.020751435153667,21.36177568305088,18.469113141288748,23.192581335464276,19.21725486016369,20.111754471035223,18.939248297157278,20.474687273562058,21.111612649480875,19.87603574815164,19.637857750008234,20.264370436179377,19.93146081401427,23.29863946166844,21.939745630681145,19.743514665635868,22.649298943378753,21.24045362673594,25.608747220451313,23.49090374555705,23.60775201354553,20.678206389277292,19.915728419308504,20.449383439914303,19.824402536620187,20.53678125858074,22.159724468145836,21.034942664057727,19.03923233773122,20.84977321505041,21.781868242254394,20.46100849412487,22.004305689493815,19.142842792258786,19.496508584409785,21.834751391009725,20.402181439081684,19.21550544985804,20.640134807950194,23.051954851262433,27.036279453018476,20.37977707208339,21.775846548503456,21.781910553210967,22.353218452653266,20.165877660805254,19.451496130670126,25.490707221646936,22.59150705240579,19.995956185326282,20.28232915493825,19.579291420064532,20.879430687458026,21.138173662648782,24.394611647917593,19.533857847503334,19.817723799212757,21.171020993541568,20.50991100623185,22.326932218927173,19.527299811101727,21.383177447413818,19.392837666571104,23.017914332247763,20.82549425312562,19.072268708954628,19.10179618438648,23.29660399040893,19.32638298997934,20.650313109616484,22.182752894747274,20.620515848299537,21.682830461889793,20.248785506000544,20.060843672034636,21.235100056704788,19.639114342316898,22.873156355744623,20.616811975583957,21.260660009666424,20.98111639712373,20.640272301462904,20.347463631132083,19.255662542640188,21.590649178257912,18.627786784209636,25.25576312419618,18.13136213470814,20.18507504172347,22.234490018926184,19.867378293238115,22.842407702796528,19.642302060721956,20.934073076796377,19.657897588782742,20.907660872216642,21.21194412605265,19.94161891161789,21.31905629271608,18.792730927848385,19.734888199833236,18.03332620487895,20.049848549450562,20.009665992350836,21.60568082130027,15.704606153972572,19.9492653280832,21.029500902796773,21.639091593296747,19.630944296038688,19.444205825873514,22.72561505846151,17.641741979338622,23.57832742490694,21.06232551474456,20.596453669823063,23.77569135147608,20.739222922576804,20.798718195952173,20.117453663452505,20.134364899685817,19.1796783064222,19.08437502019272,20.74807693982553,18.925763040720064,21.23645513023823,22.042981377352834,21.11735552640613,23.60188649068525,20.85643673732399,20.248587775712384,22.0971519060863,19.917597162486278,22.120758360865494,22.44443523329341,21.796740313790345,20.192956161016543,21.587428114342384,19.88222659302695,20.074407563972667,19.772030191805687,22.686480223069132,21.085461437053635,21.84811725117268,18.71863791145474,19.897316659199046,22.368646863289158,18.2265721237695,21.061479969798718,21.091487685756913,21.905619198419114,22.248853822319084,20.948206085285577,20.95171629836988,20.60386819333271,21.712312461652626,26.34702224299619,19.173408752888843,18.94863556098072,19.669251869992156,24.602654298813697,20.178503166977418,20.318803159054653,19.8813455186455,19.742006737354636,20.848691129006262,19.533504643608662,23.044574887856943,21.2197623270523,21.881578906408237,21.91240193642795,20.941991359524017,22.077006044437276,22.54693130601029,18.995877797816178,20.997149152437558,18.687341507608156,21.335453055117675,19.376016989288264,25.07655356851684,22.59004948235383,19.378047539426834,19.682163069515127,20.52340976581651,19.581570228112746,20.9498708711936,22.82667920636651,20.99544749522815,25.17876365321419,21.627453285628658,19.477956222501692,19.193137896037967,21.893341530106955,19.71728461871742,17.31416721288844,19.147406179743545,19.827554176093653,20.471143313052202,19.29233286786097,20.500085221471384,19.644130235297485,22.034075873229305,20.949040678200554,20.540968959330602,27.231751620760654,19.73015830855728,22.493025220696325,23.089701491122522,21.64924126399572,20.795739259102447,21.118903696326274,20.815053320149143,16.52324032760632,19.730383893455734,21.24843214899758,22.071670864625634,19.282486682508637,23.126218118696794,22.228032143928214,21.93139047566312,22.735567798975932,24.167967887858172,21.655361699836966,22.600876020595507,24.59815969618305,19.718235182480434,19.521220854266264,20.564957928128795,20.971944131576173,19.6146020174933,27.021590188993713,23.73507977629211,21.659326126254854,21.01213158467734,21.766780414954713,20.08203076321153,21.756353934255824,25.471067413053014,24.02283197509071,23.385758678777925,21.23702785682451,21.578861893090256,19.189211661593234,20.07860380095908,19.328382384869762,23.106213073613993,19.95329417355663,21.365528432523575,18.914065908446993,20.846904364921407,21.621710733177533,20.214357069981805,20.48695013292876,19.590051196125845,20.988896575504317,22.71500741252458,20.63081635106876,18.585982112110514,19.30101383466248,22.43563961615873,21.19573925103901,19.253482814272665,21.733085888220945,21.51937854264472,22.935901951961643,23.1601071574333,20.83646802652741,17.151393182854086,20.807011308942396,19.447379908647196,19.74096325611752,20.942433069954838,19.870033345928174,20.91477708386503,23.264639171673256,18.81898933836999,19.40497105732979,20.010315016400824,20.436358643191994,21.24511744232927,21.394403553854325,20.278388484789648,20.16621212904916,20.06466810918053,20.8905482708943,19.02658827934665,19.696262913257698,21.147132242697182,0,21.233800456337935,21.791167162104042,20.18131197591118,20.79775274411387,20.158015290040524,21.01738902857623,21.1949084635434,22.774677489141574,21.93032100053686,18.418089755206072,23.810210740127232,20.9793536678564,18.664791000361163,20.57237479635235,19.66651702047476,20.204034803826428,19.989961267682528,19.13829898996108,20.67820982860559,19.602487517256634,19.634034627117707,20.049508251055094,19.732903567032626,22.301656666857568,19.3351286191496,19.66019789348857,22.54105973810428,21.087085320291497,21.46282774650559,18.953344800503015,19.199864465913354,18.832769512656196,21.40009037173508,19.692718291075924,20.626400634350908,18.73677516240749,22.077517747249683,18.74408127304125,24.761947476774708,26.113208352362694,19.576225059708314,22.48651099143561,19.29251486362711,21.199347342918216,22.53052802544868,19.222080263625543,20.87727804782974,21.285530909063,20.6451326442611,19.40981146751902,20.008240359915625,19.734510138857544,19.369940873414674,21.839004592901738,19.643216988179674,20.34827538264879,20.295760247970698,21.15453445100085,23.946209069915817,20.161508356002734,21.461336053955172,18.855956110093064,20.13782559566735,20.22901210527111,18.772971741326142,20.346151863763343,0,18.758138614258336,23.01986369036341,19.826388660339887,21.76883742068305,20.30266681112443,20.98525964839946,20.015097338364022,20.523507399137607,20.436904665975636,19.162579413291002,20.375536910022955,23.70344643795525,20.910669810022966,20.379640393188353,19.217287998251134,20.77164874576569,21.098649782686852,21.445335194484834,21.911569915872246,28.094893882984294,22.184719195487645,19.636646744775323,20.26686372980413,20.815307943684378,20.695469281090816,25.87586915107988,20.22604051998145,19.449746389320087,22.323663662536706,20.105603080001295,19.720177445507503,21.133857610241805,20.04287236504247,21.180586044654405,23.433073191606535,19.844702412663505,25.31201588491192,18.563017473748218,20.726116689654802,20.55471999163649,19.12308826699376,20.688277632644148,23.08698185712261,19.4188490987079,19.38789150561276,23.032593297237835,19.840625811644227,21.990665865987562,22.00972851632279,22.544699771649725,22.804072710576072,21.602864772837197,22.7909106273511,25.661642339745672,23.190044679879648,21.797480754192584,17.862153020413583,20.62223350550368,20.658934051201378,20.583030645279486,23.466890235880964,20.879510709815698,21.526914680071894,22.700127681411185,19.683189962828273,20.26341148584,19.616726513482636,19.839377869097518,19.875302398958702,20.973185759004682,21.082163500516533,20.324838680602316,20.30745292915028,25.9613964584893,19.046718202293015,21.78315042330069,20.91962424027046,21.618102712310883,18.472822451162664,19.554830163405484,19.175778559517227,21.854300382519792,24.477823287446007,25.5492833010917,20.6322831501737,18.924421094438486,22.00276008739277,23.251480243552532,19.437442862246026,20.440974576504654,19.114316794022223,23.060566976295764,21.685850226111516,19.690963589379074,19.932730372590424,21.5156696442052,21.03832189892855,21.34321011850724,20.297903654101066,20.251155291034372,22.02017509916011,21.00569855470944,23.695624962968736,19.83550351971754,20.046496736817677,21.663202256858984,21.64914607700524,21.763695119906945,18.966254774482525,25.621581882833034,21.318917047123914,20.25569596256737,19.082528830592434,19.62869805766532,25.538294718669484,19.064589338432878,24.544735671741783,19.58243213803125,23.66501619396984,19.12391425698277,21.767558412000408,18.99637745675535,21.23800756191085,21.00116832090369,22.714272038785044,21.137414277628565,21.45606258158953,22.279207015403138,23.33275256138136,22.26485604208308,20.723080111555596,20.639447033358472,20.694537918678005,20.730065203035867,18.59276109193853,18.41344612635984,22.526317266984485,19.84998714954673,20.751047310028625,19.801872521805933,20.233535275947663,20.632626738131346,21.279464358226686,22.469580214601123,20.408744363117133,22.583074271938578,21.34410908350179,21.979041347822633,20.59404343806881,20.264946502479802,19.878697565839243,22.615047199531546,23.56945490190484,19.736798486144902,23.729801820804443,20.49274951238928,23.323658577961712,20.770035525423065,19.238103804361465,19.567540218990334,20.344212591822245,22.493051859930933,19.27583351001347,18.98927807008273,23.394529944648035,22.51205924599309,21.071056073380674,21.047075972047644,20.445539922646542,21.340147837597844,20.003307902740296,21.299913824117656,20.644505230109406,20.419105854581318,21.593824209166353,19.476347089125902,19.838668436714478,19.728765556885957,22.07230878544129,19.427802084161115,21.756421739524402,21.380030171433972,21.32600862652118,19.632965031850407,21.403421935393933,24.37658838285707,21.043820980181792,20.1938730272849,19.66793597026211,19.199993380834425,20.035891808657503,25.0888235331049,18.385719188798834,26.48889669142553,20.116390923202314,19.499674996520806,19.751886432327307,20.522012502780594,0,20.782571363318098,21.618914296187317,22.967539915594646,20.95701983082078,23.548104709979565,21.65670940491333,18.258239797953646,17.88154175390193,21.233312475041235,20.277059332832177,20.919229965693855,19.653189714082398,19.02398982442439,24.205441209659515,20.16062238071552,18.963476628745678,17.96562762596234,19.531857117715408,20.86635829845881,21.255773747330974,19.280122810790118,19.369392868205782,22.9021647605954,18.93136369208174,21.47821316674072,24.42926325370543,19.880057005564264,21.11943827672747,24.0347829295736,19.174320595643714,19.810878030088894,20.711569411286565,20.79181709109951,21.52825545142037,22.510563057499144,19.714589625374927,19.195910426040165,21.643407148870626,22.79188245530678,23.088899981103793,20.914444260183114,22.409894316390734,22.55442514417979,21.497350091305258,23.661739831337005,22.81337882106461,20.335424976569172,28.119486884464127,20.443137811296648,19.395826969085725,18.744741571834194,21.52611831315801,21.048400728737306,20.235023779555554,21.0819625214105,21.53807712673746,19.283653604184654,20.890428315584586,19.706229562174084,25.296616627387504,21.809197399053033,21.371449507411626,20.19656265688086,19.305685493406397,19.08654493081137,19.639706353282477,17.583516204502104,23.250222506325386,22.82294225806205,24.601371538637977,20.0953223269386,21.243839329097405,20.37329318617189,20.45930590947763,24.987649624596244,22.400315304318365,22.625652665035147,20.455200776646638,21.021720462702472,20.59279530263181,20.232040256335257,22.264357262987502,20.592440384261195,22.36255454353493,21.99517470550352,22.199722361671988,19.391954018620176,21.83585207290849,19.14268047046844,20.53559540130039,21.167744601954155,22.926085660752005,18.718526187000297,22.28042360433217,25.95498550595432,23.32578333060166,21.309621092817217,19.68447811713372,23.268620237666717,25.42582021905868,21.189524929277866,19.937692777054252,20.801380865239704,19.316707310363867,21.75315297668226,22.868455030888445,25.947611368302585,20.033901511817852,23.20532613123076,20.554742003246933,22.96074119343453,18.988728193664233,22.68524113143557,22.64902697475969,25.92407619036082,19.407373582124833,21.346588936497685,19.366735480432887,19.124759964560997,20.07378037525111,23.985931291943867,21.460510640680972,23.23197611805285,20.73330584327736,19.12586494836624,25.275464586661446,19.715668478243582,25.489298123537854,19.013002282515146,25.312874128977917,20.415195026786527,19.91413691399634,22.130140513245646,22.92736601506673,19.584806561357016,23.12482431901705,20.687407722542194,19.611919934246885,24.151034118830534,20.079932477068777,18.93225512877545,24.075045174066695,19.899787637164348,20.874095085342866,21.48054296008632,23.07558442006914,20.640568320462897,20.08863945104873,22.28815948936714,19.616841359021425,23.953590715604236,21.784577239189012,23.301772335078063,19.749416399115965,19.811951028236585,20.17477942437346,18.304477915148002,21.513176875338157,19.58793862702669,24.19084293977443,20.88004158687887,21.10029756186111,19.35190112925433,18.917516857772046,19.84423117509742,24.377548873947223,20.529649424214696,18.766505656199122,21.439000175436664,18.811308834423215,23.933428716720645,19.631145075685264,22.477074905595526,24.176989311672042,19.985865383441666,24.300186984861302,20.78628428680642,19.166666955002903,20.396533708238668,21.16161998720379,23.822353472570573,20.556259058220718,24.241029788998915,22.067946023704817,19.778390202711652,22.99407646829777,18.833019773765052,20.288010749874967,19.947775322749504,20.338620784352592,23.494951508290605,22.38615246636496,19.034944006994706,19.984953818018486,24.081396558763124,19.595170049801663,19.41162451128841,20.72798026434645,21.785986387464803,22.944212504137,23.12678795739032,21.831113335529626,24.147006515510686,20.371960850528456,19.32549212260673,20.504720566168093,19.431273145079818,20.869899532948505,21.94524199207118,21.941322673977073,19.72760807429288,21.84503668728796,18.979888467927786,20.204391739449186,19.721950276191546,19.579038969018928,19.030933465764726,23.8143209659356,20.855181011615457,20.74517719106974,20.090817980596317,20.872940195293037,21.393924057871644,22.20317352529318,19.736694248820182,23.68012909153893,20.722700819009958,21.714666548374087,21.05462253329071,19.397806220065235,19.129172314836154,19.83710046840733,21.397724240612124,22.701728218692,19.775576525065844,24.10746972295679,17.80002145055656,21.28200527852275,24.425354799557958,24.25287711008343,21.61681534013004,21.629837063073623,22.53034703709958,21.62524006847279,20.878967106147208,18.668189678924886,19.500769926583363,19.995538068644624,23.512127623334848,21.53372289651904,20.823420888524133,22.265101932257092,20.999782941566835,21.517654972490075,19.604530267831706,19.76742293183983,20.740147099081785,22.94133404816206,19.387966817051925,20.39789928981696],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso_log = np.linspace(0.1, 1, 5)\n","param_gridLasso_log = {'lasso__alpha': alphaslasso_log}\n","\n","GridLasso_log, \\\n","BestParametresLasso_log, \\\n","ScoresLasso_log, \\\n","SiteEnergyUse_pred_logLasso_log, \\\n","figLasso_log = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=SiteEnergyUse_train_log,\n","                            y_test=SiteEnergyUse_test_log,\n","                            y_test_name='SiteEnergyUse_test_log',\n","                            y_pred_name='SiteEnergyUse_pred_logLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso_log)\n","\n","print(BestParametresLasso_log)\n","print(ScoresLasso_log)\n","figLasso_log.show()\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.7199522436035095,1.7551798673636747,1.8244893065953232,1.9252839501068066,1.9957058376246597]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.8718322259105722,1.9013304422429655,1.9666822070954917,2.064017739936407,2.127740770610214]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.5680722612964468,1.6090292924843839,1.6822964060951546,1.786550160277206,1.8636709046391056]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.7114137229380093,1.7580256366150755,1.8327893902766323,1.930045614813185,1.9963892822155573],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.9397461772123339,1.9900749127288038,2.0645801530258527,2.162102678763376,2.2183278622757827],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.835697833948096,1.8230492238910663,1.8626544406490304,1.9557461478963312,2.036216275588467],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.5496144990317977,1.5899021309253851,1.6603806189534014,1.7561114304620293,1.8396672742616496],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.5632889848873117,1.6148474326580415,1.7020419300716996,1.8224138785991115,1.8879284937818408],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","FigRMSEGRidLasso_log = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso_log, 'alpha',\n","                                    GridLasso_log, None, None)\n","FigRMSEGRidLasso_log.show()\n","if write_data is True:\n","    FigRMSEGRidLasso_log.write_image('./Figures/ConsoGraphRMSELasso_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.488e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.221e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.004e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.776e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.474e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.224e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.008e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.477e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.491e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.227e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.481e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.779e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.783e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.016e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.012e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.231e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.234e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.494e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.786e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.497e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.790e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.026e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.238e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.484e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.505e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.021e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.794e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.496e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.492e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.501e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.242e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.246e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.488e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.798e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.802e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.250e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.036e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.030e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.806e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.513e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.046e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.041e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.500e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.254e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.509e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.521e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.517e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.811e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.504e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.508e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.513e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.259e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.264e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.815e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.268e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.820e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.052e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.070e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.526e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.058e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.825e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.064e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.531e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.534e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.541e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.274e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.279e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.536e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.518e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.830e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.528e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.836e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.523e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.077e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.284e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.290e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.546e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.842e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.098e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.848e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.084e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.563e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.091e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.296e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.552e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.302e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.557e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.557e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.539e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.551e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.860e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.854e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.545e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.309e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.106e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.867e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.570e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.315e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.114e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.874e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.131e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.576e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.585e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.122e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.590e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.322e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.330e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.564e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.583e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.578e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.881e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.888e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.140e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.337e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.597e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.345e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.896e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.168e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.149e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.904e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.621e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.158e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.617e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.605e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.609e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.353e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.613e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.361e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.593e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.912e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.370e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.601e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.921e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.930e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.379e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.178e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.188e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.210e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.629e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.939e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.657e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.638e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.199e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.388e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.654e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.626e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.398e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.645e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.647e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.948e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.958e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.635e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.408e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.418e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.222e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.968e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.979e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.666e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.258e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.234e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.429e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.246e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.698e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.697e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.990e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.440e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.687e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.677e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.686e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.001e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.675e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.451e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.665e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.012e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.271e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.463e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.285e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.313e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.709e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.024e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.721e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.745e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.475e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.299e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.708e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.036e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.733e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.745e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.733e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.488e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.720e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.501e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.049e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.514e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.062e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.327e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.076e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.342e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.373e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.528e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.758e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.358e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.798e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.771e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.089e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.542e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.784e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.800e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.785e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.758e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.772e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.104e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.557e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.390e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.572e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.118e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.813e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.406e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.441e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.133e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.587e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.827e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.861e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.148e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.423e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.858e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.603e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.814e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.843e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.845e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.164e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.619e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.829e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.459e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.180e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.636e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.874e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.515e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.197e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.653e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.477e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.925e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.214e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.496e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.928e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.891e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.911e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.671e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.908e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.877e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.231e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.689e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.893e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.707e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.534e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.249e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.268e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.595e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.943e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.574e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.554e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.726e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.745e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.999e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.980e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.002e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.286e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.961e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.305e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.964e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.983e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.946e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.616e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.765e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.785e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.019e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.324e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.344e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.682e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.638e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.660e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.083e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.826e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.080e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.806e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.039e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.059e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.385e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.062e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.022e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.042e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.364e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.704e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.848e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.869e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.101e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.406e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.427e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.727e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.774e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.891e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.169e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.751e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.913e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.167e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.123e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.448e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.147e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.145e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.470e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.104e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.798e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.936e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.125e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.959e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.492e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.190e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.871e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.982e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.514e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.822e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.260e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.847e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.260e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.537e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.237e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.213e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.236e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.006e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.191e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.214e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.560e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.029e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.053e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.896e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.972e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.583e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.606e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.078e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.357e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.284e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.630e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.922e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.947e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.331e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.355e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.102e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.308e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.332e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.307e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.283e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.654e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.998e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.127e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.151e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.382e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.702e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.678e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.076e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.176e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.050e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.726e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.432e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.452e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.458e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.024e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.403e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.226e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.428e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.407e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.201e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.774e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.128e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.379e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.251e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.750e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.509e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.799e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.102e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.477e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.180e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.276e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.154e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.483e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.560e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.534e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.551e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.823e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.301e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.501e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.526e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.327e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.847e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.871e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.352e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.206e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.232e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.896e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.586e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.284e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.611e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.258e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.401e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.649e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.377e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.663e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.575e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.944e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.637e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.624e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.920e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.426e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.309e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.600e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.968e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.688e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.451e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.335e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.992e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.385e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.713e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.475e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.360e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.764e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.745e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.500e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.673e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.739e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.015e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.721e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.039e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.524e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.547e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.062e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.410e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.085e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.434e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.483e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.813e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.789e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.459e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.862e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.571e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.838e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.837e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.768e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.594e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.814e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.791e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.108e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.130e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.640e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.174e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.152e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.576e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.506e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.553e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.955e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.886e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.530e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.932e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.196e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.684e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.903e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.925e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.909e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.882e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.217e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.860e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.705e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.598e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.238e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.727e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.978e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.620e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.258e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.043e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.748e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.007e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.000e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.768e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.641e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.987e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.946e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.298e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.278e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.788e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.022e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.683e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.808e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.317e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.967e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.064e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.336e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.703e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.743e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.723e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.085e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.827e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.125e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.082e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.026e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.105e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.355e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.845e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.064e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.045e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.864e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.373e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.391e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.762e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.882e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.780e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.408e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.816e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.144e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.163e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.199e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.100e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.899e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.151e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.181e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.135e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.916e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.933e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.118e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.425e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.441e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.949e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.457e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.833e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.850e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.473e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.964e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.882e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.217e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.234e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.866e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.167e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.266e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.250e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.213e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.198e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.980e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.183e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.994e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.517e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.009e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.912e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.940e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.531e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.897e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.297e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.268e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.327e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.544e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.282e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.926e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.036e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.227e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.255e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.312e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.557e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.049e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.241e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.570e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.954e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.062e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.340e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.967e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.582e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.992e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.316e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.980e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.074e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.354e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.086e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.594e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.367e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.380e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.097e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.605e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.281e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.305e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.004e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.617e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.293e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.392e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.037e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.108e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.015e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.426e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.404e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.129e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.119e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.359e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.349e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.328e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.638e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.648e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.026e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.139e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.415e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.657e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.047e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.149e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.338e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.057e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.076e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.437e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.667e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.158e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.467e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.067e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.447e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.395e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.675e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.387e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.368e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.457e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.167e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.176e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.378e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.684e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.184e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.692e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.085e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.700e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.094e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.110e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.476e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.102e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.485e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.192e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.493e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.502e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.427e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.708e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.404e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.200e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.412e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.420e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.716e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.214e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.723e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.118e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.730e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.139e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.125e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.510e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.221e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.133e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.517e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.532e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.454e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.736e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.525e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.434e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.448e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.234e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.743e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.441e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.749e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.240e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.146e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.755e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.538e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.152e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.760e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.557e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.545e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.158e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.472e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.551e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.477e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.460e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.257e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.766e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.771e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.262e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.170e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.175e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.776e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.185e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.569e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.180e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.267e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.579e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.483e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.497e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.574e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.272e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.492e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.781e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.276e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.488e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.785e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.190e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.790e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.281e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.203e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.584e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.195e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.285e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.598e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.794e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.514e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.589e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.199e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.289e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.510e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.593e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.501e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.802e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.293e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.506e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.296e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.806e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.809e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.602e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.218e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.211e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.300e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.613e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.215e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.528e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.606e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.813e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.517e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.610e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.524e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.303e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.521e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.816e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.306e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.309e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.819e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.222e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.822e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.312e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.225e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.626e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.617e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.315e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.825e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.539e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.620e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.537e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.531e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.828e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.623e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.318e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.234e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.534e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.830e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.321e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.629e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.236e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.833e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.632e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.241e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.239e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.542e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.549e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.323e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.637e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.635e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.547e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.326e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.544e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.835e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.328e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.330e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.837e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.840e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.842e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.250e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.248e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.647e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.642e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.332e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.558e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.556e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.644e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.334e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.551e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.844e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.554e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.846e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.336e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.338e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.252e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.848e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.254e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.258e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.849e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.340e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.651e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.649e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.654e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.559e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.256e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.851e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.564e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.563e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.341e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.653e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.343e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.853e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.561e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.344e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.854e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.260e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.264e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.856e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.261e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.656e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.263e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.661e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.346e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.658e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.570e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.347e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.659e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.857e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.569e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.566e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.568e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.858e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.349e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.266e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.350e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.860e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.662e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.861e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.269e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.267e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.666e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.268e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.351e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.575e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.664e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.352e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.665e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.862e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.863e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.572e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.574e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.353e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.573e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.271e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.864e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.668e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.354e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.272e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.274e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.355e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.865e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.671e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.669e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.866e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.356e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.579e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.273e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.578e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.867e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.576e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.357e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.670e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.275e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.358e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.868e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.577e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.672e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.869e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.276e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.277e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.277e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.583e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.673e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.359e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.674e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.360e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.870e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.580e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.581e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.582e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.871e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.361e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.361e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.278e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.872e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.872e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.280e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.675e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.362e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.279e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.678e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.280e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.873e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.585e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.363e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.585e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.676e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.677e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.363e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.874e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.584e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.583e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.874e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.281e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.364e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.678e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.875e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.282e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.283e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.679e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.282e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.365e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.588e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.680e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.365e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.586e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.680e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.875e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.587e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.366e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.876e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.587e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.284e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.366e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.876e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.681e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.285e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.284e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.877e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.590e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.683e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.367e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.682e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.285e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.877e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.589e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.588e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.367e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.682e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.368e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.878e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.368e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.589e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.878e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.286e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.879e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.286e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.683e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.287e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.369e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.286e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.684e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.684e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.879e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.591e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.684e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.369e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.590e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.591e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.879e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.591e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.369e+03, tolerance: 1.675e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.287e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.880e+03, tolerance: 1.578e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.685e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.288e+03, tolerance: 1.659e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.685e+03, tolerance: 1.739e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.592e+03, tolerance: 1.720e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha      0.276829\n","1  elasticnet__l1_ratio      0.000000\n","                    R²      RMSE       MAE\n","ElasticNet() -0.139985  2.287456  1.111688\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning:\n","\n","Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.894e+03, tolerance: 2.093e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logEN=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[20.787348160339967,20.69318236331025,20.54983070832989,20.431046096793583,21.038088253513752,20.616026106776697,21.03012393764684,20.52227527213293,20.58181308398315,20.933466285512463,20.443257088352542,20.79221686980519,20.739361569534495,20.427167679541913,21.776731369496094,21.802651646044062,20.46259674592851,20.46998466793295,79.38506765918996,20.714707496941056,20.705140978151174,20.54368503934118,20.499870503538624,20.488748983388966,20.741845457495263,20.642570789881802,20.424409092494326,21.173406348788262,20.503463495558098,20.53864577232541,20.648638451357815,20.55900606980998,20.957246084205696,21.54577946368755,20.59996473282005,20.612044655794346,20.615984670566508,22.6726626408236,20.72604650059193,20.602205869718972,20.681836727795034,21.331619171059945,20.59439213772534,22.0394497426579,20.628053838678433,20.475409982233593,20.53230430847164,20.372849202262913,20.57035170231224,20.616245548071184,20.759304286099127,20.480608478350643,20.51482793946442,20.655752115140512,20.992924999984194,21.53267920782186,20.46669802908768,20.789199729923073,20.808974301959374,20.55891797556911,21.77962646460718,20.478222861223188,20.74979747238092,20.612961126060252,20.478283747090064,20.591249235198962,20.512749883359458,20.58206318587938,20.598701308161967,20.520011270238207,20.966320690973646,20.534556797851458,20.829973362341974,20.594756350908696,20.77564631585649,20.49052553854029,20.669138544641243,20.530961495234582,20.889914729860422,20.491680322427623,20.728166507700262,21.034411174162294,20.54477127628361,20.883310454910795,20.570148003488,20.471389956795022,20.566306394453683,20.43787004937319,20.57804316044081,20.57690222257542,20.477350181069184,20.508165820139006,21.18933411758624,20.485225487587762,21.54906508705644,23.37940222561633,21.002511329954203,23.66042529297493,20.890989937429435,20.585448380604348,21.79930599951711,21.87211923466606,20.56709962083194,20.43541915562761,20.892809565487358,20.96760536014337,20.550351732957907,21.20559391493039,20.481813585394377,20.835756547822665,21.445487065191223,20.569594631056678,20.553559386185846,23.218697069326737,21.140266319253776,20.4999033946286,21.323225797165495,21.702253323671442,20.57910191506934,21.226421121790107,25.220068582451454,20.54399301569069,20.556195391309885,20.654572943476413,20.492664698938814,20.83553577651185,20.656216178594267,21.24870963322038,20.595297497562072,20.981197207307694,20.560466767239493,20.518790490874565,21.096741375732954,20.56785199810894,20.52217927328478,20.60787722274764,20.859512278884587,20.537533670465685,20.706583946306257,20.751019652922558,20.441163575383953,20.613871113782395,20.625563861906862,20.4626933368198,20.695857965394158,20.485530378000206,21.4341101413885,20.576114614030235,20.56057545054318,20.57182647085132,21.170947595589155,20.522628305528382,20.714628217173306,20.764187863107132,20.655954042319816,20.543262096962348,20.69534036182385,20.690302522192148,20.755085958915213,20.689877536738344,21.00078245322734,20.55232346500603,20.505151968332036,20.51015683530434,21.64875022597403,23.58282987429038,20.530551359257974,20.58590225397334,20.744769049314222,20.628518925000904,21.069425554794776,20.9530083553836,20.649348958850528,20.80199344400464,20.603097603763157,20.478405695671196,20.726522089060115,20.755688775102268,20.999132926343528,20.54210678522529,23.182441563808155,20.57985370253259,20.489127037873136,20.962489347053836,21.258334874208,20.649752104496507,20.55891820907969,22.061161364123393,20.550358509332693,20.70004493986484,20.81920509028557,20.505469567265703,20.67172922767813,20.465695869771547,20.618917476741654,20.678328412398674,20.729500524783003,20.549692301150944,20.63712493773141,20.480874578113617,22.348143109300555,20.430433634410974,20.60110520418635,22.965862392985752,20.55409654351924,20.519271038925154,21.121967615945934,20.468998440245162,20.427299387532138,20.700049591382687,20.61299959549506,21.548971112663892,20.85685620506207,20.62803359222269,20.66781599945222,24.046838854951343,20.61909222982162,20.46051819557683,21.136731688618003,20.649294672945707,20.60859119067952,20.986377889043077,21.244678226265503,20.572902506878968,20.555483683304054,21.75964494372905,20.735266804382196,21.699382482608364,20.699311632604644,20.7950727916902,20.513879938453222,20.741151235899494,20.587085370634085,20.570417998448598,21.123710502225357,20.644625373446406,21.438631497146332,20.57725293578469,20.983529243780616,20.553491550126058,20.589045231475154,20.69304888079449,20.56881093793415,20.945385563075465,20.997397289577886,20.484905474648123,23.083938464930323,20.629186954435394,20.98595238069244,20.633339692965887,22.722568011490825,20.541386209845847,20.571797410611055,20.799721896175004,20.577523395580837,20.626021537700584,20.539546950851246,20.485007968214546,20.7466884777452,20.60374851400538,21.94550182519656,26.22440189167066,23.20265906444612,20.59963218473765,20.545148419706216,20.521720124916634,20.94554179529393,21.4975176143119,20.452062476346978,20.631943388204995,20.43923443339893,21.682517326215944,21.93541427240241,20.627912470242617,20.533129951090046,20.79378709316905,20.572151033811657,20.793274466837833,20.515613685965707,20.5300380618363,20.53554190509247,20.495650216861414,21.169338221464464,20.46979955742509,20.486468020587335,20.662136372652256,21.217459799121123,20.85958087381828,21.68799718514108,21.881934648869596,21.086697207687685,20.600878029996593,20.4569269757554,20.513168833902444,20.704669205171776,20.860246728877765,20.496175821957955,20.54709754655862,20.482517040585368,20.996605362802313,20.441675365315792,20.489969762752608,20.49828564278343,20.43271933703697,20.391848001724004,20.531609629854778,20.49233789909406,20.5870512320813,20.4675592282238,20.484677313313547,20.69837755494798,20.663304890881264,20.591954572829053,20.664675152909684,20.99537317242539,20.556594788063535,20.55910018635386,20.596450013769235,20.694129790993493,20.62927746082488,20.81312954710502,21.03529774526088,20.639864621044545,21.050557843800274,20.566222731430322,20.928881854140826,20.746290740474574,21.40116027137748,20.585468031513404,20.49142695338038,20.558385804845823,20.577021773618046,20.908755443031495,20.576814781398742,27.759592084147016,20.582136630228412,21.90761621585848,20.772492782945967,20.755704522253502,21.14724910019049,20.50441566153116,20.66818234727436,20.804941897942015,21.013545150661802,20.556667557754697,20.91027534301716,20.572391609864017,20.45894290839365,20.59846578789098,20.491879489233476,21.63598278203581,25.531114172815343,20.567081547270387,20.565113877236257,20.429298086074784,20.478763799490416,20.867540941587684,20.746964441729364,20.46539977724633,20.621460573994785,20.704207804409776,20.513920340885917,20.65289924286969,20.680692711953558,21.313872944222997,22.970741672902562,20.549118880524684,21.645120509010592,20.63335108113993,20.54442007221745,20.592353654570935,21.009009092816086,20.54453957460406,20.561232773923116,20.568639903272487,20.975451221840785,20.545881124709826,20.588925977650476,22.49136416565404,20.825135948204156,20.686298671051357,20.787786109390535,21.31756892974065,23.04947478908069,20.737656405356837,20.543598197014003,20.647460533133856,20.5077979492231,20.6213411365754,21.01689657894919,20.548874597360186,20.642135539276147,20.423062986786675,20.904054663623285,20.515405249486456,20.64595872603451,20.59596283467602,20.616934270461364,20.94400134939567,20.5186839522502,21.202023687382475,20.643038935425306,20.670118493259118,20.758871425559747,20.50168249975989,20.63813821524672,21.316385540603864,20.578814701007985,20.526706315727438,20.892016868005424,20.728499275302145,20.531163741976567,20.79842266395038,20.637116208485136,20.471186671146015,20.560061233620686,20.59218347962465,20.734074323824867,21.07142204876656,20.83129523198788,20.549709501083782,20.368190377169654,20.631574525988608,20.768605766294517,21.11788684497428,20.814266610287962,21.250891612584958,20.529656295866065,20.62242959662126,20.70905073207589,20.675214121028844,21.99797628236185,20.885098106392352,20.72632708951395,20.889503207234018,21.076671559367448,22.505692238823922,20.73164588841896,20.49854281306934,20.69637148722534,20.661357413476058,21.675557613671188,20.51169598933922,20.673732449265707,20.807568775955147,20.661593948601986,20.49879956322465,21.505708811292134,20.666754082570336,20.517643464561026,20.70411299173035,20.523989192326994,20.607203442869654,20.46464921805445,20.58450890869,20.718226646661563,20.579958122771078,20.56415500381141,21.475305579178947,20.699981583010715,20.670353720927462,20.488679304607864,21.089066257763843,20.5668381884791,20.584574698252094,21.142118877395163,21.184722382542194,20.703515480631427,20.669705372167318,20.659253220870045,20.802521864945756,20.544302741130576,23.761339060296727,20.485543320784473,20.6674228457132,20.698170022796077,20.925788973586602,20.530213820744127,20.49442129892626,21.266528601647597,20.59343082831885,20.491945115299515,20.763417660997174,21.135789298256086,20.52744550315024,20.645768631497983,20.53589552829307,21.758499703076517,20.671860191679674,20.605517726887605,21.922925560802597,20.499478888081807,20.94198248163639,20.697694264505163,20.80842129038711,20.78143051697186,20.48239537357125,20.924550949714412,20.526040812745823,20.66331161796706,20.381957372564234,20.68227591918074,20.839470083862448,20.553582215774767,20.645719327045413,20.54780353246164,20.499453866993093,21.580879839417058,21.103878492772843,20.698204796790332,21.50828094491544,20.956841625175294,21.61933125129666,22.09068832552858,21.205911333796674,20.523902860752106,20.44927487248162,20.45699990936205,20.647091219962178,20.521861352870594,21.22476538175068,20.79517720576164,20.46628762330012,20.928922451669035,20.987364140489795,20.727923468331145,21.189395199042824,20.550905141686943,20.58474304882658,20.534588440252094,20.661597697772997,20.460894005320384,20.814867883214564,22.274527052350816,23.87891028658513,20.72989481706851,21.494738128767384,20.613725604513657,21.5129565319648,20.46123526649145,20.55424621840156,28.716633905324752,21.543943053103323,20.71591725765013,20.527173178273593,20.594580706456387,20.469015522506233,20.57986177151463,21.72224419755793,20.503536527388924,20.57606856568381,20.65901797501322,20.48560584735201,21.41898976430201,20.54695308220168,21.03353102149929,20.522700968382534,22.57103934161096,20.515989101233355,20.55426021238391,20.454658073541125,20.62344612861206,20.451648797955844,20.84492710706077,21.20361560024927,20.414517003278437,20.751226293210475,20.485464619398048,20.6405272684394,20.853744012201204,20.545826345425443,22.10151840644218,20.424585221092677,20.78838349287776,20.56342868379218,21.076192426856544,20.489816422396306,20.597944752377746,20.523146412216178,20.550017011079444,23.818028841972332,20.444352972832057,20.72114216419944,21.671374999057363,20.648591108380902,22.612961317359115,20.457063633843966,20.688805086178192,20.565762268171323,20.644484745021387,20.67007086980747,20.77113543226125,20.955845534686347,20.488743289301944,20.508463661754607,20.442868881912958,20.61060514824102,20.573016535039226,20.6253384430834,20.631552245290756,20.596922822815596,20.971499541024464,20.964115397896624,20.7172901943752,20.606614366811957,21.55515494554549,20.455393517255708,20.59138479195402,20.582925925209704,20.6031021792327,22.66554933167679,20.73761976287185,20.848010079553244,20.620207633773912,20.722149872620736,20.52536321639003,20.571364980290145,20.423848770795914,20.49032799458485,20.447989463195675,21.506482200254524,20.766489961783087,22.257832043966616,20.80112556652365,20.679687962760426,20.956669494465608,20.614544351291272,20.60086070296655,21.231540008310617,20.518833089874587,20.458925600300642,21.08518033579882,20.67784388701637,20.54040001075141,20.51268389803452,21.930003362206183,20.61589260481642,21.359245401868424,20.45992031643937,20.596157800352856,21.18813749621118,20.874610532733666,20.93522879877631,20.78193877766767,20.631250434211957,21.102075449787364,20.644455879870588,20.51365150835874,20.655468384445967,20.799155721013488,27.502496119606334,20.48415099096351,20.522662470050523,20.48802014024996,22.957498320218242,20.527538853283236,20.557180677632502,20.551561710785023,20.536996236178982,21.143591429076103,20.567343969751793,21.875535588352804,20.44883442465491,20.64542576670455,20.41873934441038,20.78720925751078,21.25749144594619,21.322118973533463,20.530364299377727,20.59906408306348,20.56010648214801,20.950604255868424,20.577615608263812,22.986199064893817,21.747534818201217,20.517152033150555,20.534912695978633,20.55334445567361,20.584905788744233,20.62442199489723,20.542568281969217,20.681971141142977,28.76345991859244,20.832065292480735,20.52529425134967,20.6273832094403,20.540211920825964,20.752944224404487,20.451533421904035,20.669210095328534,20.597504255081642,20.515658172494685,20.468974842555557,20.936118393813516,20.573790610932207,20.462649686863326,20.73635719997691,20.545442259606,26.525120252689945,20.574533774386463,22.93638436490276,20.593206514184317,20.62330819858125,20.548813909643773,20.485089930173494,20.52215698805127,20.476574759793067,20.438527121893408,20.74468341753813,21.68591324378217,20.493782556317715,21.602855413433442,20.602570672779695,20.486890906697592,20.694332538096617,23.849673708978578,20.882143816756965,20.955702490972097,24.75662852061138,20.601242620064816,20.48778609568784,20.912853167020625,20.942715429023487,20.673698886138013,24.229608011368256,20.678232458963713,21.420075010021804,20.6082292395028,21.228828758704932,20.50381473374238,20.91953824186803,24.832129595864888,22.3023552519748,20.587190623093477,20.872112548652076,20.998488484732356,20.60483144424209,20.579729165047922,20.548284190213018,20.754074446711773,20.640925010690303,20.501173481274183,20.53021852353067,20.70627985069187,20.860744740220223,20.57742388465768,20.51165060139737,20.563420744964535,21.045096523419662,22.111815260392756,20.4828035764352,20.534428202931398,20.53121673785016,21.241656461911237,20.76889882273368,20.524173152202124,20.642137559518215,20.726278710554805,22.049373430022506,21.22927541491653,20.819882633097965,20.465331448202047,20.533426043615265,20.506624471646024,20.548939403553106,20.94250208458822,20.698483497860806,20.5449537529369,20.888295444195826,20.48063838252612,20.641340003534214,20.683071353378647,20.694032904856897,20.55067908659242,21.17837438291375,20.783321235324212,20.540623369612934,20.506760371944686,20.553789327730147,20.498242775547475,20.712571922597014,20.923012556860815,22.368119442506252,20.536647001360382,21.252861001203865,20.68589436710215,20.5507825942185,20.59709557562135,20.620589126891787,20.613641676878043,20.755180430464172,20.82399122335918,20.401165543880097,22.196074097142684,20.75984423609135,20.541996635886072,20.724644977527237,20.5980368490706,20.561593805414535,20.753486311983945,20.512041815907224,20.915629445868657,20.615077006791232,20.555483683304054,20.681887448047707,20.612939902759443,21.351473533812264,20.606136501084062,20.603494715569983,20.89008633682581,20.970698753796235,20.613719846279448,20.531146659715496,20.54537048910013,20.581142007119478,20.540075376699193,20.474663109808557,20.525996251350115,20.538048731720497,21.37456065403863,20.431262472100475,23.706274048389268,27.1926876223845,20.497785737964623,20.543032904229047,20.55340753866914,21.929002510462663,20.473893885884273,20.53523599097693,20.743526864691315,20.756035618007996,20.557487385930184,20.601547733150817,20.604271762920813,20.5417153808813,20.51630474008674,20.978049133268527,20.62152320895204,20.568285244372962,20.49708512679133,20.796081636393833,23.59640597427113,20.765696112118974,20.866779916527495,20.536583496991007,20.498007568889133,20.706776483841296,20.518069090912963,20.643422667644312,21.216761776988406,20.48698402018413,23.080898326546638,20.755277079622815,20.482577584356,20.698650375474866,20.755860354750407,20.52714990627526,21.17055280027975,20.77153835140842,20.485173535330233,20.464776485774667,20.822996459395878,21.002344668892043,20.690945272342532,20.49391548475114,20.635016550657287,20.558774369953383,20.702797041796703,21.228056461204215,30.534891517484667,20.96713456757336,20.641923085880553,20.436540220822508,20.585873025748324,20.69701541470923,30.12737705147785,20.870089930178512,20.52024867280281,21.46564139732718,20.5970841874473,20.436029307852483,20.512505210101132,20.768646789870775,21.03178565696867,22.042054434018265,20.528453356553396,27.356678289041696,20.46204752131861,20.685930453378,20.47247183332948,20.586965872720555,20.506240173451534,22.15359456481673,20.604356790016414,20.85720147085354,21.72951670346846,20.570402197137643,20.597688751972267,21.97907802959699,20.530136574997346,20.955879089819938,20.904059104270168,20.848970860404407,29.98954421469811,22.708918146342185,21.48121060824581,20.545918870544764,20.59320427644081,20.68533908696471,20.802479096829558,20.681425889908844,20.837381072042234,21.16756427497695,21.501148007599305,20.52116098016671,20.49128334776465,20.63042776975627,20.571580867319412,20.52992164518953,20.630176092481157,20.921334143659628,20.68653730328556,20.707323908537173,22.429180111572478,20.52033928254495,21.11312267787757,20.86273168279347,20.970292355397973,20.49948305209212,20.546031498845093,20.522844680835277,21.081788779712003,21.24245475107845,21.915984860181446,20.463614998020038,20.5792172550597,20.943378645342783,22.414919102564767,20.452751956527056,20.735282845889838,20.56741355340813,21.27684881173972,20.625175859124948,20.638183116282242,20.646926155417685,20.663692626147395,20.723040820066867,20.87218386725243,20.499586580085012,20.79292323942764,20.55308720332871,20.788036240021146,21.393125937572226,20.477937046675887,20.473228823841293,20.77131327043656,20.55821684507532,20.602866062022276,20.570237820571773,30.086168131135203,20.495886412933213,20.59784931093312,20.503525139214876,20.402271616374822,22.250186865977405,20.63313396619031,20.80993660356208,20.488999523218,22.150569428121955,20.724594122904524,20.869655710913996,20.430032117781533,20.495164955995833,20.660677500415815,20.664741329460192,21.006514130155693,20.867544941166464,21.065819944268778,23.018169125910283,20.476436412286116,20.519059081953785,20.709195562502675,20.558947566225882,21.05092752080941,20.47858991079758,20.512318558669495,21.355393193261733,20.682905380964392,20.737156745623746,20.647238535988862,20.45608239357548,20.768308087208197,20.434208854132056,20.46343531722323,20.52430981029069,21.05987954475785,20.43852713531983,20.648866808868608,20.703482906267414,20.663439132622198,20.61460854838001,20.985656400657874,22.721004340958785,20.566962962743375,21.976490893193176,20.507983862066624,21.901207128170302,20.48365376680344,20.550380947608915,20.63660891378781,20.48178157120608,21.04210853484365,20.58537270388016,20.558883811046968,21.62988436241447,20.60588003375756,20.640776790216464,20.656844632410372,20.615345124531576,20.83210937843148,20.62012681429331,20.71823876399643,20.52295936380231,20.688603257554774,20.950157006936994,20.42903131576401,20.632382590695702,20.59695506685046,20.937808840086355,20.507607799610707,20.57179138225247,20.809598903337314,21.091972978545794,20.62053938533712,20.908995644208936,22.432615148895522,20.847783733198476,20.748767185686543,20.563842259773008,21.263123511311186,20.539517222804598,25.84661448066163,20.508902106455412,22.854364101325572,20.49136431842312,20.449255787094646,20.571621717229053,20.536164899473555,20.666283918083746,20.918548713324302,20.615488512268993,22.39346257619493,21.044008207387712,22.43075202291639,20.943584605926738,20.483123721060014,20.641623336144896,20.53618251023656,20.51232250366614,20.666431597999743,20.66740832119672,20.657586602462548,22.059251323331736,20.593892717055162,20.52033928254495,20.568674140701308,20.486950643190028,20.60689214569258,21.09679045368959,20.585600467361097,20.582056733451942,22.743644609888747,20.47807908212551,20.836070327165537,24.753893328415998,20.463647889110014,20.82359458602952,21.95240889871406,20.591975820957792,20.534146691619057,20.874019837498576,20.577641715364432,21.023066406159284,20.579288409747633,20.459732411567593,20.530243048969147,20.478838732722306,21.037663598171633,21.58796728403384,20.690093120562704,21.076559984704595,20.688458188620718,20.701400899838255,23.435545605731132,21.673662267278942,20.430842020261117,30.82825069937105,20.667856828484496,20.446266391407324,20.461857669557237,20.739008810857094,20.745683272149083,20.597525220898223,20.88268950983003,20.748740047859194,20.551914248530238,20.479614799661572,20.487811208120473,27.324959526882463,20.821884599976695,21.076427595545066,20.549806182891416,20.550969525734583,20.48173784726035,20.610046116450793,20.49933604025598,22.282568879136527,20.902060741864588,21.096062727401154,20.681477504662315,20.61374782646884,20.730288410375483,20.575789150275764,20.661252690629013,21.060352870899976,20.724560843731997,20.569784360539277,20.76588490160788,20.436732006814424,20.478602572278785,21.795900449247636,20.449482438919212,21.057219462249808,21.526023944208585,20.94095717939714,20.613740903515655,21.461262108793314,20.64137040581609,20.509561862760236,21.160622633691872,21.938832886591307,20.591345305517542,20.60205368368907,24.64034429195119,20.885257491058322,21.013337590526124,20.661257801641966,20.9524001232882,28.660889138300163,21.003541657535933,20.6973815861422,20.677594356211202,20.482765489227777,20.964419286135218,21.16460283818084,29.594209444565337,20.6498302646361,21.390681426879894,20.684010034873598,20.63507491572911,20.497366450328858,21.637406761760356,20.72613304225693,22.040039732693195,20.6524200829313,20.614871148812277,20.55319416467038,20.48697987141505,20.670138828308204,23.968368014111107,20.85872649487535,21.719396360839987,20.658685102139845,20.625091886182123,21.16358020475252,20.538278268867877,21.913196167842703,20.48698381641169,21.052810752245257,20.856340934552176,20.5669694146203,20.6090888098435,21.64339714134758,20.59967625048312,20.797651284128854,20.83425747682536,20.60321846826196,22.35305023447511,20.53943502803417,20.49408434292996,21.19128073755837,20.547951494857806,20.60125078542616,20.689790995073345,20.495878402551707,20.453714257234022,20.568625672655767,21.139200142677634,20.514743581476612,22.331863936987666,20.496043604516274,21.061012509328982,20.520044328201976,20.47430358026066,20.708734603593435,20.490798854717415,20.843569826519346,20.56155104933673,23.91744992589107,20.911978354508758,20.58386301302904,20.499909393954972,20.56447123406728,20.584374067899688,20.879848341896604,20.911890730827196,20.45072851466555,20.739223164455602,20.681739675348553,23.698896702768156,20.621711585256303,21.165325026982124,22.713802595183278,20.59266297334735,24.00510767538203,20.79876048811074,20.520486007159548,20.681397896223135,20.618518217954367,20.731158151802447,20.51369076531951,24.25187430314869,21.160296005815855,20.524696997805044,21.251916083789723,20.533621955847444,20.441105463514177,20.68318166093627,20.62561999808034,22.29568316024541,20.85473443191085,20.487794621509643,20.69873967389186,23.025637351822457,20.686474258034597,20.54476691587564,20.549466286760385,20.93289222861594,21.717289322077868,20.629462019702903,21.293323879646316,22.96424736904044,20.479005216507588,20.524213291162575,20.750870706048115,20.579627865723715,21.53612156019539,20.548760715619714,21.157444004032033,20.552336106620217,20.49687444557146,20.52031245988063,20.618317437957828,20.600097765291594,20.669317392826105,20.518107200431746,22.37469550191041,20.67503699658853,20.92552228901111,20.715132223717244,20.89096121716546,20.767901393937546,21.582060845317297,20.693246191239353,20.67370612045914,21.052656736065476,20.59135594947729,20.72086203324532,20.612145805414283,20.577936226227507,20.382195911328726,21.209789689667,21.822153072991554,20.580248521209274,21.94531055066992,20.45907705134365,20.92592198717101,21.290060756054462,22.01016661988114,20.589740275495007,21.25983274762502,20.99755039355377,20.687085422590535,20.528561920382597,20.520666732243715,20.639596998954442,20.690052304069425,21.83941293689976,20.58327441388784,20.567726300961365,20.871791282030436,20.651053239906005,21.0071825616072,20.75952029450089,20.5799900425526,20.487223463716916,20.495007071388127,20.48464773005831,20.916112309738168],"xaxis":"x","y":[21.224638239972652,20.180643707721647,20.201341431117804,19.1454180237343,21.79470380164697,21.087755789009996,22.432940196049124,19.578846464119135,19.242152996635603,24.328668011629535,21.754493646354923,21.476822454566275,22.377553705906216,19.555124576954142,22.494011033388475,22.606991826298977,18.49440443743341,20.76699650557159,29.702932107840674,20.84994793377778,20.282334104450392,19.302304652102425,20.014253448408297,19.02478155099295,20.401136284262034,21.066918904158555,20.30528553843669,22.443056343683086,18.530754586286314,19.472341179018663,21.511675814584603,20.024996233202344,21.986420084483047,22.32477661567823,19.024668956843474,20.277557571355647,20.011894681671833,23.15521167718674,21.342870607762556,20.008806098312462,21.88562823605827,23.73392435177354,20.358652273917528,22.63391493571178,20.2953770114005,18.603706022480708,19.476619646192038,22.286954060950823,20.827570650601533,19.658607240576167,20.944082894927185,17.889444547032436,20.210011039401028,20.041902410540708,22.116524819871177,23.07232514287102,18.931245369435278,19.243034483473725,22.175009164867692,18.8696149273692,22.748443989592825,20.18471556395119,19.630595419370202,19.596480627252774,22.075827573908075,20.81706640440522,20.033430227416382,19.472993206549905,19.47575297115503,19.712127286089668,22.22369094458836,19.974424393125233,20.76732896720974,19.73170766432226,20.622497836834853,19.074352191519743,22.597191120827183,21.03390016791011,22.014068840206768,21.41124459992273,20.690267441816314,22.238063995762765,22.835153147091482,21.940489607019817,20.06072495330695,19.26508618967162,20.593917077867186,20.807981392523768,19.461766940454588,19.92317059187003,20.905864572180928,19.887154874828134,22.674741834325705,19.54492482512454,21.650322094933124,23.6494594685329,21.673503182786,25.42292365756086,23.598781185238046,18.99960834234589,24.597055201180662,22.731123431811554,19.734159687698988,18.832054600441634,20.397708053035622,21.477527921557282,21.731372277489818,21.51301334156099,18.824250825082274,21.177749972976986,22.50813230656899,20.78875924862035,19.355480148534408,23.890614784737277,22.347688134832723,19.773789560999628,21.94580529932221,22.19000668879013,19.515006779114817,22.99883477706531,24.630356510701596,19.78112594827553,20.45440259621003,20.195062159996276,21.08554669425927,22.350651659007692,19.88287753302219,23.223107004233785,19.56773145357299,22.383567194160783,19.337397595787667,19.84140252838032,24.09542285957458,18.7932762528682,18.78833918294514,13.482051629033913,21.99805786914009,20.68539834867811,20.75326327581395,18.386700720994313,20.922404198298533,19.34599856610255,23.03775857233843,18.64679105166152,20.517600912543127,20.206849822052806,22.025147336797662,19.802893385023037,20.525632635954643,20.1770277231615,23.357834800271604,19.12730750072142,23.147280554074683,21.983261369651657,20.345804248394614,19.067786210059246,20.11323917875491,20.682137237753555,21.173013406383724,22.83375379636568,21.704780057376663,19.98202682053532,19.114539527933296,22.520164246985647,22.815206783859768,26.613491397737352,21.053960341071367,19.740225451250925,20.911400034394557,19.239293618022067,22.519031496965763,20.18180618902739,21.47581257946244,21.841152373314472,19.894016350184707,18.947642644803835,20.33821258053668,21.998370834730604,21.623042657920415,20.52474339677355,23.920588558527324,22.398384643288747,20.614611901935827,20.73652363425427,24.216251851014647,0,23.798062566113977,22.44169398922513,19.04780522125321,22.508853453023672,20.361268080220963,19.223547338390134,20.037360610262493,19.47969541290554,21.419330156983328,19.397345612637686,21.062803097791257,20.802720957968873,22.787613064091445,19.854352512470033,23.127238847948263,21.236410750491928,19.39740450234132,24.629767207576666,19.424905147339715,17.66325927218184,21.589387814254906,19.856012075783497,19.346653316091935,20.250448783340936,19.91174617647474,25.664004852328446,20.437440195258947,20.159778968313912,19.760702082570422,23.894186440194584,19.664395199250066,20.03503448449456,21.85979788510952,21.007452333347548,19.330905939549268,21.816602846893197,22.163197583801455,21.21476614620658,19.61795076264764,22.32608970797742,21.206937139516242,23.650902186871754,19.538495641550785,21.951268793802367,19.712186074557728,21.6397845044252,19.566140232030243,23.528005120775013,21.76194235515012,19.759231530600534,22.69657897049395,20.97108250648827,20.91944021038121,19.11091145461158,20.92382072739302,19.99564295418165,20.157995405754583,21.499214768467855,21.850862595399537,19.613806547176576,23.262476414885043,19.981118924422468,23.526907160221576,20.11872947152901,23.756207113729698,21.05135284285491,20.459271509540073,20.732604891283092,20.22193428509613,21.2413041438754,20.86782723578085,18.85414731140336,21.746512886013605,21.477990920672163,24.227674033652324,25.12798066804062,26.560353492028305,23.633181830866906,21.120826057383237,20.22792847217847,22.242317589489687,21.590387855717424,19.62630225708032,21.126056594579662,22.585399041291762,22.379011027342607,22.883846216197703,20.986097829919892,21.770367489208184,20.58669309831316,19.43536098584635,21.22950036954701,19.51332725702657,19.38984557001998,20.48325363998835,20.246730162293353,21.286321988690805,20.685520474331085,21.05996063251775,20.18838178436931,21.836335494435165,22.094100243073623,22.994379248863297,22.721264543402345,20.729598340663756,16.46658160832902,21.557923127735883,19.880806435323297,19.801676786988352,21.12828261562749,18.90461110336481,19.19842345841963,20.092049623606254,20.0459782414438,18.31795878333722,20.7963375193431,17.813568277801206,19.627334048970262,18.56499419954315,19.372221038245304,20.307717788382128,19.384134222108464,20.087817122473812,19.502286013993587,21.227181698040095,23.312865371844044,19.405079121883418,20.90714373186156,21.80480516491849,18.974285739983618,19.176540848422555,19.19474964169837,20.93275451807793,20.832136714555805,21.170532392724574,22.709608681261756,19.86611073536812,22.055547735078193,19.980960001271505,22.653172763321756,20.76687340459434,20.847191191309296,20.06982734293826,20.638856572715298,19.67156525332246,19.49519726866692,22.064314182636632,20.227612418267608,26.201435233706484,19.068639219627578,22.656078929958284,20.77129521126714,20.391385205003306,22.283464834927894,20.845264152741507,21.027619198526104,21.07443745441354,21.518584006439504,19.724170230709305,20.679280780496406,18.927945745238855,21.825732486030095,20.464091007664432,19.439137307013926,22.977548256996624,23.988556498480502,19.81821879465883,19.050281782029014,19.439732238591404,17.74375624808358,19.990444456103596,0,18.279601513401296,19.78875556523867,19.965832185850637,20.69408662857846,19.405573621751774,22.379934620081865,22.48871602661339,23.642331039751582,20.276556094805027,23.283922162181995,20.195578868662224,20.03811220566055,19.14964293785794,20.90803339180857,21.473710208099476,19.884197354704103,22.164351182655523,22.813153319462682,18.901120542977083,20.544639287700853,22.988276940898057,20.109117749689304,19.622850798933502,21.871633434683428,22.25017305320405,23.60048678836413,21.515067916471864,22.320806056618878,19.870759032200375,20.544449448312626,20.261067504600565,23.43234984844275,22.4595812045188,22.52530214820436,19.382106816183008,23.96470652783692,21.019906267101412,19.811383905702034,19.845505364796534,20.51412216383031,20.373076483151994,19.01511010272945,21.47364877893911,19.957005833266606,19.38761598733965,22.32447174476143,20.79356441567801,22.807056132685855,24.028256571904787,19.874893102286897,19.393396349595932,21.076706013187607,21.881924997624267,19.305427636738685,20.88679803779653,22.116604136681058,20.73926969664862,19.30704929356398,20.730402345071546,21.49837252262123,21.892478522473827,21.156990969891915,19.626240196846272,19.456579392491523,19.83361590003151,18.640025995609424,21.63591845810156,20.866343204364526,21.462764853897205,18.837375101562536,20.833447685328323,20.03253125195289,21.036829264514445,22.96164822427184,21.237452292596537,20.43520123557409,21.156628899315663,22.496434708234126,22.4307529287208,21.544986593746263,21.11234277813463,21.351967832505505,20.39596499860976,22.86476103379213,20.105565995835796,20.48988421076927,21.684940338873297,21.559903574554543,22.73966855399758,21.952844226236508,20.330025112864092,21.040198736231254,20.75190246631361,18.798041102461394,19.29157093266521,19.97055996653021,19.307592868706436,20.89335338152254,19.773921527816817,21.100709172660746,21.699220401665585,21.041684548715036,20.396544813616845,22.752749154816065,24.957468167964976,20.506265808043914,20.374729613814893,21.525022859298808,23.275631354399913,21.75794824506378,19.767052898077655,21.218145625261048,21.536078922759724,18.276769619768288,24.94868149530031,20.41835179368666,20.87359355439842,20.51145154622031,20.700075492290427,21.289957235167595,20.748925773432315,21.959550623213953,22.62198208884591,19.38396579774315,23.31051349089806,21.627932359551377,18.954513256380405,19.964316452607815,19.32031247988204,24.27577420165758,20.498820970198462,20.060863127783303,24.1535698900103,18.16161414446258,19.814472942891484,22.998819199975294,21.020751435153667,21.36177568305088,18.469113141288748,23.192581335464276,19.21725486016369,20.111754471035223,18.939248297157278,20.474687273562058,21.111612649480875,19.87603574815164,19.637857750008234,20.264370436179377,19.93146081401427,23.29863946166844,21.939745630681145,19.743514665635868,22.649298943378753,21.24045362673594,25.608747220451313,23.49090374555705,23.60775201354553,20.678206389277292,19.915728419308504,20.449383439914303,19.824402536620187,20.53678125858074,22.159724468145836,21.034942664057727,19.03923233773122,20.84977321505041,21.781868242254394,20.46100849412487,22.004305689493815,19.142842792258786,19.496508584409785,21.834751391009725,20.402181439081684,19.21550544985804,20.640134807950194,23.051954851262433,27.036279453018476,20.37977707208339,21.775846548503456,21.781910553210967,22.353218452653266,20.165877660805254,19.451496130670126,25.490707221646936,22.59150705240579,19.995956185326282,20.28232915493825,19.579291420064532,20.879430687458026,21.138173662648782,24.394611647917593,19.533857847503334,19.817723799212757,21.171020993541568,20.50991100623185,22.326932218927173,19.527299811101727,21.383177447413818,19.392837666571104,23.017914332247763,20.82549425312562,19.072268708954628,19.10179618438648,23.29660399040893,19.32638298997934,20.650313109616484,22.182752894747274,20.620515848299537,21.682830461889793,20.248785506000544,20.060843672034636,21.235100056704788,19.639114342316898,22.873156355744623,20.616811975583957,21.260660009666424,20.98111639712373,20.640272301462904,20.347463631132083,19.255662542640188,21.590649178257912,18.627786784209636,25.25576312419618,18.13136213470814,20.18507504172347,22.234490018926184,19.867378293238115,22.842407702796528,19.642302060721956,20.934073076796377,19.657897588782742,20.907660872216642,21.21194412605265,19.94161891161789,21.31905629271608,18.792730927848385,19.734888199833236,18.03332620487895,20.049848549450562,20.009665992350836,21.60568082130027,15.704606153972572,19.9492653280832,21.029500902796773,21.639091593296747,19.630944296038688,19.444205825873514,22.72561505846151,17.641741979338622,23.57832742490694,21.06232551474456,20.596453669823063,23.77569135147608,20.739222922576804,20.798718195952173,20.117453663452505,20.134364899685817,19.1796783064222,19.08437502019272,20.74807693982553,18.925763040720064,21.23645513023823,22.042981377352834,21.11735552640613,23.60188649068525,20.85643673732399,20.248587775712384,22.0971519060863,19.917597162486278,22.120758360865494,22.44443523329341,21.796740313790345,20.192956161016543,21.587428114342384,19.88222659302695,20.074407563972667,19.772030191805687,22.686480223069132,21.085461437053635,21.84811725117268,18.71863791145474,19.897316659199046,22.368646863289158,18.2265721237695,21.061479969798718,21.091487685756913,21.905619198419114,22.248853822319084,20.948206085285577,20.95171629836988,20.60386819333271,21.712312461652626,26.34702224299619,19.173408752888843,18.94863556098072,19.669251869992156,24.602654298813697,20.178503166977418,20.318803159054653,19.8813455186455,19.742006737354636,20.848691129006262,19.533504643608662,23.044574887856943,21.2197623270523,21.881578906408237,21.91240193642795,20.941991359524017,22.077006044437276,22.54693130601029,18.995877797816178,20.997149152437558,18.687341507608156,21.335453055117675,19.376016989288264,25.07655356851684,22.59004948235383,19.378047539426834,19.682163069515127,20.52340976581651,19.581570228112746,20.9498708711936,22.82667920636651,20.99544749522815,25.17876365321419,21.627453285628658,19.477956222501692,19.193137896037967,21.893341530106955,19.71728461871742,17.31416721288844,19.147406179743545,19.827554176093653,20.471143313052202,19.29233286786097,20.500085221471384,19.644130235297485,22.034075873229305,20.949040678200554,20.540968959330602,27.231751620760654,19.73015830855728,22.493025220696325,23.089701491122522,21.64924126399572,20.795739259102447,21.118903696326274,20.815053320149143,16.52324032760632,19.730383893455734,21.24843214899758,22.071670864625634,19.282486682508637,23.126218118696794,22.228032143928214,21.93139047566312,22.735567798975932,24.167967887858172,21.655361699836966,22.600876020595507,24.59815969618305,19.718235182480434,19.521220854266264,20.564957928128795,20.971944131576173,19.6146020174933,27.021590188993713,23.73507977629211,21.659326126254854,21.01213158467734,21.766780414954713,20.08203076321153,21.756353934255824,25.471067413053014,24.02283197509071,23.385758678777925,21.23702785682451,21.578861893090256,19.189211661593234,20.07860380095908,19.328382384869762,23.106213073613993,19.95329417355663,21.365528432523575,18.914065908446993,20.846904364921407,21.621710733177533,20.214357069981805,20.48695013292876,19.590051196125845,20.988896575504317,22.71500741252458,20.63081635106876,18.585982112110514,19.30101383466248,22.43563961615873,21.19573925103901,19.253482814272665,21.733085888220945,21.51937854264472,22.935901951961643,23.1601071574333,20.83646802652741,17.151393182854086,20.807011308942396,19.447379908647196,19.74096325611752,20.942433069954838,19.870033345928174,20.91477708386503,23.264639171673256,18.81898933836999,19.40497105732979,20.010315016400824,20.436358643191994,21.24511744232927,21.394403553854325,20.278388484789648,20.16621212904916,20.06466810918053,20.8905482708943,19.02658827934665,19.696262913257698,21.147132242697182,0,21.233800456337935,21.791167162104042,20.18131197591118,20.79775274411387,20.158015290040524,21.01738902857623,21.1949084635434,22.774677489141574,21.93032100053686,18.418089755206072,23.810210740127232,20.9793536678564,18.664791000361163,20.57237479635235,19.66651702047476,20.204034803826428,19.989961267682528,19.13829898996108,20.67820982860559,19.602487517256634,19.634034627117707,20.049508251055094,19.732903567032626,22.301656666857568,19.3351286191496,19.66019789348857,22.54105973810428,21.087085320291497,21.46282774650559,18.953344800503015,19.199864465913354,18.832769512656196,21.40009037173508,19.692718291075924,20.626400634350908,18.73677516240749,22.077517747249683,18.74408127304125,24.761947476774708,26.113208352362694,19.576225059708314,22.48651099143561,19.29251486362711,21.199347342918216,22.53052802544868,19.222080263625543,20.87727804782974,21.285530909063,20.6451326442611,19.40981146751902,20.008240359915625,19.734510138857544,19.369940873414674,21.839004592901738,19.643216988179674,20.34827538264879,20.295760247970698,21.15453445100085,23.946209069915817,20.161508356002734,21.461336053955172,18.855956110093064,20.13782559566735,20.22901210527111,18.772971741326142,20.346151863763343,0,18.758138614258336,23.01986369036341,19.826388660339887,21.76883742068305,20.30266681112443,20.98525964839946,20.015097338364022,20.523507399137607,20.436904665975636,19.162579413291002,20.375536910022955,23.70344643795525,20.910669810022966,20.379640393188353,19.217287998251134,20.77164874576569,21.098649782686852,21.445335194484834,21.911569915872246,28.094893882984294,22.184719195487645,19.636646744775323,20.26686372980413,20.815307943684378,20.695469281090816,25.87586915107988,20.22604051998145,19.449746389320087,22.323663662536706,20.105603080001295,19.720177445507503,21.133857610241805,20.04287236504247,21.180586044654405,23.433073191606535,19.844702412663505,25.31201588491192,18.563017473748218,20.726116689654802,20.55471999163649,19.12308826699376,20.688277632644148,23.08698185712261,19.4188490987079,19.38789150561276,23.032593297237835,19.840625811644227,21.990665865987562,22.00972851632279,22.544699771649725,22.804072710576072,21.602864772837197,22.7909106273511,25.661642339745672,23.190044679879648,21.797480754192584,17.862153020413583,20.62223350550368,20.658934051201378,20.583030645279486,23.466890235880964,20.879510709815698,21.526914680071894,22.700127681411185,19.683189962828273,20.26341148584,19.616726513482636,19.839377869097518,19.875302398958702,20.973185759004682,21.082163500516533,20.324838680602316,20.30745292915028,25.9613964584893,19.046718202293015,21.78315042330069,20.91962424027046,21.618102712310883,18.472822451162664,19.554830163405484,19.175778559517227,21.854300382519792,24.477823287446007,25.5492833010917,20.6322831501737,18.924421094438486,22.00276008739277,23.251480243552532,19.437442862246026,20.440974576504654,19.114316794022223,23.060566976295764,21.685850226111516,19.690963589379074,19.932730372590424,21.5156696442052,21.03832189892855,21.34321011850724,20.297903654101066,20.251155291034372,22.02017509916011,21.00569855470944,23.695624962968736,19.83550351971754,20.046496736817677,21.663202256858984,21.64914607700524,21.763695119906945,18.966254774482525,25.621581882833034,21.318917047123914,20.25569596256737,19.082528830592434,19.62869805766532,25.538294718669484,19.064589338432878,24.544735671741783,19.58243213803125,23.66501619396984,19.12391425698277,21.767558412000408,18.99637745675535,21.23800756191085,21.00116832090369,22.714272038785044,21.137414277628565,21.45606258158953,22.279207015403138,23.33275256138136,22.26485604208308,20.723080111555596,20.639447033358472,20.694537918678005,20.730065203035867,18.59276109193853,18.41344612635984,22.526317266984485,19.84998714954673,20.751047310028625,19.801872521805933,20.233535275947663,20.632626738131346,21.279464358226686,22.469580214601123,20.408744363117133,22.583074271938578,21.34410908350179,21.979041347822633,20.59404343806881,20.264946502479802,19.878697565839243,22.615047199531546,23.56945490190484,19.736798486144902,23.729801820804443,20.49274951238928,23.323658577961712,20.770035525423065,19.238103804361465,19.567540218990334,20.344212591822245,22.493051859930933,19.27583351001347,18.98927807008273,23.394529944648035,22.51205924599309,21.071056073380674,21.047075972047644,20.445539922646542,21.340147837597844,20.003307902740296,21.299913824117656,20.644505230109406,20.419105854581318,21.593824209166353,19.476347089125902,19.838668436714478,19.728765556885957,22.07230878544129,19.427802084161115,21.756421739524402,21.380030171433972,21.32600862652118,19.632965031850407,21.403421935393933,24.37658838285707,21.043820980181792,20.1938730272849,19.66793597026211,19.199993380834425,20.035891808657503,25.0888235331049,18.385719188798834,26.48889669142553,20.116390923202314,19.499674996520806,19.751886432327307,20.522012502780594,0,20.782571363318098,21.618914296187317,22.967539915594646,20.95701983082078,23.548104709979565,21.65670940491333,18.258239797953646,17.88154175390193,21.233312475041235,20.277059332832177,20.919229965693855,19.653189714082398,19.02398982442439,24.205441209659515,20.16062238071552,18.963476628745678,17.96562762596234,19.531857117715408,20.86635829845881,21.255773747330974,19.280122810790118,19.369392868205782,22.9021647605954,18.93136369208174,21.47821316674072,24.42926325370543,19.880057005564264,21.11943827672747,24.0347829295736,19.174320595643714,19.810878030088894,20.711569411286565,20.79181709109951,21.52825545142037,22.510563057499144,19.714589625374927,19.195910426040165,21.643407148870626,22.79188245530678,23.088899981103793,20.914444260183114,22.409894316390734,22.55442514417979,21.497350091305258,23.661739831337005,22.81337882106461,20.335424976569172,28.119486884464127,20.443137811296648,19.395826969085725,18.744741571834194,21.52611831315801,21.048400728737306,20.235023779555554,21.0819625214105,21.53807712673746,19.283653604184654,20.890428315584586,19.706229562174084,25.296616627387504,21.809197399053033,21.371449507411626,20.19656265688086,19.305685493406397,19.08654493081137,19.639706353282477,17.583516204502104,23.250222506325386,22.82294225806205,24.601371538637977,20.0953223269386,21.243839329097405,20.37329318617189,20.45930590947763,24.987649624596244,22.400315304318365,22.625652665035147,20.455200776646638,21.021720462702472,20.59279530263181,20.232040256335257,22.264357262987502,20.592440384261195,22.36255454353493,21.99517470550352,22.199722361671988,19.391954018620176,21.83585207290849,19.14268047046844,20.53559540130039,21.167744601954155,22.926085660752005,18.718526187000297,22.28042360433217,25.95498550595432,23.32578333060166,21.309621092817217,19.68447811713372,23.268620237666717,25.42582021905868,21.189524929277866,19.937692777054252,20.801380865239704,19.316707310363867,21.75315297668226,22.868455030888445,25.947611368302585,20.033901511817852,23.20532613123076,20.554742003246933,22.96074119343453,18.988728193664233,22.68524113143557,22.64902697475969,25.92407619036082,19.407373582124833,21.346588936497685,19.366735480432887,19.124759964560997,20.07378037525111,23.985931291943867,21.460510640680972,23.23197611805285,20.73330584327736,19.12586494836624,25.275464586661446,19.715668478243582,25.489298123537854,19.013002282515146,25.312874128977917,20.415195026786527,19.91413691399634,22.130140513245646,22.92736601506673,19.584806561357016,23.12482431901705,20.687407722542194,19.611919934246885,24.151034118830534,20.079932477068777,18.93225512877545,24.075045174066695,19.899787637164348,20.874095085342866,21.48054296008632,23.07558442006914,20.640568320462897,20.08863945104873,22.28815948936714,19.616841359021425,23.953590715604236,21.784577239189012,23.301772335078063,19.749416399115965,19.811951028236585,20.17477942437346,18.304477915148002,21.513176875338157,19.58793862702669,24.19084293977443,20.88004158687887,21.10029756186111,19.35190112925433,18.917516857772046,19.84423117509742,24.377548873947223,20.529649424214696,18.766505656199122,21.439000175436664,18.811308834423215,23.933428716720645,19.631145075685264,22.477074905595526,24.176989311672042,19.985865383441666,24.300186984861302,20.78628428680642,19.166666955002903,20.396533708238668,21.16161998720379,23.822353472570573,20.556259058220718,24.241029788998915,22.067946023704817,19.778390202711652,22.99407646829777,18.833019773765052,20.288010749874967,19.947775322749504,20.338620784352592,23.494951508290605,22.38615246636496,19.034944006994706,19.984953818018486,24.081396558763124,19.595170049801663,19.41162451128841,20.72798026434645,21.785986387464803,22.944212504137,23.12678795739032,21.831113335529626,24.147006515510686,20.371960850528456,19.32549212260673,20.504720566168093,19.431273145079818,20.869899532948505,21.94524199207118,21.941322673977073,19.72760807429288,21.84503668728796,18.979888467927786,20.204391739449186,19.721950276191546,19.579038969018928,19.030933465764726,23.8143209659356,20.855181011615457,20.74517719106974,20.090817980596317,20.872940195293037,21.393924057871644,22.20317352529318,19.736694248820182,23.68012909153893,20.722700819009958,21.714666548374087,21.05462253329071,19.397806220065235,19.129172314836154,19.83710046840733,21.397724240612124,22.701728218692,19.775576525065844,24.10746972295679,17.80002145055656,21.28200527852275,24.425354799557958,24.25287711008343,21.61681534013004,21.629837063073623,22.53034703709958,21.62524006847279,20.878967106147208,18.668189678924886,19.500769926583363,19.995538068644624,23.512127623334848,21.53372289651904,20.823420888524133,22.265101932257092,20.999782941566835,21.517654972490075,19.604530267831706,19.76742293183983,20.740147099081785,22.94133404816206,19.387966817051925,20.39789928981696],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN_log = np.logspace(-1, 3, 200)\n","l1ratioEN_log = np.linspace(0, 1, 6)\n","param_gridEN_log = {\n","    'elasticnet__alpha': alphasEN_log,\n","    'elasticnet__l1_ratio': l1ratioEN_log\n","}\n","\n","GridEN_log, \\\n","BestParametresEN_log, \\\n","ScoresEN_log, \\\n","SiteEnergyUse_pred_logEN, \\\n","figEN_log = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log,\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_logEN',\n","                         score=score,\n","                         param_grid=param_gridEN_log)\n","\n","print(BestParametresEN_log)\n","print(ScoresEN_log)\n","figEN_log.show()\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"y":[1.7180329536395043,1.7179108262969727,1.717788553495754,1.7176664434868543,1.7175448345221003,1.717424096622672,1.717304633376824,1.7171868837599231,1.7170713239693078,1.71695846926587,1.716848875813668,1.7167431425083435,1.7166419127845824,1.7165458763924009,1.7164557711316157,1.716372384533491,1.7162965554782539,1.7162291757369288,1.7161711914257824,1.7161236043615453,1.7160874733055607,1.7160639150850119,1.7160541055794614,1.7160592805610453,1.716080736376788,1.7161198304616623,1.7161779816711153,1.7162566704218776,1.7163574386298532,1.7164818894337963,1.7166316866932234,1.7168085542485936,1.7170142749311619,1.7172506893090547,1.7175196941550024,1.7178232406197806,1.718163332093757,1.718542021737028,1.7189614096564703,1.7194236397056977,1.719930895881434,1.7204853982873292,1.7210893986337776,1.7217451752401296,1.7224550275037824,1.7232212697993639,1.7240462247706325,1.7249322159781209,1.7258815598670698,1.7268965570231276,1.727979482687788,1.729132576511769,1.7303580315327007,1.7316579823736475,1.7330344926711934,1.7344895417561073,1.7360250106257975,1.7376426672657712,1.739344151396819,1.7411309587453363,1.7430044249555539,1.7449657092840745,1.7470157782382625,1.7491553893401544,1.751385075215883,1.7537051282264589,1.756115585868373,1.758616217181256,1.761206510404061,1.7638856621205097,1.7666525681283722,1.7695058162553596,1.7724436813268856,1.775464122467811,1.7785647828918365,1.7817429922988903,1.784995771963402,1.7883198425555011,1.791711634693963,1.7951673021851597,1.798682737857509,1.8022535918571159,1.8058752922285979,1.8095430675665625,1.813251971488868,1.8169969086534792,1.8207726620171516,1.8245739210168204,1.8283953103437813,1.8322314189766264,1.8360768291413527,1.839926144875842,1.8437740198905324,1.8476151844370523,1.8514444709210554,1.8552568380237946,1.8590473931281704,1.862811412878261,1.8665443617357966,1.870241908431839,1.8738999402463232,1.8775145750813482,1.88108217132562,1.8845993355366808,1.88806292799412,1.8914700662005544,1.894818126427563,1.8981047434208815,1.9013278083929772,1.9044854654417076,1.9075761065412187,1.9105983652558223,1.9135511093294126,1.9164334323024454,1.9192446443057662,1.921984262176025,1.9246519990312723,1.9272477534379635,1.9297715982921928,1.9322237695288922,1.9346046547631173,1.9369147819576764,1.939154808201416,1.9413255086725987,1.9434277658521881,1.9454625590425607,1.947430954238305,1.949334094387447,1.9511731900736522,1.9529495106427788,1.9546643757906161,1.9563191476226869,1.9579152231917114,1.9594540275135919,1.9609370070586976,1.9623656237116278,1.9637413491896176,1.965065659907187,1.9663400322725426,1.967565938399548,1.9687448422177714,1.9698781959621479,1.9709674370231145,1.9720139851376721,1.9730192399016477,1.9739845785834613,1.9749113542198906,1.9758008939746694,1.976654497741223,1.9774734369713898,1.978258953712631,1.9790122598369209,1.9797345364452503,1.980426933432464,1.9810905691979237,1.981726530488323,1.9823358723597484,1.9829196182469044,1.9834787601281803,1.9840142587760201,1.9845270440827776,1.9850180154529606,1.9854880422534624,1.9859379643140151,1.986368592470739,1.9867807091462435,1.9871750689603036,1.9875523993656479,1.9879134013039057,1.98825874987721,1.9885890950313918,1.9889050622471054,1.9892072532355922,1.9894962466361459,1.9897725987126536,1.9900368440468976,1.9902894962265507,1.9905310485260823,1.9907619745789908,1.9909827290400037,1.991193748236078,1.991395450805205,1.9915882383221795,1.9917724959106444,1.991948592840845,1.9921168831126466,1.9922777060234753,1.9924313867209407,1.9925782367399631,1.992718554524341,1.9928526259327204,1.99298072472901,1.993103113057341,1.9932200419016965,1.9933317515303972,1.9934384719256428,1.9935404231983598,1.993637815988615,1.993730851851878,1.9938197236314459]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"y":[1.8745781971752136,1.8742822932434733,1.873981476059069,1.8736760766122411,1.8733664689256244,1.8730530724783663,1.872736354629223,1.8724168330241708,1.8720950779730274,1.871771714778597,1.8714474260009257,1.8711229536384857,1.8707991012073844,1.8704767356991852,1.8701567893975197,1.869840261533472,1.8695282197596905,1.8692218014233761,1.8689222146187021,1.8686307389998411,1.868348726336649,1.8680776007961322,1.8678188589341367,1.8675740693832175,1.8673448722243342,1.8671329780319068,1.8669401665837153,1.8667682852292264,1.8666192469119975,1.8664950278438974,1.8663976648308482,1.8663292522516233,1.8662919386928385,1.8662879232445848,1.866319451462111,1.8663888109995204,1.8664983269215314,1.8666503566989818,1.866847284892849,1.8670915175301934,1.8673854761735826,1.8677315916833344,1.86813229766934,1.8685900236265385,1.869107187745292,1.8696861893853078,1.8703294011994223,1.871039160891851,1.8718177625945276,1.872667447845241,1.873590396152629,1.8745887151358753,1.8756644302314405,1.8768194739654478,1.8780556747984916,1.8793747455597827,1.880778271499512,1.8822676980020716,1.8838443180180677,1.8855092592896108,1.8872634714607306,1.8891077131825704,1.8910425393406158,1.8930682885480852,1.8951850710650835,1.897392757316562,1.8996909671928797,1.9020790603242388,1.9045561275238858,1.9071209835943266,1.9097721616854955,1.9125079093836814,1.9153261866949498,1.9182246660669442,1.9212007345685294,1.924251498318214,1.927373789220239,1.9305641740323374,1.9338189657523277,1.9371342372728004,1.9405058372151702,1.9439294078172922,1.9474004047136686,1.9509141184148777,1.9544656972641594,1.9580501716247078,1.9616624790318393,1.9652974900301607,1.9689500344074748,1.9726149275344793,1.9762869965222787,1.9799611059180944,1.9836321826729022,1.9872952401326316,1.9909454008262977,1.9945779178494416,1.9981881946687086,2.001771803202609,2.00532450006371,2.0088422408779603,2.0123211926269686,2.015757743988097,2.0191485136748195,2.0224903568053354,2.0257803693506973,2.029015890734321,2.0321945046726304,2.035314038361547,2.038372560125649,2.0413683756560603,2.0443000229696415,2.0471662662260224,2.049966088540566,2.0526986839307875,2.0553634485312577,2.05795997120791,2.0604880236971255,2.062947550388343,2.065338657861373,2.0676616042814016,2.0699167887460206,2.0721047406697037,2.074226109282175,2.076281653308183,2.0782722308874844,2.080198789785416,2.0820623579363846,2.0838640343550616,2.085604980442958,2.0872864117115384,2.088909589937055,2.0904758157568426,2.0919864217119932,2.093442765736984,2.094846225093092,2.0961981907391243,2.0975000621302193,2.0987532424331232,2.099959134144407,2.1011191350965257,2.1022346348354373,2.1033070113525554,2.104337628153224,2.1053278316434927,2.1062789488168208,2.10719228522235,2.1080691231965663,2.108910720340483,2.1097183082249087,2.1104930913068642,2.111236246040825,2.111948920169091,2.112632232176295,2.113287270893761,2.1139150952401704,2.1145167340857514,2.1150931862279285,2.1156454204671293,2.1161743757721663,2.1166809615253257,2.1171660578379834,2.1176305159282243,2.11807515855261,2.1185007804848146,2.1189081490344583,2.1192980046000076,2.1196710612501453,2.1200280073284867,2.120369506077001,2.1206961962739235,2.1210086928823415,2.1213075877060255,2.1215934500494216,2.1218668273790473,2.1221282459838333,2.122378211632242,2.122617210224218,2.1228457084363113,2.123064154358477,2.123272978121288,2.1234725925124622,2.123663393581775,2.123845761233568,2.1240200598062153,2.1241866386380144,2.124345832619086,2.124497962728972,2.1246433365597,2.1247822488241592,2.1249149818497215,2.125041806057087,2.125162980424385,2.125278752936638,2.1253893610207033,2.12549503196587,2.1255959833303093,2.1256924233336063,2.125784551235631,2.1258725577020052,2.125956625156481]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"y":[1.561487710103795,1.561539359350472,1.561595630932439,1.5616568103614674,1.5617232001185761,1.5617951207669776,1.561872912124425,1.5619569344956754,1.5620475699655882,1.5621452237531428,1.5622503256264102,1.5623633313782013,1.5624847243617805,1.5626150170856166,1.5627547528657117,1.5629045075335102,1.5630648911968172,1.5632365500504815,1.5634201682328628,1.5636164697232495,1.5638262202744724,1.5640502293738916,1.564289352224786,1.564544491738873,1.5648166005292419,1.5651066828914177,1.5654157967585154,1.5657450556145287,1.566095630347709,1.5664687510236952,1.5668657085555986,1.5672878562455639,1.5677366111694853,1.5682134553735245,1.5687199368478937,1.569257670240041,1.5698283372659827,1.5704336867750741,1.5710755344200915,1.571755761881202,1.5724763155892856,1.573239204891324,1.5740464995982153,1.5749003268537207,1.5758028672622728,1.5767563502134199,1.5777630483418428,1.5788252710643906,1.579945357139612,1.5811256662010142,1.5823685692229468,1.5836764378876629,1.585051632833961,1.5864964907818473,1.5880133105438952,1.5896043379524318,1.591271749752083,1.5930176365294708,1.5948439847755704,1.5967526582010618,1.5987453784503771,1.6008237053855785,1.6029890171359091,1.6052424901322235,1.6075850793666826,1.6100174991363556,1.6125402045438664,1.6151533740382733,1.6178568932842363,1.6206503406466928,1.623532974571249,1.6265037231270378,1.6295611759588213,1.6327035788686777,1.6359288312151437,1.6392344862795667,1.642617754706565,1.646075511078665,1.6496043036355983,1.653200367097519,1.6568596384998477,1.6605777758969396,1.6643501797435272,1.6681720167182472,1.6720382457135765,1.6759436456822505,1.6798828450024639,1.68385035200348,1.6878405862800878,1.6918479104187736,1.6958666617604268,1.6998911838335895,1.7039158571081625,1.707935128741473,1.711943541015813,1.7159357581981476,1.7199065915876321,1.7238510225539128,1.7277642234078832,1.7316415759857176,1.7354786878656778,1.739271406174599,1.7430158289764206,1.7467083142680262,1.7503454866375425,1.7539242416667873,1.7574417481824955,1.7608954484802162,1.7642830566603056,1.7676025552273549,1.7708521901127956,1.7740304642856222,1.777136130118259,1.7801681806741034,1.7831258400802747,1.78600855314414,1.7888159743654193,1.7915479564875838,1.7942045387230123,1.7967859347763828,1.7992925207802142,1.801724823245649,1.8040835071206571,1.8063693640370146,1.8085833008168917,1.8107263282997053,1.8127995505402255,1.8148041544198326,1.8167413997043464,1.8186126095740192,1.8204191616441772,1.8221624794885312,1.8238440246714296,1.8254652892901997,1.8270277890243027,1.8285330566841314,1.829982636249016,1.8313780773812505,1.8327209304006784,1.8340127417025702,1.8352550496001054,1.8364493805717401,1.8375972458930052,1.8387001386318516,1.8397595309864745,1.8407768719445727,1.841753585243215,1.8426910676088557,1.8435906872575374,1.8444537826359153,1.8452816613844372,1.8460755995047506,1.8468368407142055,1.8475665959711671,1.848266043155677,1.8489363268908945,1.8495785584915685,1.8501938160266795,1.8507831444841945,1.8513475560267145,1.8518880303275718,1.8524055149776968,1.8529009259543148,1.8533751481432155,1.85382903590702,1.8542634136924796,1.8546790766704617,1.855076791402809,1.8554572965308107,1.8558213034804967,1.856169497180442,1.856502536788185,1.8568210564217627,1.8571256658932445,1.8574169514414738,1.8576954764615532,1.8579617822288832,1.8582163886158531,1.8584597947995045,1.8586924799587194,1.8589149039596937,1.8591275080286351,1.8593307154107908,1.8595249320150735,1.8597105470436754,1.859887933606207,1.8600574493179787,1.8602194368821812,1.8603742246557673,1.8605221271989607,1.860663445808354,1.860798469033635,1.8609274731780439,1.86105072278269,1.8611684710949246,1.8612809605209764,1.8613884230631133,1.8614910807415994,1.8615891460017506,1.8616828221064108]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.7038546637951444,1.7039788026276348,1.70411040051858,1.7042498359846723,1.7043975057360676,1.7045538255737602,1.704719231317312,1.7048941797618964,1.7050791496634068,1.7052746427501602,1.7054811847594675,1.7056993264970706,1.7059296449171255,1.7061727442200771,1.706429256965372,1.7066998451955453,1.7069852015677431,1.707286050488227,1.707603149244848,1.7079372891318558,1.708289296560735,1.7086600341500229,1.7090504017862609,1.7094613376473504,1.7098938191786222,1.7103488640108946,1.7108275308086416,1.711330920035155,1.7118601746202313,1.7124164805144464,1.7130010671124891,1.7136152075263127,1.7142602186870364,1.7149374612525803,1.7156483392959792,1.7163942997471848,1.7171768315590186,1.7179974645657352,1.718857768000545,1.719759348636411,1.7207038485126085,1.7216929422080047,1.7227283336208274,1.7238117522140843,1.7249449486857706,1.7261296900237781,1.7273677539071288,1.728660922417931,1.7300109750324282,1.7314196808648494,1.7328887901445416,1.7344200249151966,1.7360150689548945,1.7376755569272317,1.7394030627868924,1.7411990874776324,1.743065045976568,1.7450022537557055,1.7470119127495394,1.7490950969359076,1.7512527376557314,1.7534856088152975,1.7557943121318644,1.7581792625990047,1.7606406743617058,1.7631785472022312,1.7657926538455897,1.76848252829761,1.771247455428722,1.7740864620121632,1.7769983094163138,1.7799814881370395,1.783034214337384,1.7861544285388156,1.7893397965809212,1.7925877129353487,1.7958953064256122,1.7992594483678268,1.8026767631093346,1.8061436409035052,1.8096562530206408,1.8132105689578784,1.816802375576196,1.820427297960976,1.8240808217748288,1.827758316848212,1.8314550617353316,1.8351662689502206,1.8388871105910056,1.842612744059175,1.8463383375850988,1.850059095280738,1.8537702814551285,1.8574672439471562,1.8611454362528612,1.8648004382502121,1.8684279753523274,1.8720239359496729,1.8755843870321456,1.8791055879124166,1.8825840020018143,1.886016306618802,1.889399400837193,1.8927304114062913,1.8960066967977267,1.8992258494537304,1.9023856963287007,1.9054842978301727,1.9085199452766524,1.9114911569983137,1.9143966732124114,1.9172354498085706,1.9200066511801435,1.9227096422367367,1.9253439797301215,1.9279094030212602,1.9304058244103959,1.9328333191453075,1.935192115215154,1.9374825830290792,1.9397052250700861,1.9418606656058575,1.9439496405292964,1.9459729873927984,1.9479316356917165,1.949826597444261,1.9516589581072707,1.953429867859945,1.955140533280804,1.9567922094368562,1.9583861923982113,1.9599238121862204,1.9614064261585882,1.9628354128308334,1.9642121661299052,1.9655380900726946,1.9668145938595698,1.9680430873708932,1.9692249770526888,1.9703616621762243,1.971454531455168,1.9725049600032112,1.973514306614489,1.974483911348855,1.9754150934039585,1.9763091492561473,1.9771673510524512,1.977990945236244,1.9787811513896394,1.9795391612762077,1.9802661380682118,1.9809632157432024,1.9816314986355144,1.9822720611289142,1.982885947477384,1.9834741717417557,1.984037717830653,1.9845775396349083,1.9850945612453539,1.9855896772445525,1.9860637530637253,1.9865176253967707,1.986952102663885,1.9873679655178955,1.9877659673869648,1.9881468350478764,1.9885112692245899,1.9888599452072557,1.9891935134872918,1.989512600404571,1.989817808803128,1.9901097186921746,1.9903888879095362,1.9906558527849407,1.9909111288008636,1.9911552112489195,1.9913885758800065,1.9916116795466592,1.9918249608362495,1.9920288406938715,1.9922237230339193,1.9924099953395094,1.9925880292490539,1.9927581811294073,1.9929207926351329,1.993076191253526,1.9932246908351399,1.9933665921096322,1.9935021831868243,1.9936317400429373,1.9937555269920246,1.9938737971426688,1.9939867928400596,1.9940947460936116,1.994197878990304,1.9942964040939697,1.99439052483077,1.9944804358611272,1.994566323438391,1.9946483657545426],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.945824126804059,1.9457259430587088,1.9456295078725656,1.945535264775067,1.9454436943911975,1.9453553162799833,1.9452706907580508,1.9451904206944985,1.9451151532622546,1.945045581630139,1.9449824465789034,1.9449265380237801,1.9448786964253908,1.9448398140704075,1.944810836203045,1.9447927619884022,1.944786645288801,1.944793595234699,1.9448147765724142,1.9448514097718608,1.9449047708787377,1.9449761910971353,1.9450670560903207,1.9451788049895158,1.9453129291027733,1.945470970318511,1.9456545192008938,1.9458652127769624,1.9461047320181175,1.946374799021272,1.946677173897521,1.9470136513785286,1.9473860571528845,1.9477962439463525,1.948246087361161,1.948737481490173,1.9492723343219076,1.9498525629518662,1.9504800886144822,1.951156831548222,1.9518847057039668,1.9526656133038602,1.9535014392543788,1.9543940454136288,1.9553452647089014,1.956356895096549,1.9574306933524355,1.958568368677841,1.9597715761029417,1.961041909668133,1.9623808953627286,1.963789983801176,1.9652705426190922,1.9668238485752834,1.9684510793516237,1.9701533050502196,1.9719314793967595,1.9737864306701711,1.9757188523915834,1.9777292938198343,1.979818150316062,1.98198565365588,1.9842318623837703,1.986556652320126,1.9889597073462664,1.9914405106061424,1.9939983362747542,1.9966322420519789,1.9993410625459804,2.0021234037122633,2.004977638512313,2.0079019039494552,2.010894099628858,2.0139518879736538,2.017072696209955,2.0202537202105786,2.023491930260882,2.026784078780955,2.030126710007061,2.033516171602636,2.036948628135988,2.0404200763291303,2.0439263619507018,2.047463198196526,2.051026185374897,2.0546108316907286,2.0582125749038984,2.0618268046229384,2.0654488849858548,2.069074177475565,2.0726980636181374,2.076315967317582,2.079923376591076,2.0835158644828313,2.0870891089527825,2.0906389115573405,2.0941612147629796,2.0976521177587495,2.1011078906602414,2.1045249870244476,2.107900054621758,2.1112299444373877,2.1145117178994104,2.1177426523538103,2.1209202448282056,2.1240422141449113,2.1271065014605566,2.130111269323493,2.1330548993516416,2.1359359886423013,2.138753345031802,2.1415059813269264,2.1441931086318733,2.146814128894379,2.149368626792729,2.1518563610819195,2.154277255512491,2.15663138942972,2.158918988154162,2.1611404132372356,2.1632961526777574,2.1653868111773127,2.1674131005042225,2.169375830027769,2.171275897476413,2.1731142799660508,2.174892025337035,2.1766102438317527,2.178270100138057,2.1798728058178845,2.1814196121348792,2.182911803289875,2.1843506900686194,2.185737603902179,2.1870738913369565,2.188360908908271,2.1896000184088598,2.190792582541497,2.1919399609431642,2.193043506566742,2.194104562405105,2.1951244585416525,2.1961045095107505,2.197046011951197,2.1979502425356796,2.1988184561592172,2.1996518843697506,2.200451734024319,2.2012191861546895,2.2019553950267587,2.2026614873786112,2.2033385618227155,2.203987688398383,2.2046099082612844,2.2052062334974933,2.205777647050232,2.206325102748192,2.2068495254249636,2.2073518111198194,2.2078328273507273,2.2082934134511314,2.208734380962639,2.209156514076356,2.2095605701161865,2.2099472800579263,2.2103173490785335,2.210671457130401,2.2110102595359438,2.2113343875982268,2.2116444492237703,2.211941029554027,2.212224691602402,2.212495976893987,2.2127554061054906,2.213003479703128,2.213240678576487,2.2134674646666177,2.2136842815868234,2.213891555234815,2.2140896943950863,2.214279091330523,2.2144601223624125,2.214633148438164,2.214798515686166,2.214956555957322,2.2151075873529065,2.21525191473848,2.215389830243671,2.21552161374772,2.2156475333507286,2.215767845830636,2.2158827970859694,2.2159926225644853,2.216097547677837,2.2161977882024493,2.216293550666807,2.2163850327253862,2.216472423519484,2.2165559040252125,2.2166356473889475],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.8386863268089417,1.8379270616987116,1.8371465972778025,1.8363449328145989,1.8355221208751724,1.834678271143825,1.8338135542957819,1.8329282059062346,1.8320225303786366,1.831096904873941,1.8301517832212917,1.829187699789618,1.8282052732985683,1.8272052105463714,1.8261883100314222,1.8251554654437712,1.8241076690022024,1.823046014612233,1.8219717008201708,1.8208860335383383,1.8197904285166824,1.8186864135362746,1.817575630300643,1.8164598360014494,1.8153409045357431,1.8142208273528684,1.8131017139100143,1.8119857917164206,1.8108754059472962,1.809773018609563,1.8086812072425722,1.8076026631378899,1.8065401890631043,1.8054966964752717,1.804475202210115,1.8034788246333253,1.8025107792402923,1.8015743736902723,1.8006730022604032,1.7998101397040824,1.7989893344970915,1.7982142014535385,1.7974884136922582,1.7968156939329223,1.796199805099849,1.7956445402106143,1.7951537115261824,1.7947311389396674,1.7943806375822473,1.7941060046273842,1.7939110052786893,1.7937993579326519,1.7937747185153399,1.793840664002151,1.7940006751419546,1.7942581184215003,1.7946162273227537,1.795078082944779,1.7956465940825748,1.796324476877662,1.7971142341786448,1.798018134773934,1.79903819268261,1.8001761467122783,1.801433440513916,1.802811203382244,1.8043102320652518,1.8059309738572968,1.8076735112559188,1.8095375484625147,1.8115224000007601,1.8136269817137793,1.8158498043814304,1.8181889701727074,1.8206421721155093,1.8232066967273466,1.8258794299067815,1.8286568661373968,1.8315351210050512,1.8345099469763348,1.8375767523328481,1.8407306231036002,1.8439663477878248,1.8472784446141721,1.8506611910407795,1.854108655165167,1.8576147286841576,1.861173161022682,1.8647775942368157,1.8684215982908832,1.8720987063108223,1.8758024494259746,1.879526390828478,1.8832641587028325,1.8870094777071076,1.8907561987207415,1.8944983266109139,1.898230045808983,1.901945743529502,1.9056400305057384,1.9093077591566214,1.9129440391396733,1.9165442502820722,1.9201040529169116,1.92361939568346,1.9270865208784438,1.930501967469819,1.9338625719050373,1.9371654668624492,1.940408078107274,1.9435881196226812,1.9467035871921718,1.9497527506119114,1.952734144711251,1.9556465593567167,1.9584890286095928,1.961260819200231,1.9639614184737126,1.96659052195182,1.9691480206457204,1.9716339882426368,1.9740486682783231,1.9763924613955761,1.9786659127775543,1.980869699833444,1.983004620203179,1.985071580137606,1.9870715833007448,1.9890057200317104,1.9908751570954695,1.9926811279439345,1.9944249235019345,1.9961078834863824,1.9977313882614156,1.9992968512274347,2.0008057117377605,2.0022594285330166,2.003659473680323,2.0050073270018625,2.006304470975363,2.007552386087426,2.0087525466194567,2.0099064168450615,2.0110154476172744,2.0120810733236594,2.013104709187342,2.014087748892142,2.0150315625103485,2.0159374947121296,2.016806863236176,2.017640957601853,2.0184410380438953,2.019208334651504,2.019944046694532,2.020649342120349,2.0213253572058423,2.0219731963499235,2.0225939319927684,2.023188604648927,2.0237582230422593,2.024303764331499,2.0248261744160447,2.0253263683123497,2.025805230592011,2.0262636158733676,2.026702349359082,2.027122227412809,2.027524018168647,2.0279084621676375,2.0282762730160835,2.0286281380609683,2.0289647190781963,2.029286652969812,2.029594552466743,2.0298890068339794,2.0301705825754435,2.030439824136099,2.0306972545991604,2.0309433763765026,2.031178671890617,2.0314036042466954,2.0316186178935935,2.0318241392726435,2.0320205774534155,2.0322083247557066,2.0323877573571583,2.0325592358860143,2.0327231059986737,2.032879698941752,2.033029332098482,2.03317230951935,2.0333089224369347,2.0334394497649786,2.033564158581776,2.0336833045980103,2.0337971326092106,2.033905876933036,2.034009761831627,2.0341090019192922,2.03420380255581],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.5478979657445637,1.5479551579004687,1.5480170485982798,1.54808392771605,1.54815610062101,1.5482338889214475,1.5483176312460736,1.5484076840515342,1.548504422458875,1.5486082411199333,1.5487195551148152,1.5488388008818064,1.5489664371812786,1.549102946095335,1.549248834065148,1.5494046329681117,1.5495709012370822,1.5497482250240942,1.5499372194109862,1.5501385296693684,1.5503528325722482,1.550580837759421,1.5508232891583718,1.551080966461933,1.551354686663222,1.5516453056474608,1.5519537198391056,1.5522808679012212,1.552627732482269,1.5529953420032996,1.5533847724760343,1.5537971493393508,1.5542336492983224,1.5546955021461246,1.5551839925448474,1.5557004617365386,1.5562463091506586,1.5568229938686367,1.5574320359003897,1.5580750172216447,1.5587535825147467,1.5594694395495416,1.5602243591349958,1.5610201745667034,1.5618587804905382,1.5627421310986627,1.5636722375712093,1.5646511646754615,1.5656810264345704,1.566763980780062,1.567902223106865,1.569097978656594,1.570353493664583,1.5716710252188046,1.5730528297945003,1.574501150447027,1.5760182026670946,1.5776061589270067,1.5792671319734486,1.5810031569513896,1.5828161724742351,1.5847080007868588,1.5866803271997851,1.5887346790037589,1.5908724041033024,1.593094649634664,1.5954023408568367,1.5977961606231124,1.6002765297540578,1.6028435886400347,1.605497180401831,1.6082368359310855,1.6110617611177591,1.6139708265498158,1.6169625599408102,1.6200351415045895,1.6231864024535725,1.626413826748939,1.6297145561787025,1.633085398784314,1.6365228405995693,1.640023060608667,1.6435819487747314,1.6471951269374951,1.650857972330436,1.6545656434247307,1.6583131077709488,1.66209517148029,1.6659065099659003,1.6697416995517909,1.6735952495520745,1.6774616344265583,1.6813353256296775,1.6852108227877929,1.6890826838641426,1.6929455540003724,1.6967941927575516,1.7006234995168286,1.704428536839363,1.7082045516257844,1.711946993956253,1.7156515335322362,1.7193140736796317,1.722930762909116,1.7264980040630222,1.7300124611082415,1.7334710636612118,1.736871009353901,1.7402097641686658,1.7434850608850077,1.7466948957926935,1.749837523833603,1.7529114523392575,1.7559154335326097,1.7588484559615591,1.7617097350282318,1.7644987027726082,1.767214997061952,1.7698584503290478,1.7724290779927434,1.7749270666840904,1.7773527623906469,1.7797066586205998,1.781989384677368,1.7842016941245467,1.7863444535105273,1.78841863141202,1.790425287846126,1.7923655640916012,1.7942406729515972,1.7960518894824764,1.7978005422062897,1.7994880048182065,1.8011156883945354,1.802685034102015,1.8041975064047073,1.8056545867610758,1.8070577678006703,1.8084085479671577,1.8097084266122823,1.810958899523586,1.81216145486738,1.8133175695274775,1.814428705819507,1.8154963085602545,1.8165218024713008,1.8175065898962999,1.8184520488114575,1.8193595311091602,1.8202303611352018,1.8210658344606676,1.8218672168702117,1.822635743549212,1.8233726184530754,1.8240790138427885,1.8247560699716399,1.8254048949088915,1.8260265644870166,1.8266221223599568,1.8271925801606654,1.8277389177470078,1.8282620835258554,1.8287629948459596,1.8292425384509,1.8297015709840876,1.8301409195384517,1.8305613822440474,1.8309637288874068,1.831348701557,1.831717015309686,1.8320693588535069,1.8324063952426362,1.8327287625807016,1.8330370747290852,1.8333319220171806,1.833613871951891,1.8338834699239843,1.834141239909181,1.8343876851621286,1.83462328890163,1.834848514985735,1.8350638085754802,1.8352695967862536,1.8354662893259233,1.8356542791190076,1.8358339429163144,1.83600564188958,1.8361697222107578,1.8363265156157031,1.8364763399520787,1.836619499711396,1.8367562865451648,1.8368869797651877,1.8370118468280927,1.8371311438042421,1.8372451158311902,1.8373539975519113,1.8374580135380338,1.8375573786983541,1.8376522986729218],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.5539016850448126,1.5539671661993402,1.5540392132115421,1.5541182561438835,1.5542047509870538,1.5542991811943443,1.5544020592669014,1.5545139283854523,1.554635364083367,1.554766975955176,1.5549094093938616,1.555063347349443,1.5552295121005486,1.555408667029812,1.55560161839309,1.555809217071625,1.5560323602954393,1.5562719933253906,1.5565291110804929,1.5568047596963044,1.5571000379994016,1.5574160988822057,1.557754150561711,1.5581154577049767,1.558501342403579,1.5589131849785753,1.5593524245969215,1.5598205596796288,1.5603191480813519,1.5608498070203995,1.5614142127375,1.5620140998608858,1.5626512604544618,1.5633275427249436,1.5640448493629093,1.564805135491681,1.5656104061969083,1.566462713608629,1.567364153506531,1.5683168614181269,1.5693230081787584,1.5703847949217005,1.571504447466427,1.5726842100733087,1.5739263385338542,1.5752330925672156,1.5766067274962063,1.5780494851797042,1.5795635841831615,1.58115120917521,1.5828144995461162,1.5845555372532254,1.5863763339095949,1.588278817144767,1.5902648162809956,1.5923360473841577,1.594494097765813,1.5967404100311924,1.5990762657869493,1.6015027691418884,1.6040208301530967,1.6066311483884015,1.6093341967932828,1.6121302060656044,1.6150191497542257,1.6180007303070123,1.621074366299433,1.624239181076282,1.6274939930356265,1.6308373077755733,1.634267312310644,1.6377818715454395,1.641378527168996,1.6450544991040628,1.6488066896119864,1.6526316901165892,1.656525790770162,1.6604849927423877,1.6645050231696648,1.6685813526590088,1.6727092151984988,1.676883630286303,1.681099427053535,1.6853512701236442,1.6896336869233983,1.6939410961385573,1.6982678369914208,1.7026081990079702,1.7069564519393308,1.7113068755057175,1.7156537886406313,1.7199915779283559,1.724314724948301,1.7286178322646477,1.7328956478283832,1.7371430875903067,1.7413552561570798,1.7455274653570712,1.7496552506177303,1.7537343850908085,1.7577608914951692,1.7617310516786413,1.7656414139297916,1.7694887980972749,1.773270298598186,1.7769832854174457,1.7806254032175268,1.784194568691802,1.7876889663054782,1.7911070425756406,1.7944474990465054,1.7977092841178395,1.8008915838838786,1.803993812137251,1.807015599687706,1.8099567831391208,1.8128173932606362,1.815597643079125,1.8182979158107795,1.8209187527396824,1.823460841141015,1.8259250023362417,1.8283121799573852,1.8306234284875025,1.832859902134821,1.8350228440887866,1.8371135761975934,1.8391334890986681,1.841084032826089,1.842966707912087,1.8447830569935777,1.8465346569291166,1.8482231114267604,1.849850044178997,1.8514170924971773,1.8529259014347064,1.8543781183855659,1.8557753881425514,1.8571193483978394,1.8584116256671284,1.8596538316175721,1.8608475597790393,1.8619943826177947,1.8630958489515272,1.864153481684687,1.8651687758432998,1.8661431968888091,1.8670781792909792,1.8679751253404975,1.8688354041826045,1.8696603510538115,1.8704512667045794,1.8712094169916402,1.8719360326245134,1.8726323090516037,1.8732994064721433,1.873938449961083,1.8745505296948644,1.875136701266844,1.8756979860818974,1.8762353718205247,1.8767498129634936,1.8772422313687613,1.8777135168930836,1.8781645280513481,1.8785960927072745,1.8790090087896716,1.8794040450289875,1.8797819417093733,1.8801434114319395,1.8804891398853294,1.880819786620118,1.8811359858239245,1.8814383470944693,1.8817274562081172,1.8820038758817466,1.8822681465260453,1.8825207869885878,1.8827622952852596,1.8829931493188137,1.8832138075835179,1.8834247098550296,1.8836262778647828,1.8838189159583107,1.8840030117370556,1.8841789366833261,1.8843470467681631,1.8845076830419665,1.8846611722078148,1.8848078271774793,1.8849479476101945,1.8850818204343112,1.885209720351992,1.885331910327166,1.885448642056981,1.8855601564270366,1.8856666839506961,1.885768445192802,1.8858656511781398,1.8859585037850073],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=0.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN_log = visuRMSEGrid(ElasticNet(), 'EN', alphasEN_log, 'alpha',\n","                                 GridEN_log, BestParametresEN_log,\n","                                 'elasticnet__l1_ratio')\n","FigRMSEGRidEN_log.show()\n","if write_data is True:\n","    FigRMSEGRidEN_log.write_image('./Figures/ConsoGraphRMSEEN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                     35\n","                             R²      RMSE       MAE\n","KNeighborsRegressor()  0.384768  1.680438  0.830577\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logkNN=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[20.949204435006916,20.703809647732033,19.773689506611564,19.368804521071013,21.827335055702296,20.274617380713277,21.899294853236217,19.05134286709525,19.935199228502306,21.982943737220282,19.854314355366878,20.98634446877412,20.015229801962516,19.531656462455683,22.97999330283146,23.682708729171186,20.52749260797385,19.93124344457044,24.98460378656429,21.029222695389326,20.739131807033782,19.576970296100882,19.36620288905055,19.749125902637505,19.800479418900416,21.072497209022277,20.066688794129107,22.022225093720532,20.635061837645306,19.652879416389087,20.57810158804025,20.45868322421906,21.72134459471919,22.361125654642894,19.645530238741365,21.25582223013157,20.466297980662617,23.85703872710407,20.358608445551965,19.739203460359533,20.926065055556823,22.3809304645984,19.70044560911188,23.54696931487492,20.21546052704444,19.857303922178634,19.656906028641952,20.02802401113801,19.380954279337193,19.71193383082141,21.056440578258563,20.26939718651956,19.116570519476475,20.783781115823054,21.64799619064791,22.62660609344828,19.584532731611436,21.671782015612283,20.528072266244486,19.021846560632678,22.837048508464367,20.156521341737726,20.61416122269547,20.312857564429493,19.744089684872762,20.736384326349505,20.315425595770563,19.757333965740607,19.912718248805145,20.148520983357955,22.162266576295163,20.650660992831018,21.192371428514285,19.89211734945677,21.170684498762537,19.59531422295543,20.767091000388426,20.45810343090893,20.916558599512204,19.355974828397045,20.404680562676695,21.81677304342319,20.189612060109038,20.750930635339014,20.894012958475244,19.794157123498895,20.763924569230458,19.79459058927883,19.754442556677695,19.76136823508541,20.244885547260164,19.81788886291968,22.205463765176937,20.374442758333274,22.298926900486254,23.827220270053303,21.59518068342498,24.850454145261875,21.375986022121346,19.75099983026706,22.352257126836594,23.26635923669517,20.491377128583096,19.461437599570544,20.80660696594909,21.486589485875417,19.791028128912505,22.042210921456636,19.80297366746008,21.14335809060782,22.564666661781136,20.72169293093731,20.209262309890505,24.007614240208255,22.521003861614037,19.918463548980696,21.858036023776016,22.561507939571523,19.30721551278584,22.009470153668172,24.523837422313687,19.894587608515945,19.900692464020125,20.301379353063602,19.431547481644873,21.1077659260451,20.436079342500456,22.34488107121927,19.696415370021796,21.678051137555762,19.08983197365328,19.03488836643939,22.4340933213921,19.26090136477759,19.542060416191394,21.010715930952074,21.302367171163144,19.7318932358685,20.918751549927865,20.65170483547597,19.941169295201103,19.711933830821405,21.007914785447035,19.42326910546221,20.685629318905036,20.106713339371677,22.761677560692068,19.721503751190976,19.578605143009096,19.372897128150942,21.370787556649848,19.074382098843063,19.752291988440533,21.25426012458157,20.517581254014484,19.762713175517696,20.346273382135408,20.36975386157304,21.340379954816225,21.206539664238782,21.66709916566081,20.19394769867339,19.706022127914824,20.47004129949347,22.55866390914977,24.824135111681525,21.027395996215397,19.658757972378176,20.103765440735653,20.231307159221302,21.886854854061955,21.394461394218165,20.229241613767968,20.939965887306744,20.490327652568926,19.71547974060932,20.868226311265726,20.733159564748586,21.684017731196445,20.626027831429184,24.031667361554582,20.62030989224545,20.363448315139994,21.686674194368155,22.25519107916246,21.02872102382361,20.609858501832278,22.86308117596647,19.612034087914612,19.837976921994912,20.912241506874075,19.706022127914824,20.412045904463568,19.696883548756958,20.187399992636852,19.258171592935703,20.849929113890813,19.629773006415093,20.69661242596817,19.53651033112043,23.48960914624039,19.292825177012585,20.535554331027154,23.771569312906006,19.55240247471278,20.101143565156043,21.947578380349967,19.650439546798005,19.43436182784177,20.424950396590557,20.256206005719235,22.64527380522236,20.9684099201994,19.50571601562944,21.371491908472034,24.1717510279898,19.775496247757964,19.369545103140968,21.766099878678023,20.123601164311044,19.69459059070773,21.22638032304244,21.958550801697232,19.800892863808762,20.24971295729743,23.040185994915714,20.50411974271207,22.81607530532653,20.3501203425817,21.05173246938839,20.083447891297567,21.477656493977683,19.752235131859265,21.2076731417113,21.931436696828747,20.10237071233604,22.467523416939304,19.47697693560705,20.940936387026273,19.49082450589103,19.85005091091503,21.151538494845948,20.35034108303557,21.4696789424575,21.832116999361205,19.80297366746008,24.16905204251731,20.38039151948225,22.302997993966784,19.969755872407546,23.919779662267633,20.654616376629725,19.775933631653466,21.157449305220197,20.284422280878815,20.264988764449892,19.763886202748456,19.802973667460083,20.975792318029583,20.861047346280948,23.353418356149994,24.346840164739856,23.30383805202602,20.893560203065608,19.610359788701647,19.648431571235733,21.300526616504307,22.262438452568706,19.397332725053143,20.42677840159758,19.8672301586202,22.535173651542635,23.049218173791704,20.629129179652367,19.73522244551261,20.736367845633882,19.43215088336941,20.974479982846837,19.83164921967834,19.43998837098891,19.72563446798919,19.566049131055486,21.52662764782048,19.485273675023848,20.308822177340797,20.195623817238516,21.606424987508163,20.91514103148243,22.984097417746742,23.213300566171934,21.94697616591378,19.92999083074094,19.66152624640132,20.26239225589328,19.914942049009614,20.609328003484023,19.335821990174253,19.702720303968817,19.600703392764057,21.566573182432354,19.57405861285191,20.42887541841321,20.445995643504602,19.50959706438322,19.958919324300165,19.573024495733986,20.038306882023104,20.092065472986814,19.375026777675956,19.50433177824218,20.89068632287434,21.136076396632852,19.645530238741365,20.68030638370191,21.592263370298998,18.95817714559179,19.037569698367914,19.869506985604545,20.20796558499378,20.99333852459952,21.114274655708204,21.683659736746343,19.90801905859982,21.560325230983832,20.4633163772932,21.42628076049698,21.09490488412318,22.399670822685383,20.58774614244597,19.32251261484015,19.798953512448996,19.817027534314626,21.326678437136383,19.680899530864885,25.193637026776198,19.32003696673768,23.652435603419743,21.175025359742804,20.870927581595655,22.166122504051,20.540770669128314,21.145694065929238,20.900562238773883,20.716981220165472,19.563326967386498,21.534091332107373,20.667318589779295,20.33033367494243,19.337097836314953,19.75570999621096,22.473082796958668,25.71163851514793,19.513586433476817,19.71139482303308,19.44184516721985,20.156521341737726,21.344917254615744,19.800479418900416,19.696883548756954,20.219004194051806,20.415237130789052,19.721404811528647,19.748986996072116,21.153057259737523,21.81679761329353,23.776592549590838,19.727253573397288,22.879989782840013,19.969755872407546,19.925186206082248,19.64187846190949,21.081012362383838,20.562150801441664,19.659246748813604,20.443304832622083,21.377425518122855,19.582258492703517,20.736384326349505,23.374505915897867,20.500969565628548,20.541285610256278,21.038636592862535,22.313547415898245,23.93969274137759,21.149971578969918,20.71929245824711,19.93868570798532,20.3673943111254,20.329364948267813,22.465084675107548,20.33990777592516,21.09821581634908,19.470704368749644,21.881300754194,20.697499378205478,20.602660914970084,19.912594387241946,20.632996156428007,21.982943737220282,19.73407466392161,21.901258564954663,20.44395632753266,20.232215722707817,20.999947163808965,20.40991945433725,21.103503969400418,22.20855072183243,20.19689799411773,19.723622380316687,21.448901508050547,20.92674531992545,19.499296345817637,20.961451610620216,20.986521485649494,20.007111731815478,19.580501618503643,20.020696428899072,21.81157634739094,21.94098589345445,21.09704077735697,19.74497536344133,19.97539217975357,19.94846903895769,21.21185260389525,21.685788453693007,20.8807279538682,22.06707775243856,19.113980559692774,20.732429852348968,20.704487803562124,20.290558285772466,23.03861567369039,21.710411647179814,21.856369900619207,21.23430501960784,22.184702544269577,23.585078133135266,20.308463121365296,19.59265182412601,20.70408544119352,19.727137797175562,22.83154211334599,20.55429726363853,21.108308342929963,21.041022568263173,20.742161521983714,19.92657588708486,22.66147836150472,21.117809596741516,19.928615809535575,20.410297471451106,19.11870604519605,19.71685801372155,20.28878607223822,19.884029917200262,20.56849795657985,19.88742709291957,19.78557636679911,22.310836039995404,20.82300936297596,20.52884911634994,20.363448315139998,21.372582571534863,20.814727726309236,19.559669327666086,22.38914379366906,22.212763618729277,20.27528725899089,20.43681209028594,19.311061347052878,20.90053647671847,20.549877814666143,24.441334173910903,19.754729830587678,21.21103539834849,20.662132216167382,21.54934135322558,20.571541615013427,20.072671621916943,22.125124195018582,20.51493851389014,19.32251261484015,20.21109770979989,21.99466931410471,19.281356553370887,20.602660914970084,19.549000710374465,23.129167080477362,20.412045904463568,20.20577755688168,23.25520052744485,20.38380331347033,21.74413514533903,20.89068632287434,20.672380146564343,20.601865558725432,20.32954199579034,21.771161832533753,19.527811434151957,20.603507505302098,19.93449041353379,20.86757534378024,20.874965301678568,20.432784826827458,20.13047866824718,19.54741688299745,19.690427349928974,22.964177651388045,21.966610241945762,20.05895094380361,22.619413208583605,21.146267845775796,22.812278608312408,22.92262792893223,22.3525772282665,20.133613886480433,20.394277928700433,19.28245422899424,20.163061600908698,19.73407466392161,22.2277253095353,21.97061160190147,19.638996440869064,21.644678978060654,21.94700153690837,20.83397678890979,21.99826151304756,19.596372142083432,20.407751930154532,19.37250563888125,20.603507505302098,19.39072363623728,21.086968717300348,23.43379607879192,24.832535488979886,20.404680562676692,22.42528575713883,20.632967090376646,22.88716623605071,20.091178436556767,20.075129954103158,24.993456044104082,22.863738018200497,20.08938299558331,20.649868378964293,20.200809148507563,19.650439546798005,20.457721138318636,22.903156415978764,19.6890268059992,19.487681384357426,21.10202431418385,19.80297366746008,22.54880581036694,20.004642441742355,21.62575426540728,19.646457050588147,23.830434192878823,20.483734219937254,18.98928418977938,19.457640182214146,20.462611014307676,19.397332725053143,21.382455627080358,22.151548495892776,19.991765072775586,21.38920848259312,19.75192033963469,20.136809029679974,21.538134994601872,19.662458383679013,23.098121386525044,19.94790516354201,20.85155864491667,20.350937607548726,21.22338986693278,19.665292694135207,19.645530238741365,20.42964657050805,19.743905360828112,24.605602281699685,20.150800248049073,20.306562550779713,22.625821833849233,20.174252238377655,23.575616390667747,19.66152624640132,21.143874704750182,19.222517583056717,20.05180732697514,20.060276579691244,21.093593192813596,21.369626901026706,19.749125902637502,19.541457668558213,19.81655140708402,19.45444437852801,19.449079240458406,20.435141073735803,20.527625274028058,21.148109556673393,21.533641939359974,21.787340145801295,20.021903830177127,20.106302785815938,22.6941737166439,19.390393721177503,21.25184522158231,20.547309238658013,20.193003225130372,23.778432740173905,20.925356533351586,21.279828425915237,20.245145518070277,20.56849795657985,19.543212889627,19.84461838435598,19.427019624030553,19.35597482839704,20.12528173120691,22.277617689910034,20.556208773461446,23.578064299468654,20.709112754330558,19.871830428784232,21.52939636597079,20.36140048516507,20.906540961215672,22.256363291859657,20.606212663320928,19.661526246401323,21.777417223438974,20.620602203966815,19.683088088627212,19.107625239362488,22.82960485060113,19.996074624689022,22.501344318383598,19.369545103140965,20.27782074587666,22.25400540423103,21.79458872623436,21.40607993181014,20.87774356010057,20.525450946750492,21.903969982856232,20.5244282745064,19.207228681057273,20.52153937108772,20.710322233990308,24.904532081584076,20.355856764372287,19.074382098843063,19.81086613320245,23.94768391804118,20.309469334278237,20.479789744502224,19.770614084445935,19.798840203747645,22.29568675288097,19.636188954447647,23.528559388904238,20.19395138998999,20.174050942785563,20.273205037797506,21.066826802910043,22.041408625632894,21.93572260688841,19.645633146818998,21.03404402167077,20.255256684097784,21.3810339243104,19.571251905429243,23.845346985775052,22.614905565701502,19.75302431147593,20.445016638840833,20.342426696099743,19.67822789831878,20.648371333153033,20.58735781139656,20.661828524894062,24.93171528080608,21.140937174609373,19.107372877729077,21.297954515645017,20.81086402970129,21.52326377101892,20.485243200951626,19.83728032482175,20.18771012459453,19.797651821613947,19.70055504198482,20.9098888189344,21.31118833788644,20.542100379293142,20.32118409608544,20.857219856361375,24.357043609008034,19.47481661090876,23.624147512212176,19.870550194477254,21.041443491054,20.24190978546767,20.366558932970456,19.94932383227889,19.914986805531427,19.900544143388398,20.38504790991361,22.4591024365152,19.544392839485727,22.802446299815585,19.650651204109483,20.716280303930812,20.760195764086777,24.449784478318048,21.72451675025837,21.72797178931985,25.148482556534777,20.37669180444495,20.495523551752072,21.37220666067342,20.737342909580452,20.214845768963947,23.960735243598553,20.72641896244172,22.164740724225076,19.985654628820022,22.11627253073143,19.402801126589917,20.71152044734858,25.133870920701995,23.2778056196684,20.36741921854582,21.236447477710428,21.28275945048298,19.348971359118032,20.459167891207528,19.51926997657632,21.175684379201186,19.661064167145987,19.643797731530643,19.3952442797573,21.201175107994704,21.156964649302417,19.773211530334976,19.866220502402086,19.67935414505822,21.818061294528427,22.901267488522116,19.89114300464975,19.754519976665755,19.516756189036283,21.735906298186244,21.148429518958455,19.6316643043697,19.465626639886615,19.946402072039906,23.426891375992493,22.201078316295035,21.099430589811956,19.696883548756958,19.746808257117543,19.390300255924455,19.13084699328047,21.473078583887833,20.367922521342244,20.648885087568893,21.540403099632808,19.652518790533925,20.206219300540475,20.268271717252027,20.140334973467887,20.729687002561334,22.08264762137766,20.857340649727913,19.81578600689889,20.64320955475918,20.253111638281666,19.388238228882955,20.264907704124273,21.467073718467145,23.154568969501366,19.856813140257824,22.4261208482872,20.684610374316694,19.69174298775434,19.886867970893853,20.040254664219347,20.529943723779848,20.242725124514585,20.79463748112124,19.83671420471719,23.670415771461293,21.46122632929926,20.562805537840955,20.533752717709135,20.32408739871887,19.63773549578508,21.004998972801445,19.170323879451832,20.86852363610626,20.183690413289238,20.24971295729743,19.96852879274269,20.170348135749112,22.096973704985764,19.427181595388372,19.42891222624503,22.04330313338973,22.00193968812675,20.002309366268083,19.499296345817637,19.610359788701647,19.310809314957623,20.74833550805435,20.102461041080932,20.684285180507207,19.84619919771332,21.863902362316406,19.353082669751014,24.564135223260617,24.796329396663186,19.62768005431754,19.805428711613047,19.57581736940035,22.848352253459,20.233557799662794,19.470874561871955,20.698159884952336,19.809397616060718,20.33489045544758,19.418755887672077,20.026953867588023,19.783554106287486,19.802594197576237,21.690772771354045,20.219004194051806,19.708342367363247,19.55600737028193,20.770659370087497,23.957439167413153,20.277793424039277,21.90965246947577,19.549876535559736,19.60025871745005,20.89234343160202,19.734074663921614,20.60266091497008,21.938694191224734,19.32498914900244,23.88951371748734,20.56176403563344,20.329541995790343,20.348599125881496,21.340379954816225,19.832932998081127,22.421510122989847,20.071870616453335,19.62845527716809,19.60193926601093,21.0573185810539,21.734863437807945,20.096674580567402,19.156399130792444,20.445454552405874,19.576783849297488,21.53322833282068,22.121319165574633,25.187296749041145,21.28649615035068,20.360633909961237,19.866498914995073,20.458539135170742,20.360184925349074,25.213501052370837,21.90543459748822,19.82317746226967,22.711125521706162,19.886867970893853,19.461437599570544,20.48236571669185,20.320404302486722,21.809904160698302,23.589962353194725,19.30165682938546,24.81456006437998,19.863396570831636,20.268271717252027,19.786714012992956,19.77977045140226,20.005574549320745,23.683356573873173,19.804180813950094,21.093619264331608,23.07179482378122,19.530251033914503,20.842820128769308,23.34874252837792,19.606840027309083,21.993504799177874,21.95030268316933,20.78024347148649,25.25684805306911,23.713753065806493,22.623006633930085,19.737894520252702,20.816188867007813,21.096974634656643,21.021195954812125,20.28279689884897,20.52703126997805,22.294718242405793,22.602591162229757,19.73407466392161,20.149524049789857,20.22139234256568,20.55183077689158,20.26738364695859,20.426214478086177,21.99612185911825,20.320893348843466,20.780397488995593,23.59452261660861,19.011087006520384,21.86065028705729,21.23922500080368,21.708412781679474,20.807180794610804,19.92717370126012,19.066984639180728,21.87741546048653,22.231088321836353,23.37855348428477,19.480652714674502,19.71533811527319,21.134391365239793,23.42433651158639,20.485243200951626,20.329537197692165,19.26090136477759,22.42968149965252,21.007914785447035,20.29970904101369,20.32875465987535,20.329650499163172,21.70215433519854,21.24082030269886,19.398807550701104,20.988993700951994,20.74089939692204,21.568090074865054,22.597860225045242,20.62514842677516,20.74255283422752,20.461328919249258,20.318672412386174,19.932834062042716,19.38095427933719,25.27478107319418,19.62663051444493,19.575336124185156,19.6890268059992,19.892012404049083,23.80782087667977,20.271940328180374,21.05228799059539,19.78255737952468,23.60238831991608,21.345304513231724,21.398794796659296,19.48297001036368,19.58211313274928,20.752990513413334,20.174880002441245,21.733688994290155,21.063567761081327,21.87673920923907,23.563093451642924,20.349939231899054,20.060724390040313,20.20439155309325,20.0987108167898,21.885654319722402,19.680424821449126,19.69803296722001,22.339765905333536,20.24645071447133,20.738580157060827,20.19368441851778,20.252120779167686,20.33366993814441,19.464469054217783,19.65441521095487,19.6316643043697,22.152104896793716,20.24079702176903,20.19588198253936,20.44133196206429,20.79678438007749,19.74662178852513,21.529710024412793,23.668135708829986,20.483779623615433,23.62669192182694,19.107625239362484,23.32199550897492,20.3161437518782,19.6367160947842,20.063401946870272,20.339689164820612,21.85570064100536,19.697603366737113,19.021846560632678,22.16861558499242,20.00331634917235,21.07665889234132,19.9182090876804,20.117204618041175,21.13381967466476,20.24876226393256,21.387753601484253,19.49026510292525,20.63939387323906,21.42875715081145,19.46114764710953,19.816555171552405,19.900209417430364,21.61542791359102,19.796140763681645,19.6871782656321,21.3123633193273,21.181677058172124,19.92894555118685,21.160175349668588,23.79682863266364,20.860945356161213,20.586468431834703,19.658984303632657,21.978262942376094,20.082515238104985,24.59836740352377,19.56312729786646,24.3242469722271,19.32251261484015,20.193951389989987,21.327524042533504,19.875527076118857,19.32384730630496,21.673378705568336,20.591284577927944,23.627875269537043,21.57240794124033,23.648994552775847,21.49047632689574,19.94718974671802,20.538105453599005,20.4788551741165,19.857667995578804,20.077355195614196,20.061177824078154,21.165103696899735,22.950824529447846,19.399930639144515,19.011087006520384,19.68725490439977,20.279927900834064,20.931276034868777,21.58214125983081,19.69760336673711,19.749869335769844,23.542677434218394,20.215776951433718,21.13378503446779,24.481183423130652,19.896568638617453,20.897953183698068,23.712103389997424,19.87767351817062,19.598447166378932,21.738262239847796,20.41545177572139,21.39323115288258,19.763921867906422,19.369545103140965,19.433519737316157,20.374624977108212,22.124914015613964,22.681086465218467,21.16842575491079,22.056302054943682,21.010253648944634,20.214009146852078,24.404477678931116,22.941140098853538,19.84219097069047,25.193200769220844,20.395223667694975,20.242211456891376,19.42724649844145,20.887143323673797,19.800479418900416,20.89992904381442,20.48161066087308,20.670355468466227,18.98928418977938,19.57130251505522,19.355461997471643,24.838540466195234,20.497133103670123,21.718859716471727,19.612034087914612,19.77061408444594,19.810565308580404,21.063048593486013,19.398807550701104,23.605788339763322,21.39205633099798,21.35800208278289,20.5631768932233,21.117504551658744,21.136660979423386,21.165925682349453,19.964285512732136,21.621242392649666,21.84239910337252,20.572089853072146,21.026859527822072,19.79459058927883,20.348299858069485,23.20479607042216,19.50288491254126,21.81155478137519,22.469428653600982,21.595382761763258,20.19523613638554,22.451341951607176,20.0980085187861,20.48987184171246,22.104667116549678,23.647549528794126,19.645530238741365,20.58613890524935,24.998473807123958,21.352839141947303,21.788618614861,20.260885398380747,21.687284032414237,24.77577465561423,21.770853023565923,20.380978645473608,20.150319655055796,20.274179996512,21.33416153332062,22.114479637415656,25.070160613388893,20.754741082049666,22.58770254596578,20.312153586321454,21.155102420030417,19.608162648277567,22.933322570095527,21.64522456020037,23.564031441778752,19.959191872984984,21.03282921096738,19.600532495160845,19.355461997471643,20.629728966995554,23.915193678461858,21.313694047119427,22.97468631949675,20.80725492292724,19.56834889086089,22.148439091693604,19.442092445383828,23.720117209364652,19.82013284091359,21.772787707996024,21.03939335307797,19.241015488474712,20.38055399883763,22.476989942516088,19.747879247542546,21.53003151137411,21.23448316874344,20.410729140440957,23.48045665443059,19.52879976870666,19.50852056470491,21.165487910034567,20.33990777592516,20.489673815358305,20.338111598440722,20.165446828734932,20.480548886217647,20.401906772993044,21.67313693910289,19.79820645206579,23.10578043529388,20.419756672110356,21.875716617318673,19.069173582561387,19.580141268973374,20.436376794127955,19.56447367832503,20.968795133008793,19.752086314160504,24.50543099247456,21.33700013776909,20.49468780922082,19.721110374148687,19.64967238205179,20.19158013878173,21.32970070343748,21.479277826890225,19.406387051138736,21.436787876286125,20.383754388719154,24.718412329453965,19.495377837032944,22.121225069204552,24.097202888596925,19.86606603388866,24.243003624684263,21.120489399013533,19.107512351530893,19.80682667823053,20.474407585740042,20.27902507018879,19.79820645206579,24.710305818044862,21.996523113135687,19.762966493181054,22.084492919588747,19.756787179175234,20.100172626093144,20.295133117424843,20.648371333153033,23.633735517093356,21.607462898968613,20.36344831514,20.384079179603987,23.752270469812768,19.91124792571541,19.596562584800417,19.77368950661156,21.97632050018503,22.31713376974996,19.93682116577477,22.316088728803855,24.141711844934292,20.26869045933391,20.133613886480433,20.744020397693657,19.88742709291957,22.36425417730932,20.33990777592516,21.701328717033505,19.799132469058968,19.50720401082227,19.735365135065837,20.176381631972674,19.5927981735977,20.574210425917876,19.03488836643939,23.62455191785311,20.214845768963947,20.948905914201813,21.032867784424223,21.59370592030254,20.908258618768325,22.814804629433162,20.329262200814853,21.348894044144366,21.999388604190365,20.924916975609626,20.79059127524843,19.65676721055591,19.513107852196608,19.97539217975357,22.698347190529738,23.218062655565873,19.939234554612245,22.71342459153072,19.59394560847956,21.39779582612492,22.82679170878833,23.64997469621601,20.44423444490145,22.49854819219226,21.6449082460781,20.29131165697709,19.406197680784597,19.128519603893867,19.90801905859982,20.323881985442718,23.285499895637074,20.30787299204559,20.575752227892863,21.169864511151722,20.249259094512958,21.738306913351398,20.781289495414498,19.674188877822175,20.09939558378079,20.40272465864045,20.271726905061886,21.67069698544609],"xaxis":"x","y":[21.224638239972652,20.180643707721647,20.201341431117804,19.1454180237343,21.79470380164697,21.087755789009996,22.432940196049124,19.578846464119135,19.242152996635603,24.328668011629535,21.754493646354923,21.476822454566275,22.377553705906216,19.555124576954142,22.494011033388475,22.606991826298977,18.49440443743341,20.76699650557159,29.702932107840674,20.84994793377778,20.282334104450392,19.302304652102425,20.014253448408297,19.02478155099295,20.401136284262034,21.066918904158555,20.30528553843669,22.443056343683086,18.530754586286314,19.472341179018663,21.511675814584603,20.024996233202344,21.986420084483047,22.32477661567823,19.024668956843474,20.277557571355647,20.011894681671833,23.15521167718674,21.342870607762556,20.008806098312462,21.88562823605827,23.73392435177354,20.358652273917528,22.63391493571178,20.2953770114005,18.603706022480708,19.476619646192038,22.286954060950823,20.827570650601533,19.658607240576167,20.944082894927185,17.889444547032436,20.210011039401028,20.041902410540708,22.116524819871177,23.07232514287102,18.931245369435278,19.243034483473725,22.175009164867692,18.8696149273692,22.748443989592825,20.18471556395119,19.630595419370202,19.596480627252774,22.075827573908075,20.81706640440522,20.033430227416382,19.472993206549905,19.47575297115503,19.712127286089668,22.22369094458836,19.974424393125233,20.76732896720974,19.73170766432226,20.622497836834853,19.074352191519743,22.597191120827183,21.03390016791011,22.014068840206768,21.41124459992273,20.690267441816314,22.238063995762765,22.835153147091482,21.940489607019817,20.06072495330695,19.26508618967162,20.593917077867186,20.807981392523768,19.461766940454588,19.92317059187003,20.905864572180928,19.887154874828134,22.674741834325705,19.54492482512454,21.650322094933124,23.6494594685329,21.673503182786,25.42292365756086,23.598781185238046,18.99960834234589,24.597055201180662,22.731123431811554,19.734159687698988,18.832054600441634,20.397708053035622,21.477527921557282,21.731372277489818,21.51301334156099,18.824250825082274,21.177749972976986,22.50813230656899,20.78875924862035,19.355480148534408,23.890614784737277,22.347688134832723,19.773789560999628,21.94580529932221,22.19000668879013,19.515006779114817,22.99883477706531,24.630356510701596,19.78112594827553,20.45440259621003,20.195062159996276,21.08554669425927,22.350651659007692,19.88287753302219,23.223107004233785,19.56773145357299,22.383567194160783,19.337397595787667,19.84140252838032,24.09542285957458,18.7932762528682,18.78833918294514,13.482051629033913,21.99805786914009,20.68539834867811,20.75326327581395,18.386700720994313,20.922404198298533,19.34599856610255,23.03775857233843,18.64679105166152,20.517600912543127,20.206849822052806,22.025147336797662,19.802893385023037,20.525632635954643,20.1770277231615,23.357834800271604,19.12730750072142,23.147280554074683,21.983261369651657,20.345804248394614,19.067786210059246,20.11323917875491,20.682137237753555,21.173013406383724,22.83375379636568,21.704780057376663,19.98202682053532,19.114539527933296,22.520164246985647,22.815206783859768,26.613491397737352,21.053960341071367,19.740225451250925,20.911400034394557,19.239293618022067,22.519031496965763,20.18180618902739,21.47581257946244,21.841152373314472,19.894016350184707,18.947642644803835,20.33821258053668,21.998370834730604,21.623042657920415,20.52474339677355,23.920588558527324,22.398384643288747,20.614611901935827,20.73652363425427,24.216251851014647,0,23.798062566113977,22.44169398922513,19.04780522125321,22.508853453023672,20.361268080220963,19.223547338390134,20.037360610262493,19.47969541290554,21.419330156983328,19.397345612637686,21.062803097791257,20.802720957968873,22.787613064091445,19.854352512470033,23.127238847948263,21.236410750491928,19.39740450234132,24.629767207576666,19.424905147339715,17.66325927218184,21.589387814254906,19.856012075783497,19.346653316091935,20.250448783340936,19.91174617647474,25.664004852328446,20.437440195258947,20.159778968313912,19.760702082570422,23.894186440194584,19.664395199250066,20.03503448449456,21.85979788510952,21.007452333347548,19.330905939549268,21.816602846893197,22.163197583801455,21.21476614620658,19.61795076264764,22.32608970797742,21.206937139516242,23.650902186871754,19.538495641550785,21.951268793802367,19.712186074557728,21.6397845044252,19.566140232030243,23.528005120775013,21.76194235515012,19.759231530600534,22.69657897049395,20.97108250648827,20.91944021038121,19.11091145461158,20.92382072739302,19.99564295418165,20.157995405754583,21.499214768467855,21.850862595399537,19.613806547176576,23.262476414885043,19.981118924422468,23.526907160221576,20.11872947152901,23.756207113729698,21.05135284285491,20.459271509540073,20.732604891283092,20.22193428509613,21.2413041438754,20.86782723578085,18.85414731140336,21.746512886013605,21.477990920672163,24.227674033652324,25.12798066804062,26.560353492028305,23.633181830866906,21.120826057383237,20.22792847217847,22.242317589489687,21.590387855717424,19.62630225708032,21.126056594579662,22.585399041291762,22.379011027342607,22.883846216197703,20.986097829919892,21.770367489208184,20.58669309831316,19.43536098584635,21.22950036954701,19.51332725702657,19.38984557001998,20.48325363998835,20.246730162293353,21.286321988690805,20.685520474331085,21.05996063251775,20.18838178436931,21.836335494435165,22.094100243073623,22.994379248863297,22.721264543402345,20.729598340663756,16.46658160832902,21.557923127735883,19.880806435323297,19.801676786988352,21.12828261562749,18.90461110336481,19.19842345841963,20.092049623606254,20.0459782414438,18.31795878333722,20.7963375193431,17.813568277801206,19.627334048970262,18.56499419954315,19.372221038245304,20.307717788382128,19.384134222108464,20.087817122473812,19.502286013993587,21.227181698040095,23.312865371844044,19.405079121883418,20.90714373186156,21.80480516491849,18.974285739983618,19.176540848422555,19.19474964169837,20.93275451807793,20.832136714555805,21.170532392724574,22.709608681261756,19.86611073536812,22.055547735078193,19.980960001271505,22.653172763321756,20.76687340459434,20.847191191309296,20.06982734293826,20.638856572715298,19.67156525332246,19.49519726866692,22.064314182636632,20.227612418267608,26.201435233706484,19.068639219627578,22.656078929958284,20.77129521126714,20.391385205003306,22.283464834927894,20.845264152741507,21.027619198526104,21.07443745441354,21.518584006439504,19.724170230709305,20.679280780496406,18.927945745238855,21.825732486030095,20.464091007664432,19.439137307013926,22.977548256996624,23.988556498480502,19.81821879465883,19.050281782029014,19.439732238591404,17.74375624808358,19.990444456103596,0,18.279601513401296,19.78875556523867,19.965832185850637,20.69408662857846,19.405573621751774,22.379934620081865,22.48871602661339,23.642331039751582,20.276556094805027,23.283922162181995,20.195578868662224,20.03811220566055,19.14964293785794,20.90803339180857,21.473710208099476,19.884197354704103,22.164351182655523,22.813153319462682,18.901120542977083,20.544639287700853,22.988276940898057,20.109117749689304,19.622850798933502,21.871633434683428,22.25017305320405,23.60048678836413,21.515067916471864,22.320806056618878,19.870759032200375,20.544449448312626,20.261067504600565,23.43234984844275,22.4595812045188,22.52530214820436,19.382106816183008,23.96470652783692,21.019906267101412,19.811383905702034,19.845505364796534,20.51412216383031,20.373076483151994,19.01511010272945,21.47364877893911,19.957005833266606,19.38761598733965,22.32447174476143,20.79356441567801,22.807056132685855,24.028256571904787,19.874893102286897,19.393396349595932,21.076706013187607,21.881924997624267,19.305427636738685,20.88679803779653,22.116604136681058,20.73926969664862,19.30704929356398,20.730402345071546,21.49837252262123,21.892478522473827,21.156990969891915,19.626240196846272,19.456579392491523,19.83361590003151,18.640025995609424,21.63591845810156,20.866343204364526,21.462764853897205,18.837375101562536,20.833447685328323,20.03253125195289,21.036829264514445,22.96164822427184,21.237452292596537,20.43520123557409,21.156628899315663,22.496434708234126,22.4307529287208,21.544986593746263,21.11234277813463,21.351967832505505,20.39596499860976,22.86476103379213,20.105565995835796,20.48988421076927,21.684940338873297,21.559903574554543,22.73966855399758,21.952844226236508,20.330025112864092,21.040198736231254,20.75190246631361,18.798041102461394,19.29157093266521,19.97055996653021,19.307592868706436,20.89335338152254,19.773921527816817,21.100709172660746,21.699220401665585,21.041684548715036,20.396544813616845,22.752749154816065,24.957468167964976,20.506265808043914,20.374729613814893,21.525022859298808,23.275631354399913,21.75794824506378,19.767052898077655,21.218145625261048,21.536078922759724,18.276769619768288,24.94868149530031,20.41835179368666,20.87359355439842,20.51145154622031,20.700075492290427,21.289957235167595,20.748925773432315,21.959550623213953,22.62198208884591,19.38396579774315,23.31051349089806,21.627932359551377,18.954513256380405,19.964316452607815,19.32031247988204,24.27577420165758,20.498820970198462,20.060863127783303,24.1535698900103,18.16161414446258,19.814472942891484,22.998819199975294,21.020751435153667,21.36177568305088,18.469113141288748,23.192581335464276,19.21725486016369,20.111754471035223,18.939248297157278,20.474687273562058,21.111612649480875,19.87603574815164,19.637857750008234,20.264370436179377,19.93146081401427,23.29863946166844,21.939745630681145,19.743514665635868,22.649298943378753,21.24045362673594,25.608747220451313,23.49090374555705,23.60775201354553,20.678206389277292,19.915728419308504,20.449383439914303,19.824402536620187,20.53678125858074,22.159724468145836,21.034942664057727,19.03923233773122,20.84977321505041,21.781868242254394,20.46100849412487,22.004305689493815,19.142842792258786,19.496508584409785,21.834751391009725,20.402181439081684,19.21550544985804,20.640134807950194,23.051954851262433,27.036279453018476,20.37977707208339,21.775846548503456,21.781910553210967,22.353218452653266,20.165877660805254,19.451496130670126,25.490707221646936,22.59150705240579,19.995956185326282,20.28232915493825,19.579291420064532,20.879430687458026,21.138173662648782,24.394611647917593,19.533857847503334,19.817723799212757,21.171020993541568,20.50991100623185,22.326932218927173,19.527299811101727,21.383177447413818,19.392837666571104,23.017914332247763,20.82549425312562,19.072268708954628,19.10179618438648,23.29660399040893,19.32638298997934,20.650313109616484,22.182752894747274,20.620515848299537,21.682830461889793,20.248785506000544,20.060843672034636,21.235100056704788,19.639114342316898,22.873156355744623,20.616811975583957,21.260660009666424,20.98111639712373,20.640272301462904,20.347463631132083,19.255662542640188,21.590649178257912,18.627786784209636,25.25576312419618,18.13136213470814,20.18507504172347,22.234490018926184,19.867378293238115,22.842407702796528,19.642302060721956,20.934073076796377,19.657897588782742,20.907660872216642,21.21194412605265,19.94161891161789,21.31905629271608,18.792730927848385,19.734888199833236,18.03332620487895,20.049848549450562,20.009665992350836,21.60568082130027,15.704606153972572,19.9492653280832,21.029500902796773,21.639091593296747,19.630944296038688,19.444205825873514,22.72561505846151,17.641741979338622,23.57832742490694,21.06232551474456,20.596453669823063,23.77569135147608,20.739222922576804,20.798718195952173,20.117453663452505,20.134364899685817,19.1796783064222,19.08437502019272,20.74807693982553,18.925763040720064,21.23645513023823,22.042981377352834,21.11735552640613,23.60188649068525,20.85643673732399,20.248587775712384,22.0971519060863,19.917597162486278,22.120758360865494,22.44443523329341,21.796740313790345,20.192956161016543,21.587428114342384,19.88222659302695,20.074407563972667,19.772030191805687,22.686480223069132,21.085461437053635,21.84811725117268,18.71863791145474,19.897316659199046,22.368646863289158,18.2265721237695,21.061479969798718,21.091487685756913,21.905619198419114,22.248853822319084,20.948206085285577,20.95171629836988,20.60386819333271,21.712312461652626,26.34702224299619,19.173408752888843,18.94863556098072,19.669251869992156,24.602654298813697,20.178503166977418,20.318803159054653,19.8813455186455,19.742006737354636,20.848691129006262,19.533504643608662,23.044574887856943,21.2197623270523,21.881578906408237,21.91240193642795,20.941991359524017,22.077006044437276,22.54693130601029,18.995877797816178,20.997149152437558,18.687341507608156,21.335453055117675,19.376016989288264,25.07655356851684,22.59004948235383,19.378047539426834,19.682163069515127,20.52340976581651,19.581570228112746,20.9498708711936,22.82667920636651,20.99544749522815,25.17876365321419,21.627453285628658,19.477956222501692,19.193137896037967,21.893341530106955,19.71728461871742,17.31416721288844,19.147406179743545,19.827554176093653,20.471143313052202,19.29233286786097,20.500085221471384,19.644130235297485,22.034075873229305,20.949040678200554,20.540968959330602,27.231751620760654,19.73015830855728,22.493025220696325,23.089701491122522,21.64924126399572,20.795739259102447,21.118903696326274,20.815053320149143,16.52324032760632,19.730383893455734,21.24843214899758,22.071670864625634,19.282486682508637,23.126218118696794,22.228032143928214,21.93139047566312,22.735567798975932,24.167967887858172,21.655361699836966,22.600876020595507,24.59815969618305,19.718235182480434,19.521220854266264,20.564957928128795,20.971944131576173,19.6146020174933,27.021590188993713,23.73507977629211,21.659326126254854,21.01213158467734,21.766780414954713,20.08203076321153,21.756353934255824,25.471067413053014,24.02283197509071,23.385758678777925,21.23702785682451,21.578861893090256,19.189211661593234,20.07860380095908,19.328382384869762,23.106213073613993,19.95329417355663,21.365528432523575,18.914065908446993,20.846904364921407,21.621710733177533,20.214357069981805,20.48695013292876,19.590051196125845,20.988896575504317,22.71500741252458,20.63081635106876,18.585982112110514,19.30101383466248,22.43563961615873,21.19573925103901,19.253482814272665,21.733085888220945,21.51937854264472,22.935901951961643,23.1601071574333,20.83646802652741,17.151393182854086,20.807011308942396,19.447379908647196,19.74096325611752,20.942433069954838,19.870033345928174,20.91477708386503,23.264639171673256,18.81898933836999,19.40497105732979,20.010315016400824,20.436358643191994,21.24511744232927,21.394403553854325,20.278388484789648,20.16621212904916,20.06466810918053,20.8905482708943,19.02658827934665,19.696262913257698,21.147132242697182,0,21.233800456337935,21.791167162104042,20.18131197591118,20.79775274411387,20.158015290040524,21.01738902857623,21.1949084635434,22.774677489141574,21.93032100053686,18.418089755206072,23.810210740127232,20.9793536678564,18.664791000361163,20.57237479635235,19.66651702047476,20.204034803826428,19.989961267682528,19.13829898996108,20.67820982860559,19.602487517256634,19.634034627117707,20.049508251055094,19.732903567032626,22.301656666857568,19.3351286191496,19.66019789348857,22.54105973810428,21.087085320291497,21.46282774650559,18.953344800503015,19.199864465913354,18.832769512656196,21.40009037173508,19.692718291075924,20.626400634350908,18.73677516240749,22.077517747249683,18.74408127304125,24.761947476774708,26.113208352362694,19.576225059708314,22.48651099143561,19.29251486362711,21.199347342918216,22.53052802544868,19.222080263625543,20.87727804782974,21.285530909063,20.6451326442611,19.40981146751902,20.008240359915625,19.734510138857544,19.369940873414674,21.839004592901738,19.643216988179674,20.34827538264879,20.295760247970698,21.15453445100085,23.946209069915817,20.161508356002734,21.461336053955172,18.855956110093064,20.13782559566735,20.22901210527111,18.772971741326142,20.346151863763343,0,18.758138614258336,23.01986369036341,19.826388660339887,21.76883742068305,20.30266681112443,20.98525964839946,20.015097338364022,20.523507399137607,20.436904665975636,19.162579413291002,20.375536910022955,23.70344643795525,20.910669810022966,20.379640393188353,19.217287998251134,20.77164874576569,21.098649782686852,21.445335194484834,21.911569915872246,28.094893882984294,22.184719195487645,19.636646744775323,20.26686372980413,20.815307943684378,20.695469281090816,25.87586915107988,20.22604051998145,19.449746389320087,22.323663662536706,20.105603080001295,19.720177445507503,21.133857610241805,20.04287236504247,21.180586044654405,23.433073191606535,19.844702412663505,25.31201588491192,18.563017473748218,20.726116689654802,20.55471999163649,19.12308826699376,20.688277632644148,23.08698185712261,19.4188490987079,19.38789150561276,23.032593297237835,19.840625811644227,21.990665865987562,22.00972851632279,22.544699771649725,22.804072710576072,21.602864772837197,22.7909106273511,25.661642339745672,23.190044679879648,21.797480754192584,17.862153020413583,20.62223350550368,20.658934051201378,20.583030645279486,23.466890235880964,20.879510709815698,21.526914680071894,22.700127681411185,19.683189962828273,20.26341148584,19.616726513482636,19.839377869097518,19.875302398958702,20.973185759004682,21.082163500516533,20.324838680602316,20.30745292915028,25.9613964584893,19.046718202293015,21.78315042330069,20.91962424027046,21.618102712310883,18.472822451162664,19.554830163405484,19.175778559517227,21.854300382519792,24.477823287446007,25.5492833010917,20.6322831501737,18.924421094438486,22.00276008739277,23.251480243552532,19.437442862246026,20.440974576504654,19.114316794022223,23.060566976295764,21.685850226111516,19.690963589379074,19.932730372590424,21.5156696442052,21.03832189892855,21.34321011850724,20.297903654101066,20.251155291034372,22.02017509916011,21.00569855470944,23.695624962968736,19.83550351971754,20.046496736817677,21.663202256858984,21.64914607700524,21.763695119906945,18.966254774482525,25.621581882833034,21.318917047123914,20.25569596256737,19.082528830592434,19.62869805766532,25.538294718669484,19.064589338432878,24.544735671741783,19.58243213803125,23.66501619396984,19.12391425698277,21.767558412000408,18.99637745675535,21.23800756191085,21.00116832090369,22.714272038785044,21.137414277628565,21.45606258158953,22.279207015403138,23.33275256138136,22.26485604208308,20.723080111555596,20.639447033358472,20.694537918678005,20.730065203035867,18.59276109193853,18.41344612635984,22.526317266984485,19.84998714954673,20.751047310028625,19.801872521805933,20.233535275947663,20.632626738131346,21.279464358226686,22.469580214601123,20.408744363117133,22.583074271938578,21.34410908350179,21.979041347822633,20.59404343806881,20.264946502479802,19.878697565839243,22.615047199531546,23.56945490190484,19.736798486144902,23.729801820804443,20.49274951238928,23.323658577961712,20.770035525423065,19.238103804361465,19.567540218990334,20.344212591822245,22.493051859930933,19.27583351001347,18.98927807008273,23.394529944648035,22.51205924599309,21.071056073380674,21.047075972047644,20.445539922646542,21.340147837597844,20.003307902740296,21.299913824117656,20.644505230109406,20.419105854581318,21.593824209166353,19.476347089125902,19.838668436714478,19.728765556885957,22.07230878544129,19.427802084161115,21.756421739524402,21.380030171433972,21.32600862652118,19.632965031850407,21.403421935393933,24.37658838285707,21.043820980181792,20.1938730272849,19.66793597026211,19.199993380834425,20.035891808657503,25.0888235331049,18.385719188798834,26.48889669142553,20.116390923202314,19.499674996520806,19.751886432327307,20.522012502780594,0,20.782571363318098,21.618914296187317,22.967539915594646,20.95701983082078,23.548104709979565,21.65670940491333,18.258239797953646,17.88154175390193,21.233312475041235,20.277059332832177,20.919229965693855,19.653189714082398,19.02398982442439,24.205441209659515,20.16062238071552,18.963476628745678,17.96562762596234,19.531857117715408,20.86635829845881,21.255773747330974,19.280122810790118,19.369392868205782,22.9021647605954,18.93136369208174,21.47821316674072,24.42926325370543,19.880057005564264,21.11943827672747,24.0347829295736,19.174320595643714,19.810878030088894,20.711569411286565,20.79181709109951,21.52825545142037,22.510563057499144,19.714589625374927,19.195910426040165,21.643407148870626,22.79188245530678,23.088899981103793,20.914444260183114,22.409894316390734,22.55442514417979,21.497350091305258,23.661739831337005,22.81337882106461,20.335424976569172,28.119486884464127,20.443137811296648,19.395826969085725,18.744741571834194,21.52611831315801,21.048400728737306,20.235023779555554,21.0819625214105,21.53807712673746,19.283653604184654,20.890428315584586,19.706229562174084,25.296616627387504,21.809197399053033,21.371449507411626,20.19656265688086,19.305685493406397,19.08654493081137,19.639706353282477,17.583516204502104,23.250222506325386,22.82294225806205,24.601371538637977,20.0953223269386,21.243839329097405,20.37329318617189,20.45930590947763,24.987649624596244,22.400315304318365,22.625652665035147,20.455200776646638,21.021720462702472,20.59279530263181,20.232040256335257,22.264357262987502,20.592440384261195,22.36255454353493,21.99517470550352,22.199722361671988,19.391954018620176,21.83585207290849,19.14268047046844,20.53559540130039,21.167744601954155,22.926085660752005,18.718526187000297,22.28042360433217,25.95498550595432,23.32578333060166,21.309621092817217,19.68447811713372,23.268620237666717,25.42582021905868,21.189524929277866,19.937692777054252,20.801380865239704,19.316707310363867,21.75315297668226,22.868455030888445,25.947611368302585,20.033901511817852,23.20532613123076,20.554742003246933,22.96074119343453,18.988728193664233,22.68524113143557,22.64902697475969,25.92407619036082,19.407373582124833,21.346588936497685,19.366735480432887,19.124759964560997,20.07378037525111,23.985931291943867,21.460510640680972,23.23197611805285,20.73330584327736,19.12586494836624,25.275464586661446,19.715668478243582,25.489298123537854,19.013002282515146,25.312874128977917,20.415195026786527,19.91413691399634,22.130140513245646,22.92736601506673,19.584806561357016,23.12482431901705,20.687407722542194,19.611919934246885,24.151034118830534,20.079932477068777,18.93225512877545,24.075045174066695,19.899787637164348,20.874095085342866,21.48054296008632,23.07558442006914,20.640568320462897,20.08863945104873,22.28815948936714,19.616841359021425,23.953590715604236,21.784577239189012,23.301772335078063,19.749416399115965,19.811951028236585,20.17477942437346,18.304477915148002,21.513176875338157,19.58793862702669,24.19084293977443,20.88004158687887,21.10029756186111,19.35190112925433,18.917516857772046,19.84423117509742,24.377548873947223,20.529649424214696,18.766505656199122,21.439000175436664,18.811308834423215,23.933428716720645,19.631145075685264,22.477074905595526,24.176989311672042,19.985865383441666,24.300186984861302,20.78628428680642,19.166666955002903,20.396533708238668,21.16161998720379,23.822353472570573,20.556259058220718,24.241029788998915,22.067946023704817,19.778390202711652,22.99407646829777,18.833019773765052,20.288010749874967,19.947775322749504,20.338620784352592,23.494951508290605,22.38615246636496,19.034944006994706,19.984953818018486,24.081396558763124,19.595170049801663,19.41162451128841,20.72798026434645,21.785986387464803,22.944212504137,23.12678795739032,21.831113335529626,24.147006515510686,20.371960850528456,19.32549212260673,20.504720566168093,19.431273145079818,20.869899532948505,21.94524199207118,21.941322673977073,19.72760807429288,21.84503668728796,18.979888467927786,20.204391739449186,19.721950276191546,19.579038969018928,19.030933465764726,23.8143209659356,20.855181011615457,20.74517719106974,20.090817980596317,20.872940195293037,21.393924057871644,22.20317352529318,19.736694248820182,23.68012909153893,20.722700819009958,21.714666548374087,21.05462253329071,19.397806220065235,19.129172314836154,19.83710046840733,21.397724240612124,22.701728218692,19.775576525065844,24.10746972295679,17.80002145055656,21.28200527852275,24.425354799557958,24.25287711008343,21.61681534013004,21.629837063073623,22.53034703709958,21.62524006847279,20.878967106147208,18.668189678924886,19.500769926583363,19.995538068644624,23.512127623334848,21.53372289651904,20.823420888524133,22.265101932257092,20.999782941566835,21.517654972490075,19.604530267831706,19.76742293183983,20.740147099081785,22.94133404816206,19.387966817051925,20.39789928981696],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors_log = np.linspace(1, 100, dtype=int)\n","param_gridkNN_log = {'kneighborsregressor__n_neighbors': n_neighbors_log}\n","\n","\n","GridkNN_log, \\\n","BestParametreskNN_log, \\\n","ScoreskNN_log, \\\n","SiteEnergyUse_pred_logkNN_log, \\\n","figkNN_log = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log,\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_logkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN_log)\n","\n","print(BestParametreskNN_log)\n","print(ScoreskNN_log)\n","figkNN_log.show()\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.180626327976927,1.7209133288676586,1.6342395252124688,1.582365977731774,1.5595762855459383,1.5532216788317876,1.5430323836669726,1.5395696541258146,1.5354034421695413,1.5337797025369055,1.5281952440524553,1.5246979234612454,1.5241037640965096,1.5197858322847426,1.5193035498414258,1.5159769970173615,1.516554947855783,1.5150535505430756,1.5156837831133716,1.5161678806074892,1.5159172712048377,1.5160917871878399,1.5154885287219506,1.5161763403883797,1.51575407059775,1.5174712749494346,1.517547393207122,1.5181902320324903,1.5181532093826684,1.5187766291698253,1.5188145429436526,1.5191266070567528,1.519524940793115,1.5198610012023672,1.52064750156064,1.5205266440063787,1.5206782179600435,1.5202792052422283,1.520536581875519,1.520498418531576,1.5194287360892145,1.5200452939917615,1.5206916301840274,1.5211568355606198,1.520638058786545,1.5214598950674996,1.5223421196078644,1.5225515594453096,1.5236434601208866,1.5242899491679034]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.44480698651506,1.8727348036593914,1.7748730683072635,1.7324410339233463,1.717025134991128,1.7155607329303904,1.70009603421898,1.7026037722222624,1.6995431873829645,1.6982645614787675,1.6943797814189165,1.6885192067526802,1.6870589254981898,1.6822252614537794,1.6856349020758197,1.681366997267723,1.6840428960286258,1.6833052967039133,1.6834620697098432,1.6843999378596082,1.6849121165453322,1.6853796747869032,1.685192796518639,1.6854537240762524,1.6840828604223101,1.6849096924022784,1.6843186048311434,1.685662747883877,1.6852544509927534,1.6855085045714446,1.6841862633901394,1.6836372291814772,1.6834497737096443,1.6837302112334764,1.6844743494169447,1.6841946788512574,1.6859017412519606,1.6847700210890506,1.6847452885475398,1.6855370594815733,1.6856405049610736,1.6863715361671325,1.6869733122442017,1.6866960485018385,1.6861780253461571,1.6865732756247402,1.6865339914154809,1.687107689229679,1.6885237257486843,1.6891010733810123]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[1.9164456694387941,1.5690918540759258,1.4936059821176741,1.4322909215402018,1.4021274361007485,1.390882624733185,1.3859687331149653,1.3765355360293667,1.371263696956118,1.3692948435950436,1.362010706685994,1.3608766401698107,1.3611486026948294,1.3573464031157059,1.352972197607032,1.3505869967670001,1.34906699968294,1.3468018043822378,1.3479054965169,1.3479358233553702,1.346922425864343,1.3468038995887766,1.3457842609252622,1.346898956700507,1.3474252807731897,1.3500328574965907,1.3507761815831005,1.3507177161811035,1.3510519677725834,1.3520447537682059,1.3534428224971657,1.3546159849320285,1.3556001078765858,1.355991791171258,1.3568206537043352,1.3568586091615,1.3554546946681265,1.355788389395406,1.3563278752034982,1.3554597775815787,1.3532169672173553,1.3537190518163904,1.3544099481238532,1.355617622619401,1.3550980922269327,1.356346514510259,1.3581502478002478,1.3579954296609402,1.358763194493089,1.3594788249547944]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[1.8444704304351398,1.6170015495581833,1.5905930166808848,1.5695169162529878,1.5531034929254321,1.55120340801019,1.5501168640696834,1.5389873879382496,1.5347217735657732,1.5212690656399468,1.5064720403625558,1.5007798849874838,1.4977568293049475,1.496626729307463,1.4968262197647264,1.4962348468078968,1.4998818526200703,1.5019425185866082,1.501318933884494,1.5018752673697389,1.5016252593898496,1.502457227614281,1.5026006341795557,1.5047742745770871,1.5058471239015807,1.5096577909044957,1.5107974604528773,1.51229837141288,1.5107546043361024,1.511757579685756,1.514192974623608,1.5143806221402134,1.515169581116489,1.5147520299012454,1.5168976723095229,1.5133024313301862,1.514855128496316,1.5153713325361802,1.5153399707099668,1.5154736729614648,1.5159055962681918,1.5168517586404817,1.5163284409550817,1.5171321726244156,1.5140442676586479,1.5143213319366773,1.5161696880757611,1.516347335463101,1.5173247119980207,1.518420995705683],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.603466254299547,1.9988969088754247,1.8733705259482927,1.84655669169999,1.8226725981493705,1.8188820183747174,1.7941425466252308,1.8002015300250183,1.7971319043144025,1.801057398542394,1.803250126999077,1.7943833247392278,1.794645601882021,1.7890357066117382,1.7953297964618162,1.787278418254096,1.7909945436583805,1.79145057141105,1.792158124703956,1.7932372282064657,1.795395843413937,1.7962522440696334,1.7972525549613816,1.7974411691192815,1.7971403710089173,1.7979456135911205,1.7963008108872611,1.7971344198524246,1.7968937639726292,1.7968813652285696,1.7933211561239806,1.7916192819080212,1.7911983497701007,1.791511697680593,1.7921103731814185,1.7927128708831643,1.7947328844993866,1.7920483394173878,1.7922604538506872,1.7924012547878403,1.7921761254652377,1.792813791800547,1.7937354117832571,1.7931879395437729,1.7938951256527358,1.7945260848800868,1.7938384278402393,1.7950708249617933,1.7971590492948204,1.7979773268527435],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.138713100004989,1.74295700868017,1.6995336617286405,1.618500761066467,1.6208648078115282,1.6245322038250183,1.6153936659111647,1.6202385548091351,1.617937022133105,1.6154005748406295,1.6072756446103353,1.6064117290586517,1.60243719004891,1.5965652247304878,1.5958501462163022,1.5958433450849323,1.596492217397533,1.592847799860031,1.5924232350575784,1.5934324323917204,1.5912475266263717,1.5908496745237208,1.5888640241299956,1.5882326101048592,1.5832342212740071,1.5822191114856914,1.5823151201675285,1.5845510941523777,1.584394861442752,1.5848792765572297,1.5851731824632516,1.5863429235901165,1.585688588887531,1.5864459566078453,1.586610722911428,1.5869256364620778,1.587845656260297,1.5886454954161648,1.588207626320181,1.5902580016232535,1.5905800863218345,1.5911576966691314,1.5921298313459737,1.591535783958813,1.5905797428047233,1.5907626907276526,1.5906596602961676,1.5903068928513147,1.5908250116283864,1.5907479505450615],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[1.993310972740129,1.5617830590313548,1.4761532513167683,1.4351848262681566,1.4147414885794636,1.3878120232235622,1.3913316568818936,1.3836893724493695,1.3772422479411666,1.3804530548895795,1.3771022933637989,1.378069794012828,1.3820292982148072,1.3793668802520191,1.3796916858259742,1.3774206158457256,1.3727439776422694,1.3639585907933074,1.3663230818125058,1.3655711166551618,1.3660703370807157,1.3653258737643876,1.3599924429221002,1.3591422273035367,1.3603485186139856,1.3617403303245355,1.3635122687693793,1.3623800995899378,1.3650742712254165,1.3641204288456703,1.3653626251249023,1.3662362086387738,1.3693475209777355,1.3695016504183528,1.369122067837253,1.3704940196789601,1.3702483381880617,1.3691106059553046,1.3698387324291972,1.3709346271566856,1.3684492745750125,1.3699294130866306,1.3697663836653595,1.3718827676398901,1.371844696512764,1.3725989310792563,1.374507267220183,1.3741719859272412,1.3746062651153,1.3742876299873994],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.3231708824048316,1.6839281181931598,1.531547170387758,1.4420706933712686,1.3864990402638981,1.3836787407254503,1.3641771848468902,1.3547314254072997,1.3499842628932586,1.3507184187719787,1.3468761149265096,1.343844884508035,1.3436499010318632,1.3373346205220047,1.32881990093831,1.3231077590941567,1.3226621479606613,1.3250682720643805,1.3261955401083243,1.3267233584143585,1.3252473895133143,1.3255739159671773,1.3287329874167197,1.3312914208371343,1.3322001181902587,1.3357935284413296,1.3348113057585638,1.3345871751548313,1.3336485459364422,1.3362444955319006,1.336022776382521,1.3370539990066392,1.336220663213719,1.3370936714037995,1.338496671563578,1.3391982616775053,1.3357090823561568,1.3362202528861036,1.3370361260675627,1.3334245361286357,1.3300325978157959,1.329473809762017,1.3314980831704637,1.3320455140362073,1.332826461303854,1.3350904367138245,1.3365355546069708,1.336860758023098,1.3383022625679055,1.340015842748629],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour les paramètres de GridSearchCV\n","FigRMSEGRidkNN_log = visuRMSEGrid(KNeighborsRegressor(), 'kNN',\n","                                  n_neighbors_log, 'n neighbors', GridkNN_log)\n","FigRMSEGRidkNN_log.show()\n","if write_data is True:\n","    FigRMSEGRidkNN_log.write_image('./Figures/ConsoGraphRMSEkNN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                    1000\n","1  randomforestregressor__max_features                    sqrt\n","                               R²      RMSE       MAE\n","RandomForestRegressor()  0.374041  1.695024  0.753997\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_log_logRF=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.016306452145407,21.38354103024903,19.76945560959943,19.086978385264793,21.88422668920781,20.687763624751767,21.84294003234071,19.062436458180777,19.95940575336327,23.294786634111208,21.04512358733028,21.05656907945318,20.87894058447876,19.714518292200175,23.242832993911982,23.35941890883964,19.336074686528352,20.200575413594105,25.376273803859352,20.755505134768107,20.553652280702067,19.957156899383264,19.7138289889866,19.368677874111036,20.344959146541253,20.983952583090847,19.574997218496705,22.377627703740263,19.772994196199893,19.20055773264164,21.25944777846761,20.08158444028386,21.613870338658273,22.430269235015864,19.488430072405595,21.010201883343157,20.135270165153944,24.050249313621986,20.364617104097128,19.864182655433815,20.412179988076463,22.83831848066343,20.132430193297292,22.828611237566125,20.251932401267474,19.446397211801447,19.446811676865433,20.238474453754403,20.096760779234295,19.418264441899634,20.837580387229053,20.411309796313535,20.269766713243868,20.500221085959716,21.44877632405882,22.704995245779195,19.173158675220954,21.697782493903194,20.893184744707856,19.12342390434395,22.914928745542042,19.905811155713984,20.64977710118983,20.256508086968434,19.340611195504547,20.363337004352083,17.601842350705606,19.941232763479142,19.664513016220432,19.54242611415427,22.115293463514387,19.673594334342642,20.992558006281737,20.396647092331307,20.612944479494153,18.930449727403825,21.3200154745515,20.866749001881765,21.39718447091599,19.882057700269133,20.26728656629084,21.72296693319571,21.102437565319676,21.020045755964752,21.064705617347148,19.325514091540466,21.433919872050716,19.853453774890212,19.75511774665057,19.869528461892887,21.23205725960682,19.521280022437125,22.785227420247605,20.4234613874049,22.491273800103322,23.96088864213233,22.130752772653324,25.63335820006665,21.424720762838696,19.479266949249535,23.85006761020629,23.65133392114068,20.343512168425594,19.573937004326613,20.652092064332397,21.68674408326982,21.160021957222693,21.599906611311383,19.37268215344353,20.938870144089552,22.56832743544226,20.850317040023118,12.259680553392863,24.335328076237122,22.38064126769982,19.939851369718877,21.906426299256466,22.621083770896345,19.210972888025605,21.910155451719213,24.778184875960676,19.544164957798223,21.968651016321758,20.7097271011036,19.963134371880418,21.36507657662396,21.197760305779312,21.82586539614721,20.020941187852358,21.835122641877643,19.29691505410434,19.187959051195218,23.08738456118511,19.46409825194531,19.17583611315004,21.732923686393118,20.56815996286711,19.686248967289966,20.830359487406344,21.089109580221855,20.37415504111381,19.625680139208644,20.384955973253625,19.18006346153104,20.785051567606033,19.925021275908527,22.915755547313722,19.83742619556414,20.07265897273241,19.448259193352914,21.63466624879241,19.243733213578196,21.131631557489847,21.51283606286619,20.871254840431664,19.718267744385134,20.44638090699481,20.671802370501158,20.995386908465555,22.63548001835145,21.68237302041039,19.573693023105175,19.21231918321235,20.904224334541894,22.47122368966882,26.18859089034962,20.519621974327382,20.410152173212477,20.209835887620827,19.66129574483106,22.243493016643807,20.931855035792907,21.097864248520388,21.170323250555974,19.902624612341636,19.533437219082057,20.705786956905165,21.10004638449153,21.939168516104733,19.801645928460996,24.364554145578346,20.135801983688392,20.65839328739534,21.82062086396815,23.141062917684604,20.440324381206903,21.950780985671944,22.787294550659706,19.407306870978847,21.038796778629063,20.82560037700427,20.01602422649312,20.34392542399038,18.979128874009287,20.45858375968921,19.209678761782875,20.909176332467116,19.81299702895764,21.435267133412548,19.73369403117322,23.190353877194514,19.49541980958834,21.21313408491899,22.957838919262063,19.36815157543699,19.13021722445579,21.61985954649277,19.112783377366682,20.518949233443546,20.311512313702924,20.130806770969357,24.219668283661477,20.893831519087037,19.85046275942054,21.321110828350662,24.074846468728378,19.902191071897853,19.76709839013443,21.995542345103463,20.14985183391046,19.492395009518816,21.693247475769027,22.16883655771755,19.690152491553274,20.198157430392783,22.772001064931,20.66815464634281,23.355908122328874,20.181525592288228,21.301546158631545,19.9362040352638,21.52817749260412,20.074237681381934,21.22399930688429,21.813329425011894,19.886222902090804,22.760772389382208,19.804716865531077,21.133987348274808,19.383563278771064,20.242008894972127,20.94211972533993,20.434631830073027,21.479064241031352,21.616158710816702,19.549843250232655,24.236026051927617,19.893066177679714,22.755297183570107,20.32032999694278,24.3859644394477,20.154056400973133,20.00386659009243,21.19588496637812,20.023723214842846,20.763330189693082,20.07488955837887,19.49264770433175,21.190385687379088,20.889619240095364,23.57683877056654,25.30890333283479,24.788108105128703,21.37950487249916,20.03104774052272,19.50996314458611,21.878270264315955,22.445441179197754,19.742853473114142,20.360984417203102,20.439005266056586,22.314436542424705,23.010753972230486,21.049653569039855,19.52831262393612,20.526369562316063,19.225715167058347,20.9802518762396,19.38650821779397,19.609875158218884,19.680370959151535,19.4920123758472,22.068489923227506,20.15743111997881,20.9865721904892,20.749061305998737,21.750745316889127,21.164493704086343,23.10285384206516,23.79323129492671,21.716531987183963,19.258427580865693,19.53662609866131,18.650653447021828,19.89278059843492,21.136496032813483,19.719380256131025,19.470514407488725,19.670230447200947,21.487872556413677,18.566872596351327,20.66841816059219,19.88003159074839,19.848326672305728,19.839642367080188,19.459375064838525,19.786686883689125,19.456044556638556,19.78524218323843,19.873726865862977,21.05349805834089,20.944905164714736,19.36581462151414,20.579065714381542,21.52389490252296,19.08298355315857,19.471442591104882,19.6650096912278,20.837444205236526,21.459413068876458,21.033817542498312,21.63760266928132,19.785834714478746,21.866862028973063,20.225541535949596,21.857860723809623,21.209269936307624,22.307998262469727,21.1537432495153,19.96975082989502,19.914120180170457,19.712111871428643,21.2363165166159,19.83676264189693,11.818766619357756,19.028513918206222,22.647265304212553,21.595991731661986,21.092684325407173,22.24254698348458,20.835869483487286,21.224118849628717,21.05691125461293,22.29817373365747,19.58881083814344,21.445683775973585,20.07965626182632,20.156728989333647,19.975982706386873,19.55544014654876,22.66970769628947,26.621682175797805,19.55794440104126,19.453617998994932,19.581650849331478,19.785158840529775,21.11604146006068,20.234942143459577,19.03051963965877,20.352632915421104,20.53334629950865,20.7379153651663,20.14518695678933,22.390690031677092,22.25225197251685,23.881767249106534,19.782367658018412,23.32094696444617,20.32032999694278,19.27815840383215,19.750889747417794,21.23122057495964,20.28713606296443,19.636469036656464,21.42913668297748,21.189532824924147,19.091652311560154,21.47638597510722,23.602162363969715,20.488910345221264,20.21976509614941,21.23386041746996,21.995679143474504,24.077170547019886,21.412944593375954,19.50233079158693,20.06954417066892,20.847972672227517,20.294969052779546,23.059928041066893,20.574752233033735,20.46841003494938,20.348410742771993,22.262287885535745,19.90653389636936,20.23217285849602,19.983858048452756,20.946890469163204,21.922835261406746,18.64800869569058,22.011862792574927,20.401238185844885,19.922796050850405,21.324556165550817,20.470804143219418,21.67342906734838,23.182573377030742,20.093085043540043,19.70683827967325,21.195406507705638,21.050728910437396,19.299749398842415,21.322150927964817,20.473402281325225,20.206970648075945,19.433288984686573,20.337623725713993,21.755854966758417,21.973537415734206,20.907033159694922,19.792131822922144,21.330861535236686,19.82597005596727,21.574560245519688,21.60595804215851,20.77847553726796,21.991907837939532,18.728064356072146,19.647090489415174,20.04937358478902,20.440772069397433,23.129621590089158,21.155290742764667,21.866898540418024,21.116858139029787,22.423378925005654,22.86864728263892,20.307661703798658,19.964122919697267,20.967957382376575,19.617266518961664,23.162947999030237,20.501824382595885,16.154498730739196,21.15222372460366,21.049952960248444,21.12827355966889,22.486645895749618,21.38162415381168,20.869667909548422,21.47004214829276,18.98972548185572,20.626524434772165,20.665649144925766,20.305602121496136,20.758485727219455,19.733579033400744,20.246791076609004,22.338498368036397,21.044365204604404,20.840708465876713,21.131414299233217,21.580693282956997,20.805888750837003,20.105495682235674,22.439539404939275,22.609253002808458,20.638896774787995,20.791817359367457,20.57497209131693,21.215645655269693,20.056340682920947,24.657231293493915,19.804262964856566,20.785866735103113,20.20651122403138,20.99024796985254,21.427855827379194,20.319083331430242,22.288629057941893,21.5882979132519,20.08396802770736,21.33086467483555,21.72988555003112,19.23158004870625,20.003464558233052,19.26758935712867,23.224903536039697,20.529084744819276,20.081491225464323,23.701905466420875,20.53424071136088,21.328453698874714,20.858392264084824,22.201060637511176,21.118635014743997,20.415502492916026,21.9769659720513,19.603444128692338,20.40251456891589,20.082981971895467,20.613206586294524,21.486670889717264,20.234946012400528,19.644596802805264,19.879038111170672,19.378686346777137,22.86761240754962,21.98415563521134,19.946091319726904,22.493943561276698,21.445246030841723,22.68275150113651,23.245629508117755,22.401823224213214,20.10159512087829,19.854335970290286,20.027210870033336,20.34148905102084,20.046360347998604,22.43952800956804,20.76045533763756,19.455193430399408,21.21779003661166,21.75851349725454,20.611378041899325,21.87105048476784,19.43733578992196,20.579079857769276,18.619413407943025,20.34238120797094,19.192082308143203,21.05091160895518,23.125758675259632,26.132952522497874,20.5022026428187,22.545907561706628,21.44460264780675,23.05209416064259,19.86721599504943,20.026080386291245,25.495031943757805,22.827974541682075,20.14055769250314,19.352654809242857,20.501963230939502,19.218246167450825,20.99558701681734,23.150605878088573,19.724834373233147,19.439912455633113,21.33108090085347,20.11896893328291,22.338970697133213,19.835297195798162,21.820581668771844,19.483684900399012,23.325550850597594,20.90432678311973,19.04317736936344,19.622254112107775,19.88064048732278,18.794523150403677,21.190999995845342,21.676946779689686,19.883843669378106,21.23326210045298,19.69142794500808,20.29646436630948,21.397553749627708,19.294476665110007,23.350563294987214,21.00590754032563,20.853415330392057,19.103970460108282,21.15742221389529,19.729712402853398,19.37607435091173,20.80012039602704,19.521243896515365,24.74113443937116,20.088639677590926,20.654582414830774,22.83372354534349,19.96381333066089,23.142316579275867,19.50784136579021,21.26569420127758,19.859917004801087,20.210817933791343,19.61753539449652,20.8945947644784,21.587430910534135,19.354255576779675,19.19761440498918,19.730176200160873,19.35159640088209,19.57637987234102,20.510972641730845,19.783255272054053,21.03540640711899,21.537973790844934,21.686114805605772,19.836498645530998,20.129592491549218,22.646490593359196,19.98808980689929,21.181073936939985,19.40075453643389,20.041875051920805,23.918141384984487,20.796212271955238,21.025566977530364,20.24589263404944,20.720172077130787,19.380554009478118,19.963678070449387,19.76201130842203,19.613272937676356,20.19737886016705,22.085320427864588,20.64435630332699,23.642022593602903,20.56975142445898,20.56183258806999,21.173417896598178,19.86977152887181,20.228188551416316,22.433036084641103,19.123555744317432,20.391593643922196,21.4789125263021,20.91513612748414,20.411624386499547,19.452463318662343,22.58433345207543,19.918111717571296,22.18185736055583,19.837964991202014,19.880881213720375,22.141726190636795,20.39442738247684,21.401374518339342,21.187658030285906,20.589429117324283,22.209366894572355,21.104485896544585,20.054366285851877,20.777439488565122,21.231460786614306,25.810338219269575,19.647675557260524,19.243733213578196,19.975326774304907,24.227629265757315,20.518421412723136,20.33004982048634,19.51970771364643,19.43312208077164,21.18078163994399,19.513290328978776,23.487179558109734,21.208149363732346,20.72122866069447,20.2939573325023,21.213535749589784,22.274831289399962,22.65169649200968,19.729801806813228,21.668262979695836,20.097418113645826,21.551025636207143,19.659417694069603,24.78328412043107,22.476900731699374,19.70487924906194,21.074768039010753,20.642745048183883,19.448503558928113,20.799954565966864,20.996376603254532,20.876539239315115,25.522480192127166,21.03877825016741,19.66889796084967,20.957767484010375,21.529846762567022,21.62802693575585,20.01838150898755,19.31691040172953,20.73021403432217,20.342239425892444,19.54795359025782,21.128481293812253,21.412356469121885,20.92411660752557,21.274440042442787,20.74841514946914,25.61497934961194,19.900217702769513,23.474250509982433,20.199251071599004,20.05805079329445,20.679962774436706,20.825580493041805,20.541789001052408,18.98168202839427,20.57357743838969,21.314108227950666,22.398396344172113,19.514522614695448,23.17377186554918,20.032290432862858,20.707919909698628,19.506939538699637,24.54855038681955,22.119844312623258,22.054933732596886,25.0616123659274,20.57432382817941,19.99656740168806,21.47239073488529,21.424234313015084,20.147061013565395,24.901895127737454,21.836860678760505,22.457301877111743,20.288325014142185,22.591571419571046,19.866450325235586,21.266619590869226,25.85140526776605,23.74083466286281,18.514308816242966,21.194697654251858,21.649078773492434,19.392582666737756,20.337710820267326,19.58142107809989,21.734390144628065,19.72098480195083,20.075138400420613,19.3119494090227,20.955995587307303,21.319004485499356,19.869682250836938,20.24307995346836,19.61059484361149,21.63327605629607,23.521519858626217,20.237874747563104,19.914170086133353,20.018140683248888,22.652700424320166,21.04389760698706,19.669679211788853,20.34670038163816,20.332662386619024,23.559265396413174,22.271898118875388,21.06248816237211,18.85341049124555,19.944515837822845,19.76080830709454,19.62309579944609,21.396096976833178,20.57949178261466,20.550079107532707,21.8313845880878,19.467796567499303,20.492734494644264,20.50984771364114,20.91841871305882,20.430111083490388,22.17320217139563,20.544356276841224,19.995325794752446,19.18599191539929,20.813828713409826,19.41281008566369,20.376831010215376,21.327392935197143,23.29292397684593,19.519823647852526,21.759849497758385,20.801646419968108,20.09844236599856,20.31287473605953,20.34345477087745,20.63855730949786,20.338376506764767,21.343938970077797,19.559114044572773,23.852031229912356,21.386704930775473,19.764670293865663,20.83352443468676,21.12136796185811,19.677184895052502,20.828917790124564,20.16751513235685,21.09258306723125,19.85656147477045,20.198157430392783,19.767222101890365,20.136746415365963,21.871968064366975,19.520782649732553,19.480248130546872,22.36178674321249,21.264770336000954,21.049888319065726,19.28702974690229,19.88425994708554,19.127944268677773,20.37431359433398,20.91762893514479,20.108678051620217,19.29875182383413,22.661254914915,19.082012643002926,24.611255086021792,25.524345226918793,20.01614435922176,19.560814578442912,19.609803012578013,22.222169032830156,21.422964510096747,19.2427835468402,20.88530216528912,21.564903605829826,20.40579191552196,19.506593648836947,20.507381109066582,19.776429551842018,20.04532489104763,21.722540755436487,20.32837695164249,19.878322269654557,19.85325744501596,21.172737397781578,23.903086229815223,20.49221249725668,22.558590389043832,19.477461839115072,19.656900676913157,20.967918167804296,19.328356275966456,20.28006471697644,21.588653004656415,20.311084585036717,23.593234858173716,20.32222989521332,20.54200700993822,20.216124204468734,21.281276748094076,19.699256058930235,21.18763532963301,19.92322537842575,19.372422880153174,19.635635639171607,21.847914076796474,22.159037209754576,20.345574096874977,19.861963363310434,20.558394920662746,19.851576109008132,20.69441970159313,21.740012796525882,24.382743232411233,21.693040091640334,20.214220115841716,20.210809154853575,20.630180427865803,20.52347794963741,25.335959247257506,21.42578750908884,19.324144710093993,22.43899725512235,20.312944823487875,19.61035812372999,20.299360302348003,20.38952377135732,21.678584257410687,23.661005333237902,19.78766130674101,25.590734803500602,19.41557591191719,20.375925803973846,20.333435564287058,19.949310485058582,20.277572090779937,22.990995122187268,19.979118547175858,21.116221649689635,23.23418874181691,20.019194933701453,20.936670360181875,22.73477514979879,22.01941678668563,21.582583706289892,21.951720013424072,21.42098427640298,25.191582983094555,23.984484688922805,22.684090578055457,20.2046721253916,20.994025003295835,21.566724370442454,21.01872612749934,17.091617056154483,20.383152879989357,21.930478364865973,22.453981526341487,19.38401769701311,20.000938861182934,19.89418695413047,20.24433681072084,20.34282595940658,20.16366265879393,21.685212407275724,19.837235893891133,20.690436366401602,24.836090594188775,19.106377336801568,21.336291226462844,21.521945670078313,21.62214735241272,20.3794005036898,19.720539840713396,19.51851688322051,21.543247047899534,22.836910019301275,24.6247226546736,19.968213169402883,19.724548924838448,21.24039486108567,23.530894668921043,21.22390051807366,19.95859434066546,19.189955819037273,22.539667411214833,20.32023221720939,20.248671369691674,20.061941593027527,20.56396692277966,21.56265015257755,21.506106831334417,19.991815101657,20.206868641270454,21.311720612454835,21.30551404460154,22.515212363559044,20.366685275265073,20.285920928026073,21.24011069857007,20.384655305447065,20.19549998589742,20.083536197857278,25.244575980069666,20.570664142425755,19.8354805920409,19.720259092057617,20.71964656962456,24.605975099403327,19.868588755848997,21.629037709664225,19.345250707588576,23.659964503013626,21.0901503964049,21.348365064512848,19.64595378165594,20.268391932586137,20.732623895206913,20.952327205608444,21.371079394508936,21.721383935248493,22.03634075925012,23.56956899694828,21.01409022071208,20.22546565536926,20.990071954862103,20.603976944764053,21.585140826062354,19.69902504845998,19.17851294496971,22.29652828160937,21.09850083549018,20.757818658631088,20.75938222418375,20.03068858729967,20.35335639167896,20.386692473142773,19.729527898599763,19.815159451867505,14.212120958502384,20.28327617116812,20.875151873178613,20.48908027328105,21.01821559570799,19.51574896306327,21.943467742738463,23.829464703299784,20.287059587311095,23.576906134878367,19.275760984853736,23.768652087034056,21.338780117474247,19.203956827726913,20.06412358552542,19.783267362574197,22.30845146166123,19.387764605607654,19.12342390434395,23.013896030020163,19.06360005813856,20.888315213306228,20.25570111754768,20.369728428025628,21.268063574955463,19.907986853486328,21.266267875531685,18.877520525425716,20.352471164276523,21.73776468070252,20.064865330114962,19.74297438738164,20.492744018072543,21.944832763615832,19.52715272443149,20.189541291808226,21.105349724285947,21.398151269446252,19.208420646025072,21.496027236353278,24.30998488621796,20.986315877839797,20.578239135214567,19.791358092587494,21.650952331659035,19.827915174872334,25.395913181231716,19.899261678568493,24.5632038637073,19.909474835942056,20.041088360118216,22.042114721984948,19.615610239710207,19.11529990251931,21.41712117346482,21.56177114225768,23.149240602456548,21.847931630271518,23.874700487247303,21.544950863526374,19.459085253593365,19.391201363707353,20.724795359340273,19.80818129037674,20.08511457432114,19.953201126829423,20.865266450203343,23.75061899779175,19.387788910920218,19.106377336801568,19.374818050772852,20.514206144827394,21.438348247765777,21.662648013131435,18.808163370495905,19.465281016657528,23.42263280386584,20.22222292636347,21.396612339663236,24.688267293955388,19.922890608250658,21.793753703832206,23.986807952698452,19.90374272026263,19.69869243702564,21.594124818596757,20.484410980812502,20.838760614123714,19.77121645403506,19.88028672770625,18.983412793948997,20.410264559938863,22.150929383771246,23.183721239080377,21.13536416213578,22.35980882926305,21.39817804265148,20.99509563505098,24.25012324869608,22.739246694952076,20.072374277621837,25.28584087748178,20.443550773043302,18.97953747295411,19.81527255366559,21.05386315417865,20.61421045932859,20.4064523854881,20.852624622388024,21.19456075893287,19.12588535248756,20.22385672002662,19.542277888387886,25.642920088324125,20.82195687947131,21.73315691650691,19.74653516559995,19.600306035959022,19.83904137037446,21.001466503464766,19.27754988360029,23.762425203953523,21.150234663640433,22.530392976405956,20.252162010822126,21.043011053641962,21.228212340527136,20.74476035901099,21.684504945226816,22.17286437333212,22.347087068506564,20.698853182097032,20.761528895492773,20.08880461188864,20.681955381028175,22.78585229191857,19.980058225684356,22.02792948403409,22.81997121902927,21.93751616468383,20.405041624830833,22.504198902621894,19.791422249063036,21.12177690242081,21.993994551190234,23.298519680961387,19.59922108865078,21.61843921245845,25.699273226727666,21.36638383056314,21.44027327960567,20.6881316594446,22.580555234893506,25.723494353247485,21.7113314745361,20.638078154483583,20.938247865122893,20.650447434135643,22.062454292609075,22.210921181046338,25.558547491824882,20.495272427756586,22.402038140053047,20.37651060431496,21.883123021109242,19.35263379636723,22.843858384885237,22.442846132875736,24.641917063086048,19.802835572927165,21.166619224980558,19.847159377791275,18.96440317505961,20.48064311671744,23.919554048252326,20.635068059524173,23.393136948035494,20.74937368143789,19.288522493248266,23.30096096732615,19.837451168593958,24.636196457875574,20.252863695043335,21.697879494063464,20.747251712172943,19.437553764860542,21.592592942544837,23.02509805822262,19.56794713720216,21.326510638212977,21.184714150143378,19.886096566942825,23.812247025240904,20.542857870019922,19.138562800040184,23.37649327435782,20.528387535159013,20.315707411509024,20.574659942950664,22.213531123977216,20.755790292559446,20.905780568934983,22.345104050032806,19.77818450256342,23.27419670389032,20.372714399354113,22.167546861977083,19.607523960581165,19.785764826718122,20.021063806554093,19.37637882435844,21.441713458168064,19.96331814221103,24.54726251013538,21.184166335214524,20.239896790023494,19.542095870026795,19.586193188143362,20.49527896547351,22.980891203736928,21.16281550898694,20.233431403342145,21.411713935439693,20.30811254091248,24.596883560712854,19.938043680620094,22.275567260958628,24.375865192627877,18.858549360570617,24.468289604129343,21.006969106774,19.268262463882785,20.42385934621307,15.8300596359265,20.401529884411758,19.973583479253374,24.631439015608734,21.86128716597444,19.687689487599506,22.560046312140198,19.1675882120355,20.442194326402884,20.013367124761064,21.138433640905962,23.692165432909924,21.383282713392497,19.426076542806996,20.51350512674195,24.006459672507955,19.772439418913287,20.006094740210052,20.34615390564715,21.988279189265025,23.040177802555004,20.55017143570561,22.409605016473964,24.403207797465967,20.427390224137152,20.079832711995767,20.78548140107847,19.852166506151292,22.435581302498832,20.04611871447962,17.859389668500953,19.837945586728807,19.49236671646452,19.54449271437748,20.51374615295053,19.66313612648078,19.722949932252078,19.10735101506332,23.579061599965943,20.18213689903154,21.286941381087246,20.602786989496522,21.458295887532998,20.282779022350482,22.604502501101713,20.193793391339465,22.25628282787241,22.066748076084455,20.439924533164298,20.800028494356283,19.417822425448364,19.907808161050465,20.03481573140776,22.40079907899948,23.898503462283635,19.380098168870052,23.299010956861473,19.316942284027764,21.328984963553893,23.546492960767463,24.05601960050969,21.287273415505982,21.854085632603557,22.00671728176748,20.85234943007299,20.726980686817022,19.320609005518836,19.783960483324172,20.068355183183392,23.295786974203004,20.65419879430306,21.00339912806214,21.37999192826042,20.51580150642719,21.925585817356765,20.276193932152175,19.423522868014796,20.835987942550716,20.39942574071058,20.214659349035866,21.37598939221192],"xaxis":"x","y":[21.224638239972652,20.180643707721647,20.201341431117804,19.1454180237343,21.79470380164697,21.087755789009996,22.432940196049124,19.578846464119135,19.242152996635603,24.328668011629535,21.754493646354923,21.476822454566275,22.377553705906216,19.555124576954142,22.494011033388475,22.606991826298977,18.49440443743341,20.76699650557159,29.702932107840674,20.84994793377778,20.282334104450392,19.302304652102425,20.014253448408297,19.02478155099295,20.401136284262034,21.066918904158555,20.30528553843669,22.443056343683086,18.530754586286314,19.472341179018663,21.511675814584603,20.024996233202344,21.986420084483047,22.32477661567823,19.024668956843474,20.277557571355647,20.011894681671833,23.15521167718674,21.342870607762556,20.008806098312462,21.88562823605827,23.73392435177354,20.358652273917528,22.63391493571178,20.2953770114005,18.603706022480708,19.476619646192038,22.286954060950823,20.827570650601533,19.658607240576167,20.944082894927185,17.889444547032436,20.210011039401028,20.041902410540708,22.116524819871177,23.07232514287102,18.931245369435278,19.243034483473725,22.175009164867692,18.8696149273692,22.748443989592825,20.18471556395119,19.630595419370202,19.596480627252774,22.075827573908075,20.81706640440522,20.033430227416382,19.472993206549905,19.47575297115503,19.712127286089668,22.22369094458836,19.974424393125233,20.76732896720974,19.73170766432226,20.622497836834853,19.074352191519743,22.597191120827183,21.03390016791011,22.014068840206768,21.41124459992273,20.690267441816314,22.238063995762765,22.835153147091482,21.940489607019817,20.06072495330695,19.26508618967162,20.593917077867186,20.807981392523768,19.461766940454588,19.92317059187003,20.905864572180928,19.887154874828134,22.674741834325705,19.54492482512454,21.650322094933124,23.6494594685329,21.673503182786,25.42292365756086,23.598781185238046,18.99960834234589,24.597055201180662,22.731123431811554,19.734159687698988,18.832054600441634,20.397708053035622,21.477527921557282,21.731372277489818,21.51301334156099,18.824250825082274,21.177749972976986,22.50813230656899,20.78875924862035,19.355480148534408,23.890614784737277,22.347688134832723,19.773789560999628,21.94580529932221,22.19000668879013,19.515006779114817,22.99883477706531,24.630356510701596,19.78112594827553,20.45440259621003,20.195062159996276,21.08554669425927,22.350651659007692,19.88287753302219,23.223107004233785,19.56773145357299,22.383567194160783,19.337397595787667,19.84140252838032,24.09542285957458,18.7932762528682,18.78833918294514,13.482051629033913,21.99805786914009,20.68539834867811,20.75326327581395,18.386700720994313,20.922404198298533,19.34599856610255,23.03775857233843,18.64679105166152,20.517600912543127,20.206849822052806,22.025147336797662,19.802893385023037,20.525632635954643,20.1770277231615,23.357834800271604,19.12730750072142,23.147280554074683,21.983261369651657,20.345804248394614,19.067786210059246,20.11323917875491,20.682137237753555,21.173013406383724,22.83375379636568,21.704780057376663,19.98202682053532,19.114539527933296,22.520164246985647,22.815206783859768,26.613491397737352,21.053960341071367,19.740225451250925,20.911400034394557,19.239293618022067,22.519031496965763,20.18180618902739,21.47581257946244,21.841152373314472,19.894016350184707,18.947642644803835,20.33821258053668,21.998370834730604,21.623042657920415,20.52474339677355,23.920588558527324,22.398384643288747,20.614611901935827,20.73652363425427,24.216251851014647,0,23.798062566113977,22.44169398922513,19.04780522125321,22.508853453023672,20.361268080220963,19.223547338390134,20.037360610262493,19.47969541290554,21.419330156983328,19.397345612637686,21.062803097791257,20.802720957968873,22.787613064091445,19.854352512470033,23.127238847948263,21.236410750491928,19.39740450234132,24.629767207576666,19.424905147339715,17.66325927218184,21.589387814254906,19.856012075783497,19.346653316091935,20.250448783340936,19.91174617647474,25.664004852328446,20.437440195258947,20.159778968313912,19.760702082570422,23.894186440194584,19.664395199250066,20.03503448449456,21.85979788510952,21.007452333347548,19.330905939549268,21.816602846893197,22.163197583801455,21.21476614620658,19.61795076264764,22.32608970797742,21.206937139516242,23.650902186871754,19.538495641550785,21.951268793802367,19.712186074557728,21.6397845044252,19.566140232030243,23.528005120775013,21.76194235515012,19.759231530600534,22.69657897049395,20.97108250648827,20.91944021038121,19.11091145461158,20.92382072739302,19.99564295418165,20.157995405754583,21.499214768467855,21.850862595399537,19.613806547176576,23.262476414885043,19.981118924422468,23.526907160221576,20.11872947152901,23.756207113729698,21.05135284285491,20.459271509540073,20.732604891283092,20.22193428509613,21.2413041438754,20.86782723578085,18.85414731140336,21.746512886013605,21.477990920672163,24.227674033652324,25.12798066804062,26.560353492028305,23.633181830866906,21.120826057383237,20.22792847217847,22.242317589489687,21.590387855717424,19.62630225708032,21.126056594579662,22.585399041291762,22.379011027342607,22.883846216197703,20.986097829919892,21.770367489208184,20.58669309831316,19.43536098584635,21.22950036954701,19.51332725702657,19.38984557001998,20.48325363998835,20.246730162293353,21.286321988690805,20.685520474331085,21.05996063251775,20.18838178436931,21.836335494435165,22.094100243073623,22.994379248863297,22.721264543402345,20.729598340663756,16.46658160832902,21.557923127735883,19.880806435323297,19.801676786988352,21.12828261562749,18.90461110336481,19.19842345841963,20.092049623606254,20.0459782414438,18.31795878333722,20.7963375193431,17.813568277801206,19.627334048970262,18.56499419954315,19.372221038245304,20.307717788382128,19.384134222108464,20.087817122473812,19.502286013993587,21.227181698040095,23.312865371844044,19.405079121883418,20.90714373186156,21.80480516491849,18.974285739983618,19.176540848422555,19.19474964169837,20.93275451807793,20.832136714555805,21.170532392724574,22.709608681261756,19.86611073536812,22.055547735078193,19.980960001271505,22.653172763321756,20.76687340459434,20.847191191309296,20.06982734293826,20.638856572715298,19.67156525332246,19.49519726866692,22.064314182636632,20.227612418267608,26.201435233706484,19.068639219627578,22.656078929958284,20.77129521126714,20.391385205003306,22.283464834927894,20.845264152741507,21.027619198526104,21.07443745441354,21.518584006439504,19.724170230709305,20.679280780496406,18.927945745238855,21.825732486030095,20.464091007664432,19.439137307013926,22.977548256996624,23.988556498480502,19.81821879465883,19.050281782029014,19.439732238591404,17.74375624808358,19.990444456103596,0,18.279601513401296,19.78875556523867,19.965832185850637,20.69408662857846,19.405573621751774,22.379934620081865,22.48871602661339,23.642331039751582,20.276556094805027,23.283922162181995,20.195578868662224,20.03811220566055,19.14964293785794,20.90803339180857,21.473710208099476,19.884197354704103,22.164351182655523,22.813153319462682,18.901120542977083,20.544639287700853,22.988276940898057,20.109117749689304,19.622850798933502,21.871633434683428,22.25017305320405,23.60048678836413,21.515067916471864,22.320806056618878,19.870759032200375,20.544449448312626,20.261067504600565,23.43234984844275,22.4595812045188,22.52530214820436,19.382106816183008,23.96470652783692,21.019906267101412,19.811383905702034,19.845505364796534,20.51412216383031,20.373076483151994,19.01511010272945,21.47364877893911,19.957005833266606,19.38761598733965,22.32447174476143,20.79356441567801,22.807056132685855,24.028256571904787,19.874893102286897,19.393396349595932,21.076706013187607,21.881924997624267,19.305427636738685,20.88679803779653,22.116604136681058,20.73926969664862,19.30704929356398,20.730402345071546,21.49837252262123,21.892478522473827,21.156990969891915,19.626240196846272,19.456579392491523,19.83361590003151,18.640025995609424,21.63591845810156,20.866343204364526,21.462764853897205,18.837375101562536,20.833447685328323,20.03253125195289,21.036829264514445,22.96164822427184,21.237452292596537,20.43520123557409,21.156628899315663,22.496434708234126,22.4307529287208,21.544986593746263,21.11234277813463,21.351967832505505,20.39596499860976,22.86476103379213,20.105565995835796,20.48988421076927,21.684940338873297,21.559903574554543,22.73966855399758,21.952844226236508,20.330025112864092,21.040198736231254,20.75190246631361,18.798041102461394,19.29157093266521,19.97055996653021,19.307592868706436,20.89335338152254,19.773921527816817,21.100709172660746,21.699220401665585,21.041684548715036,20.396544813616845,22.752749154816065,24.957468167964976,20.506265808043914,20.374729613814893,21.525022859298808,23.275631354399913,21.75794824506378,19.767052898077655,21.218145625261048,21.536078922759724,18.276769619768288,24.94868149530031,20.41835179368666,20.87359355439842,20.51145154622031,20.700075492290427,21.289957235167595,20.748925773432315,21.959550623213953,22.62198208884591,19.38396579774315,23.31051349089806,21.627932359551377,18.954513256380405,19.964316452607815,19.32031247988204,24.27577420165758,20.498820970198462,20.060863127783303,24.1535698900103,18.16161414446258,19.814472942891484,22.998819199975294,21.020751435153667,21.36177568305088,18.469113141288748,23.192581335464276,19.21725486016369,20.111754471035223,18.939248297157278,20.474687273562058,21.111612649480875,19.87603574815164,19.637857750008234,20.264370436179377,19.93146081401427,23.29863946166844,21.939745630681145,19.743514665635868,22.649298943378753,21.24045362673594,25.608747220451313,23.49090374555705,23.60775201354553,20.678206389277292,19.915728419308504,20.449383439914303,19.824402536620187,20.53678125858074,22.159724468145836,21.034942664057727,19.03923233773122,20.84977321505041,21.781868242254394,20.46100849412487,22.004305689493815,19.142842792258786,19.496508584409785,21.834751391009725,20.402181439081684,19.21550544985804,20.640134807950194,23.051954851262433,27.036279453018476,20.37977707208339,21.775846548503456,21.781910553210967,22.353218452653266,20.165877660805254,19.451496130670126,25.490707221646936,22.59150705240579,19.995956185326282,20.28232915493825,19.579291420064532,20.879430687458026,21.138173662648782,24.394611647917593,19.533857847503334,19.817723799212757,21.171020993541568,20.50991100623185,22.326932218927173,19.527299811101727,21.383177447413818,19.392837666571104,23.017914332247763,20.82549425312562,19.072268708954628,19.10179618438648,23.29660399040893,19.32638298997934,20.650313109616484,22.182752894747274,20.620515848299537,21.682830461889793,20.248785506000544,20.060843672034636,21.235100056704788,19.639114342316898,22.873156355744623,20.616811975583957,21.260660009666424,20.98111639712373,20.640272301462904,20.347463631132083,19.255662542640188,21.590649178257912,18.627786784209636,25.25576312419618,18.13136213470814,20.18507504172347,22.234490018926184,19.867378293238115,22.842407702796528,19.642302060721956,20.934073076796377,19.657897588782742,20.907660872216642,21.21194412605265,19.94161891161789,21.31905629271608,18.792730927848385,19.734888199833236,18.03332620487895,20.049848549450562,20.009665992350836,21.60568082130027,15.704606153972572,19.9492653280832,21.029500902796773,21.639091593296747,19.630944296038688,19.444205825873514,22.72561505846151,17.641741979338622,23.57832742490694,21.06232551474456,20.596453669823063,23.77569135147608,20.739222922576804,20.798718195952173,20.117453663452505,20.134364899685817,19.1796783064222,19.08437502019272,20.74807693982553,18.925763040720064,21.23645513023823,22.042981377352834,21.11735552640613,23.60188649068525,20.85643673732399,20.248587775712384,22.0971519060863,19.917597162486278,22.120758360865494,22.44443523329341,21.796740313790345,20.192956161016543,21.587428114342384,19.88222659302695,20.074407563972667,19.772030191805687,22.686480223069132,21.085461437053635,21.84811725117268,18.71863791145474,19.897316659199046,22.368646863289158,18.2265721237695,21.061479969798718,21.091487685756913,21.905619198419114,22.248853822319084,20.948206085285577,20.95171629836988,20.60386819333271,21.712312461652626,26.34702224299619,19.173408752888843,18.94863556098072,19.669251869992156,24.602654298813697,20.178503166977418,20.318803159054653,19.8813455186455,19.742006737354636,20.848691129006262,19.533504643608662,23.044574887856943,21.2197623270523,21.881578906408237,21.91240193642795,20.941991359524017,22.077006044437276,22.54693130601029,18.995877797816178,20.997149152437558,18.687341507608156,21.335453055117675,19.376016989288264,25.07655356851684,22.59004948235383,19.378047539426834,19.682163069515127,20.52340976581651,19.581570228112746,20.9498708711936,22.82667920636651,20.99544749522815,25.17876365321419,21.627453285628658,19.477956222501692,19.193137896037967,21.893341530106955,19.71728461871742,17.31416721288844,19.147406179743545,19.827554176093653,20.471143313052202,19.29233286786097,20.500085221471384,19.644130235297485,22.034075873229305,20.949040678200554,20.540968959330602,27.231751620760654,19.73015830855728,22.493025220696325,23.089701491122522,21.64924126399572,20.795739259102447,21.118903696326274,20.815053320149143,16.52324032760632,19.730383893455734,21.24843214899758,22.071670864625634,19.282486682508637,23.126218118696794,22.228032143928214,21.93139047566312,22.735567798975932,24.167967887858172,21.655361699836966,22.600876020595507,24.59815969618305,19.718235182480434,19.521220854266264,20.564957928128795,20.971944131576173,19.6146020174933,27.021590188993713,23.73507977629211,21.659326126254854,21.01213158467734,21.766780414954713,20.08203076321153,21.756353934255824,25.471067413053014,24.02283197509071,23.385758678777925,21.23702785682451,21.578861893090256,19.189211661593234,20.07860380095908,19.328382384869762,23.106213073613993,19.95329417355663,21.365528432523575,18.914065908446993,20.846904364921407,21.621710733177533,20.214357069981805,20.48695013292876,19.590051196125845,20.988896575504317,22.71500741252458,20.63081635106876,18.585982112110514,19.30101383466248,22.43563961615873,21.19573925103901,19.253482814272665,21.733085888220945,21.51937854264472,22.935901951961643,23.1601071574333,20.83646802652741,17.151393182854086,20.807011308942396,19.447379908647196,19.74096325611752,20.942433069954838,19.870033345928174,20.91477708386503,23.264639171673256,18.81898933836999,19.40497105732979,20.010315016400824,20.436358643191994,21.24511744232927,21.394403553854325,20.278388484789648,20.16621212904916,20.06466810918053,20.8905482708943,19.02658827934665,19.696262913257698,21.147132242697182,0,21.233800456337935,21.791167162104042,20.18131197591118,20.79775274411387,20.158015290040524,21.01738902857623,21.1949084635434,22.774677489141574,21.93032100053686,18.418089755206072,23.810210740127232,20.9793536678564,18.664791000361163,20.57237479635235,19.66651702047476,20.204034803826428,19.989961267682528,19.13829898996108,20.67820982860559,19.602487517256634,19.634034627117707,20.049508251055094,19.732903567032626,22.301656666857568,19.3351286191496,19.66019789348857,22.54105973810428,21.087085320291497,21.46282774650559,18.953344800503015,19.199864465913354,18.832769512656196,21.40009037173508,19.692718291075924,20.626400634350908,18.73677516240749,22.077517747249683,18.74408127304125,24.761947476774708,26.113208352362694,19.576225059708314,22.48651099143561,19.29251486362711,21.199347342918216,22.53052802544868,19.222080263625543,20.87727804782974,21.285530909063,20.6451326442611,19.40981146751902,20.008240359915625,19.734510138857544,19.369940873414674,21.839004592901738,19.643216988179674,20.34827538264879,20.295760247970698,21.15453445100085,23.946209069915817,20.161508356002734,21.461336053955172,18.855956110093064,20.13782559566735,20.22901210527111,18.772971741326142,20.346151863763343,0,18.758138614258336,23.01986369036341,19.826388660339887,21.76883742068305,20.30266681112443,20.98525964839946,20.015097338364022,20.523507399137607,20.436904665975636,19.162579413291002,20.375536910022955,23.70344643795525,20.910669810022966,20.379640393188353,19.217287998251134,20.77164874576569,21.098649782686852,21.445335194484834,21.911569915872246,28.094893882984294,22.184719195487645,19.636646744775323,20.26686372980413,20.815307943684378,20.695469281090816,25.87586915107988,20.22604051998145,19.449746389320087,22.323663662536706,20.105603080001295,19.720177445507503,21.133857610241805,20.04287236504247,21.180586044654405,23.433073191606535,19.844702412663505,25.31201588491192,18.563017473748218,20.726116689654802,20.55471999163649,19.12308826699376,20.688277632644148,23.08698185712261,19.4188490987079,19.38789150561276,23.032593297237835,19.840625811644227,21.990665865987562,22.00972851632279,22.544699771649725,22.804072710576072,21.602864772837197,22.7909106273511,25.661642339745672,23.190044679879648,21.797480754192584,17.862153020413583,20.62223350550368,20.658934051201378,20.583030645279486,23.466890235880964,20.879510709815698,21.526914680071894,22.700127681411185,19.683189962828273,20.26341148584,19.616726513482636,19.839377869097518,19.875302398958702,20.973185759004682,21.082163500516533,20.324838680602316,20.30745292915028,25.9613964584893,19.046718202293015,21.78315042330069,20.91962424027046,21.618102712310883,18.472822451162664,19.554830163405484,19.175778559517227,21.854300382519792,24.477823287446007,25.5492833010917,20.6322831501737,18.924421094438486,22.00276008739277,23.251480243552532,19.437442862246026,20.440974576504654,19.114316794022223,23.060566976295764,21.685850226111516,19.690963589379074,19.932730372590424,21.5156696442052,21.03832189892855,21.34321011850724,20.297903654101066,20.251155291034372,22.02017509916011,21.00569855470944,23.695624962968736,19.83550351971754,20.046496736817677,21.663202256858984,21.64914607700524,21.763695119906945,18.966254774482525,25.621581882833034,21.318917047123914,20.25569596256737,19.082528830592434,19.62869805766532,25.538294718669484,19.064589338432878,24.544735671741783,19.58243213803125,23.66501619396984,19.12391425698277,21.767558412000408,18.99637745675535,21.23800756191085,21.00116832090369,22.714272038785044,21.137414277628565,21.45606258158953,22.279207015403138,23.33275256138136,22.26485604208308,20.723080111555596,20.639447033358472,20.694537918678005,20.730065203035867,18.59276109193853,18.41344612635984,22.526317266984485,19.84998714954673,20.751047310028625,19.801872521805933,20.233535275947663,20.632626738131346,21.279464358226686,22.469580214601123,20.408744363117133,22.583074271938578,21.34410908350179,21.979041347822633,20.59404343806881,20.264946502479802,19.878697565839243,22.615047199531546,23.56945490190484,19.736798486144902,23.729801820804443,20.49274951238928,23.323658577961712,20.770035525423065,19.238103804361465,19.567540218990334,20.344212591822245,22.493051859930933,19.27583351001347,18.98927807008273,23.394529944648035,22.51205924599309,21.071056073380674,21.047075972047644,20.445539922646542,21.340147837597844,20.003307902740296,21.299913824117656,20.644505230109406,20.419105854581318,21.593824209166353,19.476347089125902,19.838668436714478,19.728765556885957,22.07230878544129,19.427802084161115,21.756421739524402,21.380030171433972,21.32600862652118,19.632965031850407,21.403421935393933,24.37658838285707,21.043820980181792,20.1938730272849,19.66793597026211,19.199993380834425,20.035891808657503,25.0888235331049,18.385719188798834,26.48889669142553,20.116390923202314,19.499674996520806,19.751886432327307,20.522012502780594,0,20.782571363318098,21.618914296187317,22.967539915594646,20.95701983082078,23.548104709979565,21.65670940491333,18.258239797953646,17.88154175390193,21.233312475041235,20.277059332832177,20.919229965693855,19.653189714082398,19.02398982442439,24.205441209659515,20.16062238071552,18.963476628745678,17.96562762596234,19.531857117715408,20.86635829845881,21.255773747330974,19.280122810790118,19.369392868205782,22.9021647605954,18.93136369208174,21.47821316674072,24.42926325370543,19.880057005564264,21.11943827672747,24.0347829295736,19.174320595643714,19.810878030088894,20.711569411286565,20.79181709109951,21.52825545142037,22.510563057499144,19.714589625374927,19.195910426040165,21.643407148870626,22.79188245530678,23.088899981103793,20.914444260183114,22.409894316390734,22.55442514417979,21.497350091305258,23.661739831337005,22.81337882106461,20.335424976569172,28.119486884464127,20.443137811296648,19.395826969085725,18.744741571834194,21.52611831315801,21.048400728737306,20.235023779555554,21.0819625214105,21.53807712673746,19.283653604184654,20.890428315584586,19.706229562174084,25.296616627387504,21.809197399053033,21.371449507411626,20.19656265688086,19.305685493406397,19.08654493081137,19.639706353282477,17.583516204502104,23.250222506325386,22.82294225806205,24.601371538637977,20.0953223269386,21.243839329097405,20.37329318617189,20.45930590947763,24.987649624596244,22.400315304318365,22.625652665035147,20.455200776646638,21.021720462702472,20.59279530263181,20.232040256335257,22.264357262987502,20.592440384261195,22.36255454353493,21.99517470550352,22.199722361671988,19.391954018620176,21.83585207290849,19.14268047046844,20.53559540130039,21.167744601954155,22.926085660752005,18.718526187000297,22.28042360433217,25.95498550595432,23.32578333060166,21.309621092817217,19.68447811713372,23.268620237666717,25.42582021905868,21.189524929277866,19.937692777054252,20.801380865239704,19.316707310363867,21.75315297668226,22.868455030888445,25.947611368302585,20.033901511817852,23.20532613123076,20.554742003246933,22.96074119343453,18.988728193664233,22.68524113143557,22.64902697475969,25.92407619036082,19.407373582124833,21.346588936497685,19.366735480432887,19.124759964560997,20.07378037525111,23.985931291943867,21.460510640680972,23.23197611805285,20.73330584327736,19.12586494836624,25.275464586661446,19.715668478243582,25.489298123537854,19.013002282515146,25.312874128977917,20.415195026786527,19.91413691399634,22.130140513245646,22.92736601506673,19.584806561357016,23.12482431901705,20.687407722542194,19.611919934246885,24.151034118830534,20.079932477068777,18.93225512877545,24.075045174066695,19.899787637164348,20.874095085342866,21.48054296008632,23.07558442006914,20.640568320462897,20.08863945104873,22.28815948936714,19.616841359021425,23.953590715604236,21.784577239189012,23.301772335078063,19.749416399115965,19.811951028236585,20.17477942437346,18.304477915148002,21.513176875338157,19.58793862702669,24.19084293977443,20.88004158687887,21.10029756186111,19.35190112925433,18.917516857772046,19.84423117509742,24.377548873947223,20.529649424214696,18.766505656199122,21.439000175436664,18.811308834423215,23.933428716720645,19.631145075685264,22.477074905595526,24.176989311672042,19.985865383441666,24.300186984861302,20.78628428680642,19.166666955002903,20.396533708238668,21.16161998720379,23.822353472570573,20.556259058220718,24.241029788998915,22.067946023704817,19.778390202711652,22.99407646829777,18.833019773765052,20.288010749874967,19.947775322749504,20.338620784352592,23.494951508290605,22.38615246636496,19.034944006994706,19.984953818018486,24.081396558763124,19.595170049801663,19.41162451128841,20.72798026434645,21.785986387464803,22.944212504137,23.12678795739032,21.831113335529626,24.147006515510686,20.371960850528456,19.32549212260673,20.504720566168093,19.431273145079818,20.869899532948505,21.94524199207118,21.941322673977073,19.72760807429288,21.84503668728796,18.979888467927786,20.204391739449186,19.721950276191546,19.579038969018928,19.030933465764726,23.8143209659356,20.855181011615457,20.74517719106974,20.090817980596317,20.872940195293037,21.393924057871644,22.20317352529318,19.736694248820182,23.68012909153893,20.722700819009958,21.714666548374087,21.05462253329071,19.397806220065235,19.129172314836154,19.83710046840733,21.397724240612124,22.701728218692,19.775576525065844,24.10746972295679,17.80002145055656,21.28200527852275,24.425354799557958,24.25287711008343,21.61681534013004,21.629837063073623,22.53034703709958,21.62524006847279,20.878967106147208,18.668189678924886,19.500769926583363,19.995538068644624,23.512127623334848,21.53372289651904,20.823420888524133,22.265101932257092,20.999782941566835,21.517654972490075,19.604530267831706,19.76742293183983,20.740147099081785,22.94133404816206,19.387966817051925,20.39789928981696],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_log_logRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF_log = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF_log = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF_log,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF_log, \\\n","BestParametresRF_log, \\\n","ScoresRF_log, \\\n","SiteEnergyUse_pred_logRF_log, \\\n","figRF_log = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log.ravel(),\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_log_logRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF_log)\n","print(ScoresRF_log)\n","figRF_log.show()\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[2.045928851621741,1.7950857011329737,1.681436574314852,1.5733783878716372,1.547552490914628,1.5564366767549267,1.5440268930574366,1.5443896394577337,1.5398594260627791,1.537128373683475]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[2.250561845222491,1.9481724485877376,1.8013056392629827,1.7261773675183594,1.6852090283821684,1.6802450033321852,1.6813809151592238,1.6766050333742957,1.67365140174351,1.6724383122488025]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.8412958580209913,1.6419989536782098,1.5615675093667214,1.420579408224915,1.4098959534470878,1.4326283501776682,1.4066728709556493,1.4121742455411717,1.4060674503820483,1.4018184351181477]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.9007357965887914,1.7112836608481636,1.599087967730554,1.5527136771188257,1.5023468989532998,1.4941663616897558,1.506993040898447,1.4863213303541465,1.4899243670274818,1.491928885845917],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[2.0499095700514474,2.0052808939676323,1.8043094864528222,1.788170617569167,1.7448724157667084,1.7294230179135655,1.7473548634517282,1.7408520456081158,1.7335331851387772,1.7352214257117657],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[2.0124867800960757,1.9518390493681381,1.741600632345578,1.7028176705837892,1.6637190781125002,1.6442182676118153,1.638985614080465,1.6187697285596465,1.6207396672727286,1.6278680943044272],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8400786454772515,1.627867007204007,1.487434585643461,1.4347536055274595,1.3671199622917976,1.3693403224327374,1.3483113595969385,1.3450877424241077,1.3333958771805383,1.3367376872883217],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[2.42643346589514,1.6791578942769272,1.7747501994018458,1.3884363685589447,1.459704099448833,1.5450354141267595,1.4784895872596029,1.530917350342652,1.5217040336943697,1.493885775266944],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=sqrt<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF_log = visuRMSEGrid(RandomForestRegressor(), 'RF',\n","                                 n_estimatorsRF_log, 'n estimators',\n","                                 GridRF_log, BestParametresRF_log,\n","                                 'randomforestregressor__max_features')\n","FigRMSEGRidRF_log.show()\n","if write_data is True:\n","    FigRMSEGRidRF_log.write_image('./Figures/ConsoGraphRMSERF_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                   1\n","1          adaboostregressor__loss              square\n","                           R²      RMSE       MAE\n","AdaBoostRegressor()  0.369392  1.701307  0.845429\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predAB_log=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.339198983880678,20.53085429235503,20.006166383672266,19.573355649402984,21.339198983880678,20.006166383672266,21.93869191818988,19.573355649402984,20.53085429235503,21.93869191818988,19.573355649402984,21.339198983880678,20.53085429235503,19.573355649402984,22.78434120954013,22.78434120954013,20.006166383672266,19.573355649402984,25.394935420175504,20.53085429235503,21.339198983880678,20.006166383672266,19.573355649402984,19.573355649402984,21.339198983880678,20.53085429235503,19.573355649402984,21.93869191818988,20.53085429235503,19.573355649402984,20.53085429235503,20.53085429235503,21.93869191818988,23.761820762880586,19.573355649402984,20.53085429235503,20.53085429235503,23.761820762880586,20.53085429235503,20.53085429235503,20.53085429235503,21.93869191818988,20.006166383672266,23.761820762880586,20.53085429235503,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,20.006166383672266,20.53085429235503,20.006166383672266,20.006166383672266,20.53085429235503,21.339198983880678,22.78434120954013,19.573355649402984,21.339198983880678,21.339198983880678,19.573355649402984,23.761820762880586,19.573355649402984,20.53085429235503,20.53085429235503,19.573355649402984,20.53085429235503,20.53085429235503,20.006166383672266,20.53085429235503,20.006166383672266,21.93869191818988,20.53085429235503,21.339198983880678,20.53085429235503,21.339198983880678,19.573355649402984,21.339198983880678,20.53085429235503,21.339198983880678,19.573355649402984,20.53085429235503,21.93869191818988,20.006166383672266,21.339198983880678,20.53085429235503,19.573355649402984,20.53085429235503,19.573355649402984,20.006166383672266,20.006166383672266,20.006166383672266,19.573355649402984,21.93869191818988,20.006166383672266,22.78434120954013,23.761820762880586,21.93869191818988,25.394935420175504,21.339198983880678,20.006166383672266,23.761820762880586,23.761820762880586,20.53085429235503,19.573355649402984,21.339198983880678,21.93869191818988,20.53085429235503,21.93869191818988,19.573355649402984,21.339198983880678,22.78434120954013,20.53085429235503,20.006166383672266,23.761820762880586,21.93869191818988,19.573355649402984,21.93869191818988,22.78434120954013,19.573355649402984,21.93869191818988,25.394935420175504,20.006166383672266,20.53085429235503,20.53085429235503,19.573355649402984,21.339198983880678,20.53085429235503,21.93869191818988,20.006166383672266,21.93869191818988,19.573355649402984,19.573355649402984,21.93869191818988,19.573355649402984,19.573355649402984,20.53085429235503,21.339198983880678,20.006166383672266,20.53085429235503,20.53085429235503,19.573355649402984,20.006166383672266,20.53085429235503,19.573355649402984,20.53085429235503,20.006166383672266,22.78434120954013,20.006166383672266,20.006166383672266,19.573355649402984,21.93869191818988,19.573355649402984,21.339198983880678,21.339198983880678,20.53085429235503,19.573355649402984,20.53085429235503,20.53085429235503,21.339198983880678,21.339198983880678,21.93869191818988,20.006166383672266,19.573355649402984,20.006166383672266,22.78434120954013,25.394935420175504,20.53085429235503,20.006166383672266,20.53085429235503,20.006166383672266,21.93869191818988,21.339198983880678,20.53085429235503,21.339198983880678,20.53085429235503,19.573355649402984,20.53085429235503,20.53085429235503,21.339198983880678,20.53085429235503,23.761820762880586,20.53085429235503,20.006166383672266,21.339198983880678,21.93869191818988,20.53085429235503,20.53085429235503,22.78434120954013,20.006166383672266,21.339198983880678,20.53085429235503,19.573355649402984,20.53085429235503,19.573355649402984,20.53085429235503,21.339198983880678,20.53085429235503,20.006166383672266,20.53085429235503,19.573355649402984,23.761820762880586,19.573355649402984,20.53085429235503,23.761820762880586,20.006166383672266,20.006166383672266,21.339198983880678,19.573355649402984,19.573355649402984,20.53085429235503,20.53085429235503,22.78434120954013,21.339198983880678,20.006166383672266,21.339198983880678,23.761820762880586,20.53085429235503,19.573355649402984,21.93869191818988,20.006166383672266,20.006166383672266,21.339198983880678,21.93869191818988,20.006166383672266,20.53085429235503,23.761820762880586,20.53085429235503,23.761820762880586,20.53085429235503,21.339198983880678,19.573355649402984,21.339198983880678,20.53085429235503,20.53085429235503,21.339198983880678,20.53085429235503,21.93869191818988,19.573355649402984,21.339198983880678,19.573355649402984,20.006166383672266,20.53085429235503,20.53085429235503,21.339198983880678,21.339198983880678,19.573355649402984,23.761820762880586,20.53085429235503,21.93869191818988,20.53085429235503,23.761820762880586,20.53085429235503,20.006166383672266,21.339198983880678,20.53085429235503,20.53085429235503,20.006166383672266,19.573355649402984,21.339198983880678,20.53085429235503,22.78434120954013,25.394935420175504,25.394935420175504,20.53085429235503,20.006166383672266,19.573355649402984,20.53085429235503,22.78434120954013,19.573355649402984,20.53085429235503,19.573355649402984,22.78434120954013,22.78434120954013,20.53085429235503,19.573355649402984,20.53085429235503,19.573355649402984,21.339198983880678,20.006166383672266,19.573355649402984,20.006166383672266,19.573355649402984,21.93869191818988,19.573355649402984,20.006166383672266,20.53085429235503,21.93869191818988,21.339198983880678,23.761820762880586,23.761820762880586,21.93869191818988,20.53085429235503,19.573355649402984,20.53085429235503,19.573355649402984,21.339198983880678,19.573355649402984,20.006166383672266,19.573355649402984,21.93869191818988,19.573355649402984,20.006166383672266,20.006166383672266,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,20.53085429235503,21.339198983880678,19.573355649402984,20.53085429235503,21.93869191818988,19.573355649402984,19.573355649402984,19.573355649402984,21.339198983880678,20.53085429235503,21.339198983880678,21.93869191818988,20.006166383672266,21.93869191818988,20.53085429235503,21.339198983880678,20.53085429235503,21.93869191818988,20.53085429235503,19.573355649402984,20.006166383672266,19.573355649402984,21.339198983880678,20.006166383672266,25.394935420175504,19.573355649402984,22.78434120954013,21.339198983880678,21.339198983880678,21.93869191818988,20.006166383672266,20.53085429235503,21.339198983880678,21.93869191818988,20.006166383672266,21.339198983880678,20.53085429235503,20.006166383672266,20.006166383672266,19.573355649402984,22.78434120954013,25.394935420175504,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,21.339198983880678,21.339198983880678,19.573355649402984,20.53085429235503,20.53085429235503,20.006166383672266,20.53085429235503,20.53085429235503,22.78434120954013,23.761820762880586,20.006166383672266,21.93869191818988,20.53085429235503,20.006166383672266,20.006166383672266,21.93869191818988,20.53085429235503,19.573355649402984,20.53085429235503,21.339198983880678,19.573355649402984,20.53085429235503,23.761820762880586,20.53085429235503,20.53085429235503,21.339198983880678,21.93869191818988,23.761820762880586,20.53085429235503,20.53085429235503,20.006166383672266,20.006166383672266,20.53085429235503,21.93869191818988,20.006166383672266,20.53085429235503,19.573355649402984,21.93869191818988,20.006166383672266,20.53085429235503,20.006166383672266,20.53085429235503,21.93869191818988,19.573355649402984,21.93869191818988,20.53085429235503,20.53085429235503,21.339198983880678,20.006166383672266,20.53085429235503,21.93869191818988,20.53085429235503,20.006166383672266,21.339198983880678,20.53085429235503,19.573355649402984,21.339198983880678,20.53085429235503,20.006166383672266,20.006166383672266,20.53085429235503,21.339198983880678,21.339198983880678,21.339198983880678,20.006166383672266,19.573355649402984,20.53085429235503,21.339198983880678,21.93869191818988,21.339198983880678,21.93869191818988,20.53085429235503,20.53085429235503,20.006166383672266,20.53085429235503,23.761820762880586,21.339198983880678,21.339198983880678,21.339198983880678,21.93869191818988,23.761820762880586,21.339198983880678,19.573355649402984,20.53085429235503,20.53085429235503,22.78434120954013,20.006166383672266,20.53085429235503,21.339198983880678,20.53085429235503,20.006166383672266,22.78434120954013,20.53085429235503,20.006166383672266,21.339198983880678,19.573355649402984,20.006166383672266,20.006166383672266,20.53085429235503,20.53085429235503,20.006166383672266,20.006166383672266,22.78434120954013,21.339198983880678,20.53085429235503,20.006166383672266,21.93869191818988,20.53085429235503,20.006166383672266,21.93869191818988,21.93869191818988,20.53085429235503,20.53085429235503,20.53085429235503,21.339198983880678,20.53085429235503,23.761820762880586,19.573355649402984,20.53085429235503,20.53085429235503,20.53085429235503,20.53085429235503,20.006166383672266,21.93869191818988,20.53085429235503,19.573355649402984,20.53085429235503,21.93869191818988,19.573355649402984,20.53085429235503,19.573355649402984,22.78434120954013,20.53085429235503,19.573355649402984,23.761820762880586,20.006166383672266,21.93869191818988,20.53085429235503,21.339198983880678,20.53085429235503,20.006166383672266,21.93869191818988,19.573355649402984,20.53085429235503,19.573355649402984,20.53085429235503,21.339198983880678,20.53085429235503,20.006166383672266,20.006166383672266,19.573355649402984,22.78434120954013,21.339198983880678,20.53085429235503,21.93869191818988,21.93869191818988,22.78434120954013,23.761820762880586,21.93869191818988,20.006166383672266,20.006166383672266,19.573355649402984,20.53085429235503,19.573355649402984,21.93869191818988,20.53085429235503,19.573355649402984,21.339198983880678,21.339198983880678,21.339198983880678,21.93869191818988,20.006166383672266,20.53085429235503,20.53085429235503,20.53085429235503,19.573355649402984,21.339198983880678,23.761820762880586,25.394935420175504,20.53085429235503,23.761820762880586,20.53085429235503,22.78434120954013,20.006166383672266,20.53085429235503,25.394935420175504,22.78434120954013,20.53085429235503,20.53085429235503,20.53085429235503,19.573355649402984,20.53085429235503,22.78434120954013,19.573355649402984,19.573355649402984,21.339198983880678,19.573355649402984,21.93869191818988,20.006166383672266,21.93869191818988,20.006166383672266,23.761820762880586,20.006166383672266,19.573355649402984,19.573355649402984,20.53085429235503,19.573355649402984,21.339198983880678,21.93869191818988,19.573355649402984,21.339198983880678,19.573355649402984,20.53085429235503,21.93869191818988,20.006166383672266,23.761820762880586,19.573355649402984,20.53085429235503,20.53085429235503,21.339198983880678,19.573355649402984,19.573355649402984,20.006166383672266,20.53085429235503,23.761820762880586,20.006166383672266,20.53085429235503,21.93869191818988,20.53085429235503,22.78434120954013,19.573355649402984,20.53085429235503,19.573355649402984,20.53085429235503,20.53085429235503,21.339198983880678,21.93869191818988,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,20.53085429235503,20.53085429235503,20.53085429235503,21.93869191818988,21.93869191818988,20.53085429235503,20.53085429235503,22.78434120954013,19.573355649402984,20.53085429235503,20.53085429235503,19.573355649402984,23.761820762880586,21.339198983880678,21.339198983880678,20.53085429235503,20.53085429235503,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,20.006166383672266,21.93869191818988,21.339198983880678,23.761820762880586,20.53085429235503,20.53085429235503,21.339198983880678,19.573355649402984,20.53085429235503,21.93869191818988,20.006166383672266,19.573355649402984,21.93869191818988,20.53085429235503,20.006166383672266,19.573355649402984,22.78434120954013,20.53085429235503,21.93869191818988,19.573355649402984,20.53085429235503,21.93869191818988,21.93869191818988,21.93869191818988,21.339198983880678,21.339198983880678,21.93869191818988,20.53085429235503,20.006166383672266,20.53085429235503,20.53085429235503,25.394935420175504,19.573355649402984,19.573355649402984,19.573355649402984,23.761820762880586,20.006166383672266,20.53085429235503,20.006166383672266,19.573355649402984,21.339198983880678,19.573355649402984,23.761820762880586,20.006166383672266,20.53085429235503,19.573355649402984,21.339198983880678,21.93869191818988,22.78434120954013,20.006166383672266,20.53085429235503,20.53085429235503,21.93869191818988,20.006166383672266,23.761820762880586,22.78434120954013,19.573355649402984,20.006166383672266,20.006166383672266,20.006166383672266,20.53085429235503,20.53085429235503,20.53085429235503,25.394935420175504,20.53085429235503,20.006166383672266,21.339198983880678,20.006166383672266,21.339198983880678,20.006166383672266,19.573355649402984,20.53085429235503,20.006166383672266,19.573355649402984,21.339198983880678,20.53085429235503,20.006166383672266,21.339198983880678,20.53085429235503,25.394935420175504,20.006166383672266,23.761820762880586,20.53085429235503,20.53085429235503,20.006166383672266,20.006166383672266,19.573355649402984,19.573355649402984,19.573355649402984,21.339198983880678,22.78434120954013,19.573355649402984,23.761820762880586,20.53085429235503,20.006166383672266,20.53085429235503,23.761820762880586,21.93869191818988,21.339198983880678,25.394935420175504,20.53085429235503,20.006166383672266,21.339198983880678,21.339198983880678,20.53085429235503,25.394935420175504,21.339198983880678,22.78434120954013,20.53085429235503,22.78434120954013,19.573355649402984,21.339198983880678,25.394935420175504,23.761820762880586,20.53085429235503,21.339198983880678,21.93869191818988,19.573355649402984,20.53085429235503,20.006166383672266,21.339198983880678,20.53085429235503,19.573355649402984,19.573355649402984,20.53085429235503,21.339198983880678,20.006166383672266,20.006166383672266,20.006166383672266,21.93869191818988,23.761820762880586,19.573355649402984,19.573355649402984,19.573355649402984,22.78434120954013,21.339198983880678,19.573355649402984,20.53085429235503,20.53085429235503,23.761820762880586,21.93869191818988,21.339198983880678,19.573355649402984,19.573355649402984,19.573355649402984,19.573355649402984,21.93869191818988,20.53085429235503,20.53085429235503,21.93869191818988,19.573355649402984,20.53085429235503,20.53085429235503,20.53085429235503,20.53085429235503,21.93869191818988,20.53085429235503,19.573355649402984,20.53085429235503,20.006166383672266,19.573355649402984,20.53085429235503,21.339198983880678,23.761820762880586,20.006166383672266,21.93869191818988,20.53085429235503,20.53085429235503,20.53085429235503,20.53085429235503,20.53085429235503,20.53085429235503,21.339198983880678,19.573355649402984,23.761820762880586,21.339198983880678,20.53085429235503,20.53085429235503,20.53085429235503,20.006166383672266,21.339198983880678,20.006166383672266,20.53085429235503,20.006166383672266,20.53085429235503,20.53085429235503,20.53085429235503,21.93869191818988,20.006166383672266,19.573355649402984,21.93869191818988,21.93869191818988,20.53085429235503,19.573355649402984,20.006166383672266,19.573355649402984,20.53085429235503,20.53085429235503,20.006166383672266,19.573355649402984,22.78434120954013,19.573355649402984,25.394935420175504,25.394935420175504,19.573355649402984,20.006166383672266,19.573355649402984,22.78434120954013,20.006166383672266,19.573355649402984,20.53085429235503,21.339198983880678,20.53085429235503,20.006166383672266,20.53085429235503,20.006166383672266,20.006166383672266,21.93869191818988,20.53085429235503,20.006166383672266,19.573355649402984,21.339198983880678,23.761820762880586,20.53085429235503,21.93869191818988,20.006166383672266,19.573355649402984,20.53085429235503,19.573355649402984,20.53085429235503,22.78434120954013,19.573355649402984,23.761820762880586,20.53085429235503,20.006166383672266,20.53085429235503,21.339198983880678,20.006166383672266,20.53085429235503,20.53085429235503,19.573355649402984,19.573355649402984,20.53085429235503,21.93869191818988,20.53085429235503,19.573355649402984,20.53085429235503,20.006166383672266,21.339198983880678,21.93869191818988,25.394935420175504,21.93869191818988,20.53085429235503,19.573355649402984,20.53085429235503,20.53085429235503,25.394935420175504,21.93869191818988,20.006166383672266,22.78434120954013,20.53085429235503,19.573355649402984,20.006166383672266,20.53085429235503,21.339198983880678,23.761820762880586,19.573355649402984,25.394935420175504,19.573355649402984,20.53085429235503,19.573355649402984,20.006166383672266,19.573355649402984,22.78434120954013,20.006166383672266,21.339198983880678,22.78434120954013,20.006166383672266,20.53085429235503,23.761820762880586,20.53085429235503,21.93869191818988,21.93869191818988,21.339198983880678,25.394935420175504,23.761820762880586,22.78434120954013,19.573355649402984,20.53085429235503,20.53085429235503,21.339198983880678,20.53085429235503,20.53085429235503,21.93869191818988,22.78434120954013,19.573355649402984,20.006166383672266,20.006166383672266,20.53085429235503,20.006166383672266,20.53085429235503,21.93869191818988,20.53085429235503,20.53085429235503,23.761820762880586,19.573355649402984,21.93869191818988,21.339198983880678,21.339198983880678,20.53085429235503,20.006166383672266,19.573355649402984,21.93869191818988,22.78434120954013,23.761820762880586,19.573355649402984,20.006166383672266,21.339198983880678,23.761820762880586,20.006166383672266,20.53085429235503,19.573355649402984,22.78434120954013,20.53085429235503,20.006166383672266,20.006166383672266,20.53085429235503,21.339198983880678,21.339198983880678,19.573355649402984,20.006166383672266,20.53085429235503,21.339198983880678,21.93869191818988,20.006166383672266,19.573355649402984,21.339198983880678,20.53085429235503,20.53085429235503,19.573355649402984,25.394935420175504,20.006166383672266,20.006166383672266,19.573355649402984,19.573355649402984,23.761820762880586,20.53085429235503,21.339198983880678,19.573355649402984,23.761820762880586,21.339198983880678,21.339198983880678,19.573355649402984,19.573355649402984,20.53085429235503,20.53085429235503,21.93869191818988,21.339198983880678,21.93869191818988,23.761820762880586,20.53085429235503,19.573355649402984,21.339198983880678,20.53085429235503,21.93869191818988,19.573355649402984,19.573355649402984,21.93869191818988,20.53085429235503,20.53085429235503,20.53085429235503,19.573355649402984,20.53085429235503,19.573355649402984,19.573355649402984,19.573355649402984,21.93869191818988,19.573355649402984,20.53085429235503,20.53085429235503,20.53085429235503,20.53085429235503,21.93869191818988,23.761820762880586,20.53085429235503,23.761820762880586,19.573355649402984,23.761820762880586,20.006166383672266,19.573355649402984,20.53085429235503,20.006166383672266,21.93869191818988,20.006166383672266,19.573355649402984,22.78434120954013,20.53085429235503,21.339198983880678,20.53085429235503,20.53085429235503,21.339198983880678,20.53085429235503,21.339198983880678,20.006166383672266,20.53085429235503,21.339198983880678,19.573355649402984,20.006166383672266,20.53085429235503,21.93869191818988,19.573355649402984,20.006166383672266,21.339198983880678,21.93869191818988,20.006166383672266,21.339198983880678,23.761820762880586,21.339198983880678,20.53085429235503,20.006166383672266,22.78434120954013,19.573355649402984,25.394935420175504,19.573355649402984,25.394935420175504,19.573355649402984,20.006166383672266,20.53085429235503,19.573355649402984,20.53085429235503,21.339198983880678,20.53085429235503,23.761820762880586,21.93869191818988,23.761820762880586,21.339198983880678,20.006166383672266,20.53085429235503,20.53085429235503,20.006166383672266,20.53085429235503,20.53085429235503,20.53085429235503,23.761820762880586,19.573355649402984,19.573355649402984,20.006166383672266,20.006166383672266,20.53085429235503,21.93869191818988,20.006166383672266,19.573355649402984,23.761820762880586,20.006166383672266,21.339198983880678,25.394935420175504,19.573355649402984,21.339198983880678,23.761820762880586,20.53085429235503,20.006166383672266,21.339198983880678,20.53085429235503,21.93869191818988,20.006166383672266,19.573355649402984,19.573355649402984,20.53085429235503,21.93869191818988,22.78434120954013,20.53085429235503,21.93869191818988,19.573355649402984,20.53085429235503,23.761820762880586,23.761820762880586,19.573355649402984,25.394935420175504,20.53085429235503,19.573355649402984,19.573355649402984,20.53085429235503,21.339198983880678,20.53085429235503,20.53085429235503,20.53085429235503,19.573355649402984,19.573355649402984,19.573355649402984,25.394935420175504,20.53085429235503,21.93869191818988,20.006166383672266,20.006166383672266,19.573355649402984,20.53085429235503,19.573355649402984,23.761820762880586,21.339198983880678,21.93869191818988,20.53085429235503,20.53085429235503,20.53085429235503,20.53085429235503,20.53085429235503,21.93869191818988,21.93869191818988,20.53085429235503,21.339198983880678,19.573355649402984,20.006166383672266,23.761820762880586,19.573355649402984,21.93869191818988,21.93869191818988,21.93869191818988,20.53085429235503,22.78434120954013,20.53085429235503,20.53085429235503,21.93869191818988,23.761820762880586,19.573355649402984,20.53085429235503,25.394935420175504,21.339198983880678,21.93869191818988,20.53085429235503,21.93869191818988,25.394935420175504,21.93869191818988,20.53085429235503,20.53085429235503,20.006166383672266,21.93869191818988,21.93869191818988,25.394935420175504,20.53085429235503,21.93869191818988,20.53085429235503,20.53085429235503,19.573355649402984,23.761820762880586,21.339198983880678,23.761820762880586,20.006166383672266,20.53085429235503,20.006166383672266,19.573355649402984,20.53085429235503,23.761820762880586,21.339198983880678,23.761820762880586,20.53085429235503,19.573355649402984,22.78434120954013,20.006166383672266,23.761820762880586,19.573355649402984,21.93869191818988,20.53085429235503,19.573355649402984,20.53085429235503,22.78434120954013,20.006166383672266,21.339198983880678,21.339198983880678,20.53085429235503,23.761820762880586,20.53085429235503,19.573355649402984,22.78434120954013,20.006166383672266,20.53085429235503,20.53085429235503,20.53085429235503,20.006166383672266,20.53085429235503,21.93869191818988,19.573355649402984,23.761820762880586,20.006166383672266,21.93869191818988,19.573355649402984,19.573355649402984,20.53085429235503,19.573355649402984,21.339198983880678,20.006166383672266,23.761820762880586,21.339198983880678,20.53085429235503,19.573355649402984,20.006166383672266,20.53085429235503,21.93869191818988,21.339198983880678,19.573355649402984,21.339198983880678,20.53085429235503,23.761820762880586,20.006166383672266,21.93869191818988,23.761820762880586,20.006166383672266,23.761820762880586,21.339198983880678,19.573355649402984,20.53085429235503,20.53085429235503,21.339198983880678,19.573355649402984,25.394935420175504,21.93869191818988,20.006166383672266,21.93869191818988,19.573355649402984,20.006166383672266,20.006166383672266,20.53085429235503,23.761820762880586,21.339198983880678,20.006166383672266,20.53085429235503,23.761820762880586,20.53085429235503,20.006166383672266,20.006166383672266,21.93869191818988,22.78434120954013,20.53085429235503,21.93869191818988,23.761820762880586,20.006166383672266,20.006166383672266,21.339198983880678,20.006166383672266,22.78434120954013,20.006166383672266,21.93869191818988,20.006166383672266,19.573355649402984,19.573355649402984,20.53085429235503,20.006166383672266,20.006166383672266,19.573355649402984,23.761820762880586,20.53085429235503,21.339198983880678,20.53085429235503,21.339198983880678,21.339198983880678,22.78434120954013,20.53085429235503,21.339198983880678,21.93869191818988,20.53085429235503,21.339198983880678,20.006166383672266,19.573355649402984,19.573355649402984,21.93869191818988,23.761820762880586,20.006166383672266,22.78434120954013,19.573355649402984,21.339198983880678,22.78434120954013,23.761820762880586,20.53085429235503,21.93869191818988,21.93869191818988,20.53085429235503,20.53085429235503,19.573355649402984,20.006166383672266,20.53085429235503,23.761820762880586,20.53085429235503,20.006166383672266,21.339198983880678,20.53085429235503,21.93869191818988,20.53085429235503,20.006166383672266,20.006166383672266,20.006166383672266,20.006166383672266,21.339198983880678],"xaxis":"x","y":[21.224638239972652,20.180643707721647,20.201341431117804,19.1454180237343,21.79470380164697,21.087755789009996,22.432940196049124,19.578846464119135,19.242152996635603,24.328668011629535,21.754493646354923,21.476822454566275,22.377553705906216,19.555124576954142,22.494011033388475,22.606991826298977,18.49440443743341,20.76699650557159,29.702932107840674,20.84994793377778,20.282334104450392,19.302304652102425,20.014253448408297,19.02478155099295,20.401136284262034,21.066918904158555,20.30528553843669,22.443056343683086,18.530754586286314,19.472341179018663,21.511675814584603,20.024996233202344,21.986420084483047,22.32477661567823,19.024668956843474,20.277557571355647,20.011894681671833,23.15521167718674,21.342870607762556,20.008806098312462,21.88562823605827,23.73392435177354,20.358652273917528,22.63391493571178,20.2953770114005,18.603706022480708,19.476619646192038,22.286954060950823,20.827570650601533,19.658607240576167,20.944082894927185,17.889444547032436,20.210011039401028,20.041902410540708,22.116524819871177,23.07232514287102,18.931245369435278,19.243034483473725,22.175009164867692,18.8696149273692,22.748443989592825,20.18471556395119,19.630595419370202,19.596480627252774,22.075827573908075,20.81706640440522,20.033430227416382,19.472993206549905,19.47575297115503,19.712127286089668,22.22369094458836,19.974424393125233,20.76732896720974,19.73170766432226,20.622497836834853,19.074352191519743,22.597191120827183,21.03390016791011,22.014068840206768,21.41124459992273,20.690267441816314,22.238063995762765,22.835153147091482,21.940489607019817,20.06072495330695,19.26508618967162,20.593917077867186,20.807981392523768,19.461766940454588,19.92317059187003,20.905864572180928,19.887154874828134,22.674741834325705,19.54492482512454,21.650322094933124,23.6494594685329,21.673503182786,25.42292365756086,23.598781185238046,18.99960834234589,24.597055201180662,22.731123431811554,19.734159687698988,18.832054600441634,20.397708053035622,21.477527921557282,21.731372277489818,21.51301334156099,18.824250825082274,21.177749972976986,22.50813230656899,20.78875924862035,19.355480148534408,23.890614784737277,22.347688134832723,19.773789560999628,21.94580529932221,22.19000668879013,19.515006779114817,22.99883477706531,24.630356510701596,19.78112594827553,20.45440259621003,20.195062159996276,21.08554669425927,22.350651659007692,19.88287753302219,23.223107004233785,19.56773145357299,22.383567194160783,19.337397595787667,19.84140252838032,24.09542285957458,18.7932762528682,18.78833918294514,13.482051629033913,21.99805786914009,20.68539834867811,20.75326327581395,18.386700720994313,20.922404198298533,19.34599856610255,23.03775857233843,18.64679105166152,20.517600912543127,20.206849822052806,22.025147336797662,19.802893385023037,20.525632635954643,20.1770277231615,23.357834800271604,19.12730750072142,23.147280554074683,21.983261369651657,20.345804248394614,19.067786210059246,20.11323917875491,20.682137237753555,21.173013406383724,22.83375379636568,21.704780057376663,19.98202682053532,19.114539527933296,22.520164246985647,22.815206783859768,26.613491397737352,21.053960341071367,19.740225451250925,20.911400034394557,19.239293618022067,22.519031496965763,20.18180618902739,21.47581257946244,21.841152373314472,19.894016350184707,18.947642644803835,20.33821258053668,21.998370834730604,21.623042657920415,20.52474339677355,23.920588558527324,22.398384643288747,20.614611901935827,20.73652363425427,24.216251851014647,0,23.798062566113977,22.44169398922513,19.04780522125321,22.508853453023672,20.361268080220963,19.223547338390134,20.037360610262493,19.47969541290554,21.419330156983328,19.397345612637686,21.062803097791257,20.802720957968873,22.787613064091445,19.854352512470033,23.127238847948263,21.236410750491928,19.39740450234132,24.629767207576666,19.424905147339715,17.66325927218184,21.589387814254906,19.856012075783497,19.346653316091935,20.250448783340936,19.91174617647474,25.664004852328446,20.437440195258947,20.159778968313912,19.760702082570422,23.894186440194584,19.664395199250066,20.03503448449456,21.85979788510952,21.007452333347548,19.330905939549268,21.816602846893197,22.163197583801455,21.21476614620658,19.61795076264764,22.32608970797742,21.206937139516242,23.650902186871754,19.538495641550785,21.951268793802367,19.712186074557728,21.6397845044252,19.566140232030243,23.528005120775013,21.76194235515012,19.759231530600534,22.69657897049395,20.97108250648827,20.91944021038121,19.11091145461158,20.92382072739302,19.99564295418165,20.157995405754583,21.499214768467855,21.850862595399537,19.613806547176576,23.262476414885043,19.981118924422468,23.526907160221576,20.11872947152901,23.756207113729698,21.05135284285491,20.459271509540073,20.732604891283092,20.22193428509613,21.2413041438754,20.86782723578085,18.85414731140336,21.746512886013605,21.477990920672163,24.227674033652324,25.12798066804062,26.560353492028305,23.633181830866906,21.120826057383237,20.22792847217847,22.242317589489687,21.590387855717424,19.62630225708032,21.126056594579662,22.585399041291762,22.379011027342607,22.883846216197703,20.986097829919892,21.770367489208184,20.58669309831316,19.43536098584635,21.22950036954701,19.51332725702657,19.38984557001998,20.48325363998835,20.246730162293353,21.286321988690805,20.685520474331085,21.05996063251775,20.18838178436931,21.836335494435165,22.094100243073623,22.994379248863297,22.721264543402345,20.729598340663756,16.46658160832902,21.557923127735883,19.880806435323297,19.801676786988352,21.12828261562749,18.90461110336481,19.19842345841963,20.092049623606254,20.0459782414438,18.31795878333722,20.7963375193431,17.813568277801206,19.627334048970262,18.56499419954315,19.372221038245304,20.307717788382128,19.384134222108464,20.087817122473812,19.502286013993587,21.227181698040095,23.312865371844044,19.405079121883418,20.90714373186156,21.80480516491849,18.974285739983618,19.176540848422555,19.19474964169837,20.93275451807793,20.832136714555805,21.170532392724574,22.709608681261756,19.86611073536812,22.055547735078193,19.980960001271505,22.653172763321756,20.76687340459434,20.847191191309296,20.06982734293826,20.638856572715298,19.67156525332246,19.49519726866692,22.064314182636632,20.227612418267608,26.201435233706484,19.068639219627578,22.656078929958284,20.77129521126714,20.391385205003306,22.283464834927894,20.845264152741507,21.027619198526104,21.07443745441354,21.518584006439504,19.724170230709305,20.679280780496406,18.927945745238855,21.825732486030095,20.464091007664432,19.439137307013926,22.977548256996624,23.988556498480502,19.81821879465883,19.050281782029014,19.439732238591404,17.74375624808358,19.990444456103596,0,18.279601513401296,19.78875556523867,19.965832185850637,20.69408662857846,19.405573621751774,22.379934620081865,22.48871602661339,23.642331039751582,20.276556094805027,23.283922162181995,20.195578868662224,20.03811220566055,19.14964293785794,20.90803339180857,21.473710208099476,19.884197354704103,22.164351182655523,22.813153319462682,18.901120542977083,20.544639287700853,22.988276940898057,20.109117749689304,19.622850798933502,21.871633434683428,22.25017305320405,23.60048678836413,21.515067916471864,22.320806056618878,19.870759032200375,20.544449448312626,20.261067504600565,23.43234984844275,22.4595812045188,22.52530214820436,19.382106816183008,23.96470652783692,21.019906267101412,19.811383905702034,19.845505364796534,20.51412216383031,20.373076483151994,19.01511010272945,21.47364877893911,19.957005833266606,19.38761598733965,22.32447174476143,20.79356441567801,22.807056132685855,24.028256571904787,19.874893102286897,19.393396349595932,21.076706013187607,21.881924997624267,19.305427636738685,20.88679803779653,22.116604136681058,20.73926969664862,19.30704929356398,20.730402345071546,21.49837252262123,21.892478522473827,21.156990969891915,19.626240196846272,19.456579392491523,19.83361590003151,18.640025995609424,21.63591845810156,20.866343204364526,21.462764853897205,18.837375101562536,20.833447685328323,20.03253125195289,21.036829264514445,22.96164822427184,21.237452292596537,20.43520123557409,21.156628899315663,22.496434708234126,22.4307529287208,21.544986593746263,21.11234277813463,21.351967832505505,20.39596499860976,22.86476103379213,20.105565995835796,20.48988421076927,21.684940338873297,21.559903574554543,22.73966855399758,21.952844226236508,20.330025112864092,21.040198736231254,20.75190246631361,18.798041102461394,19.29157093266521,19.97055996653021,19.307592868706436,20.89335338152254,19.773921527816817,21.100709172660746,21.699220401665585,21.041684548715036,20.396544813616845,22.752749154816065,24.957468167964976,20.506265808043914,20.374729613814893,21.525022859298808,23.275631354399913,21.75794824506378,19.767052898077655,21.218145625261048,21.536078922759724,18.276769619768288,24.94868149530031,20.41835179368666,20.87359355439842,20.51145154622031,20.700075492290427,21.289957235167595,20.748925773432315,21.959550623213953,22.62198208884591,19.38396579774315,23.31051349089806,21.627932359551377,18.954513256380405,19.964316452607815,19.32031247988204,24.27577420165758,20.498820970198462,20.060863127783303,24.1535698900103,18.16161414446258,19.814472942891484,22.998819199975294,21.020751435153667,21.36177568305088,18.469113141288748,23.192581335464276,19.21725486016369,20.111754471035223,18.939248297157278,20.474687273562058,21.111612649480875,19.87603574815164,19.637857750008234,20.264370436179377,19.93146081401427,23.29863946166844,21.939745630681145,19.743514665635868,22.649298943378753,21.24045362673594,25.608747220451313,23.49090374555705,23.60775201354553,20.678206389277292,19.915728419308504,20.449383439914303,19.824402536620187,20.53678125858074,22.159724468145836,21.034942664057727,19.03923233773122,20.84977321505041,21.781868242254394,20.46100849412487,22.004305689493815,19.142842792258786,19.496508584409785,21.834751391009725,20.402181439081684,19.21550544985804,20.640134807950194,23.051954851262433,27.036279453018476,20.37977707208339,21.775846548503456,21.781910553210967,22.353218452653266,20.165877660805254,19.451496130670126,25.490707221646936,22.59150705240579,19.995956185326282,20.28232915493825,19.579291420064532,20.879430687458026,21.138173662648782,24.394611647917593,19.533857847503334,19.817723799212757,21.171020993541568,20.50991100623185,22.326932218927173,19.527299811101727,21.383177447413818,19.392837666571104,23.017914332247763,20.82549425312562,19.072268708954628,19.10179618438648,23.29660399040893,19.32638298997934,20.650313109616484,22.182752894747274,20.620515848299537,21.682830461889793,20.248785506000544,20.060843672034636,21.235100056704788,19.639114342316898,22.873156355744623,20.616811975583957,21.260660009666424,20.98111639712373,20.640272301462904,20.347463631132083,19.255662542640188,21.590649178257912,18.627786784209636,25.25576312419618,18.13136213470814,20.18507504172347,22.234490018926184,19.867378293238115,22.842407702796528,19.642302060721956,20.934073076796377,19.657897588782742,20.907660872216642,21.21194412605265,19.94161891161789,21.31905629271608,18.792730927848385,19.734888199833236,18.03332620487895,20.049848549450562,20.009665992350836,21.60568082130027,15.704606153972572,19.9492653280832,21.029500902796773,21.639091593296747,19.630944296038688,19.444205825873514,22.72561505846151,17.641741979338622,23.57832742490694,21.06232551474456,20.596453669823063,23.77569135147608,20.739222922576804,20.798718195952173,20.117453663452505,20.134364899685817,19.1796783064222,19.08437502019272,20.74807693982553,18.925763040720064,21.23645513023823,22.042981377352834,21.11735552640613,23.60188649068525,20.85643673732399,20.248587775712384,22.0971519060863,19.917597162486278,22.120758360865494,22.44443523329341,21.796740313790345,20.192956161016543,21.587428114342384,19.88222659302695,20.074407563972667,19.772030191805687,22.686480223069132,21.085461437053635,21.84811725117268,18.71863791145474,19.897316659199046,22.368646863289158,18.2265721237695,21.061479969798718,21.091487685756913,21.905619198419114,22.248853822319084,20.948206085285577,20.95171629836988,20.60386819333271,21.712312461652626,26.34702224299619,19.173408752888843,18.94863556098072,19.669251869992156,24.602654298813697,20.178503166977418,20.318803159054653,19.8813455186455,19.742006737354636,20.848691129006262,19.533504643608662,23.044574887856943,21.2197623270523,21.881578906408237,21.91240193642795,20.941991359524017,22.077006044437276,22.54693130601029,18.995877797816178,20.997149152437558,18.687341507608156,21.335453055117675,19.376016989288264,25.07655356851684,22.59004948235383,19.378047539426834,19.682163069515127,20.52340976581651,19.581570228112746,20.9498708711936,22.82667920636651,20.99544749522815,25.17876365321419,21.627453285628658,19.477956222501692,19.193137896037967,21.893341530106955,19.71728461871742,17.31416721288844,19.147406179743545,19.827554176093653,20.471143313052202,19.29233286786097,20.500085221471384,19.644130235297485,22.034075873229305,20.949040678200554,20.540968959330602,27.231751620760654,19.73015830855728,22.493025220696325,23.089701491122522,21.64924126399572,20.795739259102447,21.118903696326274,20.815053320149143,16.52324032760632,19.730383893455734,21.24843214899758,22.071670864625634,19.282486682508637,23.126218118696794,22.228032143928214,21.93139047566312,22.735567798975932,24.167967887858172,21.655361699836966,22.600876020595507,24.59815969618305,19.718235182480434,19.521220854266264,20.564957928128795,20.971944131576173,19.6146020174933,27.021590188993713,23.73507977629211,21.659326126254854,21.01213158467734,21.766780414954713,20.08203076321153,21.756353934255824,25.471067413053014,24.02283197509071,23.385758678777925,21.23702785682451,21.578861893090256,19.189211661593234,20.07860380095908,19.328382384869762,23.106213073613993,19.95329417355663,21.365528432523575,18.914065908446993,20.846904364921407,21.621710733177533,20.214357069981805,20.48695013292876,19.590051196125845,20.988896575504317,22.71500741252458,20.63081635106876,18.585982112110514,19.30101383466248,22.43563961615873,21.19573925103901,19.253482814272665,21.733085888220945,21.51937854264472,22.935901951961643,23.1601071574333,20.83646802652741,17.151393182854086,20.807011308942396,19.447379908647196,19.74096325611752,20.942433069954838,19.870033345928174,20.91477708386503,23.264639171673256,18.81898933836999,19.40497105732979,20.010315016400824,20.436358643191994,21.24511744232927,21.394403553854325,20.278388484789648,20.16621212904916,20.06466810918053,20.8905482708943,19.02658827934665,19.696262913257698,21.147132242697182,0,21.233800456337935,21.791167162104042,20.18131197591118,20.79775274411387,20.158015290040524,21.01738902857623,21.1949084635434,22.774677489141574,21.93032100053686,18.418089755206072,23.810210740127232,20.9793536678564,18.664791000361163,20.57237479635235,19.66651702047476,20.204034803826428,19.989961267682528,19.13829898996108,20.67820982860559,19.602487517256634,19.634034627117707,20.049508251055094,19.732903567032626,22.301656666857568,19.3351286191496,19.66019789348857,22.54105973810428,21.087085320291497,21.46282774650559,18.953344800503015,19.199864465913354,18.832769512656196,21.40009037173508,19.692718291075924,20.626400634350908,18.73677516240749,22.077517747249683,18.74408127304125,24.761947476774708,26.113208352362694,19.576225059708314,22.48651099143561,19.29251486362711,21.199347342918216,22.53052802544868,19.222080263625543,20.87727804782974,21.285530909063,20.6451326442611,19.40981146751902,20.008240359915625,19.734510138857544,19.369940873414674,21.839004592901738,19.643216988179674,20.34827538264879,20.295760247970698,21.15453445100085,23.946209069915817,20.161508356002734,21.461336053955172,18.855956110093064,20.13782559566735,20.22901210527111,18.772971741326142,20.346151863763343,0,18.758138614258336,23.01986369036341,19.826388660339887,21.76883742068305,20.30266681112443,20.98525964839946,20.015097338364022,20.523507399137607,20.436904665975636,19.162579413291002,20.375536910022955,23.70344643795525,20.910669810022966,20.379640393188353,19.217287998251134,20.77164874576569,21.098649782686852,21.445335194484834,21.911569915872246,28.094893882984294,22.184719195487645,19.636646744775323,20.26686372980413,20.815307943684378,20.695469281090816,25.87586915107988,20.22604051998145,19.449746389320087,22.323663662536706,20.105603080001295,19.720177445507503,21.133857610241805,20.04287236504247,21.180586044654405,23.433073191606535,19.844702412663505,25.31201588491192,18.563017473748218,20.726116689654802,20.55471999163649,19.12308826699376,20.688277632644148,23.08698185712261,19.4188490987079,19.38789150561276,23.032593297237835,19.840625811644227,21.990665865987562,22.00972851632279,22.544699771649725,22.804072710576072,21.602864772837197,22.7909106273511,25.661642339745672,23.190044679879648,21.797480754192584,17.862153020413583,20.62223350550368,20.658934051201378,20.583030645279486,23.466890235880964,20.879510709815698,21.526914680071894,22.700127681411185,19.683189962828273,20.26341148584,19.616726513482636,19.839377869097518,19.875302398958702,20.973185759004682,21.082163500516533,20.324838680602316,20.30745292915028,25.9613964584893,19.046718202293015,21.78315042330069,20.91962424027046,21.618102712310883,18.472822451162664,19.554830163405484,19.175778559517227,21.854300382519792,24.477823287446007,25.5492833010917,20.6322831501737,18.924421094438486,22.00276008739277,23.251480243552532,19.437442862246026,20.440974576504654,19.114316794022223,23.060566976295764,21.685850226111516,19.690963589379074,19.932730372590424,21.5156696442052,21.03832189892855,21.34321011850724,20.297903654101066,20.251155291034372,22.02017509916011,21.00569855470944,23.695624962968736,19.83550351971754,20.046496736817677,21.663202256858984,21.64914607700524,21.763695119906945,18.966254774482525,25.621581882833034,21.318917047123914,20.25569596256737,19.082528830592434,19.62869805766532,25.538294718669484,19.064589338432878,24.544735671741783,19.58243213803125,23.66501619396984,19.12391425698277,21.767558412000408,18.99637745675535,21.23800756191085,21.00116832090369,22.714272038785044,21.137414277628565,21.45606258158953,22.279207015403138,23.33275256138136,22.26485604208308,20.723080111555596,20.639447033358472,20.694537918678005,20.730065203035867,18.59276109193853,18.41344612635984,22.526317266984485,19.84998714954673,20.751047310028625,19.801872521805933,20.233535275947663,20.632626738131346,21.279464358226686,22.469580214601123,20.408744363117133,22.583074271938578,21.34410908350179,21.979041347822633,20.59404343806881,20.264946502479802,19.878697565839243,22.615047199531546,23.56945490190484,19.736798486144902,23.729801820804443,20.49274951238928,23.323658577961712,20.770035525423065,19.238103804361465,19.567540218990334,20.344212591822245,22.493051859930933,19.27583351001347,18.98927807008273,23.394529944648035,22.51205924599309,21.071056073380674,21.047075972047644,20.445539922646542,21.340147837597844,20.003307902740296,21.299913824117656,20.644505230109406,20.419105854581318,21.593824209166353,19.476347089125902,19.838668436714478,19.728765556885957,22.07230878544129,19.427802084161115,21.756421739524402,21.380030171433972,21.32600862652118,19.632965031850407,21.403421935393933,24.37658838285707,21.043820980181792,20.1938730272849,19.66793597026211,19.199993380834425,20.035891808657503,25.0888235331049,18.385719188798834,26.48889669142553,20.116390923202314,19.499674996520806,19.751886432327307,20.522012502780594,0,20.782571363318098,21.618914296187317,22.967539915594646,20.95701983082078,23.548104709979565,21.65670940491333,18.258239797953646,17.88154175390193,21.233312475041235,20.277059332832177,20.919229965693855,19.653189714082398,19.02398982442439,24.205441209659515,20.16062238071552,18.963476628745678,17.96562762596234,19.531857117715408,20.86635829845881,21.255773747330974,19.280122810790118,19.369392868205782,22.9021647605954,18.93136369208174,21.47821316674072,24.42926325370543,19.880057005564264,21.11943827672747,24.0347829295736,19.174320595643714,19.810878030088894,20.711569411286565,20.79181709109951,21.52825545142037,22.510563057499144,19.714589625374927,19.195910426040165,21.643407148870626,22.79188245530678,23.088899981103793,20.914444260183114,22.409894316390734,22.55442514417979,21.497350091305258,23.661739831337005,22.81337882106461,20.335424976569172,28.119486884464127,20.443137811296648,19.395826969085725,18.744741571834194,21.52611831315801,21.048400728737306,20.235023779555554,21.0819625214105,21.53807712673746,19.283653604184654,20.890428315584586,19.706229562174084,25.296616627387504,21.809197399053033,21.371449507411626,20.19656265688086,19.305685493406397,19.08654493081137,19.639706353282477,17.583516204502104,23.250222506325386,22.82294225806205,24.601371538637977,20.0953223269386,21.243839329097405,20.37329318617189,20.45930590947763,24.987649624596244,22.400315304318365,22.625652665035147,20.455200776646638,21.021720462702472,20.59279530263181,20.232040256335257,22.264357262987502,20.592440384261195,22.36255454353493,21.99517470550352,22.199722361671988,19.391954018620176,21.83585207290849,19.14268047046844,20.53559540130039,21.167744601954155,22.926085660752005,18.718526187000297,22.28042360433217,25.95498550595432,23.32578333060166,21.309621092817217,19.68447811713372,23.268620237666717,25.42582021905868,21.189524929277866,19.937692777054252,20.801380865239704,19.316707310363867,21.75315297668226,22.868455030888445,25.947611368302585,20.033901511817852,23.20532613123076,20.554742003246933,22.96074119343453,18.988728193664233,22.68524113143557,22.64902697475969,25.92407619036082,19.407373582124833,21.346588936497685,19.366735480432887,19.124759964560997,20.07378037525111,23.985931291943867,21.460510640680972,23.23197611805285,20.73330584327736,19.12586494836624,25.275464586661446,19.715668478243582,25.489298123537854,19.013002282515146,25.312874128977917,20.415195026786527,19.91413691399634,22.130140513245646,22.92736601506673,19.584806561357016,23.12482431901705,20.687407722542194,19.611919934246885,24.151034118830534,20.079932477068777,18.93225512877545,24.075045174066695,19.899787637164348,20.874095085342866,21.48054296008632,23.07558442006914,20.640568320462897,20.08863945104873,22.28815948936714,19.616841359021425,23.953590715604236,21.784577239189012,23.301772335078063,19.749416399115965,19.811951028236585,20.17477942437346,18.304477915148002,21.513176875338157,19.58793862702669,24.19084293977443,20.88004158687887,21.10029756186111,19.35190112925433,18.917516857772046,19.84423117509742,24.377548873947223,20.529649424214696,18.766505656199122,21.439000175436664,18.811308834423215,23.933428716720645,19.631145075685264,22.477074905595526,24.176989311672042,19.985865383441666,24.300186984861302,20.78628428680642,19.166666955002903,20.396533708238668,21.16161998720379,23.822353472570573,20.556259058220718,24.241029788998915,22.067946023704817,19.778390202711652,22.99407646829777,18.833019773765052,20.288010749874967,19.947775322749504,20.338620784352592,23.494951508290605,22.38615246636496,19.034944006994706,19.984953818018486,24.081396558763124,19.595170049801663,19.41162451128841,20.72798026434645,21.785986387464803,22.944212504137,23.12678795739032,21.831113335529626,24.147006515510686,20.371960850528456,19.32549212260673,20.504720566168093,19.431273145079818,20.869899532948505,21.94524199207118,21.941322673977073,19.72760807429288,21.84503668728796,18.979888467927786,20.204391739449186,19.721950276191546,19.579038969018928,19.030933465764726,23.8143209659356,20.855181011615457,20.74517719106974,20.090817980596317,20.872940195293037,21.393924057871644,22.20317352529318,19.736694248820182,23.68012909153893,20.722700819009958,21.714666548374087,21.05462253329071,19.397806220065235,19.129172314836154,19.83710046840733,21.397724240612124,22.701728218692,19.775576525065844,24.10746972295679,17.80002145055656,21.28200527852275,24.425354799557958,24.25287711008343,21.61681534013004,21.629837063073623,22.53034703709958,21.62524006847279,20.878967106147208,18.668189678924886,19.500769926583363,19.995538068644624,23.512127623334848,21.53372289651904,20.823420888524133,22.265101932257092,20.999782941566835,21.517654972490075,19.604530267831706,19.76742293183983,20.740147099081785,22.94133404816206,19.387966817051925,20.39789928981696],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predAB_log"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB_log = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB_log = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB_log,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB_log, \\\n","BestParametresAB_log, \\\n","ScoresAB_log, \\\n","SiteEnergyUse_pred_logAB, \\\n","figAB_log = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log.ravel(),\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_predAB_log',\n","                         score=score,\n","                         param_grid=param_gridAB_log)\n","\n","print(BestParametresAB_log)\n","print(ScoresAB_log)\n","figAB_log.show()\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.555092548991254,1.5288584519959465,1.5363763280299865,1.6173866662118748,1.572337311307257,1.5856093074848776,1.6264723118975337,1.6040677513474015,1.6391995447664223,2.3154789983420745,2.2486945811975447,2.217820137205904,2.556435903837998,2.863531275669408,2.8206416758132136,3.266646184826746,3.296696673864357,3.7829465194111593,3.7991041627434328,4.264601751315409,4.402894900352772,3.891379654473181,4.599254135086597,4.28177483592277,4.941340607614743,6.083988093889654,5.196599987941977,4.758699824714103,3.6712934574241674,3.7879914343976226]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.7433828043307082,1.6993843891407403,1.7153555350638312,1.7651169111295748,1.7453514208176377,1.7448934610526323,1.779906820558197,1.8081256990440062,1.786961326477874,2.91616097263669,2.844569143619573,2.405687762473699,2.9187917692655065,3.1574542976219604,3.036670374254296,3.513632030493334,3.574580672296216,4.492196701464407,4.23423807488145,5.0163989878736945,5.3264193403413795,4.141059617993139,5.732768280524803,5.939188922735059,6.702125598426044,7.450866285123398,6.844309315045409,6.398657577419482,4.431045264955953,4.774106766965293]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.3668022936517996,1.3583325148511527,1.3573971209961417,1.4696564212941747,1.3993232017968762,1.426325153917123,1.4730378032368705,1.4000098036507969,1.4914377630549704,1.714797024047459,1.6528200187755167,2.029952511938109,2.194080038410489,2.569608253716855,2.604612977372131,3.019660339160158,3.018812675432498,3.073696337357911,3.363970250605415,3.512804514757123,3.479370460364164,3.6416996909532235,3.465739989648392,2.6243607491104806,3.1805556168034412,4.71710990265591,3.548890660838545,3.1187420720087244,2.911541649892382,2.8018761018299525]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5573604703966164,1.5270056141391126,1.5278452539343974,1.5280190421714654,1.5214419715732221,1.6930074844999181,1.6873213201803832,1.5547353929601484,1.5263245181647833,2.981874793925465,2.4233012539778085,1.9104779449903715,1.970702336542149,2.5371669636857126,2.765428881923524,2.8589433363109014,2.7659838354343744,2.5261085838115105,3.1192243761517804,2.7988616755216515,2.767761028022676,3.5682199453838455,3.0387928660392896,2.4811141477192025,3.251850643253893,3.6187717204895553,3.2921553180459,2.481345484998133,2.765774137848273,2.284364769020179],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.8750404886550072,1.802347534310652,1.8362394652525178,1.8241310985361798,1.8040546604342058,1.8181307384666061,1.8064719760365413,1.924297365089437,1.8345103164617396,1.9018257388499982,2.8513236884231357,2.4805943539131268,2.622767757773691,3.2812938635840396,3.01781706784919,3.3719510125918437,3.412168702044858,4.28245281672702,4.434791785093031,4.333104359936365,5.112298590454023,4.055141170722135,5.2360915430548705,5.187209658370145,3.557773798201254,7.134339678962201,5.938327494315052,5.061785864512757,4.603437291667468,3.7787221727992364],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.6078234688749045,1.6107549081695018,1.6047869577378362,1.750373359971568,1.744268804581235,1.5943464001747236,1.74805285161707,1.730155588959002,1.6039358696062138,2.859330796244141,1.7530124900176047,2.188024948068312,2.359626916788368,2.947575990041787,2.7532511388264704,3.1249856098945137,3.3737441167946933,3.4851330044183055,3.6573242812158098,4.801492952948203,5.361408301177585,3.650675736952805,5.865030653712268,7.060264244871519,7.015072976608346,6.912028094762827,6.5381741726074285,5.6085697164704165,2.9179357127905394,4.13961706854454],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.397795138459245,1.3785122672779837,1.376375634358762,1.4237765164853338,1.367603993238866,1.3971469672320096,1.4212893225342094,1.4708470141581036,1.4483943084120119,1.3805973779756198,1.3675306203748505,2.3232256801769946,2.8314374687178865,2.5223725745537644,3.086747655913471,3.4214535231998933,3.3475055909450813,4.16791399561782,3.745498926802214,4.609455376929702,4.676576791257208,3.945706374234671,5.408725081441001,3.0404999750706656,7.163427683887584,7.184685387691262,3.1601423650119393,3.474999723022889,4.462233571485535,3.419094263171478],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.337443178570497,1.3256719360824822,1.3366343288664184,1.5606333138948258,1.4243171267087567,1.4254149470511313,1.4692260891194646,1.3403033955703172,1.782832711187363,2.4537662847151473,2.848304853194325,2.1867777588807153,2.997645039367894,3.0292469864817337,2.479963634553413,3.555897442136578,3.5840811241027803,4.4531241964811406,4.03868144445433,4.780094391241122,4.096429790852368,4.23715504507245,3.447630531185555,3.6397861535823157,3.7185779361226348,5.570115587542423,7.054200589729563,7.16679833456632,3.6070865733290196,5.3181588984526815],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=square<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB_log = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB_log,\n","                                 'n estimators', GridAB_log,\n","                                 BestParametresAB_log,\n","                                 'adaboostregressor__loss')\n","FigRMSEGRidAB_log.show()\n","if write_data is True:\n","    FigRMSEGRidAB_log.write_image('./Figures/ConsoGraphRMSEAB_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.8 Modèle GradientBoostRegressor"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 paramètre GradientBoostingRegressor()\n","0  gradientboostingregressor__n_estimators                          59\n","1          gradientboostingregressor__loss                       huber\n","                                   R²      RMSE       MAE\n","GradientBoostingRegressor()  0.395121  1.666239  0.777481\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predGB_log=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.194991949091975,20.5191591665107,19.900679626897805,19.67146163284344,21.876373809468024,20.092030874602635,21.775615725564602,19.306038759188407,20.28746575040009,22.009215991306412,20.31404595153595,21.256606350701432,20.761225244711383,19.768475680549766,23.177716239838592,22.724752330336564,20.293567380143735,20.191046707760513,26.509816242533407,20.699583169943136,20.862897624317526,19.65825599701821,19.790187630744057,19.548462389068003,21.138980953755798,21.174485327608842,20.0494620929676,22.02196816886516,20.514953358651475,19.41849843848909,20.699583169943136,20.421783402240383,21.71477878851028,22.621460414744355,19.434036021541935,20.538455585096084,20.516052209569615,24.169899349335957,20.441315430519523,20.062891056456976,20.78525524739685,22.521753456222168,19.718465367432927,23.372401211069572,20.157754018222697,19.819808966211905,19.548462389068003,19.808791479755403,19.41849843848909,19.763624285953618,20.905992137539187,19.973463366847497,20.014231457419783,20.78525524739685,21.767185960411926,22.716676467932988,19.67146163284344,21.581118394775817,20.784628346573818,19.306038759188407,22.806987470796397,19.819808966211905,20.585656781594796,20.31362790668887,19.973463366847497,21.07082615065166,20.61633125816414,19.894802656577628,20.14018025523493,20.14405778197776,21.735768643049777,20.502752381176403,21.309157891454824,20.031481601949054,20.932349314647123,19.548462389068003,21.326783890709503,20.502752381176403,21.299350136494883,19.67146163284344,20.47541574921162,21.853917943700555,20.41305261461471,21.01036081549019,20.55590785893841,19.758856965912845,21.024270086241028,19.853353924081485,19.772908089314086,19.772908089314086,20.293567380143735,19.660922068368688,22.330597361666385,20.188201476148997,22.70020657680597,23.95885947620779,21.700923867625974,25.198117028563225,21.75886102225069,19.808273367519806,23.55315455991692,23.58942805891209,20.32306907447999,19.788478301380692,21.01036081549019,21.683339957759774,20.897591465649388,21.96319294807973,19.277626284548084,21.11477145747815,22.599030123118716,20.897591465649388,20.108315590641617,23.95885947620779,21.969832395958463,19.751367901736597,22.276837046826685,22.724752330336564,19.1784275394617,22.01275586365803,24.876592349611922,20.033993484487738,21.024270086241028,20.364754949178046,19.788783174334206,21.139963735666868,20.651725081863432,22.02196816886516,19.718465367432927,21.675057181524913,19.306038759188407,19.306038759188407,21.861490850267625,19.38716777365788,19.373369331638365,21.024270086241028,21.170782452814326,20.024199844172767,20.78525524739685,20.790050553947765,19.99503105277305,19.7034149155389,21.07082615065166,19.67146163284344,20.564318085031392,20.22602912937426,22.74477838128353,20.033993484487738,19.97955076260658,19.41849843848909,21.9476666062641,19.306038759188407,21.326783890709503,21.456856999039804,20.571905085413754,19.660922068368688,20.47541574921162,20.242204052815712,21.326783890709503,21.326783890709503,21.695285107284885,20.13495321986349,19.660922068368688,20.293567380143735,22.714944575376624,25.035896519425854,20.627970793916973,19.718465367432927,20.502676010263144,19.808273367519806,21.873537100455408,21.593417733742996,20.548486014389404,21.256606350701432,20.585107076376037,19.575112094206922,20.520067966118194,20.790050553947765,21.898860014870273,20.61633125816414,23.95885947620779,20.68750659678519,20.188201476148997,21.540572814449927,22.411527133327915,20.964400458744645,21.024270086241028,23.186216903434214,19.718465367432927,21.270772895373327,20.915799892499127,19.660922068368688,20.477992002796025,19.67146163284344,20.364754949178046,21.20084250471678,20.540497863074105,19.718465367432927,20.62502465999336,19.82582938349987,23.3556248129175,19.738053999128688,21.024270086241028,23.722551205919437,19.763114448999115,20.109330340440223,21.53408239072031,19.67146163284344,19.67146163284344,20.490475032788073,20.252267326945248,23.043000858367407,21.037546501589986,19.894802656577628,21.20084250471678,24.183062939210792,19.996291070181346,19.67146163284344,21.80906216497762,19.763624285953618,19.763114448999115,21.432647502762155,22.28706180719268,20.024199844172767,20.310868097004917,23.077755205444877,20.56765424908673,22.9335484822394,20.502676010263144,21.11477145747815,19.76840924006619,21.581118394775817,20.12797927775986,20.976601436219717,21.876373809468024,20.14840917471577,22.411862616863626,19.498959505494273,21.265578293494233,19.3511976777091,19.996619353746446,21.174485327608842,20.496234899246673,21.231619389868648,21.53408239072031,19.548462389068003,24.169899349335957,20.17313819628055,22.19788866094215,20.12797927775986,23.84516666550472,20.514953358651475,20.024199844172767,21.299350136494883,20.40289149671879,20.364754949178046,20.07030254077362,19.548462389068003,21.10496370251821,20.737360878245898,23.1094377431516,25.07425399375874,25.248762186740983,21.024270086241028,19.65825599701821,19.548462389068003,21.398893282180172,22.57758616178646,19.67146163284344,20.44659043997524,20.016044274203598,22.724752330336564,23.098815410475723,20.78525524739685,19.548462389068003,20.905992137539187,19.41849843848909,21.013624917099662,19.935407068435342,19.548462389068003,20.024199844172767,19.548462389068003,21.96319294807973,19.819808966211905,20.374536359079755,20.83181131180748,21.871501853566755,21.361586576744827,23.237076646540743,23.629018808287455,21.95338519311979,20.041449988702038,19.67146163284344,20.637939180669957,19.649908491233262,21.013624917099662,19.67146163284344,19.764812653305075,19.702830139724433,21.802892408441146,19.42114309703796,20.188201476148997,20.293567380143735,19.790187630744057,19.42114309703796,19.548462389068003,19.92729613790941,19.501794670053783,19.768475680549766,19.82582938349987,20.699583169943136,21.002958613327653,19.3511976777091,20.32342154700384,21.524707121344104,19.306038759188407,19.306038759188407,19.884081136831647,20.985028331178768,21.07082615065166,21.170782452814326,21.87341162492595,19.818067007834777,21.700923867625974,20.32306907447999,21.478071395384134,20.905992137539187,22.330726400948322,20.38495199063122,19.67146163284344,20.024199844172767,19.817551631147754,21.341360438755107,20.033993484487738,25.651951251475886,19.24046291651038,22.724752330336564,21.326783890709503,21.013247520191122,22.236457434724976,20.293567380143735,21.174485327608842,20.942157069607063,22.009215991306412,19.718465367432927,21.135561389275217,20.826386194851903,20.391909969589978,19.87183148215449,19.64881649309406,22.716676467932988,25.966624339381827,19.463657357009783,19.613372739235032,19.758856965912845,19.819808966211905,21.170782452814326,21.194991949091975,19.67146163284344,20.364754949178046,20.506817312032407,20.14405778197776,20.226970376385726,21.174485327608842,22.682322959442455,23.942766454363692,19.969582375853594,22.898017387134722,20.12797927775986,20.033993484487738,19.718465367432927,21.524707121344104,20.514953358651475,19.89466535606808,20.502752381176403,21.479735877395605,19.24046291651038,21.07082615065166,23.71856299136069,20.65115866310768,20.6027359004856,21.10496370251821,22.503276860986894,23.858330255379556,20.905992137539187,20.68750659678519,19.978477830633274,20.303361020458706,20.364754949178046,21.861490850267625,20.303361020458706,21.024270086241028,19.67146163284344,21.816161305890493,20.303361020458706,20.539352582539646,20.13495321986349,20.78525524739685,22.009215991306412,18.963245062835572,21.97106978154533,20.397712890223666,20.269641755922105,21.21751464569769,20.293567380143735,21.014254740205352,22.707005000564045,20.18666369175181,20.024199844172767,21.503945373673254,20.761225244711383,19.38716777365788,21.10496370251821,20.981402387538314,20.303361020458706,19.718465367432927,20.318867313220878,21.46515932935,21.876373809468024,21.11477145747815,19.969582375853594,21.362872326405498,20.12797927775986,21.326783890709503,21.700923867625974,21.10496370251821,22.088915155297578,20.699707574260263,20.62502465999336,20.429034566861645,20.397712890223666,23.01189480007436,21.54468684721459,21.46515932935,21.13235536734435,22.032068836208406,23.342484365786014,21.138980953755798,19.599970068069627,20.728408479179517,20.157754018222697,23.292460761084264,20.303361020458706,21.07082615065166,21.333367387732473,20.689701120644745,20.208906885356814,22.716676467932988,21.174485327608842,20.14405778197776,21.46515932935,19.306038759188407,19.763114448999115,20.391909969589978,20.22086576412446,20.728408479179517,20.033993484487738,20.104742519885157,22.70020657680597,21.06568299090456,20.651725081863432,20.188201476148997,21.542291031210304,20.737360878245898,19.829714435274745,22.079107400337637,22.503276860986894,20.185339173755622,20.477992002796025,21.174485327608842,21.170782452814326,20.514953358651475,24.345376162481436,19.565978149261223,21.07082615065166,20.450668095521095,21.209900881545064,20.502752381176403,20.312984185547855,22.236457434724976,20.699707574260263,19.67146163284344,20.72017955969868,21.96319294807973,19.306038759188407,20.539352582539646,19.41849843848909,23.310525156487703,20.477992002796025,19.51279862749207,23.353618111356653,20.188201476148997,21.73898828478793,20.699583169943136,21.333367387732473,20.790050553947765,20.303361020458706,21.707549454037423,19.548462389068003,20.47541574921162,19.489776580992505,20.785096708130872,21.469928122435665,20.68750659678519,19.7034149155389,19.97955076260658,19.629591403537475,22.716676467932988,21.876373809468024,20.30539898720803,22.512541151015036,21.514899366384164,22.824939631051507,23.475245729720555,22.503276860986894,20.18325925604756,20.22602912937426,19.67146163284344,20.352553971702974,19.548462389068003,22.453484438029164,21.302214893659446,19.790187630744057,21.309157891454824,21.773801050368185,21.10496370251821,21.9476666062641,19.718465367432927,20.845419399754647,20.737360878245898,20.47541574921162,19.67146163284344,21.170782452814326,23.338496151227268,25.021898221592757,20.63564633661511,22.692392541111392,21.029351210809732,22.848367550146243,20.188201476148997,20.208343640795032,25.719120619766322,23.07426965694513,20.437087194786542,20.502752381176403,20.46441700284193,19.67146163284344,21.01163517417036,23.310525156487703,19.660922068368688,19.41849843848909,21.45657143755288,19.548462389068003,22.512541151015036,19.955122348778964,21.87341162492595,20.024199844172767,23.488546070597998,20.303361020458706,19.306038759188407,19.68897739303666,20.516052209569615,19.325400049947678,21.21739688007241,22.01275586365803,19.92729613790941,21.46515932935,19.548462389068003,20.352553971702974,21.514899366384164,19.718465367432927,23.342484365786014,21.162273039274744,20.915799892499127,20.502752381176403,21.497319787261805,19.548462389068003,19.402705356710722,20.427926014931213,20.86403949883754,24.575582005493086,19.973463366847497,20.867106639506808,22.805193136917136,20.14840917471577,23.177716239838592,19.67146163284344,21.174485327608842,19.35754643819003,20.364754949178046,20.663926059338504,21.023010446095736,21.62262849623491,19.548462389068003,19.819808966211905,19.853353924081485,19.51871868754723,19.41849843848909,20.689701120644745,20.516052209569615,20.860741281787462,21.585544058398426,21.836909659678735,20.326525247620072,20.352553971702974,22.716676467932988,19.67146163284344,20.976601436219717,21.552980026429278,19.911903750121503,23.945695886332956,21.10496370251821,21.309157891454824,20.118255369859625,20.728408479179517,19.548462389068003,19.789007753013454,19.67146163284344,19.67146163284344,20.178233089396013,22.646696384409186,21.581118394775817,23.342484365786014,20.585656781594796,20.38395553452376,21.767185960411926,19.783041533016426,20.981402387538314,22.294785516884254,20.303361020458706,19.67146163284344,21.96319294807973,20.571905085413754,20.031848695009817,19.460406509844837,23.07426965694513,20.18666369175181,22.512541151015036,19.67146163284344,20.310868097004917,21.80906216497762,21.736192901703344,21.685477352324945,21.194991949091975,21.04061191731329,22.11489843876989,20.53115997885444,19.969582375853594,20.71393610095874,20.915799892499127,25.785263381467683,19.973463366847497,19.306038759188407,19.548462389068003,23.858330255379556,20.188201476148997,20.55260666263711,19.935407068435342,19.660922068368688,21.206497468813936,19.728598811135267,23.03513102064458,20.109330340440223,20.303659519935884,20.05094087765718,21.11477145747815,22.01275586365803,22.727942559184527,20.024199844172767,20.86403949883754,20.699707574260263,21.70497103355034,19.7034149155389,23.95885947620779,22.724752330336564,19.69897747566742,20.293567380143735,20.374536359079755,19.718465367432927,20.78525524739685,20.831640513980275,20.728408479179517,25.64680910053221,21.031024024157876,20.024199844172767,21.129816010131893,20.940972311375102,21.581118394775817,20.14405778197776,19.61412049438655,20.39774365315772,20.14405778197776,19.790187630744057,21.21739688007241,21.024270086241028,20.293567380143735,21.194991949091975,20.875271863859872,25.950115460325637,19.785065353708557,23.338496151227268,20.12797927775986,21.07082615065166,20.52660565858407,20.109330340440223,20.62988576166596,19.790187630744057,20.016044274203598,21.194991949091975,23.01323391234024,19.548462389068003,23.13668394817684,19.996291070181346,20.427926014931213,20.598046449658003,24.345376162481436,21.816161305890493,21.603225488702936,25.198117028563225,20.310868097004917,20.178233089396013,21.21751464569769,21.38164151447609,20.19356809323646,24.756694386622836,21.326783890709503,22.330726400948322,20.364754949178046,22.51467952075393,19.768475680549766,21.02794472535639,25.414162694517618,23.614333619473882,20.516052209569615,21.299350136494883,21.524707121344104,19.50120292735401,20.516052209569615,19.718465367432927,21.22321113810433,19.95120188407539,19.660922068368688,19.35754643819003,21.174485327608842,21.256606350701432,20.033993484487738,19.900679626897805,19.763114448999115,21.844110188740615,23.64577204563682,19.942754176782415,19.599970068069627,19.548462389068003,22.426430159531623,20.932349314647123,19.548462389068003,21.07082615065166,20.427363286886308,23.702019063976838,21.96319294807973,21.11477145747815,19.67146163284344,19.57916710128547,19.819808966211905,19.460406509844837,21.62262849623491,20.506817312032407,20.651184916709266,21.707549454037423,19.575112094206922,20.410657648732826,20.409913867698737,20.905992137539187,20.68750659678519,22.079107400337637,20.665296733277753,19.660922068368688,20.514953358651475,20.287022519367806,19.67146163284344,20.347442018283253,21.231619389868648,23.230108515042918,20.024199844172767,21.85862508055592,20.728408479179517,20.265025316192048,20.12797927775986,20.25605629589217,20.62502465999336,20.535110747511972,21.013624917099662,19.874367145512032,23.945695886332956,20.946409160796918,20.514953358651475,20.536252686265943,20.310868097004917,19.763114448999115,20.869105401822726,20.04615074888221,21.05207067302096,19.93996157509832,20.310868097004917,19.754276976103856,20.352553971702974,22.01275586365803,19.737632520121934,19.48617518125567,21.996811182680112,22.009215991306412,20.897591465649388,19.38716777365788,19.65825599701821,19.399880914404807,20.553859408748657,20.421783402240383,20.422306431797978,19.63130073290084,22.682322959442455,19.67146163284344,24.863428759737086,25.719120619766322,19.63130073290084,20.033993484487738,19.69897747566742,23.07426965694513,20.293567380143735,19.420851169341297,20.716437992599968,21.528566854022426,20.209908361629164,19.602908069590523,20.352553971702974,20.033993484487738,19.97955076260658,21.683339957759774,20.364754949178046,20.024199844172767,19.548462389068003,21.503945373673254,23.962847690766537,20.719750304869457,21.816161305890493,19.512420636672495,19.599970068069627,20.905992137539187,19.277626284548084,20.477992002796025,22.727942559184527,19.828944074276805,23.75843727601965,20.47075080475306,20.303361020458706,20.47541574921162,21.326783890709503,19.97955076260658,21.359957283225633,20.3521850541881,19.565978149261223,19.790187630744057,20.943052144711306,21.853917943700555,20.745761550135697,19.67146163284344,20.477992002796025,19.718465367432927,21.270772895373327,22.01275586365803,26.303317576013576,21.524707121344104,20.457517091742453,20.016044274203598,20.516052209569615,20.761225244711383,25.64680910053221,21.816161305890493,19.97955076260658,22.692130714402396,20.12797927775986,19.788478301380692,20.303361020458706,20.568177891776028,21.478071395384134,23.384701852631466,19.306038759188407,25.785263381467683,19.955373737562798,20.409913867698737,19.788478301380692,20.024199844172767,20.191046707760513,23.176409148474274,19.772908089314086,21.469928122435665,23.202261993369184,20.18501420761049,21.024270086241028,23.264138047951054,21.168872246875413,21.919434668152633,22.05061438939727,21.49216958742087,25.64680910053221,24.169899349335957,22.599030123118716,19.764812653305075,21.024270086241028,21.174485327608842,21.013624917099662,20.95867115407083,20.585656781594796,21.732902873338073,22.706868712973048,19.548462389068003,20.188201476148997,19.818067007834777,20.331297993960828,20.18325925604756,20.312127379246007,21.836909659678735,20.409913867698737,20.905992137539187,23.858330255379556,19.306038759188407,21.95338519311979,21.469928122435665,21.53408239072031,20.431577042555354,20.033993484487738,19.306038759188407,21.87341162492595,22.727942559184527,23.615008191916065,19.790187630744057,20.024199844172767,21.170782452814326,23.65017411290113,20.188201476148997,20.3521850541881,19.38716777365788,22.727942559184527,21.07082615065166,19.818067007834777,19.978477830633274,20.19356809323646,21.46515932935,21.469928122435665,19.67146163284344,20.257480374534914,20.93529969093746,21.46515932935,22.42226019009407,20.562301626154436,20.18059731698129,21.333367387732473,20.310868097004917,20.14018025523493,19.41849843848909,25.64680910053221,20.227908337613748,19.763114448999115,19.660922068368688,20.016044274203598,24.06937012850772,20.266024682645153,21.194991949091975,19.548462389068003,23.666207219194707,21.270772895373327,21.309157891454824,19.790187630744057,19.64547643677433,20.78525524739685,20.699583169943136,21.712869017151085,21.223480747525773,22.11489843876989,23.850160787415156,21.841236533885215,19.942754176782415,20.985028331178768,20.17687005143684,21.914754880754877,19.575112094206922,19.819808966211905,22.284977761924313,20.761225244711383,20.70058663470859,20.61461244968733,19.92729613790941,20.489951390098774,19.790187630744057,19.67146163284344,19.548462389068003,22.034122448981403,20.82424187731199,20.202912936743388,20.506817312032407,20.78525524739685,19.996291070181346,21.62262849623491,23.858330255379556,20.32306907447999,23.384701852631466,19.419297566471908,23.680205517027805,20.41305261461471,19.534830414293157,20.14840917471577,20.303361020458706,21.66740099182519,19.718465367432927,19.306038759188407,23.225262790040848,21.024270086241028,21.0728890039842,20.25605629589217,20.364754949178046,21.503945373673254,20.38395553452376,21.326783890709503,20.090799830448397,20.745761550135697,21.432647502762155,19.768475680549766,19.85437606412066,20.1362081972407,21.695285107284885,19.660922068368688,19.80921714559997,21.160974697854385,21.64021240610111,19.894802656577628,21.479735877395605,23.945695886332956,21.11477145747815,20.755569305095637,19.763114448999115,22.727942559184527,19.677690239716068,25.719120619766322,19.819808966211905,24.72302179754875,19.67146163284344,20.109330340440223,21.024270086241028,19.629591403537475,21.174485327608842,21.61291141496839,20.738540643720242,23.213191957076155,21.747311930108893,24.057820323034303,21.479735877395605,20.109330340440223,20.477992002796025,20.553859408748657,19.900679626897805,20.312127379246007,20.29880071281831,21.174485327608842,23.64867324780527,19.48617518125567,19.306038759188407,19.763114448999115,20.374536359079755,21.024270086241028,21.844110188740615,19.718465367432927,19.773247892701455,23.58421013963625,20.254801462424627,21.24365600994194,24.876592349611922,19.751367901736597,21.503945373673254,23.261086632821787,20.108049974977668,20.18501420761049,21.135561389275217,20.514953358651475,21.98740244435738,20.033993484487738,19.67146163284344,19.548462389068003,21.018941577582673,22.04856770844016,23.067546611898,21.174485327608842,22.20794663675641,21.047987013753573,20.905992137539187,24.015637757787367,23.237076646540743,20.157163095534862,27.334911982861577,20.216093587971937,19.973463366847497,19.790187630744057,20.915799892499127,21.194991949091975,20.768762441066684,21.037807322247858,20.790050553947765,19.306038759188407,19.99503105277305,19.656255905533918,25.785263381467683,20.81138925051117,21.91316678554431,19.718465367432927,19.900679626897805,19.533256661758482,21.024270086241028,19.67146163284344,23.756057830473267,21.469928122435665,21.95338519311979,20.539352582539646,21.024270086241028,20.905992137539187,20.538455585096084,20.600994657307893,21.87341162492595,21.630854277778575,20.60653761784917,20.942157069607063,19.853353924081485,20.412283903857993,23.077755205444877,19.68897739303666,21.812700163401086,22.50190545706633,21.514899366384164,20.352553971702974,22.57758616178646,20.17238269597327,20.514953358651475,22.03429550412748,23.24123579257576,19.3511976777091,21.174485327608842,25.414162694517618,21.469928122435665,21.73236269837648,20.364754949178046,21.683339957759774,25.688403152622282,21.844110188740615,20.539352582539646,20.761225244711383,20.303361020458706,22.009215991306412,22.04856770844016,25.64680910053221,20.62502465999336,22.02732435660105,20.083547671293317,20.993564544192694,19.63130073290084,23.237076646540743,21.326783890709503,23.34949895569428,19.93996157509832,21.07082615065166,19.718465367432927,19.325400049947678,20.506817312032407,24.254057326541783,21.170782452814326,23.160909029890675,20.62502465999336,19.480909797524358,22.524365447019385,19.785065353708557,23.261086632821787,19.548462389068003,21.542291031210304,21.032510063803752,19.35754643819003,21.174485327608842,22.724752330336564,19.763114448999115,21.46515932935,21.299350136494883,20.32306907447999,23.716062820959873,20.768762441066684,19.548462389068003,22.448243706480657,20.303361020458706,20.439200553004675,20.441315430519523,20.616305425145764,20.188201476148997,20.81418521737683,22.30878381471735,19.702830139724433,23.230108515042918,20.188201476148997,21.87341162492595,19.37506219838325,19.819808966211905,20.506817312032407,19.548462389068003,21.469928122435665,19.97955076260658,24.345376162481436,21.231619389868648,20.516052209569615,19.629591403537475,19.763114448999115,20.53525279491533,22.276617463856645,21.309157891454824,19.828944074276805,21.581118394775817,20.127484936863816,24.575582005493086,19.763114448999115,22.209026483633536,24.06937012850772,19.808273367519806,24.274381775150445,21.160974697854385,19.37506219838325,20.37572661504292,21.174485327608842,21.326783890709503,19.872031808997615,24.863428759737086,22.01275586365803,20.024199844172767,22.368609589675504,19.63130073290084,20.497275873584716,19.978477830633274,20.78525524739685,23.666207219194707,21.75886102225069,20.14405778197776,20.35878682520956,23.95885947620779,19.86176414780136,19.65825599701821,19.900679626897805,21.919434668152633,23.292460761084264,20.12797927775986,22.3303148343808,24.240893736666948,20.303361020458706,20.104333472005436,20.78045447784406,20.033993484487738,22.63242249029653,20.303361020458706,22.026754161491127,19.97955076260658,19.548462389068003,19.565978149261223,20.364754949178046,19.763114448999115,19.976766048765228,19.306038759188407,23.762425490578394,20.19356809323646,21.282278294196182,20.699583169943136,21.479735877395605,21.138980953755798,23.07426965694513,20.17313819628055,21.270772895373327,21.9495585354936,20.964400458744645,20.97528257538034,19.7034149155389,19.52238902342548,19.55023439780996,22.209106150454033,23.264138047951054,20.033993484487738,23.252543406445838,19.67146163284344,21.309157891454824,22.441624529244883,23.220775465212988,20.699707574260263,22.069497578412378,21.695285107284885,20.95867115407083,20.266714707589824,19.525985610186595,19.818067007834777,20.409913867698737,22.806987470796397,20.53525279491533,20.940972311375102,21.139963735666868,20.83181131180748,21.683465433289232,20.728408479179517,19.763624285953618,20.188201476148997,20.421196543941825,20.188201476148997,21.479735877395605],"xaxis":"x","y":[21.224638239972652,20.180643707721647,20.201341431117804,19.1454180237343,21.79470380164697,21.087755789009996,22.432940196049124,19.578846464119135,19.242152996635603,24.328668011629535,21.754493646354923,21.476822454566275,22.377553705906216,19.555124576954142,22.494011033388475,22.606991826298977,18.49440443743341,20.76699650557159,29.702932107840674,20.84994793377778,20.282334104450392,19.302304652102425,20.014253448408297,19.02478155099295,20.401136284262034,21.066918904158555,20.30528553843669,22.443056343683086,18.530754586286314,19.472341179018663,21.511675814584603,20.024996233202344,21.986420084483047,22.32477661567823,19.024668956843474,20.277557571355647,20.011894681671833,23.15521167718674,21.342870607762556,20.008806098312462,21.88562823605827,23.73392435177354,20.358652273917528,22.63391493571178,20.2953770114005,18.603706022480708,19.476619646192038,22.286954060950823,20.827570650601533,19.658607240576167,20.944082894927185,17.889444547032436,20.210011039401028,20.041902410540708,22.116524819871177,23.07232514287102,18.931245369435278,19.243034483473725,22.175009164867692,18.8696149273692,22.748443989592825,20.18471556395119,19.630595419370202,19.596480627252774,22.075827573908075,20.81706640440522,20.033430227416382,19.472993206549905,19.47575297115503,19.712127286089668,22.22369094458836,19.974424393125233,20.76732896720974,19.73170766432226,20.622497836834853,19.074352191519743,22.597191120827183,21.03390016791011,22.014068840206768,21.41124459992273,20.690267441816314,22.238063995762765,22.835153147091482,21.940489607019817,20.06072495330695,19.26508618967162,20.593917077867186,20.807981392523768,19.461766940454588,19.92317059187003,20.905864572180928,19.887154874828134,22.674741834325705,19.54492482512454,21.650322094933124,23.6494594685329,21.673503182786,25.42292365756086,23.598781185238046,18.99960834234589,24.597055201180662,22.731123431811554,19.734159687698988,18.832054600441634,20.397708053035622,21.477527921557282,21.731372277489818,21.51301334156099,18.824250825082274,21.177749972976986,22.50813230656899,20.78875924862035,19.355480148534408,23.890614784737277,22.347688134832723,19.773789560999628,21.94580529932221,22.19000668879013,19.515006779114817,22.99883477706531,24.630356510701596,19.78112594827553,20.45440259621003,20.195062159996276,21.08554669425927,22.350651659007692,19.88287753302219,23.223107004233785,19.56773145357299,22.383567194160783,19.337397595787667,19.84140252838032,24.09542285957458,18.7932762528682,18.78833918294514,13.482051629033913,21.99805786914009,20.68539834867811,20.75326327581395,18.386700720994313,20.922404198298533,19.34599856610255,23.03775857233843,18.64679105166152,20.517600912543127,20.206849822052806,22.025147336797662,19.802893385023037,20.525632635954643,20.1770277231615,23.357834800271604,19.12730750072142,23.147280554074683,21.983261369651657,20.345804248394614,19.067786210059246,20.11323917875491,20.682137237753555,21.173013406383724,22.83375379636568,21.704780057376663,19.98202682053532,19.114539527933296,22.520164246985647,22.815206783859768,26.613491397737352,21.053960341071367,19.740225451250925,20.911400034394557,19.239293618022067,22.519031496965763,20.18180618902739,21.47581257946244,21.841152373314472,19.894016350184707,18.947642644803835,20.33821258053668,21.998370834730604,21.623042657920415,20.52474339677355,23.920588558527324,22.398384643288747,20.614611901935827,20.73652363425427,24.216251851014647,0,23.798062566113977,22.44169398922513,19.04780522125321,22.508853453023672,20.361268080220963,19.223547338390134,20.037360610262493,19.47969541290554,21.419330156983328,19.397345612637686,21.062803097791257,20.802720957968873,22.787613064091445,19.854352512470033,23.127238847948263,21.236410750491928,19.39740450234132,24.629767207576666,19.424905147339715,17.66325927218184,21.589387814254906,19.856012075783497,19.346653316091935,20.250448783340936,19.91174617647474,25.664004852328446,20.437440195258947,20.159778968313912,19.760702082570422,23.894186440194584,19.664395199250066,20.03503448449456,21.85979788510952,21.007452333347548,19.330905939549268,21.816602846893197,22.163197583801455,21.21476614620658,19.61795076264764,22.32608970797742,21.206937139516242,23.650902186871754,19.538495641550785,21.951268793802367,19.712186074557728,21.6397845044252,19.566140232030243,23.528005120775013,21.76194235515012,19.759231530600534,22.69657897049395,20.97108250648827,20.91944021038121,19.11091145461158,20.92382072739302,19.99564295418165,20.157995405754583,21.499214768467855,21.850862595399537,19.613806547176576,23.262476414885043,19.981118924422468,23.526907160221576,20.11872947152901,23.756207113729698,21.05135284285491,20.459271509540073,20.732604891283092,20.22193428509613,21.2413041438754,20.86782723578085,18.85414731140336,21.746512886013605,21.477990920672163,24.227674033652324,25.12798066804062,26.560353492028305,23.633181830866906,21.120826057383237,20.22792847217847,22.242317589489687,21.590387855717424,19.62630225708032,21.126056594579662,22.585399041291762,22.379011027342607,22.883846216197703,20.986097829919892,21.770367489208184,20.58669309831316,19.43536098584635,21.22950036954701,19.51332725702657,19.38984557001998,20.48325363998835,20.246730162293353,21.286321988690805,20.685520474331085,21.05996063251775,20.18838178436931,21.836335494435165,22.094100243073623,22.994379248863297,22.721264543402345,20.729598340663756,16.46658160832902,21.557923127735883,19.880806435323297,19.801676786988352,21.12828261562749,18.90461110336481,19.19842345841963,20.092049623606254,20.0459782414438,18.31795878333722,20.7963375193431,17.813568277801206,19.627334048970262,18.56499419954315,19.372221038245304,20.307717788382128,19.384134222108464,20.087817122473812,19.502286013993587,21.227181698040095,23.312865371844044,19.405079121883418,20.90714373186156,21.80480516491849,18.974285739983618,19.176540848422555,19.19474964169837,20.93275451807793,20.832136714555805,21.170532392724574,22.709608681261756,19.86611073536812,22.055547735078193,19.980960001271505,22.653172763321756,20.76687340459434,20.847191191309296,20.06982734293826,20.638856572715298,19.67156525332246,19.49519726866692,22.064314182636632,20.227612418267608,26.201435233706484,19.068639219627578,22.656078929958284,20.77129521126714,20.391385205003306,22.283464834927894,20.845264152741507,21.027619198526104,21.07443745441354,21.518584006439504,19.724170230709305,20.679280780496406,18.927945745238855,21.825732486030095,20.464091007664432,19.439137307013926,22.977548256996624,23.988556498480502,19.81821879465883,19.050281782029014,19.439732238591404,17.74375624808358,19.990444456103596,0,18.279601513401296,19.78875556523867,19.965832185850637,20.69408662857846,19.405573621751774,22.379934620081865,22.48871602661339,23.642331039751582,20.276556094805027,23.283922162181995,20.195578868662224,20.03811220566055,19.14964293785794,20.90803339180857,21.473710208099476,19.884197354704103,22.164351182655523,22.813153319462682,18.901120542977083,20.544639287700853,22.988276940898057,20.109117749689304,19.622850798933502,21.871633434683428,22.25017305320405,23.60048678836413,21.515067916471864,22.320806056618878,19.870759032200375,20.544449448312626,20.261067504600565,23.43234984844275,22.4595812045188,22.52530214820436,19.382106816183008,23.96470652783692,21.019906267101412,19.811383905702034,19.845505364796534,20.51412216383031,20.373076483151994,19.01511010272945,21.47364877893911,19.957005833266606,19.38761598733965,22.32447174476143,20.79356441567801,22.807056132685855,24.028256571904787,19.874893102286897,19.393396349595932,21.076706013187607,21.881924997624267,19.305427636738685,20.88679803779653,22.116604136681058,20.73926969664862,19.30704929356398,20.730402345071546,21.49837252262123,21.892478522473827,21.156990969891915,19.626240196846272,19.456579392491523,19.83361590003151,18.640025995609424,21.63591845810156,20.866343204364526,21.462764853897205,18.837375101562536,20.833447685328323,20.03253125195289,21.036829264514445,22.96164822427184,21.237452292596537,20.43520123557409,21.156628899315663,22.496434708234126,22.4307529287208,21.544986593746263,21.11234277813463,21.351967832505505,20.39596499860976,22.86476103379213,20.105565995835796,20.48988421076927,21.684940338873297,21.559903574554543,22.73966855399758,21.952844226236508,20.330025112864092,21.040198736231254,20.75190246631361,18.798041102461394,19.29157093266521,19.97055996653021,19.307592868706436,20.89335338152254,19.773921527816817,21.100709172660746,21.699220401665585,21.041684548715036,20.396544813616845,22.752749154816065,24.957468167964976,20.506265808043914,20.374729613814893,21.525022859298808,23.275631354399913,21.75794824506378,19.767052898077655,21.218145625261048,21.536078922759724,18.276769619768288,24.94868149530031,20.41835179368666,20.87359355439842,20.51145154622031,20.700075492290427,21.289957235167595,20.748925773432315,21.959550623213953,22.62198208884591,19.38396579774315,23.31051349089806,21.627932359551377,18.954513256380405,19.964316452607815,19.32031247988204,24.27577420165758,20.498820970198462,20.060863127783303,24.1535698900103,18.16161414446258,19.814472942891484,22.998819199975294,21.020751435153667,21.36177568305088,18.469113141288748,23.192581335464276,19.21725486016369,20.111754471035223,18.939248297157278,20.474687273562058,21.111612649480875,19.87603574815164,19.637857750008234,20.264370436179377,19.93146081401427,23.29863946166844,21.939745630681145,19.743514665635868,22.649298943378753,21.24045362673594,25.608747220451313,23.49090374555705,23.60775201354553,20.678206389277292,19.915728419308504,20.449383439914303,19.824402536620187,20.53678125858074,22.159724468145836,21.034942664057727,19.03923233773122,20.84977321505041,21.781868242254394,20.46100849412487,22.004305689493815,19.142842792258786,19.496508584409785,21.834751391009725,20.402181439081684,19.21550544985804,20.640134807950194,23.051954851262433,27.036279453018476,20.37977707208339,21.775846548503456,21.781910553210967,22.353218452653266,20.165877660805254,19.451496130670126,25.490707221646936,22.59150705240579,19.995956185326282,20.28232915493825,19.579291420064532,20.879430687458026,21.138173662648782,24.394611647917593,19.533857847503334,19.817723799212757,21.171020993541568,20.50991100623185,22.326932218927173,19.527299811101727,21.383177447413818,19.392837666571104,23.017914332247763,20.82549425312562,19.072268708954628,19.10179618438648,23.29660399040893,19.32638298997934,20.650313109616484,22.182752894747274,20.620515848299537,21.682830461889793,20.248785506000544,20.060843672034636,21.235100056704788,19.639114342316898,22.873156355744623,20.616811975583957,21.260660009666424,20.98111639712373,20.640272301462904,20.347463631132083,19.255662542640188,21.590649178257912,18.627786784209636,25.25576312419618,18.13136213470814,20.18507504172347,22.234490018926184,19.867378293238115,22.842407702796528,19.642302060721956,20.934073076796377,19.657897588782742,20.907660872216642,21.21194412605265,19.94161891161789,21.31905629271608,18.792730927848385,19.734888199833236,18.03332620487895,20.049848549450562,20.009665992350836,21.60568082130027,15.704606153972572,19.9492653280832,21.029500902796773,21.639091593296747,19.630944296038688,19.444205825873514,22.72561505846151,17.641741979338622,23.57832742490694,21.06232551474456,20.596453669823063,23.77569135147608,20.739222922576804,20.798718195952173,20.117453663452505,20.134364899685817,19.1796783064222,19.08437502019272,20.74807693982553,18.925763040720064,21.23645513023823,22.042981377352834,21.11735552640613,23.60188649068525,20.85643673732399,20.248587775712384,22.0971519060863,19.917597162486278,22.120758360865494,22.44443523329341,21.796740313790345,20.192956161016543,21.587428114342384,19.88222659302695,20.074407563972667,19.772030191805687,22.686480223069132,21.085461437053635,21.84811725117268,18.71863791145474,19.897316659199046,22.368646863289158,18.2265721237695,21.061479969798718,21.091487685756913,21.905619198419114,22.248853822319084,20.948206085285577,20.95171629836988,20.60386819333271,21.712312461652626,26.34702224299619,19.173408752888843,18.94863556098072,19.669251869992156,24.602654298813697,20.178503166977418,20.318803159054653,19.8813455186455,19.742006737354636,20.848691129006262,19.533504643608662,23.044574887856943,21.2197623270523,21.881578906408237,21.91240193642795,20.941991359524017,22.077006044437276,22.54693130601029,18.995877797816178,20.997149152437558,18.687341507608156,21.335453055117675,19.376016989288264,25.07655356851684,22.59004948235383,19.378047539426834,19.682163069515127,20.52340976581651,19.581570228112746,20.9498708711936,22.82667920636651,20.99544749522815,25.17876365321419,21.627453285628658,19.477956222501692,19.193137896037967,21.893341530106955,19.71728461871742,17.31416721288844,19.147406179743545,19.827554176093653,20.471143313052202,19.29233286786097,20.500085221471384,19.644130235297485,22.034075873229305,20.949040678200554,20.540968959330602,27.231751620760654,19.73015830855728,22.493025220696325,23.089701491122522,21.64924126399572,20.795739259102447,21.118903696326274,20.815053320149143,16.52324032760632,19.730383893455734,21.24843214899758,22.071670864625634,19.282486682508637,23.126218118696794,22.228032143928214,21.93139047566312,22.735567798975932,24.167967887858172,21.655361699836966,22.600876020595507,24.59815969618305,19.718235182480434,19.521220854266264,20.564957928128795,20.971944131576173,19.6146020174933,27.021590188993713,23.73507977629211,21.659326126254854,21.01213158467734,21.766780414954713,20.08203076321153,21.756353934255824,25.471067413053014,24.02283197509071,23.385758678777925,21.23702785682451,21.578861893090256,19.189211661593234,20.07860380095908,19.328382384869762,23.106213073613993,19.95329417355663,21.365528432523575,18.914065908446993,20.846904364921407,21.621710733177533,20.214357069981805,20.48695013292876,19.590051196125845,20.988896575504317,22.71500741252458,20.63081635106876,18.585982112110514,19.30101383466248,22.43563961615873,21.19573925103901,19.253482814272665,21.733085888220945,21.51937854264472,22.935901951961643,23.1601071574333,20.83646802652741,17.151393182854086,20.807011308942396,19.447379908647196,19.74096325611752,20.942433069954838,19.870033345928174,20.91477708386503,23.264639171673256,18.81898933836999,19.40497105732979,20.010315016400824,20.436358643191994,21.24511744232927,21.394403553854325,20.278388484789648,20.16621212904916,20.06466810918053,20.8905482708943,19.02658827934665,19.696262913257698,21.147132242697182,0,21.233800456337935,21.791167162104042,20.18131197591118,20.79775274411387,20.158015290040524,21.01738902857623,21.1949084635434,22.774677489141574,21.93032100053686,18.418089755206072,23.810210740127232,20.9793536678564,18.664791000361163,20.57237479635235,19.66651702047476,20.204034803826428,19.989961267682528,19.13829898996108,20.67820982860559,19.602487517256634,19.634034627117707,20.049508251055094,19.732903567032626,22.301656666857568,19.3351286191496,19.66019789348857,22.54105973810428,21.087085320291497,21.46282774650559,18.953344800503015,19.199864465913354,18.832769512656196,21.40009037173508,19.692718291075924,20.626400634350908,18.73677516240749,22.077517747249683,18.74408127304125,24.761947476774708,26.113208352362694,19.576225059708314,22.48651099143561,19.29251486362711,21.199347342918216,22.53052802544868,19.222080263625543,20.87727804782974,21.285530909063,20.6451326442611,19.40981146751902,20.008240359915625,19.734510138857544,19.369940873414674,21.839004592901738,19.643216988179674,20.34827538264879,20.295760247970698,21.15453445100085,23.946209069915817,20.161508356002734,21.461336053955172,18.855956110093064,20.13782559566735,20.22901210527111,18.772971741326142,20.346151863763343,0,18.758138614258336,23.01986369036341,19.826388660339887,21.76883742068305,20.30266681112443,20.98525964839946,20.015097338364022,20.523507399137607,20.436904665975636,19.162579413291002,20.375536910022955,23.70344643795525,20.910669810022966,20.379640393188353,19.217287998251134,20.77164874576569,21.098649782686852,21.445335194484834,21.911569915872246,28.094893882984294,22.184719195487645,19.636646744775323,20.26686372980413,20.815307943684378,20.695469281090816,25.87586915107988,20.22604051998145,19.449746389320087,22.323663662536706,20.105603080001295,19.720177445507503,21.133857610241805,20.04287236504247,21.180586044654405,23.433073191606535,19.844702412663505,25.31201588491192,18.563017473748218,20.726116689654802,20.55471999163649,19.12308826699376,20.688277632644148,23.08698185712261,19.4188490987079,19.38789150561276,23.032593297237835,19.840625811644227,21.990665865987562,22.00972851632279,22.544699771649725,22.804072710576072,21.602864772837197,22.7909106273511,25.661642339745672,23.190044679879648,21.797480754192584,17.862153020413583,20.62223350550368,20.658934051201378,20.583030645279486,23.466890235880964,20.879510709815698,21.526914680071894,22.700127681411185,19.683189962828273,20.26341148584,19.616726513482636,19.839377869097518,19.875302398958702,20.973185759004682,21.082163500516533,20.324838680602316,20.30745292915028,25.9613964584893,19.046718202293015,21.78315042330069,20.91962424027046,21.618102712310883,18.472822451162664,19.554830163405484,19.175778559517227,21.854300382519792,24.477823287446007,25.5492833010917,20.6322831501737,18.924421094438486,22.00276008739277,23.251480243552532,19.437442862246026,20.440974576504654,19.114316794022223,23.060566976295764,21.685850226111516,19.690963589379074,19.932730372590424,21.5156696442052,21.03832189892855,21.34321011850724,20.297903654101066,20.251155291034372,22.02017509916011,21.00569855470944,23.695624962968736,19.83550351971754,20.046496736817677,21.663202256858984,21.64914607700524,21.763695119906945,18.966254774482525,25.621581882833034,21.318917047123914,20.25569596256737,19.082528830592434,19.62869805766532,25.538294718669484,19.064589338432878,24.544735671741783,19.58243213803125,23.66501619396984,19.12391425698277,21.767558412000408,18.99637745675535,21.23800756191085,21.00116832090369,22.714272038785044,21.137414277628565,21.45606258158953,22.279207015403138,23.33275256138136,22.26485604208308,20.723080111555596,20.639447033358472,20.694537918678005,20.730065203035867,18.59276109193853,18.41344612635984,22.526317266984485,19.84998714954673,20.751047310028625,19.801872521805933,20.233535275947663,20.632626738131346,21.279464358226686,22.469580214601123,20.408744363117133,22.583074271938578,21.34410908350179,21.979041347822633,20.59404343806881,20.264946502479802,19.878697565839243,22.615047199531546,23.56945490190484,19.736798486144902,23.729801820804443,20.49274951238928,23.323658577961712,20.770035525423065,19.238103804361465,19.567540218990334,20.344212591822245,22.493051859930933,19.27583351001347,18.98927807008273,23.394529944648035,22.51205924599309,21.071056073380674,21.047075972047644,20.445539922646542,21.340147837597844,20.003307902740296,21.299913824117656,20.644505230109406,20.419105854581318,21.593824209166353,19.476347089125902,19.838668436714478,19.728765556885957,22.07230878544129,19.427802084161115,21.756421739524402,21.380030171433972,21.32600862652118,19.632965031850407,21.403421935393933,24.37658838285707,21.043820980181792,20.1938730272849,19.66793597026211,19.199993380834425,20.035891808657503,25.0888235331049,18.385719188798834,26.48889669142553,20.116390923202314,19.499674996520806,19.751886432327307,20.522012502780594,0,20.782571363318098,21.618914296187317,22.967539915594646,20.95701983082078,23.548104709979565,21.65670940491333,18.258239797953646,17.88154175390193,21.233312475041235,20.277059332832177,20.919229965693855,19.653189714082398,19.02398982442439,24.205441209659515,20.16062238071552,18.963476628745678,17.96562762596234,19.531857117715408,20.86635829845881,21.255773747330974,19.280122810790118,19.369392868205782,22.9021647605954,18.93136369208174,21.47821316674072,24.42926325370543,19.880057005564264,21.11943827672747,24.0347829295736,19.174320595643714,19.810878030088894,20.711569411286565,20.79181709109951,21.52825545142037,22.510563057499144,19.714589625374927,19.195910426040165,21.643407148870626,22.79188245530678,23.088899981103793,20.914444260183114,22.409894316390734,22.55442514417979,21.497350091305258,23.661739831337005,22.81337882106461,20.335424976569172,28.119486884464127,20.443137811296648,19.395826969085725,18.744741571834194,21.52611831315801,21.048400728737306,20.235023779555554,21.0819625214105,21.53807712673746,19.283653604184654,20.890428315584586,19.706229562174084,25.296616627387504,21.809197399053033,21.371449507411626,20.19656265688086,19.305685493406397,19.08654493081137,19.639706353282477,17.583516204502104,23.250222506325386,22.82294225806205,24.601371538637977,20.0953223269386,21.243839329097405,20.37329318617189,20.45930590947763,24.987649624596244,22.400315304318365,22.625652665035147,20.455200776646638,21.021720462702472,20.59279530263181,20.232040256335257,22.264357262987502,20.592440384261195,22.36255454353493,21.99517470550352,22.199722361671988,19.391954018620176,21.83585207290849,19.14268047046844,20.53559540130039,21.167744601954155,22.926085660752005,18.718526187000297,22.28042360433217,25.95498550595432,23.32578333060166,21.309621092817217,19.68447811713372,23.268620237666717,25.42582021905868,21.189524929277866,19.937692777054252,20.801380865239704,19.316707310363867,21.75315297668226,22.868455030888445,25.947611368302585,20.033901511817852,23.20532613123076,20.554742003246933,22.96074119343453,18.988728193664233,22.68524113143557,22.64902697475969,25.92407619036082,19.407373582124833,21.346588936497685,19.366735480432887,19.124759964560997,20.07378037525111,23.985931291943867,21.460510640680972,23.23197611805285,20.73330584327736,19.12586494836624,25.275464586661446,19.715668478243582,25.489298123537854,19.013002282515146,25.312874128977917,20.415195026786527,19.91413691399634,22.130140513245646,22.92736601506673,19.584806561357016,23.12482431901705,20.687407722542194,19.611919934246885,24.151034118830534,20.079932477068777,18.93225512877545,24.075045174066695,19.899787637164348,20.874095085342866,21.48054296008632,23.07558442006914,20.640568320462897,20.08863945104873,22.28815948936714,19.616841359021425,23.953590715604236,21.784577239189012,23.301772335078063,19.749416399115965,19.811951028236585,20.17477942437346,18.304477915148002,21.513176875338157,19.58793862702669,24.19084293977443,20.88004158687887,21.10029756186111,19.35190112925433,18.917516857772046,19.84423117509742,24.377548873947223,20.529649424214696,18.766505656199122,21.439000175436664,18.811308834423215,23.933428716720645,19.631145075685264,22.477074905595526,24.176989311672042,19.985865383441666,24.300186984861302,20.78628428680642,19.166666955002903,20.396533708238668,21.16161998720379,23.822353472570573,20.556259058220718,24.241029788998915,22.067946023704817,19.778390202711652,22.99407646829777,18.833019773765052,20.288010749874967,19.947775322749504,20.338620784352592,23.494951508290605,22.38615246636496,19.034944006994706,19.984953818018486,24.081396558763124,19.595170049801663,19.41162451128841,20.72798026434645,21.785986387464803,22.944212504137,23.12678795739032,21.831113335529626,24.147006515510686,20.371960850528456,19.32549212260673,20.504720566168093,19.431273145079818,20.869899532948505,21.94524199207118,21.941322673977073,19.72760807429288,21.84503668728796,18.979888467927786,20.204391739449186,19.721950276191546,19.579038969018928,19.030933465764726,23.8143209659356,20.855181011615457,20.74517719106974,20.090817980596317,20.872940195293037,21.393924057871644,22.20317352529318,19.736694248820182,23.68012909153893,20.722700819009958,21.714666548374087,21.05462253329071,19.397806220065235,19.129172314836154,19.83710046840733,21.397724240612124,22.701728218692,19.775576525065844,24.10746972295679,17.80002145055656,21.28200527852275,24.425354799557958,24.25287711008343,21.61681534013004,21.629837063073623,22.53034703709958,21.62524006847279,20.878967106147208,18.668189678924886,19.500769926583363,19.995538068644624,23.512127623334848,21.53372289651904,20.823420888524133,22.265101932257092,20.999782941566835,21.517654972490075,19.604530267831706,19.76742293183983,20.740147099081785,22.94133404816206,19.387966817051925,20.39789928981696],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle GradientBoostingRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predGB_log"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle GradientBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsGB_log = np.logspace(1, 4, 10, dtype=int)\n","param_gridGB_log = {\n","    'gradientboostingregressor__n_estimators':\n","    n_estimatorsGB_log,\n","    'gradientboostingregressor__loss':\n","    ['squared_error', 'absolute_error', 'huber', 'quantile']\n","}\n","\n","GridGB_log, \\\n","BestParametresGB_log, \\\n","ScoresGB_log, \\\n","SiteEnergyUse_pred_logGB, \\\n","figGB_log = reg_modelGrid(model=GradientBoostingRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log.ravel(),\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_predGB_log',\n","                         score=score,\n","                         param_grid=param_gridGB_log)\n","\n","print(BestParametresGB_log)\n","print(ScoresGB_log)\n","figGB_log.show()\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"y":[1.921148596301094,1.851644497963079,1.6383769032141586,1.5080052255107321,1.4950070262969188,1.5785546115584788,1.5573223139458405,1.5679540735787434,1.6321870370958096,1.7662013756050576]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"y":[2.058224471336473,1.9945436151082252,1.7997241287161188,1.6820239126355772,1.6684901603670788,1.7468323264273309,1.721121714572047,1.7703544166168288,1.8973366135463041,2.0018489077770343]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"y":[1.7840727212657146,1.708745380817933,1.4770296777121985,1.333986538385887,1.321523892226759,1.4102768966896266,1.393522913319634,1.365553730540658,1.367037460645315,1.5305538434330812]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[1.9193541056966,1.8521331605778388,1.6415365883067206,1.5029817079658983,1.4819920696147935,1.4698441348500548,1.450726482869682,1.4195750211821778,1.4238447366961766,1.578006263754407],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[2.1511641841800277,2.0917788591930444,1.9115643417370731,1.7979798830723246,1.7837013399629802,1.7738022051907463,1.7636034904819011,1.7391067374594247,1.7032408287357252,1.6785946561024765],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[1.9678210928222923,1.8983150882390927,1.6873296066321246,1.5759683448556001,1.5685448906407504,1.5630820924674522,1.5435910589537825,1.5403727847001405,1.5655029553626205,1.9952936228252793],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[1.7604959721901432,1.6857593926781715,1.4653125813276497,1.345844192593018,1.3374870356076596,1.332210941195394,1.3189558843612452,1.2939997819550684,1.3614689196876957,1.488738690365239],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,7,21,59,166,464,1291,3593,10000],"xaxis":"x","y":[1.8069076266164061,1.7302359891272483,1.486141398067225,1.3172519990668188,1.3033097956584108,1.7538336840887474,1.7097346530625916,1.8467160425969054,2.10687774499683,2.090373644977885],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle GB pour le paramètre<br>gradientboostingregressor__loss=huber<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["FigRMSEGRidGB_log = visuRMSEGrid(GradientBoostingRegressor(), 'GB',\n","                                 n_estimatorsGB_log, 'n estimators',\n","                                 GridGB_log, BestParametresGB_log,\n","                                 'gradientboostingregressor__loss')\n","FigRMSEGRidGB_log.show()\n","if write_data is True:\n","    FigRMSEGRidGB_log.write_image('./Figures/ConsoGraphRMSEGB_log.pdf')\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()</th>\n","      <td>0.883546</td>\n","      <td>9.865173e+06</td>\n","      <td>3.280866e+06</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()</th>\n","      <td>0.855511</td>\n","      <td>1.098865e+07</td>\n","      <td>3.289552e+06</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()</th>\n","      <td>0.849277</td>\n","      <td>1.122320e+07</td>\n","      <td>3.303228e+06</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()</th>\n","      <td>0.272898</td>\n","      <td>2.465043e+07</td>\n","      <td>3.653788e+06</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()</th>\n","      <td>0.377635</td>\n","      <td>2.280601e+07</td>\n","      <td>3.103676e+06</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()</th>\n","      <td>0.279939</td>\n","      <td>2.453079e+07</td>\n","      <td>4.238622e+06</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor()</th>\n","      <td>0.333480</td>\n","      <td>2.360117e+07</td>\n","      <td>3.215257e+06</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   R²          RMSE           MAE\n","Lasso()                      0.883546  9.865173e+06  3.280866e+06\n","Ridge()                      0.855511  1.098865e+07  3.289552e+06\n","ElasticNet()                 0.849277  1.122320e+07  3.303228e+06\n","KNeighborsRegressor()        0.272898  2.465043e+07  3.653788e+06\n","RandomForestRegressor()      0.377635  2.280601e+07  3.103676e+06\n","AdaBoostRegressor()          0.279939  2.453079e+07  4.238622e+06\n","GradientBoostingRegressor()  0.333480  2.360117e+07  3.215257e+06"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["Scores = ScoresLasso.append(\n","    [ScoresRidge, ScoresEN, ScoreskNN, ScoresRF, ScoresAB, ScoresGB])\n","Scores\n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()_log</th>\n","      <td>-0.104695</td>\n","      <td>2.251772</td>\n","      <td>1.113607</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()_log</th>\n","      <td>-0.159119</td>\n","      <td>2.306572</td>\n","      <td>1.110623</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()_log</th>\n","      <td>-0.139985</td>\n","      <td>2.287456</td>\n","      <td>1.111688</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()_log</th>\n","      <td>0.384768</td>\n","      <td>1.680438</td>\n","      <td>0.830577</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()_log</th>\n","      <td>0.374041</td>\n","      <td>1.695024</td>\n","      <td>0.753997</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()_log</th>\n","      <td>0.369392</td>\n","      <td>1.701307</td>\n","      <td>0.845429</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor()_log</th>\n","      <td>0.394810</td>\n","      <td>1.666668</td>\n","      <td>0.777784</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       R²      RMSE       MAE\n","Lasso()_log                     -0.104695  2.251772  1.113607\n","Ridge()_log                     -0.159119  2.306572  1.110623\n","ElasticNet()_log                -0.139985  2.287456  1.111688\n","KNeighborsRegressor()_log        0.384768  1.680438  0.830577\n","RandomForestRegressor()_log      0.374041  1.695024  0.753997\n","AdaBoostRegressor()_log          0.369392  1.701307  0.845429\n","GradientBoostingRegressor()_log  0.394810  1.666668  0.777784"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["ScoresLog = ScoresLasso_log.append(\n","    [ScoresRidge_log, ScoresEN_log, ScoreskNN_log, ScoresRF_log,\n","     ScoresAB_log, ScoresGB_log]).rename('{}_log'.format)\n","ScoresLog\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()</th>\n","      <td>0.883546</td>\n","      <td>9.865173e+06</td>\n","      <td>3.280866e+06</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()</th>\n","      <td>0.855511</td>\n","      <td>1.098865e+07</td>\n","      <td>3.289552e+06</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()</th>\n","      <td>0.849277</td>\n","      <td>1.122320e+07</td>\n","      <td>3.303228e+06</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()</th>\n","      <td>0.272898</td>\n","      <td>2.465043e+07</td>\n","      <td>3.653788e+06</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()</th>\n","      <td>0.377635</td>\n","      <td>2.280601e+07</td>\n","      <td>3.103676e+06</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()</th>\n","      <td>0.279939</td>\n","      <td>2.453079e+07</td>\n","      <td>4.238622e+06</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor()</th>\n","      <td>0.333480</td>\n","      <td>2.360117e+07</td>\n","      <td>3.215257e+06</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso()_log</th>\n","      <td>-0.104695</td>\n","      <td>2.251772e+00</td>\n","      <td>1.113607e+00</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()_log</th>\n","      <td>-0.159119</td>\n","      <td>2.306572e+00</td>\n","      <td>1.110623e+00</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()_log</th>\n","      <td>-0.139985</td>\n","      <td>2.287456e+00</td>\n","      <td>1.111688e+00</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()_log</th>\n","      <td>0.384768</td>\n","      <td>1.680438e+00</td>\n","      <td>8.305766e-01</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()_log</th>\n","      <td>0.374041</td>\n","      <td>1.695024e+00</td>\n","      <td>7.539974e-01</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()_log</th>\n","      <td>0.369392</td>\n","      <td>1.701307e+00</td>\n","      <td>8.454289e-01</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor()_log</th>\n","      <td>0.394810</td>\n","      <td>1.666668e+00</td>\n","      <td>7.777838e-01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       R²          RMSE           MAE\n","Lasso()                          0.883546  9.865173e+06  3.280866e+06\n","Ridge()                          0.855511  1.098865e+07  3.289552e+06\n","ElasticNet()                     0.849277  1.122320e+07  3.303228e+06\n","KNeighborsRegressor()            0.272898  2.465043e+07  3.653788e+06\n","RandomForestRegressor()          0.377635  2.280601e+07  3.103676e+06\n","AdaBoostRegressor()              0.279939  2.453079e+07  4.238622e+06\n","GradientBoostingRegressor()      0.333480  2.360117e+07  3.215257e+06\n","Lasso()_log                     -0.104695  2.251772e+00  1.113607e+00\n","Ridge()_log                     -0.159119  2.306572e+00  1.110623e+00\n","ElasticNet()_log                -0.139985  2.287456e+00  1.111688e+00\n","KNeighborsRegressor()_log        0.384768  1.680438e+00  8.305766e-01\n","RandomForestRegressor()_log      0.374041  1.695024e+00  7.539974e-01\n","AdaBoostRegressor()_log          0.369392  1.701307e+00  8.454289e-01\n","GradientBoostingRegressor()_log  0.394810  1.666668e+00  7.777838e-01"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["CompareScores = Scores.append(ScoresLog)\n","if write_data is True:\n","    CompareScores.to_latex('./Tableaux/ConsoScoresModèles.tex')\n","CompareScores\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()","GradientBoostingRegressor()"],"xaxis":"x","y":[0.8835456688347771,0.855510838298664,0.8492768899704516,0.2728980175846296,0.3776353270309972,0.27993905222061255,0.33347959571239083],"yaxis":"y"},{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()","GradientBoostingRegressor()"],"xaxis":"x3","y":[9865172.656611884,10988654.114662597,11223202.299562344,24650431.85546094,22806011.254154198,24530787.856306165,23601173.217069294],"yaxis":"y3"},{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()","GradientBoostingRegressor()"],"xaxis":"x5","y":[3280866.3086337247,3289551.60741985,3303227.7508444726,3653787.847466484,3103675.862738696,4238621.636775493,3215256.9565728786],"yaxis":"y5"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log","GradientBoostingRegressor()_log"],"xaxis":"x2","y":[-0.10469517524253025,-0.15911860551991497,-0.1399847189075052,0.3847679532136269,0.37404139608936315,0.3693922522874372,0.3948097808923974],"yaxis":"y2"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log","GradientBoostingRegressor()_log"],"xaxis":"x4","y":[2.2517717882345822,2.3065723601694406,2.287455531029177,1.6804384874947127,1.6950243899216086,1.7013074202615655,1.6666679933180435],"yaxis":"y4"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log","GradientBoostingRegressor()_log"],"xaxis":"x6","y":[1.1136068207473344,1.1106230173351972,1.1116882779206922,0.830576604693885,0.7539974128621129,0.8454289215963532,0.7777837974180236],"yaxis":"y6"}],"layout":{"annotations":[{"font":{"size":16},"showarrow":false,"text":"Consommation brute","x":0.22,"xanchor":"center","xref":"paper","y":1,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Consommation log2","x":0.76,"xanchor":"center","xref":"paper","y":1,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"R²","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.8666666666666667,"yanchor":"middle","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"RMSE","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.5,"yanchor":"middle","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"MAE","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.13333333333333333,"yanchor":"middle","yref":"paper"}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Comparaison des scores des modèles de consommation"},"xaxis":{"anchor":"y","domain":[0,0.44],"matches":"x5","showticklabels":false},"xaxis2":{"anchor":"y2","domain":[0.54,0.98],"matches":"x6","showticklabels":false},"xaxis3":{"anchor":"y3","domain":[0,0.44],"matches":"x5","showticklabels":false},"xaxis4":{"anchor":"y4","domain":[0.54,0.98],"matches":"x6","showticklabels":false},"xaxis5":{"anchor":"y5","domain":[0,0.44]},"xaxis6":{"anchor":"y6","domain":[0.54,0.98]},"yaxis":{"anchor":"x","domain":[0.7333333333333333,1]},"yaxis2":{"anchor":"x2","domain":[0.7333333333333333,1]},"yaxis3":{"anchor":"x3","domain":[0.36666666666666664,0.6333333333333333]},"yaxis4":{"anchor":"x4","domain":[0.36666666666666664,0.6333333333333333]},"yaxis5":{"anchor":"x5","domain":[0,0.26666666666666666]},"yaxis6":{"anchor":"x6","domain":[0,0.26666666666666666]}}}},"metadata":{},"output_type":"display_data"}],"source":["fig = make_subplots(3,\n","                    2,\n","                    column_titles=(\"Consommation brute\", \"Consommation log2\"),\n","                    row_titles=('R²', 'RMSE', 'MAE'),\n","                    shared_xaxes=True)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['R²']), row=1, col=1)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['RMSE']), row=2, col=1)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['MAE']), row=3, col=1)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['R²']), row=1, col=2)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['RMSE']), row=2, col=2)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['MAE']), row=3, col=2)\n","fig.update_layout(\n","    title_text=\"Comparaison des scores des modèles de consommation\",\n","    showlegend=False)\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Modèle de prédiction sur la consommation énergétique (SiteEnergyUse) avec les données catégorielles"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["BEBCat = pd.read_csv('BEBCat.csv')"]}],"metadata":{"interpreter":{"hash":"117c35e9bc21f93c21d2b781ffd59753bc10bfa2757aefbea5ab108f880d10a5"},"kernelspec":{"display_name":"Python 3.9.9 64-bit ('.env': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
