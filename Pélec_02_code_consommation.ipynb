{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","pd.options.plotting.backend = 'plotly'\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","from sklearn import metrics\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, \\\n","                             GradientBoostingRegressor\n","\n","from Pélec_04_fonctions import *\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 17] File exists: './Figures/'\n","[Errno 17] File exists: './Tableaux/'\n"]}],"source":["write_data = True\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["BEBNum = pd.read_csv('BEBNum.csv')\n","\n","BEBNumM = BEBNum.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n","SiteEnergyUse = np.array(BEBNum['SiteEnergyUse(kBtu)']).reshape(-1, 1)\n","TotalGHGEmissions = np.array(BEBNum.TotalGHGEmissions).reshape(-1, 1)\n","\n","BEBNumM_train, BEBNumM_test, SiteEnergyUse_train, SiteEnergyUse_test = train_test_split(\n","    BEBNumM, SiteEnergyUse, test_size=.2)\n","\n","score = 'neg_root_mean_squared_error'\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Scaler moins sensible aux outlier d'après la doc\n","scaler = RobustScaler(quantile_range=(10, 90))\n"]},{"cell_type":"markdown","metadata":{},"source":[" # 1. Modèle de prédiction sur la consommation énergétique\n"," (SiteEnergyUse) avec les données numériques uniquement\n"," ## 1.1 Consommation énergétique brute"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.8452540008574219\n","rmse : 11371966.097141473\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLR=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[2709760,3194368,1989632,108314368,4784384,931584,859648,8779520,1070336,56800512,995072,1699328,1118464,1508864,400384,13911808,38363904,6731264,55788800,534528,1272320,1115136,790784,877056,8416000,738560,2293760,-164608,7432448,4133376,2593792,1083136,9649408,2704384,5775872,1231616,2891264,247040,4011008,654080,3243264,14060288,1054208,9475584,151454976,2865152,35133440,517120,120489728,5182976,1848576,1334784,883968,804864,1668608,1381888,4483072,4056064,15666176,891392,572672,6161152,811264,2793984,7483648,277504,6131456,1974528,1074944,6079488,816640,958976,5438464,518400,947712,2622976,30234624,737024,1490176,1505280,842752,2228480,1216768,425984,3704064,9623296,1566208,1842176,4017920,1373440,10504448,12960256,279808,3102464,16219136,9923584,1630976,39390464,6702592,1177856,2538496,488448,14838016,1092096,4161536,978944,2627328,3204096,1471488,2046976,1316608,1419008,41447680,1197568,253440,5662464,12646912,947456,23904000,3941632,4156160,32768,1269760,1358336,819968,1408512,720640,1053184,6560256,791040,1220864,8519680,2002944,986624,398592,9082368,1033984,5853184,410368,1830400,1089024,867584,3252224,1422848,91884032,3496704,2067456,25552128,1148672,4317440,-54272,667904,92416,159245312,942080,4730624,7922176,2636032,367360,1876480,790272,-933376,1977344,1294336,14718208,5232640,4238848,954368,7767808,7578624,473856,3222016,4850944,11147264,10777088,2495744,912640,940288,2574848,287232,12598528,906496,1619200,4101376,327424,1109248,26775296,3151872,18070272,563200,945152,1701632,1357824,4150016,1529600,7189248,1916928,4188160,2755840,22377728,2303744,1202944,439040,1184512,638976,2810624,3340288,927744,3248896,3196160,1513728,2585600,1219840,2360832,1076736,1743872,9607680,6859264,5601792,27091456,2137088,818944,1442816,925440,2365696,1720064,852480,12413952,42982400,1210624,2057984,2271232,2169344,2022144,21165568,27986944,3862016,385536,929536,2157312,451840,8011008,2948864,809472,130560,805632,5884416,1504256,1355008,7435520,12409088,1536000,5313536,2200320,8970496,-544256,4165888,755456,1862400,1225984,2040064,119296,6132224,999168,322304,1257472,4318976,3099136,583424,1654272,2388992,1251328,823552,1445120,750080,2034176,2044160,6478080,1692672,1936384,5158656,2103552,2182656,471040,2001920,10957568,4878592,568064,1848576,1649664,12689920,1951488,4080128,30187008,1256704,539392,2377984,3544576,2499584,3836928,16890624,5374464,1070336,2903552,6408448,4403968,710912,3804672,1585152,1218304,1097472,2389504,1765632,1731840,1860352,1159936,12577792,1455872,434944,1624320,4015616,3005440,3454464,2352896,960768,9073920,751616,3925504,6373120,5880832,950784,1468160,2397952,3832064,4936704,1347072,4981504,1158144,756480,2541568,375040,22592768,3138304,852224,4557568,1003008,2151936,1267200,4285184,7229440,1532416,280320,2759168,6215424,5718272,216832,14420224,1456896,-40704,325376,2818304,867328,4404480,60672,2128128,2775040,1428480,7091456,510208,10678784,678912,1404928,310528,738560,2191872,928512,2472448,1799424,2173696,262400,8611072,1019136,8497408,889088,6730240,5796096,2453504,735232,6217984,547072,8819200,3201024,5620480,9273856,459776,2368512,815360,1949184,4053248,2856960,326912,1600768,1607936,878592,15879936,1948160,4070400,16321280,2398976,2125056,2014976,925696,25698816,1908224,2104064,1627136,1408512,43630848,9467392,3047936,1648128,3851520,637184,814848,832000,1449216,17979648,1972224,1376768,-311808,2305280,1133056,-1018624,625920,493824,12061184,829184,1250304,4849408,2722304,6883840,16288256,9761280,1095424,7818496,9785856,11250688,1588224,1353216,6703360,6234624,3926016,4287232,6830080,1142016,2159104,671744,3954432,1857280,13367808,475392,3131904,63217408,7806720,10527232,3653120,8753664,8105472,587520,3178752,1355520,35726848,3013888,27249920,6163968,3237632,2119424,1657344,915456,21017088,1825536,1752064,7763200,5204736,1183232,657408,4212992,3420160,3578368,1111808,20328960,839168,766976,2416896,2339072,1071872,3158528,2260736,3461888,474368,1558528,2708736,1459200,1125888,569344,1572096,5841920,1114624,1456384,-958464,-379904,-927232,777984,1227776,927744,2331136,5345792,1336064,120576,1833984,48350976,3428608,768256,837632,4545536,10924800,1967872,3916800,1007104,125184,1513216,2775808,1249792,1394176,10123776,-63488,1144576,7592704,745472,1765376,2230528,6544384,1135104,2530816,1442560,2353920,1388288,5848576,1172480,5715456,2543616,1985024,716544,3573760,1373696,138496,1536000,1615872,1308160,4037120,3021312,6603776,4363264,3660800,1808896,2660096,6577152,1694464,-83968,3266048,4374528,966656,5323008,6223104,2405632,-210944,782080,3229184,800256,1249536,11035136,14870784,-395520,1157888,10127104,1084416,6777856,5757952,3726848,2398464,4462848,5266432,3790592,1459200,-42496,805376,1184000,2549248,1045760,1148928,16052480,1233664,898048,-580864,2318336,5821696,750080,1908992,809984,1016576,1010944,1708288,7579136,2083840,1877248,2573568,2620160,12174336,699648,12850944,1231104,-304128,5409792,2748416,6941696,7595520,3544576,439296,1537536,2653440,257024,11019008,2029824,1295616,1842944,1403904,1584896,12122880,695296,2423552,1847552,1239552,2235136,547840,1363968,1848576,4049152,21069056,21377792,755200,4598016,11147776,997120,1604864,1396480,22584320,14456320,21090816,-84736,1241856,2277120,2755072,8837888,752896,1715712,1882880,12657152,232192,561664,1332480,4469760,4879616,1049600,2114304,8070144,2066688,-267008,720384,3362048,2176000,11446528,614912,1523968,2311424,1114112,6433280,1517824,3412736,134912,1518848,148736,1502720,1812224,5444352,1020416,393472,3894272,751872,1190400,1673728,1165056,2682112,1363712,855552,1851136,3418112,790528,1368320,4130560,1583360,19968,1254912,1903872,1558528,76301056,1953536,5970176,15709952,11532800,5246976,816128,2360576,9493504,5664512,4660480,6085376,1466880,4852224,910336,2427136,2029312,4016896,242432,1253888,1624320,708096,1734912,1180928,350976,681216,5427712,1029888,1770496,-406528,2072576,2487040,2613760,3637248,2622208,1961984,741888,2383872,441088,1910272,15530752,1457920,4106752,952064,7413248,3375872,737024,6451712,1431552,6992128,870400,5364992,17631232,51756544,3044864,1299968,1226496,415232,16354048,1708032,854784,1644800,5433344,8180736,1699328,1998336,2779392,3012096,354048,9399808,1279744,1224960,4778240,4390656,662784,3751424,1101312,1645056,440832,7680256,1144064,907264,2702336,3081216,1138688,1660928,2842880,1308928,718080,3677440,11782912,5510912,11040512,11166720,3419136,1193728,5228032,14407680,2987776,5669888,18262784,24708608,1571840,5836800,1795072,1514496,1351936,2385664,2427648,1937664,3417088,33514240,2664448,1007616,1202176,3712000,5298176,4009472,680192,901888,22654720,1849344,1123840,1079040,2105088,4878336,8355840,1583360,6700288,1796096,1700352,71901952,3277312,893952,545792,4944896,2019840,1634048,3847936,4033792,4902912,2818048,4510976,5877760,2954496,788224,4740096,4835840,938496,365056,10922752,3096832,6436864,45024512,4912128,2027776,1526016,912640,1341952,2108416,7599104,7925504,5237760,1558784,3329792,3320320,-441088,4377856,973312,739328,907264,2327808,932608,1426688,1196288,3040512,822784,1609984,1994240,4331264,26835968,955904,4753408,1532928,1140736,1805056,1045760,4428544,1137920,3168512,1103616,3194368,3705088,4020736,2017536,4621056,1122560,1404160,3836160,9327360,3761664,2990848,1361920,628992,670976,400640,846848,4079872,61696,4233984,626688,76301056,6124288,4133120,1592832,967168,1529600,14862080,17789184,2431744,1723136,5903360,2041344,226816,-366336,4653824,502016,860928,404992,720896,1581824,15155456,810496,495104,52736,853248,1264640,229120,535296,7160576,-148480,1355776,4302336,2555392,1763328,808704,6174720,1830656,4435456,1437184,2997760,15288320,698624,7812864,5886976,2197248,-258048,163072,2391296,16114176,2541824,498688,3411456,254208,2880000,91592192,2946048,7062784,2410496,2589184,4387840,6340352,4540928,3573760,1013504,5081344,849920,65536,2619392,1078784,1512192,21444352,1504256,5147136,4796672,2732544,-25600,1452544,715008,1082880,1746432,1265152,665600,3550208,402432,1886976,2264832,12045312,4132096,2297856,3033600,44526848,5026560,9493248,3956480,20646144,1113856,1559552,126720,5224192,2091008,3985920,3834368,61843456,167680,1027072,2143744,22637312,1038080,2201600,1840896,1571328,2915072,-1132288,3685376,600064,19795712,6004224,1792256,749312,9877504,734464,3184128,2024704,440832,5036544,5213952,734720,1845504,589312,1475584,1796096,1312256,3151616,2335744,2213376,1748480,-75008,642048,1299968,1199360,2083840,1134336,217600,1330944,1108224,9054208,3836160,16085248,10284032,1964288,62208,4422912,962560,866048,5338112,2876928,1285632,1235456,2612224,170496,2020864,1406464,5071104,7521280,1317120,2042368,19766272,1144832,2058240,1560320,692224,2860800,5021696,4221440,1076224,17511680,6326784,4128512,2706688,2230016,9790720,8556288,1521920,1172480,7409408,912896,2646784,891136,2788352,1165824,12441856,2474752,755968,4411392,4035072,9370112,8002048,849664,1884416,4740608,4355840,3584512,6258944,14867968,4233216,301056,769024,1046528,4699904,1655808,16062720,1984512,12545024,6230272,1054976,7890176,2835200,1849856,2844416,22592768,661248,917760,20856576,3518976,921002752,930304,1004288,522496,84928512,1510912,1133568,5476608,3240448,1477632,1148672,4489472,92672,10434048,1145344,-105472,2371328,3249920,2495232,10342912,7342336,18481920,913152,613888,6700288,1671680,3574784,760064,395520,1142272,1859072,1837056,722432,833024,1706752,1056768,310272,1708544,378112,1571840,705792,8271104,3922688,251136,15459072,3112704,1291776,834304,10096128,467200,1812224,9417216,467712,1436672,543744,814336,2725632,4340992,1753344,3363328,6190336,10993408,1086464,1657600,2402816,6171904,2513920,56581376,852224,600064,758784,3173888,3024384,467200,4816640,4008448,1313792,9333760,1969408,240896,5065728,5568000,84736,72448,16925952,2981888,15393024,1124096,5906176,4614912,4871424,1450752,1383936,199936,835840,1104896,1998080,9775872,4827392,708864,2542592,5351680,7131136,2858496,3968256,553472,1246720,851456,12618496,-124928,2861824,5057024,1761536,2120704,4086784,3314176,10955520,-652544,11114496,4302848,1171456,6007552,6775808,3453952,1694720,13378304,2910464,2425344,2130176,1399040,8454912,4923136,4916992,1666048,10259456,13088256,116992,786944,507904,739840,18927872,1756160,10657536,1404928,966400,828160,233472,2067456,2354688,192768,520960,1322752,4259584,2442752,6773760,1107968,-76544,2713600,-197888,1373440],"xaxis":"x","y":[4395510,1499128,805751,58761304,3639670.25,1327591,476069,17714332,555153.125,143423024,415453,1160989,1696978.75,1334876,4585966,11962615,13730204,2356376.75,18470340,1396691.875,2056762,659176.1875,619409,791035.5,9683979,1241355.375,29540052,1237655.25,4018570,6118300.5,1568550.75,526752.8125,7575080,2058584,2711035,965786,5135040,669217.6875,1692323,803938,3497215.5,63668488,809291.8125,21069722,291614432,10105379,32141070,1523508,94560088,3073937.5,1024866,1516831.75,553230,1394408.25,1138386,3546459,4107827,2014500.375,20675922,1115107,1396280.25,151754.2969,1127895,2615247.75,2735128,489557.0938,2311532,5227992,848497,22965152,666967,673030,3242458,433443,614044.6875,4216639.5,12549118,733249,770736.125,1848074,1051523.25,853314.5,1017787.125,501194,2298815.75,2255026,2202407,1045337,1436109,1362523,19526080,13146645,635516.375,1532390.25,6227419,274682208,1751483.875,30135280,2265487,3218824,1725558,816300.5,666392,2292169,2663520,931148.625,7520554,7613116,1232712,4970047,942962,1166069.875,9387211,5826745,1423463,2660396,4103027.5,784584,8233527.5,4034028,31803568,345399,2360656,711118.375,549438,901679,1939297,1094343,2455923.5,1992158.25,817695.6875,2986728,1008536.813,1078936,654446,2591768,555709,5401277.5,2099386.75,1983256,862945.125,1417728.25,14172606,643740,163945984,1051944,1569569,8747819,534703.6875,5032796,518107,556700.6875,1261963,114648520,9313513,7251589,2612378,2923484,754241.3125,162960,836437.6875,2547848,1563070,677476.6875,7860942,10040121,2148565,730672,3009967,9986032,505601,308400.4063,14064603,13631685,4748137,10434387,604183.1875,2139713,920068.875,1128060.75,11624548,596242.1875,3814590,2844685,1278437,1119679,21346100,2136086,4991893.5,590629,659368.625,1437569,450518,1047461,858701,9788717,1769137,2870914.75,1025659,10444297,4916451.5,641713,867184.375,688641.8125,746858.1875,15324777,1829981.375,743748.5,1260902,1307883,966812.125,3540225.75,1063960.125,922153.6875,794715.6875,1200811,4581493,6426022,4178233.5,8437401,1063480.5,739869.6875,824320.1875,8218155.5,913117.6875,764832.6875,2402535,6843855,37022180,706505.6875,1858224.625,2583451,2004777,1237252.75,10775646,16467486,2001643.75,576052,701422,6797528,270838.5,3265995,31371618,727374.6875,811900,784584,0,2545405,1631031,1536998.25,4998015,132105,1524499,1279570,6268627,1060818.25,991918,3158151,1441445.625,648022,1702639,3686962.25,2960159.75,255971.2031,488181,1219744.75,3321035.75,3043767,965651.125,282898,1645748.25,768479.625,508051,331059.5,633408,2454861,10834087,3752132.25,1108604,907617,7456330,1076693,966764,576644.125,975721,1987934.625,6962073,455798.9063,919180,3242066,6657348.5,1128179.125,2625396,10564729,1832031.5,452614,1365778,11508035,1587454.75,2009703.25,6860521,1544146.625,887022.875,3220635.25,7771933.5,2584082,536642.875,14619984,30038490,848734,2018011.875,1612167.25,3468606,1287422,785901,1287040.625,2225512,722062.875,645028,5975230,1878442.25,3655453,2322314,3073778,1425226,25631512,2946756.25,2483074.25,5662234,4934334,1123416,1826203,1187499.75,3518459.5,5917000,904059.875,4917420,601590,2204020,2559403,1140137,13609040,19991412,552015.625,1544117,2839882.25,1339220.625,2466100,4636396.5,1730431,1700057.375,1072487.875,1090545,6858533.5,2306723.5,918514,1157976.5,2156407,858932,668915.8125,1904641.75,1769998,0,530700.125,2804534,1678495.25,1003551,5572010,634561,13901935,1360847,650313,2195160,1335007.875,1107290.625,1031667.5,1234149,2114860.25,2805826,591845.3125,2251442,1898509.75,6299357,1076389,3091387.5,9230223,1544388.5,841841,1381590.125,662424,5053635,2401148,6182350.5,22078064,790671,4608963,662685.3125,1822830,3106930,1733511,362823,830286.5,495481,601818.625,21027396,952879,2337651.25,8037287,1809586.75,1657201,1167813,720170.6875,55030192,1862176.875,1509292,2323265,1072855,25970248,9128267,9029489,5976246,1934971,3316562,948676.5,755269,598857.8125,8190809,1026921,885958.125,1878742.875,3212269,811907.625,564754.1875,336461,1186835.125,6411570.5,1960538,847298.8125,2456132,2828950.5,6298131.5,12448381,4677797,753721.125,13458948,5722325.5,12931002,1482752,645055.5,2903161,6722886,2373568.25,3375211,2822377.25,7768057.5,1831722.875,640374,1485945.875,1224045.25,5216213,730292,455153,37797084,9375125,624180,1860084,1015060.188,16016644,670779,1427294.375,776248.6875,39403320,2664566.5,7182436,41399504,786253.6875,1208168.75,974179,1325597.625,26941110,463371,938833.625,5155090.5,1811506.375,876339,1083467.125,5804784.5,1044642,3652708.75,588856.125,15088676,552822.125,648879.3125,967432.8125,892227,507451,405050.4063,1750419,405062,593042,1116040,1827424.125,222600.0938,568033.375,622051,2003447.625,2654755,2233268.5,878927.8125,560334.5,919746.3125,1247972,587903,1732954.25,880115.5,7592451,1527348,742565,4133811,2442220,131373880,1190733.5,2133797.75,1581036.75,1041652.813,1912690,2534671,2483320,2698077,1191926,1239315.375,6782435,403800,604591,3060149,1826107,1035554.688,4784731,6901554,1712796,608199.8125,5820965,550767,1332912.625,3245772,304248,1003176.875,6057734,726124.625,7802205.5,2108926,2827818,2014245,8354235,888561,527438,335477,1288669.5,1456808,3835202,1775433,2843033.5,4285049,8422861,1651120,8188381,5671256,1191803,531563,8688676,2964206,1580597,2133372,6917478.5,5447076.5,862826,3999505.25,3721129,593824,929688,13214416,602245,4865551.5,632157,14431245,4729846.5,11208498,1920352.125,3053785,2147062,3322250,1702992,2759674,791052.875,2683383,737570,695059,6471427,951675.5,16808.90039,4222682,1324534.625,1167887.75,1391714.25,12204922,6577130,751292,1761048,552391,3039626,669117,95544,4338616,1012559,9997511,57133.19922,4247457,26416770,842511,8819864,712149,1960558,3298570,1670929,4983934,4217623,2996502,493000,4801472,2235765.5,635698.6875,3467698,1310481,470666.6875,1213869.75,1093627,1372706,61762380,842923,1820292,961491.3125,1467029.75,3042875.5,718362,1453543,1222509,2594119,7791957,15571642,896172,2763597,4039667,4475652,776237.375,811021,6352846.5,6387710,12271749,783651.5,626612,1255834,2527198,3894142.5,1972156,1691963,699382,6083307,619870,586827.5,1452804,2058955.25,13962749,24126,10591549,3240830,2953338,1260416,704799,1617421,1224642.5,3822102.5,3863772,1162226.375,921786.375,2100766,2019254,5134446,1536287,782494.3125,713326.875,2892779,317794.4063,456397,3182301,919048.6875,1976663,3065502,593953.375,767297.8125,764906,642825,3397709,1084553.75,736838,1403820,1084048,6983736,1171380,1985171,851439,1677877,1107870,990361.5,847514.5,80469216,1321065,2411402.25,17750994,5436057,4757299.5,509171.4063,3258819.5,7015692,1271021,3435687,1987218.75,1034941.313,340270,559234.6875,2158629,966677.125,3637973,689373.8125,858780.625,1705515.25,512166,97690.39844,675772.875,815800,2184336.75,2751694,1987073,1169948.875,938908,1280245,1375978.875,2845946,1194835.5,606111,753784,892634,3113412,1762174,2921368,18868768,5348309,6376820,2058532.875,7782092,1212551,1224276,1958352,544172.375,8393893,767361,6522551.5,10939331,39523852,4519115,10020111,2601945,620104,8800292,968698.3125,1756668,2065156,2773779,9343428,219483,3303553,6917280,1091162.75,1306087.75,2849048,1429855.75,410433.0938,8873485,9049598,696522,3046543,1433504.375,1503823,551219,3789270,958725,960987.8125,1438479,1988784,43943,738983,1516377,1181963,806398.1875,2326565,49102164,1983509,9273278,6931901,1784796.5,1963054,2125323.5,3833052,4103800.75,318962.8125,7286313.5,25747022,1324965,5086863,1422406.25,1758581,1486177,1162493.625,1192100,970118,2646523.5,12263687,4767227,571644,559805,12250422,13410041,2726369,447947,1988760.875,13300859,1348497.125,661380,1625308.625,4236396.5,1906597,5450441.5,106918,2270531,41680640,771512.6875,35685224,0,1852926.625,851053.8125,1739286.75,1748629.875,863703,882896.5,2837589,571309,2237482,4765753.5,10513817,1234095.5,1372475,552729,3886929,809257,728167,6583940,2203758.5,1165598.875,59124384,1727783,1048813.75,838571,948157,1966424,3786217,5328224,3869754,1637199,1054581.375,1108310.875,2063212,1608224,1907697,1126157.25,502667.6875,662280,2439061,1019487.188,1558348,1162120.375,5762597,611997,4467292,1530744,2082754.75,16020642,514471,3751417,207572,2245568.25,4520838,855665.125,1688612.5,2473302,6136327.5,1423438.5,288191,2585187,2145603.5,10403123,2053611,1337968,839115,4369513.5,4498367,2291581,22530168,694479,508014.5938,1620721,607373,520011.8125,3234787.75,452744,930611.875,723424,85357952,2733003.75,3625434,2228675.5,524053,717056.375,10249433,18829136,3040924.25,3030041,2057457,1115504.375,1419453,602618,1539478.5,635697.3125,323739.4063,569115,1257144.75,630040,8767617,544043,619545.125,917783.5,755856.875,677536.3125,385719,558893.375,11521702,1382309,1823782,1602030,2147013,32579658,574650.375,6668568.5,1198291,3986851,2470566,4207110.5,6576563,962577,4734631,1049748.625,1623657,824516,647792.875,1225404,6481807.5,3064039.5,510517,1698074.625,1135172.25,1813718,74130576,2330029,6366084.5,342726.0938,2938366,1045413,6832991,13133225,1183042.125,1286882,4025787,557193,1475475.5,3147042,956958.875,916864,12720069,942712.5,2246747.5,2150120,14084606,2214469.5,1913558.375,1103762,684941.375,1216543,3542708.5,780410,4742869,1117658.75,806615,1149140.25,7539963.5,3202213,1366000.125,6807227,19827916,13253979,5149928.5,1706301,7811542.5,945245.625,209698.7031,1926134.875,17320768,1182575,10557910,10343027,157606480,1646088.5,116486.6016,2261586,13969552,1652833.25,1121259,7492939,928584.875,1856230.625,10018321,13010578,540465,13239050,11924933,3494929.5,2014475.875,19036416,1161814.625,5326222,1430356.75,607579.6875,5968153.5,3528086.75,1093757.75,8631444,770082.125,2520919,729096.875,1294615.25,4107443,485971,1210798.375,1858029,786979,955223,1118490,747326,1943899.25,867456.875,628609,2447861,1101536,12719042,3366528,4545148,9941308,998633.5,1210229,1833381,2057836,1534068.25,1813285.25,3303989,1380283,209644.9063,1210262,954352.6875,1594733.625,851585.1875,2266570,3591653,696709,1111858,48729588,969682.8125,4270584,1094940.75,984178,533742.8125,6014078,2331952.75,44293.5,11195917,2910252,29195468,3104171.5,2485521,14465798,3590731,1921890,1509775,4785692,188745.7031,7240274,1014405.375,3095882,1293390,5499097,226375,974940.375,1889880,4224872.5,4028943,3574542.25,4602793.5,8518313,3312125,5948007,3377209.75,2581741.75,22325364,3241917,1658751,577033.875,1649327.625,1588933,729397.1875,6505995,526059,1738376,2434208,1545485,16402563,2023258,537141,2503690,13567774,481206,1139980.625,8320719,1777005,873923712,666930,541950.875,863728.8125,61576184,5462958,2874181,3897738,2093011,666687.5,845961.375,3548858.25,3395651,20725996,1022114.5,2217620,360045,2184156.5,1355558,5525072.5,4089677.5,20560622,758875,621751,2188452,824483,1042652.625,698326.625,533226.8125,1045640,2217046,7801590,525668,789630.125,841821,866343,1481017,2798967,1359579,850615.8125,1860901,3091348.25,1207092.75,2094410,6125692,4039168,1130960,803533.3125,5121586,1049088,735178,8168547,524144.6875,2001936.5,383524,2772124.75,10642754,2759071.5,1353079.5,3245111.5,7380088,6836900,1807738.25,1296959,1604129.75,3605749.75,1551294,25210246,2159932,862744,1862752,3888945,1443367.75,1676426.5,5444253.5,4099807,2233262,1358022,760826,678663.3125,0,6609358,1028277.313,2307938,61827040,0,4321141.5,572139,1997181.5,1594422.5,2755578,523133,650205.8125,519317,108200.3984,844076.8125,1350190,465533.3125,2189030,464155.1875,637246,2131817,11026945,5424718,2401890,501741,1008665,730765,7564643,1784385.25,1383950,227566.7031,702385,1641667,816846,448676,1083729.625,801645.875,13315803,11702959,609921.6875,3195230,2506927.5,2249592.5,2619088.5,3780078,1387773,1072500,1075019.375,1908612,2992693,5134607.5,11458215,1897180,6579348,15590743,1261634.5,1392015,541542.6875,1962435,20781390,1795462.75,0,819378,400354.3125,5047936,1961581.5,2422767,1913034.125,622572,561368,1787119,3494383,1244824,2549649.75,866096.6875,1539176.625,2267094,1444429,1666221],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données de consommation prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBNumM_train, SiteEnergyUse_train)\n","\n","SiteEnergyUse_predLR = pipeLR.predict(BEBNumM_test)\n","\n","LRr2 = metrics.r2_score(SiteEnergyUse_test, SiteEnergyUse_predLR)\n","print(\"r2 :\", LRr2)\n","LRrmse = metrics.mean_squared_error(SiteEnergyUse_test,\n","                                    SiteEnergyUse_predLR,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=SiteEnergyUse_predLR.squeeze(),\n","    y=SiteEnergyUse_test.squeeze(),\n","    labels={\n","        'x': f'{SiteEnergyUse_predLR=}'.partition('=')[0],\n","        'y': f'{SiteEnergyUse_test=}'.partition('=')[0]\n","    },\n","    title=\n","    'Visualisation des données de consommation prédites par le modèle de régression linéaire<br>vs les données test'\n",")\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.2 Modèle Ridge"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre      Ridge()\n","0  ridge__alpha  1175.087131\n","               R²          RMSE           MAE\n","Ridge()  0.826684  1.203498e+07  3.217855e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRidge=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4602076.592788653,2686455.9574540365,2315568.8992865803,82441790.13606,6354626.0578184435,1267739.527718815,1210303.022129214,7066082.930080797,1217800.1403769837,45740138.66590384,1315626.8605810814,1705553.555519388,1452618.6230291268,1701837.8464421947,1442664.8260886297,21271077.689821035,26383871.9156712,5063254.15777612,43113411.992480054,1274708.8294147467,2847978.2081543147,1485623.7040795616,1158383.8163376283,1083692.2472192822,9168256.893222243,1109570.9074575338,2743898.705824498,942708.8179620965,6438800.950395087,2669076.5358387437,3696653.7773257876,1092071.508039189,8299055.879984372,3030121.347098166,5350994.727517393,1494090.2886630571,4490751.20321797,1159647.7210376232,4022245.0261618975,1328458.0163785233,2834067.632953963,13881450.224816877,1067102.1897225273,7261063.30624137,123697604.26310627,4408730.924010603,34977006.744406424,1674846.9311048067,108193254.63875003,5984718.219275472,2100460.614943391,1985191.3869689512,1231735.6098061218,1559892.4038417365,2684019.8463302604,1938057.4324711827,4095318.164990115,3764666.3248655736,17845890.066518225,1548231.0645133443,1781142.5984221827,4791458.5318207005,1173803.9619269548,2383827.0069028605,6504147.894514566,842808.8088877175,6030181.25739977,1899917.6263838387,1515980.509417681,6842684.919354641,1322662.8126487392,1133696.685763639,4474603.690742866,1123822.7977197142,1391587.3179181593,3691244.856075936,23172729.707375593,1488084.0034641318,1539288.9555306027,3740550.741206348,1046056.5005105403,2238250.8899596906,1903202.0575958355,996082.5986071713,3354865.5291525456,7664005.910678604,1818634.229604113,2237294.581490013,4180283.3369408576,1460158.1111402148,11501423.303930514,10089128.107816398,972374.8592219613,3437685.9445743794,12433108.346648589,19622772.845809672,1503888.4237321194,38973770.57424004,5193194.392189182,3112125.322210606,2924886.690941712,888168.8337665685,11710851.371037867,2687645.032039614,3711125.2574991137,1148833.8929017852,8046542.977928655,3240813.1182615915,2473941.879337116,1819364.7967981568,1979853.816316883,1625618.4085415404,26217930.46908313,2819403.819434421,1318255.9055385683,4988413.38614074,14840613.26716388,1419710.7014035832,20706156.20213671,2689532.080525412,3414783.9883910883,827797.0967911913,1525941.1520962324,1249744.8044945193,1180382.2809168966,1627637.23762016,1105545.060579707,1430035.8923934568,6427243.197733838,1301220.3983829583,1490435.9832862709,6882842.265699988,2241004.650089842,1589224.0590615298,1272941.8034958832,7703627.198373102,1209906.0563310315,6620262.228149074,1074303.3566768796,2736879.350754157,1534982.8250287082,1064801.172901235,6155018.05614336,1636832.73728352,48159254.92689802,3301797.1561434725,2407199.1214237846,20052339.46543123,1141444.2677701518,6063341.777918398,1052353.202664291,940655.7340496846,903067.7272962003,138158319.41328117,1026555.1121309269,2745162.174153467,7460726.408958165,4671046.272709633,870938.0094210147,1549125.3505200224,1039839.6820414965,195008.99306724034,1916908.578551074,1531399.9042984997,12645080.086525865,6178623.892249765,5683826.562019049,1424874.3281376236,6287066.900948876,11931789.589948129,879190.6297561382,2461441.175725559,4388859.0112154875,13096340.879116066,8570026.528909942,3239892.956581237,1101795.3753930598,1708476.006691346,2498951.726493842,1174132.3006514583,10381568.904181872,1273486.0286478885,1573783.093033043,3232609.135042823,1644897.6157967316,1754763.2488867834,29343618.349798955,3226942.549105916,10614798.429791411,1326307.6401572928,1126340.286032951,2110893.829605983,1447867.3611069275,3269391.1336962623,1497916.7406661045,5561131.8389504505,2012857.938902142,3581185.0230860515,2365120.594432523,17219955.158300772,3425655.699707009,1679842.8275248609,1059540.020442853,1466674.9490013623,1033781.9032851425,3602404.870033646,2199414.857559327,1116295.9710160503,2878899.312564103,2571943.143168396,2444378.694024523,2444981.919866201,1291450.6250522432,2122911.6042806385,1530174.2763569413,2021520.7870640866,11685293.033581331,6463858.1068114145,5323380.562315385,27370067.218808353,2458191.104876194,1174331.5789089578,1501941.0799748027,3205013.538988554,2342932.2417257745,1853525.3309672743,1682403.9494333714,10165553.162865195,31396894.066131804,1466748.4682038005,1854942.8137743892,2420888.410905084,1892176.2498653983,2526759.74414051,16442688.112658717,22147243.948851727,3536025.67305565,939566.0052739067,1404644.228878232,5375533.115808645,895637.2044963597,6619025.189504674,5460033.572316872,1165489.7523095733,1300589.9564633109,1314174.6591133303,4831170.368324999,1320995.4041756848,1481444.1662581114,5759249.504082926,9906041.115980972,1440619.2052008845,4722097.56896728,2005522.6529303298,9917017.834368061,761823.4129486822,2704661.37045447,1504833.6145826213,2409068.4317041086,1486733.8889323694,2302218.2053162907,2000065.0992311395,7248988.364169538,1493017.2153560922,882632.2123425063,1315708.760016927,4374681.154554983,2905953.160052767,922972.5031380844,1529957.021161065,2931312.4624160174,1499003.451638355,1186253.253778888,1365248.5826537712,1171246.4163522408,2061824.6425579023,2115947.8435948417,7431068.474132678,2264772.8366554477,2166668.212519582,5672384.532584927,2309305.7959000743,2179827.7818781137,910208.5347321453,1782857.8213506653,8423185.75655827,6609073.840231477,983418.8589750486,2068085.177594472,2224988.809107654,12495151.835378151,1795963.1455311547,3478606.50724703,23027278.732661493,1131072.968820907,1113359.0933881104,2355571.866391588,2811596.719686533,2368872.136983963,3456230.187678098,16836210.58350089,4474846.121139037,2075993.22839612,2608618.4852650543,5633579.899323817,5083391.517656998,953918.908456956,8865347.800024003,1891613.7780519703,1624204.4669941468,1527490.9081983627,2501063.201894342,1765138.1828088644,2186053.8575187316,1968578.0712924248,2020514.0114973553,9658141.361347497,1654902.5382387019,879934.1204558527,1829086.979131979,4856534.327546106,3890187.574157837,4073388.685130882,4063353.8725700225,1286696.4039478954,8549076.66066008,1055794.8301575324,4904493.037692161,5804240.237309248,8265226.116546092,1279198.5349916173,1361877.9948282412,2366634.212182753,3691401.031089239,4496411.188826498,1572708.918170824,4181946.0703067463,1438634.352231718,1195843.9363730298,2766177.2813587934,1186172.4275067288,17665366.731260456,8794495.501095705,1056313.0193658262,3604442.185781397,1391100.1535646887,2180531.445918909,1578452.234600294,5463922.082167557,7789257.912883803,1922876.2308743517,1327958.5902780537,2049456.8705797088,5793740.312460974,5266130.773605726,1026122.6059123827,11036051.61859557,1553725.4239922417,929231.1681064093,938747.8090895065,2684657.0313094133,1824704.927926993,3462072.218950048,773489.2259956906,2162490.202875746,2793055.6835124143,1641218.2832768147,5843443.877181146,1275536.5654521056,13101385.641493402,1077039.0116232913,2631477.2508268477,1103639.9101293467,970582.6686581946,2145911.956964895,1295973.2891116613,2291943.4955905653,2007138.6455544403,1932310.3846594444,894397.206866994,6942673.552253994,2063041.23749121,9027241.699531674,1217532.2121302066,7004949.213795476,4511745.751566349,2868774.8712411686,1116438.1909501483,5371565.913014742,1116117.7432871186,7646648.3258612165,3604551.733733941,4119267.439070163,7152721.376145915,1018109.0681796302,5445167.619249191,1444706.3194177875,3241416.8048165813,3628012.0874648998,3476706.6507352223,1082525.8334771523,1904204.5391294938,1343429.0965119426,1123020.2145949965,16984507.213544544,2004714.8476177019,3737996.276168069,12779127.923201274,2508065.9285610546,2169181.4331792593,1946758.585150596,1109307.2976907743,19023057.90423391,1996267.7795877368,2140128.156959234,1421163.9920132556,1601907.7414043264,34779508.53922879,7664311.425771122,3255835.516950528,1376886.5683640132,3444605.7453666753,1282188.1037042695,1212703.9619384848,1518319.5926781876,1366734.0095224679,14029677.617497854,2331119.8199482313,1807845.3000181948,645949.3133303896,3512503.4528536685,1268233.919299873,309447.67468270566,1033497.1572982226,1374935.7615709254,23746421.66437561,1328641.6499811118,1639869.291114296,4233358.903765157,3599231.768550056,6333917.872398352,16329485.22880511,7945935.790927497,1378207.337678157,11806836.787433006,7815995.556514435,12195414.165425844,1457333.179747873,1434318.7901427106,5291678.417624095,6980508.696122234,3107365.785810934,3514449.0578193497,5705024.061320586,1979282.9048951103,2603263.6587497834,1375157.5772177689,3121697.2621801444,2239377.1848929874,11687938.669243984,1092685.9098228337,2649544.4724815222,49793330.87767571,6519819.376852424,8279625.7085197335,3043819.204201274,7149281.781866048,8142928.241547328,1418453.8371763648,2816685.327996874,1294513.9698586955,31673423.351035737,2691697.2671879753,20508930.47255454,7820505.912203317,2579374.9858802995,2155208.4545382718,2124526.909168242,1738148.925531399,18660832.88466359,1660674.5856063645,1968165.8461281653,6140162.848190981,3221696.426463194,1456176.5362048969,1050899.6795815509,3456871.083004158,3008272.9193662964,2837097.526794262,1157170.9993981244,17405473.560558494,1186427.1976969158,1617204.7352842381,3156772.628304755,2472872.4761388367,1372143.816204347,2519604.2380684605,1802566.7704690006,2767919.228489546,1119976.5964143947,2021554.0124650323,2834881.1678026454,1200082.0805232802,1402804.3782370521,1106657.8387799999,2277652.9826485114,4979184.661076434,1252743.039097751,1653700.0498211854,580812.1666320916,944110.8859022208,1027590.9463423828,1442937.1491704455,1338119.7167414075,1544626.5688681798,6371061.95868214,4604786.3555521,1418057.1893718487,1239705.8013247317,1515597.1440551565,37255171.24111166,3005334.540321341,1219027.182200951,1009219.6362957184,4273988.762391064,8553125.99097133,1876745.620849111,3243785.954572953,1318526.9797056797,851044.7451352635,2055432.6321938867,2538721.950761074,1382638.7261596948,1336710.0590049182,8514991.467052978,1452351.4391731187,2035924.0468836757,9391551.733946577,1126456.620604854,1711078.65824272,2099507.088867602,7444743.665604256,1327187.0920829019,2467077.3623517603,3000581.992814464,2062447.1862823945,1638151.4705471685,5539006.600111354,1721105.8520039883,4875695.463217914,1985008.6114430833,2456451.8901204984,1105474.3259669116,4923003.3373114215,1741411.7203955874,946246.4287542922,1863924.0697566583,1669244.8040526095,2115131.621382013,3186830.915189028,3193831.785863274,5124078.531933909,4615162.298785895,5672903.279194431,5811168.27813438,7916602.743515594,7314803.431191195,1560231.4354373573,747025.2568261777,4386018.586882548,4431973.128871921,2037276.5521094808,5015358.98980476,7844314.398283626,2158056.131506266,956602.1711705702,1374893.3801695593,3058541.08439899,1168640.3351929141,2070356.6114827301,10530694.196884096,11580911.136624806,2810558.9358848715,1596615.1834144243,18041722.301021416,2122795.056045337,12309127.542105436,5053521.704604647,3847929.3954490153,3511405.821805061,2924615.854909928,4399443.023899486,3430521.0485410485,1493357.0509794424,1111236.8532536298,1484537.8717250107,1596688.7026168625,4147198.736136268,1483899.4699011543,1384752.841372888,16486069.219711319,1342646.7319602924,987904.5213958488,735589.2128283379,2067214.1288203313,4648524.220076368,1272441.2375642355,1725184.5524754736,1161153.3172193193,2163790.1684614266,1172883.6612521107,1853775.794726017,7874231.148203433,2209851.141422012,2318442.720957589,2357829.894795218,3015184.6240134872,40446385.02098869,1079547.1977635566,11783160.961097475,1514768.76141225,952249.9541902402,4604543.925155928,2922995.917925476,6378448.203766813,7744290.91379037,3906234.7865034984,886229.5009946148,1443846.7203756156,2277725.087710247,865262.8144512572,8906411.41009704,1814876.0414996296,1249033.5722216712,1522246.1976578936,1579774.470335408,2148886.9092045967,10293429.292828303,1208297.1285953554,2008240.5621195147,1950431.7504965279,1207085.988443982,2027734.8199256547,1116683.620189479,1520099.4510257775,2100460.614943391,4453812.923218443,16490214.34897551,17881160.899654616,1554738.7934184785,3604696.6990274326,10163454.947517492,2965296.208525524,1767229.103357282,1844683.5191441218,18124160.353687372,11245737.527952697,23441402.055195607,1095024.2532862339,1339433.5925772833,2284620.321260823,2786869.2975068055,6675936.86050616,1503275.8863889622,2233916.7314501666,2114944.709704453,12625092.069791216,1168491.5712649997,958553.8432869555,1516978.859980565,3511303.509455421,4258918.776802426,2266104.2852855367,2202725.924878792,7247001.213180489,2627661.9083270077,1101700.2654412468,1246764.5731196231,4060915.7640399914,2339434.6116575943,9075213.133492373,1974927.1244577086,1704699.705646435,2019685.220233811,1545476.8429532277,5555108.204106534,2334761.763908984,2960618.63493894,1137777.8676307313,1949302.769163176,1334739.0620200192,1264064.8609558675,1796600.2812934732,6936237.294828615,1183210.9147201919,961022.2025777586,8066173.378785473,1062927.4339802677,1309896.6062361724,1627330.6025183003,1582400.3108322571,4679537.046177474,1300667.881171867,1338822.779058454,2150601.006409502,3148360.6781014916,2352587.0518714935,1704901.9386398345,6045630.425439382,1759486.5558701812,1038559.9210512699,1787169.7108763717,2198924.0167475888,1732427.6738621048,60948507.97816287,1677998.0439500206,5068979.359405823,16549171.25683815,9294986.32615518,5872457.053027753,1167541.0560806305,4228494.906112032,7474803.2939044675,3485487.42950455,4515406.746569165,5070208.536954971,1801639.350576634,4235480.94214901,1208312.516242955,3381465.587391999,1667464.357726184,4438225.965679421,903106.9584098784,1360966.996674217,1358849.242101269,1237498.338843468,1723835.5603129552,1114068.3752707688,1208737.6625627489,1660977.3544313654,6048698.649357639,2500509.998469584,1891580.552651025,1074051.1203152826,2411301.728965899,2853193.6418886376,3753463.8075900003,3183226.369339196,3940320.114395639,2046103.2069158277,1660501.066504833,2083667.5701209172,1171211.2857391178,3707114.9155026795,12324266.853717089,3131125.125897169,2799016.7702518054,1735414.4581274572,8062532.569547833,3256122.718789337,1120965.2061690334,5347110.572002742,2501537.0164137855,13707377.853432775,1292758.5435678263,4190709.4653521227,15877905.553945005,45553089.30646962,3301372.547561097,6045586.813592635,1911948.5880336105,1538148.0772724946,12649187.688788213,2389930.9797162944,1421681.1909943605,2265278.6643648725,4957111.178073467,7875945.106288088,1566951.2236528895,2912171.1698412783,2372863.141919624,2690565.513383254,1514957.3813836696,7777640.894942428,1230318.0798318977,1199163.7118109334,6663060.488477917,8763000.793179158,1303757.963401908,3717989.161035953,1105723.2883086386,3644053.765915933,1039017.0153404861,8081325.040800109,1708199.677782847,1417742.8293311738,3029414.0009702155,2892937.9912984734,2580956.6112588337,1766506.0594721353,2712423.2594454405,1690557.7106984933,1093481.9164841862,3484805.7635656074,14865432.3683439,4870993.928900914,11792741.399376847,8760187.42671565,4097812.7223663144,1671512.7895186238,4225897.79331222,11464416.90058015,3861603.1610591365,4551620.269039196,18255673.98864431,20563383.312462673,1750786.1984963869,6750202.462562136,2011835.7592090126,2551887.2128296746,2141763.0868167295,2217209.484558608,2268989.3671615073,2027712.207589108,3981931.666425366,25777249.635811582,4354983.554347339,1180419.7901287465,1427510.0581333027,2938494.6150408983,6070893.356178189,3303376.973238844,1078029.2962024226,1233748.6536568212,19911410.224203244,3037878.533846434,1412745.4839487202,1186701.0845938334,1721717.8057400072,4242091.735831389,6595295.1181088695,1393462.2683543945,5719338.981111986,3041521.591815388,1497390.3681052383,51074445.89117165,2577711.357710383,1694764.6935139312,545047.5218939236,5799890.518162715,1938939.5178227802,1357817.5987624826,3040918.334368168,5421758.716625475,4132175.1820672536,3959601.8802437317,3937229.98551642,5150820.057118233,2446473.315659636,1156403.2471793662,4007823.7327735103,3660255.0754636563,1431799.2271434374,884707.0116168945,7718904.684608023,2613855.9677124466,5351092.807418454,41044470.56913229,5929830.7525757775,2779955.8880645297,1857840.8930562818,1673572.9429142056,2274989.9797360552,4364642.803451534,9778384.14853179,7615855.849845765,4529383.2583125485,1931921.0201782936,2642647.3724209066,2792074.7172636,1108213.6556284046,3592012.4533631103,1239409.6551611954,684819.116776993,1246377.6746547027,4358435.140525094,1550745.0988892694,1889746.5302602248,1567321.7473628765,2879612.699195328,1327189.8278676243,1754384.9733531433,2019538.2961127071,4178721.485010604,29655989.00803327,1239573.8041374302,5774734.901872771,1581555.7792896205,1415536.6085401657,3024726.4238443878,1060806.8091837657,4655023.263670427,1145013.464870425,4923185.03587758,1818916.6680605425,2591381.410138621,3645196.314563199,2939567.1151798307,2245888.0780079034,4199510.249637261,1588562.9492278676,1762844.3080724953,4871254.697589066,8148952.80670385,4113556.18984453,5992347.826856755,1848612.8369592356,1026354.7689416595,1491077.6419029103,989144.0643223689,1557145.9020395463,3498071.8530518375,720104.3291033981,3376213.942531595,1288568.134129764,61078448.212575935,6691218.82438341,3904977.417486625,1688693.9951910512,1291506.3576179605,1568997.4929045346,16684066.276883803,15786900.63361201,3310868.110885235,2197083.1838121354,5311703.792015181,2096710.9297567443,1448196.13995163,671227.2623481466,3698036.651021272,1395493.009478854,1059708.28077999,1011854.924027177,951059.9155267535,1475349.9000832664,12610593.726642702,1316367.4321099776,893937.2298249411,820409.0589538165,1195551.9627474805,1374754.6831202793,880255.6835655817,944726.2056806227,8538177.390132988,1199328.0197371086,3390794.5510980627,4239046.614378804,2215375.419145348,3171461.82622845,1163438.4485385162,5229617.66506344,2369317.4193060496,3636292.3209728273,1273374.5592868407,3949304.0382289537,11841888.850693509,2978420.81389069,7457525.793166276,4239484.101898527,1543528.080963349,1139066.665487204,816306.1943412302,1840836.607639483,16164494.643472575,2002425.423805039,927892.1879309141,3132831.5379790124,1588582.8416235493,2449539.462189486,83411292.185496,3602543.202594577,7763836.699971902,2793433.0777322818,3883404.042003062,3740305.4271825342,5934180.4717223095,3801681.426836363,3165274.4595251,1326378.521725933,5729384.38245162,1252960.4490080588,749117.0453722617,2804713.553154521,1720675.6732181935,1705940.4539843088,21063357.450383,1989411.7915911886,3331053.753593368,3987449.4347500643,2805276.4657723648,1328756.8364103986,1486148.3288313458,1804035.717615135,1228693.2707474255,1828286.9827326317,1563592.054029659,1345472.583369699,3239074.926397216,909632.4252534318,1838637.8368793628,1702591.5225511538,11210148.357361749,3255810.0880396077,2290948.176492022,3700148.541083164,34650581.749389626,4499027.766700845,11180044.538225148,3696445.683438683,15817893.78816485,1112160.1380729906,1451615.5448765585,1649567.797700983,7960334.30807905,2293904.0342334625,5417659.679376898,5003759.63597067,46121964.70945063,822077.8400669757,1049772.2095877337,2183257.621125479,18536779.08908592,1193396.6989626829,2414729.836829256,1689385.6290836292,1315701.1282962728,2675106.1681343997,2814759.5776929185,3068434.8494539605,1696674.0034306676,21562400.999087073,10988389.806464273,1635197.9483958024,975604.8261666449,10648390.036288809,1108972.7966863078,4816168.695094835,2507283.5640094047,950557.1356181237,4134636.9726581033,4966558.287684146,1091808.7289548772,4938614.86389189,1139563.3748147262,1450935.6385887468,1911527.713459236,1750988.2160921288,4818427.1176830605,1897139.6454717945,3086194.038093861,1835620.4692846732,926615.4317982066,1530281.0790093238,2131256.410194042,1468201.4203800596,2036438.4867711766,1654855.7267115365,1070804.3903073927,3190004.2165055014,1290338.7224195623,12984925.851869613,4239633.495143187,16356128.985298257,10710491.546399247,1897771.973176046,1026269.6137448982,4048491.8513197745,1432655.135545082,1023063.6081103771,3847413.967021944,2738558.719678116,1880928.4505051905,1203973.6654809988,3029953.231531506,922019.5627860047,1889598.0616996451,1898508.2608940382,4400575.5284127155,5959390.94908114,1555487.932569183,2248225.221072304,19390182.414216984,1275448.8498049704,2168092.4396296637,1598846.0490613214,1081000.1499398155,2540168.9284492433,5462871.090919206,3174302.2227705587,2136164.0508724744,14636880.181633241,6726417.1982702855,3544724.22280415,3581875.2339703855,2226722.4821946067,10514160.53513347,11496526.116849512,2033510.426408951,1670209.8010851347,6454350.727106833,1102149.048457035,2463320.312499797,1423005.4566978593,2337341.606137811,1696362.440780003,9776100.88156791,2574665.4591215784,1541498.2283001211,3851597.78475651,3464189.9735229965,9392912.618581317,6975488.955937044,2023406.4868989747,1551594.091319488,6427323.730273447,3715341.200680417,3070524.1726546893,4862263.879228571,15991687.9729792,4690310.478141464,1117112.1834053968,1258983.5358793368,2600183.7401938597,3647168.841689568,1291223.7576001936,12854064.069032796,1782196.6503883046,9788081.595760558,4992204.113641633,1250696.77606597,6231577.7391165905,4273862.947658524,1679136.3195458795,3197273.4571623392,17535426.496847395,1202272.5016715205,1087591.9777171446,17775608.146423105,3754819.8713389058,746718118.3316473,1263636.9201767007,1310990.850829044,946000.2488064538,77404023.59013712,1573786.9547886774,1420525.8996517423,4306809.6877661925,2996797.320904251,1248078.8384641805,1138402.6794199636,2794675.6204968663,2130005.3336442015,8724734.752684202,1559090.3633023198,764891.5319884517,2235419.055241154,4392703.909637584,2590227.073936495,9263747.823478676,6113729.590389091,14879141.278023105,1348681.362381653,1447333.233873812,5771587.019007818,1883730.5996937528,2975734.2467720537,1128212.6113665686,894276.7107151896,1904620.667021281,2606939.1163410954,1746338.1430132268,1251503.7921768932,1970823.6132246125,2110544.430415431,1362170.2358002411,1404329.0041811112,1852502.5716957056,1331936.1095429808,1460811.0445399184,1238913.0310993697,9207304.253077164,3519396.196904101,1017904.2757308632,14341926.604809202,4517643.553366105,2281037.9739701,1184234.4247002685,8906340.23120448,1196749.1431480423,1650771.739815054,10122618.809628788,1216889.4340208126,4055139.849455687,965027.8596483287,2702203.6224312037,2123417.8051772523,5210338.382248937,2041698.679002482,3539972.3339957553,7974254.632696688,8751611.873522507,1796895.84770846,1956393.514832133,2369958.7389841215,8069230.900000052,3308884.836784319,40381988.26302607,1194703.1473939396,1232205.4911191375,1339884.7202531048,3252356.1510297367,3384611.4842955605,874670.8605678305,3971375.219478399,3594130.2079360583,1546112.7037598754,7346358.325219964,2191708.2654490387,1156879.2791875687,4241917.041204853,5488393.985157839,916781.3520073129,1155968.0917561136,15518591.446684826,2808796.276764771,14255299.313424148,1271384.5021832138,4564115.211006509,3762870.5846678233,4194392.99380508,1376321.4421701606,1459075.904620987,1308636.631220121,1421896.059150651,1111358.3607975147,2637223.7984224665,7695225.535504184,4073606.9226729306,948089.0617893606,2626160.2572363936,4885418.755391698,8067924.157164634,4620691.4376310315,3705358.2446508626,1284271.1031838707,1503993.1344543677,1310396.5956831435,12461476.044995379,705703.6370292671,2726711.6512300456,4098434.0210554944,1615545.9026431064,1874891.467842291,3231899.9814055273,2787476.9674319196,8834041.329262111,995199.9485249631,13226281.113529127,3369956.0161798988,1886326.6185956087,5808926.326941809,5821628.864946373,2937661.154803331,1673021.5965925907,12393748.323972005,2622840.014245929,2113383.941055686,1883591.8252160857,1475736.7985481874,7265259.460733174,4530498.441510118,8959175.32732749,2358079.1655420293,9494028.36360963,19035150.704086997,773127.4928831384,1155554.4318258255,938077.9721734049,1553699.7448835433,19636598.162980963,2162489.8358452604,8058331.16468434,1583601.6517702248,1558030.219320252,1442883.0803046096,1172370.9646073799,3131243.846487288,3516929.090645512,1026356.8171735893,1096312.051704497,1362758.8320410845,3502886.841241315,2057350.0774669084,6527361.787517624,1389364.8018059877,704076.8265952861,5982872.245510997,1763439.4796708522,1321077.7095772063],"xaxis":"x","y":[4395510,1499128,805751,58761304,3639670.25,1327591,476069,17714332,555153.125,143423024,415453,1160989,1696978.75,1334876,4585966,11962615,13730204,2356376.75,18470340,1396691.875,2056762,659176.1875,619409,791035.5,9683979,1241355.375,29540052,1237655.25,4018570,6118300.5,1568550.75,526752.8125,7575080,2058584,2711035,965786,5135040,669217.6875,1692323,803938,3497215.5,63668488,809291.8125,21069722,291614432,10105379,32141070,1523508,94560088,3073937.5,1024866,1516831.75,553230,1394408.25,1138386,3546459,4107827,2014500.375,20675922,1115107,1396280.25,151754.2969,1127895,2615247.75,2735128,489557.0938,2311532,5227992,848497,22965152,666967,673030,3242458,433443,614044.6875,4216639.5,12549118,733249,770736.125,1848074,1051523.25,853314.5,1017787.125,501194,2298815.75,2255026,2202407,1045337,1436109,1362523,19526080,13146645,635516.375,1532390.25,6227419,274682208,1751483.875,30135280,2265487,3218824,1725558,816300.5,666392,2292169,2663520,931148.625,7520554,7613116,1232712,4970047,942962,1166069.875,9387211,5826745,1423463,2660396,4103027.5,784584,8233527.5,4034028,31803568,345399,2360656,711118.375,549438,901679,1939297,1094343,2455923.5,1992158.25,817695.6875,2986728,1008536.813,1078936,654446,2591768,555709,5401277.5,2099386.75,1983256,862945.125,1417728.25,14172606,643740,163945984,1051944,1569569,8747819,534703.6875,5032796,518107,556700.6875,1261963,114648520,9313513,7251589,2612378,2923484,754241.3125,162960,836437.6875,2547848,1563070,677476.6875,7860942,10040121,2148565,730672,3009967,9986032,505601,308400.4063,14064603,13631685,4748137,10434387,604183.1875,2139713,920068.875,1128060.75,11624548,596242.1875,3814590,2844685,1278437,1119679,21346100,2136086,4991893.5,590629,659368.625,1437569,450518,1047461,858701,9788717,1769137,2870914.75,1025659,10444297,4916451.5,641713,867184.375,688641.8125,746858.1875,15324777,1829981.375,743748.5,1260902,1307883,966812.125,3540225.75,1063960.125,922153.6875,794715.6875,1200811,4581493,6426022,4178233.5,8437401,1063480.5,739869.6875,824320.1875,8218155.5,913117.6875,764832.6875,2402535,6843855,37022180,706505.6875,1858224.625,2583451,2004777,1237252.75,10775646,16467486,2001643.75,576052,701422,6797528,270838.5,3265995,31371618,727374.6875,811900,784584,0,2545405,1631031,1536998.25,4998015,132105,1524499,1279570,6268627,1060818.25,991918,3158151,1441445.625,648022,1702639,3686962.25,2960159.75,255971.2031,488181,1219744.75,3321035.75,3043767,965651.125,282898,1645748.25,768479.625,508051,331059.5,633408,2454861,10834087,3752132.25,1108604,907617,7456330,1076693,966764,576644.125,975721,1987934.625,6962073,455798.9063,919180,3242066,6657348.5,1128179.125,2625396,10564729,1832031.5,452614,1365778,11508035,1587454.75,2009703.25,6860521,1544146.625,887022.875,3220635.25,7771933.5,2584082,536642.875,14619984,30038490,848734,2018011.875,1612167.25,3468606,1287422,785901,1287040.625,2225512,722062.875,645028,5975230,1878442.25,3655453,2322314,3073778,1425226,25631512,2946756.25,2483074.25,5662234,4934334,1123416,1826203,1187499.75,3518459.5,5917000,904059.875,4917420,601590,2204020,2559403,1140137,13609040,19991412,552015.625,1544117,2839882.25,1339220.625,2466100,4636396.5,1730431,1700057.375,1072487.875,1090545,6858533.5,2306723.5,918514,1157976.5,2156407,858932,668915.8125,1904641.75,1769998,0,530700.125,2804534,1678495.25,1003551,5572010,634561,13901935,1360847,650313,2195160,1335007.875,1107290.625,1031667.5,1234149,2114860.25,2805826,591845.3125,2251442,1898509.75,6299357,1076389,3091387.5,9230223,1544388.5,841841,1381590.125,662424,5053635,2401148,6182350.5,22078064,790671,4608963,662685.3125,1822830,3106930,1733511,362823,830286.5,495481,601818.625,21027396,952879,2337651.25,8037287,1809586.75,1657201,1167813,720170.6875,55030192,1862176.875,1509292,2323265,1072855,25970248,9128267,9029489,5976246,1934971,3316562,948676.5,755269,598857.8125,8190809,1026921,885958.125,1878742.875,3212269,811907.625,564754.1875,336461,1186835.125,6411570.5,1960538,847298.8125,2456132,2828950.5,6298131.5,12448381,4677797,753721.125,13458948,5722325.5,12931002,1482752,645055.5,2903161,6722886,2373568.25,3375211,2822377.25,7768057.5,1831722.875,640374,1485945.875,1224045.25,5216213,730292,455153,37797084,9375125,624180,1860084,1015060.188,16016644,670779,1427294.375,776248.6875,39403320,2664566.5,7182436,41399504,786253.6875,1208168.75,974179,1325597.625,26941110,463371,938833.625,5155090.5,1811506.375,876339,1083467.125,5804784.5,1044642,3652708.75,588856.125,15088676,552822.125,648879.3125,967432.8125,892227,507451,405050.4063,1750419,405062,593042,1116040,1827424.125,222600.0938,568033.375,622051,2003447.625,2654755,2233268.5,878927.8125,560334.5,919746.3125,1247972,587903,1732954.25,880115.5,7592451,1527348,742565,4133811,2442220,131373880,1190733.5,2133797.75,1581036.75,1041652.813,1912690,2534671,2483320,2698077,1191926,1239315.375,6782435,403800,604591,3060149,1826107,1035554.688,4784731,6901554,1712796,608199.8125,5820965,550767,1332912.625,3245772,304248,1003176.875,6057734,726124.625,7802205.5,2108926,2827818,2014245,8354235,888561,527438,335477,1288669.5,1456808,3835202,1775433,2843033.5,4285049,8422861,1651120,8188381,5671256,1191803,531563,8688676,2964206,1580597,2133372,6917478.5,5447076.5,862826,3999505.25,3721129,593824,929688,13214416,602245,4865551.5,632157,14431245,4729846.5,11208498,1920352.125,3053785,2147062,3322250,1702992,2759674,791052.875,2683383,737570,695059,6471427,951675.5,16808.90039,4222682,1324534.625,1167887.75,1391714.25,12204922,6577130,751292,1761048,552391,3039626,669117,95544,4338616,1012559,9997511,57133.19922,4247457,26416770,842511,8819864,712149,1960558,3298570,1670929,4983934,4217623,2996502,493000,4801472,2235765.5,635698.6875,3467698,1310481,470666.6875,1213869.75,1093627,1372706,61762380,842923,1820292,961491.3125,1467029.75,3042875.5,718362,1453543,1222509,2594119,7791957,15571642,896172,2763597,4039667,4475652,776237.375,811021,6352846.5,6387710,12271749,783651.5,626612,1255834,2527198,3894142.5,1972156,1691963,699382,6083307,619870,586827.5,1452804,2058955.25,13962749,24126,10591549,3240830,2953338,1260416,704799,1617421,1224642.5,3822102.5,3863772,1162226.375,921786.375,2100766,2019254,5134446,1536287,782494.3125,713326.875,2892779,317794.4063,456397,3182301,919048.6875,1976663,3065502,593953.375,767297.8125,764906,642825,3397709,1084553.75,736838,1403820,1084048,6983736,1171380,1985171,851439,1677877,1107870,990361.5,847514.5,80469216,1321065,2411402.25,17750994,5436057,4757299.5,509171.4063,3258819.5,7015692,1271021,3435687,1987218.75,1034941.313,340270,559234.6875,2158629,966677.125,3637973,689373.8125,858780.625,1705515.25,512166,97690.39844,675772.875,815800,2184336.75,2751694,1987073,1169948.875,938908,1280245,1375978.875,2845946,1194835.5,606111,753784,892634,3113412,1762174,2921368,18868768,5348309,6376820,2058532.875,7782092,1212551,1224276,1958352,544172.375,8393893,767361,6522551.5,10939331,39523852,4519115,10020111,2601945,620104,8800292,968698.3125,1756668,2065156,2773779,9343428,219483,3303553,6917280,1091162.75,1306087.75,2849048,1429855.75,410433.0938,8873485,9049598,696522,3046543,1433504.375,1503823,551219,3789270,958725,960987.8125,1438479,1988784,43943,738983,1516377,1181963,806398.1875,2326565,49102164,1983509,9273278,6931901,1784796.5,1963054,2125323.5,3833052,4103800.75,318962.8125,7286313.5,25747022,1324965,5086863,1422406.25,1758581,1486177,1162493.625,1192100,970118,2646523.5,12263687,4767227,571644,559805,12250422,13410041,2726369,447947,1988760.875,13300859,1348497.125,661380,1625308.625,4236396.5,1906597,5450441.5,106918,2270531,41680640,771512.6875,35685224,0,1852926.625,851053.8125,1739286.75,1748629.875,863703,882896.5,2837589,571309,2237482,4765753.5,10513817,1234095.5,1372475,552729,3886929,809257,728167,6583940,2203758.5,1165598.875,59124384,1727783,1048813.75,838571,948157,1966424,3786217,5328224,3869754,1637199,1054581.375,1108310.875,2063212,1608224,1907697,1126157.25,502667.6875,662280,2439061,1019487.188,1558348,1162120.375,5762597,611997,4467292,1530744,2082754.75,16020642,514471,3751417,207572,2245568.25,4520838,855665.125,1688612.5,2473302,6136327.5,1423438.5,288191,2585187,2145603.5,10403123,2053611,1337968,839115,4369513.5,4498367,2291581,22530168,694479,508014.5938,1620721,607373,520011.8125,3234787.75,452744,930611.875,723424,85357952,2733003.75,3625434,2228675.5,524053,717056.375,10249433,18829136,3040924.25,3030041,2057457,1115504.375,1419453,602618,1539478.5,635697.3125,323739.4063,569115,1257144.75,630040,8767617,544043,619545.125,917783.5,755856.875,677536.3125,385719,558893.375,11521702,1382309,1823782,1602030,2147013,32579658,574650.375,6668568.5,1198291,3986851,2470566,4207110.5,6576563,962577,4734631,1049748.625,1623657,824516,647792.875,1225404,6481807.5,3064039.5,510517,1698074.625,1135172.25,1813718,74130576,2330029,6366084.5,342726.0938,2938366,1045413,6832991,13133225,1183042.125,1286882,4025787,557193,1475475.5,3147042,956958.875,916864,12720069,942712.5,2246747.5,2150120,14084606,2214469.5,1913558.375,1103762,684941.375,1216543,3542708.5,780410,4742869,1117658.75,806615,1149140.25,7539963.5,3202213,1366000.125,6807227,19827916,13253979,5149928.5,1706301,7811542.5,945245.625,209698.7031,1926134.875,17320768,1182575,10557910,10343027,157606480,1646088.5,116486.6016,2261586,13969552,1652833.25,1121259,7492939,928584.875,1856230.625,10018321,13010578,540465,13239050,11924933,3494929.5,2014475.875,19036416,1161814.625,5326222,1430356.75,607579.6875,5968153.5,3528086.75,1093757.75,8631444,770082.125,2520919,729096.875,1294615.25,4107443,485971,1210798.375,1858029,786979,955223,1118490,747326,1943899.25,867456.875,628609,2447861,1101536,12719042,3366528,4545148,9941308,998633.5,1210229,1833381,2057836,1534068.25,1813285.25,3303989,1380283,209644.9063,1210262,954352.6875,1594733.625,851585.1875,2266570,3591653,696709,1111858,48729588,969682.8125,4270584,1094940.75,984178,533742.8125,6014078,2331952.75,44293.5,11195917,2910252,29195468,3104171.5,2485521,14465798,3590731,1921890,1509775,4785692,188745.7031,7240274,1014405.375,3095882,1293390,5499097,226375,974940.375,1889880,4224872.5,4028943,3574542.25,4602793.5,8518313,3312125,5948007,3377209.75,2581741.75,22325364,3241917,1658751,577033.875,1649327.625,1588933,729397.1875,6505995,526059,1738376,2434208,1545485,16402563,2023258,537141,2503690,13567774,481206,1139980.625,8320719,1777005,873923712,666930,541950.875,863728.8125,61576184,5462958,2874181,3897738,2093011,666687.5,845961.375,3548858.25,3395651,20725996,1022114.5,2217620,360045,2184156.5,1355558,5525072.5,4089677.5,20560622,758875,621751,2188452,824483,1042652.625,698326.625,533226.8125,1045640,2217046,7801590,525668,789630.125,841821,866343,1481017,2798967,1359579,850615.8125,1860901,3091348.25,1207092.75,2094410,6125692,4039168,1130960,803533.3125,5121586,1049088,735178,8168547,524144.6875,2001936.5,383524,2772124.75,10642754,2759071.5,1353079.5,3245111.5,7380088,6836900,1807738.25,1296959,1604129.75,3605749.75,1551294,25210246,2159932,862744,1862752,3888945,1443367.75,1676426.5,5444253.5,4099807,2233262,1358022,760826,678663.3125,0,6609358,1028277.313,2307938,61827040,0,4321141.5,572139,1997181.5,1594422.5,2755578,523133,650205.8125,519317,108200.3984,844076.8125,1350190,465533.3125,2189030,464155.1875,637246,2131817,11026945,5424718,2401890,501741,1008665,730765,7564643,1784385.25,1383950,227566.7031,702385,1641667,816846,448676,1083729.625,801645.875,13315803,11702959,609921.6875,3195230,2506927.5,2249592.5,2619088.5,3780078,1387773,1072500,1075019.375,1908612,2992693,5134607.5,11458215,1897180,6579348,15590743,1261634.5,1392015,541542.6875,1962435,20781390,1795462.75,0,819378,400354.3125,5047936,1961581.5,2422767,1913034.125,622572,561368,1787119,3494383,1244824,2549649.75,866096.6875,1539176.625,2267094,1444429,1666221],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","SiteEnergyUse_predRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","print(ScoresRidge)\n","figRidge.show()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[10859984.91928906,10859984.88711631,10859984.854344832,10859984.820963481,10859984.786960911,10859984.752325559,10859984.71704565,10859984.68110919,10859984.64450396,10859984.607217515,10859984.569237178,10859984.530550033,10859984.491142934,10859984.451002479,10859984.41011502,10859984.368466657,10859984.32604323,10859984.282830313,10859984.238813218,10859984.193976974,10859984.148306344,10859984.1017858,10859984.054399524,10859984.006131407,10859983.956965033,10859983.906883696,10859983.855870362,10859983.803907687,10859983.75097801,10859983.69706333,10859983.642145323,10859983.586205313,10859983.529224282,10859983.471182864,10859983.412061319,10859983.35183955,10859983.290497083,10859983.228013065,10859983.164366247,10859983.099534998,10859983.033497272,10859982.96623062,10859982.897712175,10859982.827918638,10859982.756826287,10859982.684410946,10859982.610648004,10859982.535512378,10859982.458978528,10859982.381020432,10859982.301611586,10859982.220725,10859982.138333168,10859982.054408086,10859981.968921218,10859981.881843507,10859981.793145347,10859981.702796582,10859981.610766498,10859981.517023813,10859981.421536654,10859981.324272562,10859981.22519847,10859981.1242807,10859981.021484941,10859980.916776253,10859980.810119037,10859980.701477032,10859980.59081331,10859980.478090249,10859980.363269532,10859980.246312123,10859980.127178263,10859980.00582746,10859979.882218452,10859979.756309228,10859979.628056983,10859979.497418117,10859979.364348223,10859979.228802072,10859979.090733582,10859978.95009582,10859978.806840975,10859978.66092036,10859978.512284363,10859978.360882465,10859978.206663202,10859978.049574148,10859977.889561906,10859977.72657209,10859977.560549287,10859977.391437074,10859977.219177963,10859977.043713402,10859976.864983747,10859976.682928247,10859976.497485023,10859976.30859104,10859976.116182094,10859975.920192787,10859975.720556501,10859975.517205376,10859975.310070302,10859975.099080876,10859974.884165378,10859974.665250769,10859974.442262635,10859974.215125198,10859973.98376125,10859973.748092156,10859973.508037824,10859973.26351666,10859973.014445562,10859972.760739883,10859972.502313394,10859972.239078265,10859971.970945034,10859971.697822575,10859971.419618068,10859971.136236966,10859970.847582962,10859970.553557957,10859970.254062032,10859969.948993407,10859969.638248412,10859969.32172144,10859968.999304933,10859968.670889324,10859968.336363014,10859967.995612318,10859967.64852145,10859967.294972464,10859966.934845228,10859966.56801736,10859966.194364224,10859965.813758845,10859965.426071899,10859965.031171663,10859964.628923953,10859964.219192091,10859963.801836869,10859963.376716478,10859962.94368648,10859962.502599746,10859962.053306416,10859961.595653843,10859961.129486548,10859960.654646147,10859960.170971323,10859959.678297762,10859959.176458087,10859958.66528181,10859958.144595284,10859957.614221627,10859957.073980667,10859956.523688886,10859955.963159358,10859955.392201677,10859954.810621906,10859954.21822249,10859953.61480222,10859953.000156129,10859952.374075461,10859951.73634756,10859951.086755836,10859950.425079664,10859949.751094319,10859949.064570908,10859948.365276277,10859947.652972942,10859946.927419009,10859946.188368086,10859945.435569208,10859944.668766748,10859943.887700321,10859943.092104714,10859942.281709785,10859941.456240367,10859940.6154162,10859939.758951794,10859938.88655638,10859937.997933768,10859937.092782283,10859936.170794634,10859935.231657833,10859934.275053078,10859933.300655643,10859932.308134776,10859931.297153585,10859930.267368918,10859929.218431255,10859928.149984587,10859927.061666291,10859925.953107024,10859924.82393057,10859923.673753746,10859922.502186248,10859921.308830524,10859920.093281657,10859918.8551272,10859917.593947068,10859916.30931336,10859915.000790263,10859913.667933846,10859912.310291965,10859910.92740408,10859909.518801104,10859908.08400526,10859906.62252989,10859905.133879326,10859903.617548699,10859902.073023776,10859900.499780793,10859898.897286272,10859897.264996842,10859895.602359053,10859893.908809204,10859892.183773141,10859890.42666606,10859888.63689233,10859886.813845266,10859884.956906946,10859883.065448005,10859881.138827406,10859879.176392239,10859877.177477492,10859875.141405841,10859873.067487415,10859870.95501956,10859868.803286616,10859866.611559663,10859864.379096285,10859862.105140321,10859859.788921615,10859857.42965575,10859855.02654379,10859852.578772012,10859850.085511632,10859847.545918543,10859844.959133003,10859842.324279372,10859839.640465815,10859836.906784002,10859834.122308798,10859831.286097968,10859828.397191849,10859825.454613049,10859822.457366088,10859819.404437112,10859816.294793507,10859813.1273836,10859809.901136275,10859806.614960631,10859803.267745625,10859799.858359685,10859796.385650355,10859792.848443897,10859789.245544907,10859785.575735917,10859781.837776998,10859778.030405335,10859774.15233482,10859770.20225563,10859766.178833779,10859762.080710677,10859757.90650271,10859753.654800743,10859749.324169679,10859744.913147973,10859740.420247164,10859735.843951369,10859731.182716798,10859726.434971232,10859721.599113513,10859716.673513012,10859711.656509103,10859706.546410613,10859701.341495255,10859696.040009083,10859690.640165903,10859685.14014669,10859679.538098995,10859673.832136342,10859668.020337608,10859662.100746384,10859656.071370363,10859649.93018066,10859643.675111176,10859637.304057907,10859630.814878272,10859624.205390405,10859617.473372461,10859610.616561878,10859603.632654656,10859596.519304607,10859589.274122585,10859581.894675737,10859574.378486695,10859566.723032787,10859558.925745219,10859550.984008249,10859542.895158352,10859534.656483356,10859526.26522156,10859517.718560884,10859509.013637928,10859500.147537079,10859491.117289575,10859481.91987255,10859472.552208085,10859463.011162203,10859453.293543905,10859443.396104123,10859433.315534718,10859423.048467409,10859412.591472724,10859401.941058904,10859391.093670812,10859380.045688802,10859368.793427596,10859357.333135104,10859345.66099127,10859333.773106864,10859321.665522259,10859309.33420622,10859296.775054615,10859283.983889163,10859270.956456132,10859257.68842501,10859244.175387174,10859230.412854532,10859216.39625812,10859202.120946724,10859187.582185434,10859172.775154186,10859157.694946317,10859142.336567026,10859126.6949319,10859110.764865322,10859094.541098937,10859078.018270042,10859061.190919967,10859044.053492438,10859026.600331903,10859008.825681835,10858990.723683022,10858972.288371805,10858953.51367832,10858934.393424679,10858914.921323162,10858895.090974351,10858874.89586526,10858854.329367403,10858833.384734869,10858812.055102367,10858790.333483214,10858768.212767314,10858745.68571911,10858722.7449755,10858699.383043727,10858675.592299232,10858651.36498349,10858626.693201799,10858601.568921058,10858575.983967498,10858549.930024391,10858523.398629729,10858496.381173857,10858468.868897105,10858440.852887351,10858412.324077593,10858383.273243442,10858353.691000639,10858323.567802485,10858292.893937286,10858261.659525726,10858229.854518246,10858197.46869236,10858164.491649963,10858130.912814585,10858096.721428627,10858061.906550571,10858026.457052145,10857990.361615453,10857953.608730113,10857916.186690286,10857878.083591765,10857839.287328973,10857799.785591941,10857759.565863285,10857718.615415107,10857676.921305899,10857634.470377427,10857591.249251544,10857547.244327018,10857502.441776313,10857456.827542342,10857410.387335202,10857363.106628876,10857314.970657919,10857265.964414105,10857216.072643075,10857165.279840935,10857113.570250856,10857060.927859638,10857007.33639427,10856952.77931845,10856897.239829112,10856840.700852912,10856783.145042747,10856724.554774186,10856664.91214197,10856604.198956436,10856542.396739991,10856479.486723516,10856415.449842837,10856350.266735133,10856283.917735374,10856216.382872758,10856147.641867148,10856077.674125504,10856006.458738366,10855933.974476278,10855860.199786296,10855785.112788478,10855708.691272382,10855630.91269363,10855551.754170451,10855471.192480275,10855389.204056386,10855305.764984533,10855220.850999692,10855134.437482763,10855046.499457402,10854957.011586836,10854865.948170787,10854773.283142434,10854678.990065437,10854583.042131046,10854485.41215527,10854386.07257615,10854284.99545108,10854182.152454276,10854077.514874283,10853971.053611625,10853862.739176568,10853752.541686963,10853640.430866253,10853526.376041587,10853410.346142082,10853292.309697216,10853172.234835401,10853050.089282671,10852925.840361603,10852799.454990337,10852670.899681848,10852540.140543375,10852407.143276062,10852271.873174816,10852134.29512838,10851994.373619638,10851852.072726164,10851707.356121019,10851560.187073827,10851410.52845209,10851258.342722826,10851103.591954479,10850946.237819131,10850786.241595067,10850623.564169627,10850458.166042443,10850290.007329006,10850119.047764597,10849945.246708648,10849768.563149426,10849588.955709208,10849406.382649798,10849220.801878562,10849032.170954851,10848840.44709693,10848645.587189373,10848447.547790963,10848246.285143096,10848041.755178725,10847833.91353184,10847622.715547496,10847408.116292445,10847190.07056634,10846968.53291354,10846743.457635554,10846514.79880412,10846282.51027493,10846046.545702064,10845806.858553043,10845563.402124655,10845316.12955948,10845064.99386313,10844809.947922267,10844550.944523389,10844287.93637239,10844020.876114918,10843749.716357563,10843474.409689847,10843194.908707097,10842911.166034136,10842623.13434987,10842330.766412746,10842034.015087102,10841732.833370458,10841427.17442167,10841116.991590055,10840802.23844545,10840482.868809175,10840158.836786028,10839830.09679716,10839496.603613958,10839158.312392896,10838815.178711344,10838467.15860435,10838114.208602432,10837756.285770277,10837393.347746463,10837025.352784144,10836652.259792646,10836274.028380062,10835890.618896762,10835501.992479831,10835108.111098435,10834708.93760005,10834304.435757598,10833894.570317417,10833479.307048073,10833058.612789955,10832632.45550567,10832200.804331144,10831763.629627466,10831320.903033357,10830872.597518288,10830418.687436167,10829959.148579556,10829493.958234366,10829023.095234975,10828546.540019702,10828064.274686586,10827576.283049399,10827082.550693814,10826583.06503367,10826077.815367252,10825566.79293347,10825049.99096792,10824527.404758658,10823999.031701665,10823464.87135584,10822924.925497476,10822379.198174056,10821827.695757328,10821270.426995456,10820707.403064217,10820138.63761707,10819564.146833966,10818983.949468818,10818398.06689544,10817806.523151848,10817209.344982812,10816606.561880425,10815998.20612267,10815384.312809717,10814764.919897888,10814140.068231072,10813509.801569505,10812874.166615676,10812233.21303729,10811586.993487034,10810935.563619103,10810278.982102223,10809617.310629047,10808950.613921834,10808278.959734121,10807602.41884837,10806921.065069353,10806234.975213164,10805544.229091678,10804848.909492398,10804149.10215342,10803444.89573353,10802736.381777192,10802023.654674394,10801306.811615188,10800585.952538818,10799861.180077406,10799132.599494029,10798400.318615142,10797664.447757352,10796925.099648343,10796182.38934205,10795436.434127986,10794687.35343473,10793935.268727535,10793180.30340014,10792422.58266076,10791662.233412314,10790899.384126965,10790134.164715027,10789366.706388393,10788597.141518494,10787825.603489041,10787052.226543635,10786277.145628411,10785500.496229954,10784722.414208656,10783943.035627766,10783162.496578377,10782380.93300063,10781598.480501404,10780815.274168855,10780031.448384045,10779247.136630122,10778462.471299332,10777677.583498288,10776892.602851927,10776107.657306489,10775322.872932104,10774538.373725297,10773754.281411964,10772970.715251323,10772187.791841265,10771405.624925667,10770624.325204222,10769844.000145229,10769064.753801972,10768286.686633183,10767509.895328185,10766734.472637221,10765960.507207584,10765188.083426071,10764417.281268368,10763648.17615584,10762880.838820377,10762115.33517782,10761351.726210425,10760590.067859013,10759830.41092527,10759072.800984673,10758317.278310623,10757563.877810145,10756812.628971703,10756063.555825537,10755316.676916914,10754572.005292695,10753829.54850163,10753089.308608625,10752351.282223392,10751615.460543673,10750881.829413384,10750150.369395807,10749421.055862093,10748693.859095156,10747968.744409159,10747245.672284612,10746524.598519165,10745805.474394087,10745088.24685644,10744372.858716857,10743659.248862842,10742947.352487465,10742237.101333288,10741528.423951298,10740821.245974619,10740115.49040677,10739411.077924067,10738707.927191967,10738005.955194829,10737305.077578846,10736605.209007565,10735906.263529696,10735208.15495857,10734510.797262853,10733814.104967933,10733117.993567424,10732422.379944216,10731727.182800528,10731032.323096229,10730337.724494977,10729643.3138174,10728949.021500736,10728254.782064289,10727560.534579994,10726866.2231475,10726171.79737303,10725477.212851424,10724782.431650644,10724087.42279815,10723392.162768412,10722696.635971023,10722000.835238673,10721304.762314454,10720608.428337831,10719911.854328725,10719215.071669117,10718518.122581633,10717821.060604563,10717123.951062815,10716426.871534312,10715729.91231137,10715033.176856607,10714336.782252988,10713640.859647613,10712945.554688882,10712251.027956719,10711557.455385555,10710865.028679784,10710173.955721477,10709484.46097011,10708796.78585415,10708111.189154323,10707427.947378466,10706747.355127854,10706069.725454923,10705395.390212366,10704724.700393593,10704058.026464552,10703395.758686963,10702738.307433013,10702086.103491608,10701439.598366296,10700799.264564939,10700165.595881347,10699539.107668962,10698920.337106833,10698309.843457991,10697708.20832049,10697116.035871262,10696533.953103045,10695962.610054547,10695402.680034097,10694854.859836984,10694319.86995667,10693798.454790141,10693291.382837512,10692799.446896126,10692323.464249305,10691864.27684989,10691422.751498688,10690999.780018015,10690596.27942032,10690213.192071998,10689851.485852461,10689512.154308368,10689196.216803059,10688904.718661102,10688638.731307792,10688399.352403486,10688187.705972593,10688004.942526888,10687852.23918296,10687730.799773356,10687641.854951074,10687586.662286937,10687566.506359339,10687582.698835785,10687636.578545641,10687729.511543375,10687862.89116154,10688038.13805276,10688256.70021977,10688520.053032659,10688829.699232299,10689187.168918904,10689594.019524649,10690051.835769158,10690562.229596626,10691126.840093348,10691747.333384221,10692425.402506959,10693162.767262429,10693961.17403973,10694822.395614412,10695748.23091824,10696740.504778927,10697801.067628084,10698931.79517576,10700134.588049775,10701411.371398121,10702764.094452629,10704194.730052074,10705705.274122989,10707297.745116238,10708974.183397643,10710736.650590813,10712587.228870332,10714528.020203622,10716561.14553964,10718688.743942773,10720912.971670229,10723236.001191327,10725660.020147165,10728187.23024918,10730819.846115228,10733560.094041938,10736410.210712112,10739372.441836197,10742449.04072683,10745642.266805787,10748954.384042634,10752387.659324706,10755944.360758131,10759626.75589986,10763437.109920848,10767377.68370077,10771450.731854904,10775658.50069402,10780003.226118436,10784487.131447619,10789112.42518702,10793881.298734155,10798795.92402615,10803858.451131392,10809071.00578821,10814435.686893718,10819954.563946497,10825629.674446885,10831463.021259142,10837456.569939982,10843612.246038279,10849931.932371136,10856417.466281716,10863070.636884546,10869893.182304306,10876886.786914285,10884053.078581002,10891393.625921609,10898909.935580935,10906603.449535118,10914475.542428918,10922527.5189539,10930760.611274658,10939175.976510305,10947774.694278453,10956557.764308717,10965526.104132798,10974680.546857966,10984021.839030568,10993550.638596017,11003267.512961328,11013172.937166119,11023267.292167421,11033550.863243535,11044023.838521475,11054686.307632321,11065538.26049824,11076579.586254407,11087810.072308639,11099229.403541028,11110837.161645245,11122632.824612763,11134615.766360626,11146785.256503006,11159140.460266028,11171680.438545136,11184404.148103531,11197310.44190992,11210398.069613269,11223665.678151853,11237111.812493607,11250734.916504255,11264533.333939532,11278505.309557457,11292648.990346348,11306962.426864129,11321443.574684221,11336090.29594324,11350900.3609856,11365871.450100023,11381001.155342953,11396286.982443882,11411726.352787599,11427316.605468396,11443054.99941146,11458938.7155566,11474964.859099768,11491130.461787835,11507432.484262269,11523867.818447609,11540433.289980609,11557125.660676325,11573941.631027421,11590877.84273327,11607930.881255487,11625097.278396849,11642373.514900565,11659756.023067176,11677241.189386422,11694825.35718153,11712504.82926369,11730275.870594319,11748134.710953131,11766077.547609901,11784100.547998007,11802199.852387898,11820371.576558633,11838611.81446581,11856916.64090412,11875282.114162834,11893704.27867259,11912179.167641802,11930702.805681039,11949271.211413704,11967880.400071379,11986526.386072077,12005205.18557977,12023912.819043389,12042645.313713612,12061398.706135575,12080169.044615746,12098952.391661124,12117744.82638884,12136542.446904382,12155341.372646417,12174137.746696401,12192927.738050964,12211707.54385519,12230473.391594887,12249221.541245788,12267948.287377965,12286649.961213415,12305322.93263502,12323963.612145035,12342568.452771345,12361133.951919701,12379656.653170284,12398133.148016984,12416560.077547804,12434934.134064948,12453252.062643146,12471510.662624951,12489706.789051767,12507837.354029477,12525899.32802764,12543889.741111366,12561805.684105039,12579644.309687177,12597402.833415868,12615078.534684308,12632668.757606061,12650170.911829844,12667582.473283691,12684900.984848555,12702124.056961363,12719249.36814791,12736274.665485788,12753197.764997972,12770016.551977554,12786728.981244298,12803333.07733392,12819826.934620883,12836208.717375765,12852476.65975829,12868629.06574719,12884664.3090082,12900580.83270149,12916377.149230069,12932051.839930542,12947603.554707846,12963031.011615667,12978332.99638411,12993508.361896435,13008556.027616644,13023474.978969749,13038264.266676549,13052923.006044868,13067450.376219148,13081845.619390313,13096108.039967924,13110237.00371652,13124231.936858196,13138092.325143296,13151817.712891296,13165407.702003751,13178861.95095132,13192180.17373675,13205362.138835805,13218407.66811795,13231316.635748733,13244088.967075635,13256724.637499277,13269223.67133165,13281586.140643211,13293812.164100436,13305901.905795582,13317855.574070176,13329673.420333866,13341355.737880092,13352902.860700117,13364315.162296737,13375593.054499168,13386736.986280287,13397747.442577621,13408624.943119226,13419370.041255616,13429983.322798917]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[13325635.907302873,13325635.892380374,13325635.877180172,13325635.861697106,13325635.845925909,13325635.829861218,13325635.813497573,13325635.796829415,13325635.779851072,13325635.762556773,13325635.74494064,13325635.726996679,13325635.708718799,13325635.690100782,13325635.671136295,13325635.651818898,13325635.632142019,13325635.612098966,13325635.59168293,13325635.570886966,13325635.549704008,13325635.528126856,13325635.506148174,13325635.483760493,13325635.460956192,13325635.43772753,13325635.414066605,13325635.389965372,13325635.365415642,13325635.340409067,13325635.314937145,13325635.288991224,13325635.26256247,13325635.235641917,13325635.2082204,13325635.180288605,13325635.151837029,13325635.12285601,13325635.093335688,13325635.063266031,13325635.032636821,13325635.001437642,13325634.969657892,13325634.937286764,13325634.904313257,13325634.87072616,13325634.836514067,13325634.801665332,13325634.76616812,13325634.730010362,13325634.693179764,13325634.655663816,13325634.617449759,13325634.578524606,13325634.538875123,13325634.498487841,13325634.457349027,13325634.415444693,13325634.3727606,13325634.329282243,13325634.284994842,13325634.239883345,13325634.193932414,13325634.14712644,13325634.099449504,13325634.050885411,13325634.00141765,13325633.951029409,13325633.899703559,13325633.84742266,13325633.794168945,13325633.739924312,13325633.684670322,13325633.628388206,13325633.571058823,13325633.5126627,13325633.453179987,13325633.392590467,13325633.330873547,13325633.268008266,13325633.203973247,13325633.138746731,13325633.072306551,13325633.004630134,13325632.935694475,13325632.86547615,13325632.793951305,13325632.721095623,13325632.646884354,13325632.571292281,13325632.494293706,13325632.415862482,13325632.335971948,13325632.254594957,13325632.171703862,13325632.087270493,13325632.001266167,13325631.913661655,13325631.8244272,13325631.733532475,13325631.640946604,13325631.54663812,13325631.450574988,13325631.352724575,13325631.25305363,13325631.151528291,13325631.048114067,13325630.94277583,13325630.835477788,13325630.72618349,13325630.614855811,13325630.501456928,13325630.385948317,13325630.26829075,13325630.148444252,13325630.026368115,13325629.90202087,13325629.775360279,13325629.646343324,13325629.51492618,13325629.381064218,13325629.244711958,13325629.105823098,13325628.964350464,13325628.820246013,13325628.673460796,13325628.523944968,13325628.371647757,13325628.216517437,13325628.05850133,13325627.89754577,13325627.733596105,13325627.566596672,13325627.39649075,13325627.223220594,13325627.046727363,13325626.866951127,13325626.68383086,13325626.497304384,13325626.307308368,13325626.11377832,13325625.916648533,13325625.71585209,13325625.511320816,13325625.302985294,13325625.090774797,13325624.874617305,13325624.654439438,13325624.430166468,13325624.20172228,13325623.969029337,13325623.732008664,13325623.490579836,13325623.244660914,13325622.99416845,13325622.739017442,13325622.479121314,13325622.214391882,13325621.944739329,13325621.670072157,13325621.390297193,13325621.105319511,13325620.81504245,13325620.519367527,13325620.218194459,13325619.911421083,13325619.598943347,13325619.280655276,13325618.956448924,13325618.626214333,13325618.289839525,13325617.947210431,13325617.598210875,13325617.242722524,13325616.880624851,13325616.511795092,13325616.136108216,13325615.753436858,13325615.363651318,13325614.966619464,13325614.562206738,13325614.150276074,13325613.730687879,13325613.303299956,13325612.867967486,13325612.424542973,13325611.972876176,13325611.512814078,13325611.044200834,13325610.5668777,13325610.080682999,13325609.585452069,13325609.081017183,13325608.567207526,13325608.04384911,13325607.510764733,13325606.967773914,13325606.414692823,13325605.851334248,13325605.277507495,13325604.69301836,13325604.097669024,13325603.491258046,13325602.873580215,13325602.244426565,13325601.603584252,13325600.95083649,13325600.28596251,13325599.608737433,13325598.918932248,13325598.216313709,13325597.500644255,13325596.771681946,13325596.02918037,13325595.272888567,13325594.50255094,13325593.717907183,13325592.918692185,13325592.104635932,13325591.275463447,13325590.430894662,13325589.570644356,13325588.694422051,13325587.801931908,13325586.892872635,13325585.96693739,13325585.023813667,13325584.063183226,13325583.084721943,13325582.08809974,13325581.072980454,13325580.039021732,13325578.985874925,13325577.913184976,13325576.82059028,13325575.707722599,13325574.574206907,13325573.419661291,13325572.243696833,13325571.045917442,13325569.825919766,13325568.583293047,13325567.317618981,13325566.028471584,13325564.715417046,13325563.378013618,13325562.015811443,13325560.62835239,13325559.215169968,13325557.7757891,13325556.30972604,13325554.816488152,13325553.295573793,13325551.746472139,13325550.168663012,13325548.561616722,13325546.924793895,13325545.257645287,13325543.559611628,13325541.83012342,13325540.068600774,13325538.274453204,13325536.447079465,13325534.585867323,13325532.690193389,13325530.759422917,13325528.792909589,13325526.789995316,13325524.75001002,13325522.672271434,13325520.556084886,13325518.400743067,13325516.205525812,13325513.969699875,13325511.692518687,13325509.373222135,13325507.011036327,13325504.605173318,13325502.154830897,13325499.659192316,13325497.117426042,13325494.528685488,13325491.892108766,13325489.206818407,13325486.47192108,13325483.686507342,13325480.849651316,13325477.960410453,13325475.017825194,13325472.020918712,13325468.968696585,13325465.860146513,13325462.694237992,13325459.469922002,13325456.186130695,13325452.841777058,13325449.4357546,13325445.966936998,13325442.434177773,13325438.836309932,13325435.172145626,13325431.440475792,13325427.640069788,13325423.769675018,13325419.828016588,13325415.813796893,13325411.72569526,13325407.562367536,13325403.322445713,13325399.004537513,13325394.60722598,13325390.129069088,13325385.568599284,13325380.9243231,13325376.194720706,13325371.378245465,13325366.473323505,13325361.478353256,13325356.391705006,13325351.211720433,13325345.936712125,13325340.56496313,13325335.094726454,13325329.524224581,13325323.851648979,13325318.075159593,13325312.192884348,13325306.202918636,13325300.103324775,13325293.892131506,13325287.567333456,13325281.126890581,13325274.568727639,13325267.890733635,13325261.09076124,13325254.166626256,13325247.116107013,13325239.936943837,13325232.62683839,13325225.18345315,13325217.604410782,13325209.887293525,13325202.029642595,13325194.028957572,13325185.882695757,13325177.58827157,13325169.14305589,13325160.544375423,13325151.789512066,13325142.875702238,13325133.800136227,13325124.559957545,13325115.15226223,13325105.574098175,13325095.822464487,13325085.894310763,13325075.786536418,13325065.495989993,13325055.019468453,13325044.353716496,13325033.495425854,13325022.44123456,13325011.187726261,13324999.73142951,13324988.068817021,13324976.196304988,13324964.110252341,13324951.806960028,13324939.282670312,13324926.533566024,13324913.555769857,13324900.345343642,13324886.898287624,13324873.210539741,13324859.277974915,13324845.096404316,13324830.661574684,13324815.969167588,13324801.01479874,13324785.79401729,13324770.302305136,13324754.535076229,13324738.487675907,13324722.15538021,13324705.533395234,13324688.616856452,13324671.400828106,13324653.880302556,13324636.05019965,13324617.90536617,13324599.44057519,13324580.65052553,13324561.529841213,13324542.073070899,13324522.274687393,13324502.129087154,13324481.630589798,13324460.773437686,13324439.551795479,13324417.959749745,13324395.991308618,13324373.640401434,13324350.900878463,13324327.766510617,13324304.230989233,13324280.2879259,13324255.930852275,13324231.153220015,13324205.948400673,13324180.309685744,13324154.230286647,13324127.703334853,13324100.721882014,13324073.27890018,13324045.367282039,13324016.979841283,13323988.109312985,13323958.748354059,13323928.88954382,13323898.525384583,13323867.64830237,13323836.25064771,13323804.324696468,13323771.86265084,13323738.856640406,13323705.298723267,13323671.180887345,13323636.495051714,13323601.23306809,13323565.386722472,13323528.947736798,13323491.907770868,13323454.258424275,13323415.991238564,13323377.097699467,13323337.569239343,13323297.397239741,13323256.57303414,13323215.087910827,13323172.933116008,13323130.099857029,13323086.579305826,13323042.36260259,13322997.440859556,13322951.805165075,13322905.446587889,13322858.356181594,13322810.524989355,13322761.944048889,13322712.604397645,13322662.49707828,13322611.613144387,13322559.943666471,13322507.479738291,13322454.21248339,13322400.13306198,13322345.23267817,13322289.502587447,13322232.934104552,13322175.518611666,13322117.247566946,13322058.112513453,13321998.105088433,13321937.217033008,13321875.440202234,13321812.7665756,13321749.188267939,13321684.69754076,13321619.286814068,13321552.948678588,13321485.67590852,13321417.461474728,13321348.298558464,13321278.180565596,13321207.101141337,13321135.054185571,13321062.033868646,13320988.034647834,13320913.051284276,13320837.078860598,13320760.112799082,13320682.1488805,13320603.18326354,13320523.212504946,13320442.233580258,13320360.243905252,13320277.24135812,13320193.22430229,13320108.191610001,13320022.14268661,13319935.077495653,13319846.996584648,13319757.901111716,13319667.792872928,13319576.674330503,13319484.548641821,13319391.419689221,13319297.292110668,13319202.171331266,13319106.063595634,13319008.97600112,13318910.916531935,13318811.894094154,13318711.91855161,13318611.0007627,13318509.152618095,13318406.387079382,13318302.718218576,13318198.16125865,13318092.732614875,13317986.449937185,13317879.332153434,13317771.399513539,13317662.673634652,13317553.17754715,13317442.935741583,13317331.974216564,13317220.320527494,13317108.003836224,13316995.054961607,13316881.506430864,13316767.392531838,13316652.749366092,13316537.614902765,13316422.029033272,13316306.033626728,13316189.672586145,13316072.99190531,13315956.03972633,13315838.86639786,13315721.524533883,13315604.069073094,13315486.55733876,13315369.049099103,13315251.606628044,13315134.29476639,13315017.180983268,13314900.335437845,13314783.831041249,13314667.743518595,13314552.151471078,13314437.136438074,13314322.7829591,13314209.178635653,13314096.414192775,13313984.583540263,13313873.78383349,13313764.115533672,13313655.682467517,13313548.591886172,13313442.954523318,13313338.88465238,13313236.500142645,13313135.92251427,13313037.276991975,13312940.692557413,13312846.30199996,13312754.241965905,13312664.65300589,13312577.67962043,13312493.470303439,13312412.17758358,13312333.958063329,13312258.972455649,13312187.38561804,13312119.366583977,13312055.088591417,13311994.72910842,13311938.469855595,13311886.496825356,13311839.000297766,13311796.174852867,13311758.219379365,13311725.337079577,13311697.73547043,13311675.626380473,13311659.225942738,13311648.754583338,13311644.437005732,13311646.502170514,13311655.183270631,13311670.71770194,13311693.347029123,13311723.316946654,13311760.877235029,13311806.281711975,13311859.78817876,13311921.65836144,13311992.157847099,13312071.556015048,13312160.12596296,13312258.144427963,13312365.891702792,13312483.651546858,13312611.711092496,13312750.36074631,13312899.894085787,13313060.607751196,13313232.801332995,13313416.777254762,13313612.840651885,13313821.299246132,13314042.463216282,13314276.645065062,13314524.159482513,13314785.323206129,13315060.454877941,13315349.874898838,13315653.905280411,13315972.869494619,13316307.092321604,13316656.899695955,13317022.61855181,13317404.576667134,13317803.10250758,13318218.525070254,13318651.173727883,13319101.378073744,13319569.467767783,13320055.772384383,13320560.621262182,13321084.343356496,13321627.267094683,13322189.720234968,13322772.029729256,13323374.521590304,13323997.52076377,13324641.351005666,13325306.334765589,13325992.793076258,13326701.045449818,13327431.409781372,13328184.20226013,13328959.737288702,13329758.327410894,13330580.283248479,13331425.913447246,13332295.524632817,13333189.421376556,13334107.90617186,13335051.279421212,13336019.83943432,13337013.882437509,13338033.702594738,13339079.592040382,13340151.840923974,13341250.737467129,13342376.568032712,13343529.617206376,13344710.16789056,13345918.501410937,13347154.897635337,13348419.635105077,13349712.991178695,13351035.242187884,13352386.663605552,13353767.530225787,13355178.116355537,13356618.696017714,13358089.543165486,13359590.93190736,13361123.136742739,13362686.43280758,13364281.096129632,13365907.403892878,13367565.634710673,13369256.068906978,13370978.98880522,13372734.679024147,13374523.426780058,13376345.522194805,13378201.25860884,13380090.932898724,13382014.845798207,13383973.302222412,13385966.611594107,13387995.088171482,13390059.051376613,13392158.826123772,13394294.743146852,13396467.139325086,13398676.358006194,13400922.749326225,13403206.670525238,13405528.486257974,13407888.568898764,13410287.298839796,13412725.064782022,13415202.264017787,13417719.302704524,13420276.596128624,13422874.5689588,13425513.655488133,13428194.299864123,13430916.956305992,13433682.089308552,13436490.173831988,13439341.695476865,13442237.150643758,13445177.04667693,13448161.901991382,13451192.246182865,13454268.620120209,13457391.576019544,13460561.67749995,13463779.499620035,13467045.628895164,13470360.663294809,13473725.212219834,13477139.896459302,13480605.348126562,13484122.210574396,13487691.138288984,13491312.796762511,13494987.862344284,13498717.022070244,13502500.973470783,13506340.42435681,13510236.092584059,13514188.705795638,13518199.00114288,13522267.724984542,13526395.632564442,13530583.487667724,13534832.062255831,13539142.13608039,13543514.496276263,13547949.936933927,13552449.258651493,13557013.268066619,13561642.777368618,13566338.603791136,13571101.569085646,13575932.498976264,13580832.22259614,13585801.571905935,13590841.381094728,13595952.485963877,13601135.723294217,13606391.930197123,13611721.943449916,13617126.598816127,13622606.730351092,13628163.169693533,13633796.745343555,13639508.281927664,13645298.599451484,13651168.512540601,13657118.829670291,13663150.352384714,13669263.874506174,13675460.181335183,13681740.048841959,13688104.242850048,13694553.518212842,13701088.61798363,13707710.27258005,13714419.19894361,13721216.099695163,13728101.662287097,13735076.558153134,13742141.441856578,13749296.950237924,13756543.70156279,13763882.294671094,13771313.308128487,13778837.29938112,13786454.803914713,13794166.334419148,13801972.37995965,13809873.405155763,13817869.849369392,13825962.125903059,13834150.621209886,13842435.694116458,13850817.6750601,13859296.865341967,13867873.536397472,13876547.929085564,13885320.25299845,13894190.685793426,13903159.372548427,13912226.425143078,13921391.921666967,13930655.905856902,13940018.386565086,13949479.337259913,13959038.695561392,13968696.362813067,13978452.203692291,13988306.045860907,13998257.67965813,14008306.857837677,14018453.295350982,14028696.669178432,14039036.618210487,14049472.743180504,14060004.60665102,14070631.733055277,14081353.60879555,14092169.68239989,14103079.364738697,14114082.02930253,14125177.012542281,14136363.614272881,14147641.098141396,14159008.692160293,14170465.589306455,14182010.948186252,14193643.893766915,14205363.518174043,14217168.881554982,14229059.013007483,14241032.911572773,14253089.547291972,14265227.862324353,14277446.772125782,14289745.166685348,14302121.911817688,14314575.85050854,14327105.804310359,14339710.5747848,14352388.944988385,14365139.680997372,14377961.533467626,14390853.239224857,14403813.522880396,14416841.09846738,14429934.671091905,14443092.938593568,14456314.593209498,14469598.323235875,14482942.814680712,14496346.752901591,14509808.824221902,14523327.717519136,14536902.12577869,14550530.747606743,14564212.288695704,14577945.46323597,14591728.995267762,14605561.619967038,14619442.084859729,14633369.150958728,14647341.593818527,14661358.204502491,14675417.790458484,14689519.17629862,14703661.20447963,14717842.735880725,14732062.650276273,14746319.846701248,14760613.24370789,14774941.779512618,14789304.412032714,14803700.118812982,14818127.89684314,14832586.762267154,14847075.749986507,14861593.913159685,14876140.322600953,14890714.066081818,14905314.247539114,14919939.98619424,14934590.415588249,14949264.682538161,14963961.946020063,14978681.375984924,14993422.152113376,15008183.462515937,15022964.502385315,15037764.472607695,15052582.578339946,15067418.027559744,15082270.0295958,15097137.793645209,15112020.527284974,15126917.434984745,15141827.716627542,15156750.566045288,15171685.169575647,15186630.704646517,15201586.33839436,15216551.226322155,15231524.511002619,15246505.320831988,15261492.76883938,15276485.951556303,15291483.947950765,15306485.818429854,15321490.603914462,15336497.324989442,15351504.981131988,15366512.550020915,15381518.98692885,15396523.224199262,15411524.170809675,15426520.712022211,15441511.709122155,15456495.999244941,15471472.395291641,15486439.685932668,15501396.635699108,15516341.985160828,15531274.451190168,15546192.727309812,15561095.484123046,15575981.369824579,15590849.010789603,15605697.012238776,15620523.958976472,15635328.416199524,15650108.930373438,15664864.030172996,15679592.227483947,15694292.018462319,15708961.884647988,15723600.29412868,15738205.702750888,15752776.555373805,15767311.28716251,15781808.324916603,15796266.088430189,15810682.99187959,15825057.4452347,15839387.855690084,15853672.629112119,15867910.171498178,15882098.890444187,15896237.196616784,15910323.505226484,15924356.23749823,15938333.822135892,15952254.696777275,15966117.309436427,15979920.119930033,15993661.601284843,16007340.241123222,16020954.543024017,16034503.02785611,16047984.235082038,16061396.724029422,16074739.075127874,16088009.891109312,16101207.798169758,16114331.44709079,16127379.514319066,16140350.703002319,16153243.743980618,16166057.396731548,16178790.45026839,16191441.723990355,16204010.068484044,16216494.366275664,16228893.532533372,16241206.51571948,16253432.298192337,16265569.896757673,16277618.363169633,16289576.784581427,16301444.283946108,16313220.020367643,16324903.189402832,16336493.023314768,16347988.791278295,16359389.799538378,16370695.391522188,16381904.947905803,16393017.886636525,16404033.66291187,16414951.769116359,16425771.734717239,16436493.126120426,16447115.54648786,16457638.635517662,16468062.069188315,16478385.559468394,16488608.8539931,16498731.735709094,16508754.022489019,16518675.56671723,16528496.25484804,16538216.006938105,16547834.77615425,16557352.548258316,16566769.341070348,16576085.203911668,16585300.21702914,16594414.491002144,16603428.166133547,16612341.411826065,16621154.425945364,16629867.434171207,16638480.689337889,16646994.470765306,16655409.08358173,16663724.858039653,16671942.14882573,16680061.334365945,16688082.816127203]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[8394333.931275247,8394333.881852245,8394333.831509491,8394333.780229857,8394333.727995913,8394333.6747899,8394333.620593727,8394333.565388964,8394333.50915685,8394333.451878257,8394333.393533716,8394333.334103387,8394333.27356707,8394333.211904176,8394333.149093745,8394333.085114416,8394333.01994444,8394332.95356166,8394332.885943506,8394332.817066982,8394332.74690868,8394332.675444743,8394332.602650873,8394332.52850232,8394332.452973874,8394332.376039863,8394332.29767412,8394332.217850002,8394332.136540378,8394332.053717595,8394331.9693535,8394331.883419402,8394331.795886094,8394331.706723811,8394331.615902238,8394331.523390494,8394331.429157138,8394331.33317012,8394331.235396806,8394331.135803964,8394331.034357723,8394330.931023598,8394330.825766457,8394330.71855051,8394330.609339317,8394330.498095732,8394330.384781942,8394330.269359425,8394330.151788937,8394330.032030502,8394329.910043407,8394329.785786184,8394329.659216577,8394329.530291567,8394329.398967313,8394329.265199173,8394329.128941668,8394328.990148472,8394328.848772397,8394328.704765383,8394328.558078466,8394328.408661779,8394328.256464526,8394328.101434961,8394327.943520378,8394327.782667095,8394327.618820423,8394327.451924656,8394327.281923063,8394327.108757839,8394326.932370119,8394326.752699934,8394326.569686204,8394326.383266715,8394326.193378082,8394325.999955757,8394325.802933978,8394325.602245767,8394325.3978229,8394325.189595878,8394324.977493918,8394324.761444908,8394324.541375399,8394324.317210585,8394324.08887425,8394323.85628878,8394323.6193751,8394323.378052672,8394323.13223946,8394322.8818519,8394322.626804868,8394322.367011666,8394322.102383979,8394321.832831847,8394321.558263632,8394321.278586002,8394320.99370388,8394320.703520425,8394320.407936988,8394320.1068531,8394319.800166398,8394319.487772632,8394319.169565616,8394318.845437177,8394318.515277127,8394318.178973246,8394317.836411202,8394317.487474564,8394317.13204471,8394316.770000821,8394316.401219837,8394316.025576394,8394315.642942807,8394315.253189016,8394314.856182536,8394314.451788414,8394314.039869199,8394313.62028487,8394313.192892812,8394312.757547751,8394312.314101707,8394311.862403955,8394311.402300967,8394310.93363635,8394310.456250811,8394309.969982084,8394309.474664899,8394308.97013089,8394308.45620859,8394307.932723306,8394307.399497129,8394306.856348824,8394306.303093784,8394305.739543969,8394305.165507853,8394304.580790326,8394303.985192671,8394303.378512466,8394302.760543521,8394302.131075814,8394301.489895418,8394300.836784422,8394300.17152087,8394299.493878676,8394298.803627538,8394298.10053289,8394297.38435579,8394296.654852856,8394295.911776178,8394295.154873244,8394294.383886836,8394293.598554956,8394292.798610732,8394291.98378234,8394291.153792884,8394290.30836033,8394289.447197402,8394288.570011472,8394287.676504483,8394286.766372822,8394285.839307249,8394284.894992746,8394283.933108473,8394282.953327592,8394281.955317214,8394280.938738246,8394279.90324529,8394278.848486539,8394277.77410363,8394276.679731552,8394275.564998493,8394274.42952574,8394273.272927541,8394272.094810972,8394270.894775791,8394269.672414336,8394268.427311353,8394267.159043876,8394265.867181083,8394264.551284125,8394263.210906021,8394261.845591461,8394260.454876687,8394259.038289312,8394257.59534818,8394256.125563184,8394254.628435109,8394253.103455473,8394251.550106335,8394249.967860136,8394248.356179511,8394246.714517105,8394245.0423154,8394243.33900652,8394241.604012031,8394239.836742759,8394238.036598582,8394236.202968225,8394234.335229065,8394232.432746906,8394230.494875776,8394228.520957697,8394226.51032248,8394224.462287476,8394222.376157364,8394220.251223907,8394218.086765718,8394215.88204801,8394213.636322347,8394211.348826405,8394209.018783689,8394206.645403296,8394204.22787964,8394201.765392173,8394199.257105116,8394196.702167166,8394194.099711224,8394191.448854098,8394188.74869619,8394185.998321213,8394183.19679587,8394180.343169536,8394177.436473958,8394174.475722903,8394171.459911842,8394168.388017595,8394165.258998016,8394162.071791604,8394158.825317176,8394155.518473491,8394152.150138872,8394148.719170837,8394145.224405717,8394141.664658254,8394138.038721219,8394134.345364982,8394130.583337117,8394126.751361974,8394122.848140253,8394118.872348564,8394114.822638977,8394110.697638582,8394106.495949022,8394102.216146013,8394097.85677889,8394093.416370079,8394088.893414654,8394084.286379786,8394079.593704255,8394074.813797913,8394069.945041161,8394064.985784398,8394059.93434747,8394054.789019112,8394049.548056358,8394044.209683988,8394038.7720939,8394033.233444527,8394027.591860207,8394021.845430575,8394015.992209896,8394010.030216437,8394003.957431797,8393997.771800235,8393991.471227966,8393985.053582504,8393978.516691897,8393971.85834404,8393965.076285927,8393958.168222893,8393951.131817851,8393943.964690529,8393936.664416652,8393929.228527151,8393921.654507337,8393913.939796071,8393906.081784898,8393898.077817192,8393889.92518727,8393881.62113949,8393873.162867337,8393864.547512501,8393855.772163918,8393846.833856808,8393837.729571687,8393828.456233384,8393819.010710003,8393809.3898119,8393799.59029062,8393789.608837832,8393779.442084225,8393769.08659841,8393758.538885765,8393747.795387309,8393736.85247852,8393725.70646811,8393714.353596874,8393702.790036391,8393691.011887802,8393679.015180506,8393666.795870872,8393654.349840913,8393641.672896923,8393628.760768102,8393615.60910518,8393602.213478964,8393588.569378898,8393574.672211614,8393560.517299388,8393546.099878656,8393531.415098425,8393516.458018722,8393501.223608963,8393485.706746336,8393469.902214112,8393453.804699983,8393437.408794302,8393420.708988367,8393403.699672598,8393386.375134759,8393368.729558082,8393350.75701941,8393332.451487273,8393313.806819936,8393294.81676346,8393275.474949637,8393255.774893977,8393235.709993629,8393215.273525245,8393194.458642842,8393173.258375607,8393151.665625658,8393129.67316581,8393107.273637233,8393084.459547132,8393061.223266378,8393037.557027038,8393013.452919964,8392988.902892254,8392963.898744725,8392938.432129301,8392912.49454641,8392886.077342281,8392859.171706233,8392831.768667914,8392803.859094474,8392775.43368772,8392746.482981216,8392716.997337291,8392686.966944085,8392656.381812476,8392625.231772974,8392593.506472576,8392561.195371563,8392528.287740247,8392494.772655664,8392460.63899821,8392425.875448227,8392390.470482547,8392354.412370957,8392317.68917261,8392280.288732419,8392242.198677337,8392203.406412605,8392163.899117975,8392123.663743794,8392082.687007116,8392040.955387685,8391998.455123898,8391955.172208678,8391911.09238533,8391866.201143242,8391820.483713653,8391773.925065229,8391726.509899657,8391678.222647136,8391629.047461808,8391578.968217133,8391527.968501186,8391476.03161188,8391423.140552118,8391369.278024914,8391314.426428383,8391258.567850696,8391201.684064992,8391143.75652412,8391084.766355423,8391024.694355391,8390963.520984232,8390901.226360401,8390837.790255025,8390773.192086266,8390707.41091364,8390640.42543219,8390572.213966643,8390502.754465472,8390432.024494886,8390360.001232717,8390286.661462273,8390211.981566092,8390135.937519591,8390058.504884716,8389979.658803407,8389899.373991095,8389817.624730043,8389734.38486264,8389649.627784625,8389563.326438209,8389475.453305151,8389385.98039975,8389294.879261725,8389202.120949086,8389107.676030857,8389011.514579803,8388913.606164994,8388813.91984439,8388712.42415728,8388609.087116688,8388503.876201698,8388396.758349712,8388287.699948637,8388176.666829022,8388063.6242560875,8387948.536921753,8387831.368936551,8387712.083821496,8387590.644499916,8387467.013289189,8387341.151892462,8387213.0213903,8387082.5822322685,8386949.794228516,8386814.616541252,8386677.007676239,8386536.925474204,8386394.327102231,8386249.169045128,8386101.407096734,8385950.996351264,8385797.891194534,8385642.045295271,8385483.411596334,8385321.942305962,8385157.588889011,8384990.302058176,8384820.031765247,8384646.727192333,8384470.336743151,8384290.808034285,8384108.087886518,8383922.122316153,8383732.856526417,8383540.234898871,8383344.200984913,8383144.697497284,8382941.666301714,8382735.048408581,8382524.783964677,8382310.81224508,8382093.071645095,8381871.499672332,8381646.032938875,8381416.607153604,8381183.1571146455,8380945.6167019475,8380703.918870052,8380457.995641019,8380207.778097503,8379953.196376066,8379694.179660665,8379430.656176366,8379162.553183284,8378889.796970731,8378612.312851701,8378330.025157515,8378042.857232844,8377750.73143095,8377453.569109291,8377151.290625427,8376843.815333264,8376531.061579663,8376212.946701427,8375889.387022651,8375560.297852504,8375225.593483422,8374885.187189741,8374538.99122677,8374186.916830389,8373828.874217079,8373464.772584498,8373094.520112584,8372718.023965212,8372335.190292412,8371945.924233157,8371550.129918805,8371147.710477138,8370738.568037039,8370322.603733866,8369899.717715511,8369469.809149146,8369032.776228717,8368588.516183189,8368136.9252855405,8367677.898862584,8367211.331305573,8366737.116081644,8366255.14574611,8365765.311955626,8365267.505482267,8364761.616228464,8364247.533242924,8363725.144737465,8363194.338104811,8362654.999937403,8362107.016047172,8361550.271486333,8360984.650569228,8360410.036895193,8359826.313372476,8359233.3622432565,8358631.065109689,8358019.302961088,8357397.956202196,8356766.904682528,8356126.027726853,8355475.204166796,8354814.312373517,8354143.230291561,8353461.83547377,8352770.005117336,8352067.616100952,8351354.545023052,8350630.668241151,8349895.861912236,8349150.002034245,8348392.964488542,8347624.6250834465,8346844.859598731,8346053.543831086,8345250.553640518,8344435.764997654,8343609.054031877,8342770.2970803045,8341919.370737519,8341056.151906023,8340180.517847365,8339292.34623385,8338391.51520083,8337477.903399423,8336551.390049668,8335611.854993999,8334659.178750951,8333693.242569033,8332713.928480683,8331721.119356137,8330714.698957241,8329694.551990952,8328660.564162531,8327612.62222825,8326550.6140475,8325474.428634196,8324383.9562073,8323279.088240368,8322159.717509976,8321025.738142809,8319877.045661363,8318713.5370280165,8317535.110687356,8316341.66660655,8315133.106313652,8313909.332933585,8312670.251221714,8311415.767594703,8310145.79015863,8308860.228734015,8307558.9948776215,8306242.001900931,8304909.164884904,8303560.400691006,8302195.627968192,8300814.767155698,8299417.740481416,8298004.471955672,8296574.887360187,8295128.914232031,8293666.481842408,8292187.5211700285,8290691.964868935,8289179.747230536,8287650.804139763,8286105.073025098,8284542.492802321,8282963.003811913,8281366.547749828,8279753.067591602,8278122.507509664,8276474.812783673,8274809.9297038745,8273127.805467285,8271428.388066759,8269711.626172745,8267977.4690077985,8266225.866213772,8264456.767711724,8262670.123554476,8260865.883771954,8259043.998209329,8257204.416357985,8255347.087179498,8253471.9589226935,8251578.978933926,8249668.093460798,8247739.247449451,8245792.384335674,8243827.445830129,8241844.371697837,8239843.09953236,8237823.564524918,8235785.699228792,8233729.433319473,8231654.693350797,8229561.402507713,8227449.480355911,8225318.842588962,8223169.400773389,8221001.062092226,8218813.729087565,8216607.299402776,8214381.665524868,8212136.714527685,8209872.327816548,8207588.380874997,8205284.743014313,8202961.277126466,8200617.839441249,8198254.279288257,8195870.438864433,8193466.153007938,8191041.248979083,8188595.546248989,8186128.856296813,8183640.982416218,8181131.719531838,8178600.854026508,8176048.163579907,8173473.417019432,8170876.374183944,8168256.785801115,8165614.393379014,8162948.929112699,8160260.115806313,8157547.666811448,8154811.28598227,8152050.667648072,8149265.496603731,8146455.4481186345,8143620.187964524,8140759.372462781,8137872.648551511,8134959.653872843,8132020.016880815,8129053.356970141,8126059.284626134,8123037.401596053,8119987.301082052,8116908.567955904,8113800.778995618,8110663.503144018,8107496.301789394,8104298.729068076,8101070.33218913,8097810.651780818,8094519.222258968,8091195.572216922,8087839.224836979,8084449.698323035,8081026.506354224,8077569.158559253,8074077.161011076,8070550.01674158,8066987.226275968,8063388.288186264,8059752.699663729,8056079.957109563,8052369.556743498,8048620.995229814,8044833.770320192,8041007.381512979,8037141.330728274,8033235.122998323,8029288.267172665,8025300.276637498,8021270.670048692,8017198.972077923,8013084.714171355,8008927.435320354,8004726.682843674,8000482.013180586,7996192.992694474,7991859.198486336,7987480.219217744,7983055.655942766,7978585.1229484165,7974068.248603194,7969504.676213264,7964894.0648859395,7960236.0904000625,7955530.446082955,7950776.843693603,7945975.014311807,7941124.709233006,7936225.700868557,7931277.783651235,7926280.774945788,7921234.515964362,7916138.872686688,7910993.736784926,7905799.026553036,7900554.687840675,7895260.694991549,7889917.051786223,7884523.792389386,7879080.9823015835,7873588.719315492,7868047.13447676,7862456.393049488,7856816.695486429,7851128.278403997,7845391.415562173,7839606.418849365,7833773.63927236,7827893.467951388,7821966.337120444,7815992.72113283,7809973.137472054,7803908.147768034,7797798.358818612,7791644.4236164065,7785447.042380806,7779206.9635951305,7772924.985048695,7766601.954883653,7760238.772646284,7753836.390342496,7747395.813497086,7740918.102216333,7734404.372253438,7727855.796076135,7721273.603935826,7714659.084937489,7708013.588109409,7701338.523471789,7694635.363103228,7687905.642203728,7681150.960153079,7674372.981563082,7667573.437322099,7660754.125630265,7653916.913023515,7647063.735384472,7640196.5989381485,7633317.58123017,7626428.832085157,7619532.574542729,7612631.105768448,7605726.797936831,7598822.099083478,7591919.533923095,7585021.7046301495,7578131.291578665,7571251.054037489,7564383.830817305,7557532.540865384,7550700.183804031,7543889.8404084,7537104.673019362,7530347.925886856,7523622.925439008,7516933.0804722905,7510281.882257717,7503672.904558092,7497109.803551122,7490596.317653163,7484136.267238291,7477733.554247247,7471392.161680892,7465116.152972562,7458909.671233894,7452776.938368559,7446722.254048374,7440749.994546337,7434864.611421149,7429070.630047869,7423372.647989476,7417775.333204223,7412283.4220838435,7406901.717317855,7401635.085579436,7396488.455028599,7391466.812628676,7386575.201272504,7381818.716714961,7377202.504309043,7372731.755542985,7368411.704376529,7364247.623374866,7360244.819639425,7356408.630535241,7352744.419215289,7349257.569942893,7345953.4832139965,7342837.5706818905,7339915.249887755,7337191.938801266,7334673.050176338,7332363.985727948,7330270.130137002,7328396.844891073,7326749.461969748,7325333.277384453,7324153.54458341,7323215.467733484,7322524.194891579,7322084.811079185,7321902.331274645,7321981.693338575,7322327.750888696,7322945.266141232,7323838.902736666,7325013.218568435,7326472.65863372,7328221.547925996,7330264.084389525,7332604.331956245,7335246.213685901,7338193.505030179,7341449.82724192,7345018.640950162,7348903.239921729,7353106.745029625,7357632.09844817,7362482.058094097,7367659.192332305,7373165.874963927,7379004.280513711,7385176.379832352,7391683.936028587,7398528.50074433,7405711.410785014,7413233.785115757,7421096.52223254,7429300.2979160305,7437845.563374167,7446732.543777872,7455961.237192811,7465531.41390827,7475442.616162874,7485694.1582649015,7496285.127103765,7507214.383047378,7518480.561218888,7530082.07314472,7542017.108764593,7554283.638792975,7566879.417420261,7579801.985340903,7593048.673094849,7606616.604707772,7620502.701614881,7634703.686852505,7649216.089501166,7664036.249363504,7679160.321860101,7694584.283126162,7710303.935291965,7726314.911929989,7742612.683651818,7759192.563838175,7776049.7144856565,7793179.152154248,7810575.754000023,7828234.263878021,7846149.298500856,7864315.353639062,7882726.810350031,7901377.941222854,7920262.916627159,7939375.810954672,7958710.608842931,7978261.2113712765,7998021.442219891,8017985.053783402,8038145.733231073,8058497.108506466,8079032.7542597875,8099746.197707,8120630.924410126,8141680.383973803,8162887.9956536405,8184247.153872324,8205751.233639978,8227393.595875571,8249167.59262656,8271066.572184354,8293083.8840934355,8315212.8840522645,8337446.938704362,8359779.430318179,8382203.761354552,8404713.358920762,8427301.679110305,8449962.2112277,8472688.481897712,8495474.059058495,8518312.555838302,8541197.63431536,8564123.009160776,8587082.451164154,8610069.790641915,8633078.920728123,8656103.80054787,8679138.45827317,8702176.994061388,8725213.58287634,8748242.477192132,8771258.009579957,8794254.59517795,8817226.734044513,8840169.013395214,8863076.109723784,8885942.790807484,8908763.917597378,8931534.445994005,8954249.428509017,8976904.015813475,8999493.458173502,9022013.10677411,9044458.414932057,9066824.939198716,9089108.340353968,9111304.384292316,9133408.942802314,9155417.994240742,9177327.62410281,9199134.02548993,9220833.499476593,9242422.455378044,9263897.410920408,9285254.992315201,9306491.934240028,9327605.079727555,9348591.379964754,9369447.894004552,9390171.788392175,9410760.336708395,9431210.919032048,9451521.021324243,9471688.234736707,9491710.254846767,9511584.880821552,9531310.014514029,9550883.65949344,9570303.92001286,9589568.999916567,9608677.201489925,9627626.924254492,9646416.6637111,9665045.010033695,9683510.646716572,9701812.349177867,9719948.983321937,9737919.504063386,9755722.953815423,9773358.460945178,9790825.23819873,9808122.581098277,9825249.866314197,9842206.550014403,9858992.166193545,9875606.32498448,9892048.71095438,9908319.081387859,9924417.26455936,9940343.15799702,9956096.726740237,9971678.00159295,9987087.077374754,10002324.111171734,10017389.320589019,10032282.982006805,10047005.428841667,10061557.04981482,10075938.287229028,10090149.635255584,10104191.63823303,10118064.888978843,10131770.027115589,10145307.737412723,10158678.748145286,10171883.82947063]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[8268274.365997141,8268274.289305087,8268274.211185796,8268274.131612705,8268274.050558756,8268273.967996391,8268273.883897543,8268273.798233613,8268273.710975479,8268273.622093471,8268273.53155737,8268273.439336393,8268273.34539919,8268273.249713812,8268273.152247737,8268273.052967813,8268272.951840299,8268272.848830805,8268272.7439043075,8268272.637025127,8268272.52815694,8268272.4172627125,8268272.304304755,8268272.189244655,8268272.072043294,8268271.952660826,8268271.831056656,8268271.70718944,8268271.5810170695,8268271.452496642,8268271.321584459,8268271.188236011,8268271.052405965,8268270.914048138,8268270.773115479,8268270.629560085,8268270.483333145,8268270.334384939,8268270.182664823,8268270.028121221,8268269.870701585,8268269.710352387,8268269.547019122,8268269.38064625,8268269.211177207,8268269.038554373,8268268.862719061,8268268.683611484,8268268.501170753,8268268.315334835,8268268.126040548,8268267.933223538,8268267.736818244,8268267.536757895,8268267.332974464,8268267.12539868,8268266.91395996,8268266.698586419,8268266.479204838,8268266.255740628,8268266.028117809,8268265.796258991,8268265.560085355,8268265.319516603,8268265.074470941,8268264.824865061,8268264.5706140995,8268264.311631612,8268264.04782956,8268263.779118244,8268263.505406311,8268263.226600704,8268262.94260664,8268262.653327555,8268262.3586651115,8268262.058519126,8268261.7527875565,8268261.441366462,8268261.124149961,8268260.801030213,8268260.471897368,8268260.136639518,8268259.795142695,8268259.447290798,8268259.092965558,8268258.732046526,8268258.364410989,8268257.989933969,8268257.608488152,8268257.219943865,8268256.824169003,8268256.421029026,8268256.010386873,8268255.592102947,8268255.166035038,8268254.732038303,8268254.289965199,8268253.839665432,8268253.380985928,8268252.913770747,8268252.437861057,8268251.9530950645,8268251.459307971,8268250.956331915,8268250.443995901,8268249.922125763,8268249.390544087,8268248.849070157,8268248.297519904,8268247.735705823,8268247.163436931,8268246.5805186825,8268245.986752914,8268245.381937785,8268244.765867688,8268244.138333192,8268243.499120972,8268242.848013734,8268242.184790142,8268241.509224744,8268240.821087886,8268240.120145646,8268239.40615975,8268238.678887489,8268237.938081652,8268237.183490399,8268236.41485724,8268235.631920882,8268234.834415199,8268234.022069087,8268233.19460642,8268232.351745926,8268231.493201094,8268230.6186801,8268229.727885676,8268228.820515029,8268227.896259734,8268226.954805629,8268225.995832707,8268225.019015007,8268224.0240205,8268223.010510991,8268221.978141985,8268220.926562589,8268219.855415365,8268218.7643362405,8268217.652954375,8268216.520892012,8268215.367764382,8268214.193179553,8268212.996738312,8268211.778034002,8268210.536652421,8268209.272171647,8268207.984161929,8268206.672185496,8268205.335796467,8268203.9745406415,8268202.587955378,8268201.175569437,8268199.736902814,8268198.271466566,8268196.778762669,8268195.258283828,8268193.709513322,8268192.131924809,8268190.524982165,8268188.888139299,8268187.220839958,8268185.522517544,8268183.792594926,8268182.0304842405,8268180.235586685,8268178.407292343,8268176.54497992,8268174.648016604,8268172.715757794,8268170.747546909,8268168.742715168,8268166.700581333,8268164.6204515295,8268162.501618948,8268160.343363667,8268158.144952359,8268155.905638069,8268153.624659956,8268151.301243036,8268148.934597908,8268146.523920498,8268144.068391793,8268141.567177543,8268139.01942799,8268136.424277585,8268133.780844686,8268131.088231259,8268128.345522581,8268125.551786925,8268122.706075245,8268119.8074208535,8268116.854839097,8268113.84732702,8268110.783863026,8268107.66340652,8268104.484897601,8268101.24725663,8268097.949383926,8268094.590159361,8268091.16844201,8268087.68306972,8268084.132858763,8268080.516603403,8268076.833075508,8268073.081024122,8268069.259175051,8268065.366230417,8268061.40086824,8268057.36174197,8268053.247480048,8268049.056685431,8268044.7879351275,8268040.439779707,8268036.010742815,8268031.499320687,8268026.903981605,8268022.223165421,8268017.455283002,8268012.598715708,8268007.651814822,8268002.612901034,8267997.480263827,8267992.252160938,8267986.92681774,8267981.502426663,8267975.977146575,8267970.349102155,8267964.616383275,8267958.777044344,8267952.82910365,8267946.770542702,8267940.599305539,8267934.313298034,8267927.910387202,8267921.388400466,8267914.74512493,8267907.978306643,8267901.085649812,8267894.064816063,8267886.913423617,8267879.629046525,8267872.209213816,8267864.651408694,8267856.953067662,8267849.111579686,8267841.124285298,8267832.988475709,8267824.7013918925,8267816.260223657,8267807.662108709,8267798.904131669,8267789.983323132,8267780.896658616,8267771.641057587,8267762.213382406,8267752.6104372805,8267742.82896718,8267732.865656768,8267722.717129255,8267712.379945295,8267701.850601809,8267691.125530835,8267680.201098294,8267669.073602817,8267657.739274463,8267646.194273475,8267634.434688999,8267622.45653777,8267610.255762758,8267597.828231837,8267585.169736395,8267572.275989914,8267559.142626545,8267545.765199657,8267532.139180333,8267518.259955864,8267504.122828213,8267489.723012438,8267475.055635101,8267460.115732644,8267444.8982497165,8267429.398037511,8267413.609852032,8267397.5283523565,8267381.1480988525,8267364.463551358,8267347.469067356,8267330.15890008,8267312.527196613,8267294.567995939,8267276.275226958,8267257.642706472,8267238.66413713,8267219.333105347,8267199.643079165,8267179.58740609,8267159.159310899,8267138.351893375,8267117.158126026,8267095.570851788,8267073.582781616,8267051.186492107,8267028.374423021,8267005.138874811,8266981.472006062,8266957.365830902,8266932.812216386,8266907.802879805,8266882.329385965,8266856.383144395,8266829.955406543,8266803.037262881,8266775.619639997,8266747.693297598,8266719.248825492,8266690.276640487,8266660.766983278,8266630.709915226,8266600.095315109,8266568.912875833,8266537.152101045,8266504.802301708,8266471.852592628,8266438.291888889,8266404.108902258,8266369.292137498,8266333.829888634,8266297.7102351645,8266260.921038166,8266223.449936377,8266185.284342183,8266146.411437549,8266106.818169857,8266066.491247713,8266025.417136626,8265983.582054679,8265940.971968052,8265897.572586533,8265853.3693589,8265808.347468285,8265762.491827361,8265715.787073556,8265668.217564129,8265619.767371143,8265570.420276397,8265520.159766244,8265468.9690263355,8265416.83093627,8265363.728064141,8265309.64266101,8265254.556655288,8265198.451647007,8265141.308901977,8265083.109345921,8265023.833558423,8264963.461766823,8264901.97384,8264839.349282062,8264775.567225896,8264710.606426672,8264644.445255175,8264577.061691071,8264508.43331604,8264438.537306815,8264367.350428096,8264294.849025334,8264221.009017443,8264145.8058893215,8264069.214684337,8263991.209996628,8263911.765963317,8263830.856256563,8263748.45407552,8263664.5321381595,8263579.062672963,8263492.017410468,8263403.367574714,8263313.083874502,8263221.136494577,8263127.495086633,8263032.128760195,8262935.006073357,8262836.09502336,8262735.363037057,8262632.776961231,8262528.303052705,8262421.906968396,8262313.55375515,8262203.207839448,8262090.833016954,8261976.392441917,8261859.848616416,8261741.163379408,8261620.297895702,8261497.212644655,8261371.86740882,8261244.221262354,8261114.232559287,8260981.858921621,8260847.057227264,8260709.783597792,8260569.993386038,8260427.641163503,8260282.680707623,8260135.064988806,8259984.746157363,8259831.675530204,8259675.803577369,8259517.079908421,8259355.453258603,8259190.871474855,8259023.281501617,8258852.629366487,8258678.860165663,8258501.918049217,8258321.746206193,8258138.286849501,8257951.481200641,8257761.269474245,8257567.590862411,8257370.383518884,8257169.58454303,8256965.129963631,8256756.954722493,8256544.992657877,8256329.176487733,8256109.437792776,8255885.706999338,8255657.913362079,8255425.984946492,8255189.848611249,8254949.4299903335,8254704.653475037,8254455.442195754,8254201.718003606,8253943.401451908,8253680.411777439,8253412.666881573,8253140.083311228,8252862.576239645,8252580.059447026,8252292.445301018,8251999.644736997,8251701.567238274,8251398.12081609,8251089.211989497,8250774.745765101,8250454.625616662,8250128.753464574,8249797.029655196,8249459.35294009,8249115.620455124,8248765.727699487,8248409.568514565,8248047.035062762,8247678.017806191,8247302.405485294,8246920.085097397,8246530.941875182,8246134.859265087,8245731.718905666,8245321.400605896,8244903.782323427,8244478.740142851,8244046.148253877,8243605.878929562,8243157.802504498,8242701.7873530155,8242237.699867418,8241765.404436221,8241284.763422454,8240795.637141991,8240297.883841963,8239791.359679228,8239275.918698937,8238751.412813184,8238217.691779799,8237674.603181219,8237121.9924035305,8236559.702615654,8235987.574748688,8235405.447475449,8234813.157190156,8234210.537988407,8233597.421647298,8232973.637605817,8232339.012945494,8231693.372371307,8231036.538192883,8230368.330305976,8229688.566174304,8228997.06081167,8228293.626764472,8227578.074094544,8226850.210362398,8226109.840610854,8225356.767349088,8224590.790537094,8223811.7075706115,8223019.313266495,8222213.3998485645,8221393.756933947,8220560.171519918,8219712.427971273,8218850.308008213,8217973.590694798,8217082.052427955,8216175.466927033,8215253.605224,8214316.2356541585,8213363.1238475405,8212394.032720864,8211408.722470135,8210406.950563894,8209388.471737074,8208353.037985511,8207300.398561108,8206230.2999676475,8205142.485957243,8204036.697527457,8202912.672919067,8201770.147614459,8200608.854336687,8199428.523049139,8198228.880955867,8197009.652502467,8195770.559377629,8194511.32051522,8193231.652096955,8191931.267555602,8190609.877578728,8189267.190112902,8187902.910368422,8186516.740824413,8185108.381234374,8183677.528632079,8182223.877337746,8180747.118964605,8179246.942425535,8177723.033940019,8176175.077041142,8174602.752582678,8173005.738746187,8171383.71104803,8169736.342346266,8168063.302847345,8166364.260112495,8164638.879063774,8162886.8219896415,8161107.748550022,8159301.315780693,8157467.178096993,8155604.987296654,8153714.392561728,8151795.040459435,8149846.574941907,8147868.637344595,8145860.866383324,8143822.898149834,8141754.366105642,8139654.901074184,8137524.131231004,8135361.682091946,8133167.176499144,8130940.234604711,8128680.4738519685,8126387.508954106,8124060.951870032,8121700.4117774265,8119305.495042708,8116875.805187841,8114410.942853846,8111910.50576079,8109374.088664272,8106801.2833080655,8104191.678372969,8101544.8594215475,8098860.408838842,8096137.905768725,8093376.926045897,8090577.042123386,8087737.822995415,8084858.834115577,8081939.637310173,8078979.790686677,8075978.848537242,8072936.361237133,8069851.875138107,8066724.932456644,8063555.071157026,8060341.824829223,8057084.722561619,8053783.288808548,8050437.043252711,8047045.500662469,8043608.170744154,8040124.557989377,8036594.1615175055,8033016.474913428,8029390.986060687,8025717.176970225,8021994.523604836,8018222.495699608,8014400.556578524,8010528.162967523,8006604.764804225,8002629.805044704,7998602.719467557,7994522.93647566,7990389.876895954,7986202.953777696,7981961.572189531,7977665.129015907,7973313.012753237,7968904.603306329,7964439.271785636,7959916.380305778,7955335.281785997,7950695.319753061,7945995.828147275,7941236.131132181,7936415.542908638,7931533.367533911,7926588.898746487,7921581.419797305,7916510.203288099,7911374.511017631,7906173.593836508,7900906.691511365,7895573.032599169,7890171.834332441,7884702.3025161065,7879163.631436854,7873555.003785683,7867875.590594522,7862124.55118762,7856301.033148552,7850404.172303574,7844433.092722106,7838386.906735125,7832264.714972175,7826065.606417759,7819788.658487817,7813432.937126995,7806997.496927377,7800481.381269384,7793883.622485402,7787203.242046837,7780439.250775106,7773590.649077201,7766656.427206303,7759635.565547942,7752527.034932227,7745329.7969725225,7738042.804430998,7730665.001611411,7723195.324779482,7715632.702611074,7707976.056668538,7700224.301905353,7692376.347199303,7684431.095914287,7676387.4464909015,7668244.293065821,7660000.526120048,7651655.033156006,7643206.699403401,7634654.408553817,7625997.043523884,7617233.487246907,7608362.623492748,7599383.3377157105,7590294.5179302925,7581095.055614366,7571783.846639629,7562359.792228921,7552821.799940047,7543168.784675778,7533399.669719589,7523513.387796722,7513508.882160187,7503385.107701171,7493141.032083486,7482775.636901504,7472287.918861169,7461676.890983546,7450941.58383045,7440081.046751643,7429094.34915313,7417980.581786028,7406738.858055562,7395368.315349652,7383868.116386689,7372237.450581936,7360475.535432205,7348581.617918282,7336554.975924699,7324394.919676435,7312100.793192144,7299671.975753518,7287107.883390419,7274407.9703814145,7261571.730769406,7248598.699892,7235488.4559263745,7222240.62144828,7208854.865004994,7195330.902701924,7181668.4998026565,7167867.472342225,7153927.688753389,7139849.071505735,7125631.598757441,7111275.306019484,7096780.287832161,7082146.699453773,7067374.758561228,7052464.746962515,7037417.012320769,7022231.969889816,7006910.10426099,6991451.971120956,6975858.199020401,6960129.491153253,6944266.627146192,6928270.464858079,6912141.942188984,6895882.078898368,6879491.978431968,6862972.829756866,6846325.909204125,6829552.5823183805,6812654.305713609,6795632.62893429,6778489.196321032,6761225.748879644,6743844.12615259,6726346.268091548,6708734.216929788,6691010.11905285,6673176.22686596,6655234.900656451,6637188.610449259,6619039.937853494,6600791.577897881,6582446.3408526685,6564007.154035466,6545477.06359833,6526859.236293066,6508156.961211771,6489373.651499143,6470512.846033162,6451578.211070315,6432573.541851425,6413502.764163912,6394369.935856035,6375179.2482984895,6355935.0277884705,6336641.736891057,6317303.975712562,6297926.483100234,6278514.137762438,6259071.959303237,6239605.109164999,6220118.891472483,6200618.753771513,6181110.287655249,6161599.229270711,6142091.459698075,6122593.005194991,6103110.03729798,6083648.872772816,6064215.973405537,6044817.945625633,6025461.539952766,6006153.65025825,5986901.312832418,5967711.705248926,5948592.145016905,5929550.0880119605,5910593.126676853,5891728.987982875,5872965.531142887,5854310.745067109,5835772.745552927,5817359.772200077,5799080.185042928,5780942.460891713,5762955.189374997,5745127.068676002,5727466.9009558605,5709983.587457388,5692686.123283518,5675583.591845147,5658685.158973922,5642000.066696122,5625537.62666482,5609307.213248221,5593318.2562732175,5577580.233424145,5562102.662297933,5546895.092117977,5531967.095110394,5517328.257547588,5502988.17046553,5488956.420062547,5475242.577789013,5461856.190138807,5448806.7681551045,5436103.776664671,5423756.623256489,5411774.647022314,5400167.107078355,5388943.170889061,5378111.90241565,5367682.250113648,5357663.034805376,5348062.937454818,5338890.486873866,5330154.047390268,5321861.80650899,5314021.762599837,5306641.712645269,5299729.240083279,5293291.702780941,5287336.221174847,5281869.66661509,5276898.649949603,5272429.510385776,5268468.304665993,5265020.796593414,5262092.446943624,5259688.403796978,5257813.493325377,5256472.211065937,5255668.713712499,5255406.811454236,5255689.960888673,5256521.258534385,5257903.434966333,5259838.849594399,5262329.486103084,5265376.948567683,5268982.4582594605,5273146.851149436,5277870.576117542,5283153.693870915,5288995.876572141,5295396.408175314,5302354.185464882,5309867.7197894,5317935.139479528,5326554.192936997,5335722.25237868,5345436.31821756,5355693.024060104,5366488.642297531,5377819.090266529,5389679.936953341,5402066.410213579,5414973.404478912,5428395.488920616,5442326.916039155,5456761.630648287,5471693.279221679,5487115.219569792,5503020.530814719,5519402.023630695,5536252.250718373,5553563.517481289,5571327.892873585,5589537.220388753,5608183.12915997,5627257.045143605,5646750.202358437,5666653.654154272,5686958.284484868,5707654.819161226,5728733.8370626755,5750185.781284406,5772000.9702014625,5794169.608430546,5816681.797672237,5839527.547417659,5862696.78550475,5886179.36851071,5909965.091968314,5934043.700394983,5958404.897124714,5983038.353933898,6007933.7204532875,6033080.633359194,6058468.725338003,6084087.633818976,6109927.009471051,6135976.524460189,6162225.880464474,6188664.816444839,6215283.116169921,6242070.615493994,6269017.209387582,6296112.858720681,6323347.596799002,6350711.535654027,6378194.8720879285,6405787.893474829,6433480.983320036,6461264.6265791925,6489129.4147395035,6517066.050665337,6545065.353210717,6573118.261601398,6601215.839589239,6629349.279381884,6657509.905350684,6685689.177520021,6713878.694841225,6742070.19825436,6770255.57354125,6798426.85397313,6826576.222756413,6854696.015280129,6882778.7211685795,6910816.986142896,6938803.613695139,6966731.566578721,6994593.968118869,7022384.103347035,7050095.4199630385,7077721.529128888,7105256.20609824,7132693.390685425,7160027.187578173,7187251.866497962,7214361.862212206,7241351.774402336,7268216.367391969,7294950.569739346,7321549.473698251,7348008.334551629,7374322.569822219,7400487.75836438,7426499.6393415155,7452354.111093283,7478047.2298969515,7503575.20862722,7528934.4153187405,7554121.371635712,7579132.751252771,7603965.378151485,7628616.224836668,7653082.41047676,7677361.198972473,7701449.996957819,7725346.351737698,7749047.949166076,7772552.611468813,7795858.295015109,7818963.088041478,7841865.208332151,7864563.000859651,7887054.935389306,7909339.604051375,7931415.71888432,7953282.109352773,7974937.719843611,7996381.607143469,8017612.937900974,8038630.986076838,8059435.130384899,8080024.8517271,8100399.73062528,8120559.444652602,8140503.765867267,8160232.558251153,8179745.775155873,8199043.456758645,8218125.727530281,8236992.7937175,8255644.940841671,8274082.531215994,8292306.001483006,8310315.8601742685,8328112.685293891,8345697.121927551,8363069.879878507,8380231.731332066,8397183.508549793,8413926.10159476,8430460.456088953,8446787.571003933,8462908.496485734],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[15265326.930465126,15265326.977822004,15265327.02606018,15265327.07519608,15265327.125246396,15265327.176228145,15265327.22815867,15265327.281055622,15265327.33493698,15265327.389821073,15265327.445726562,15265327.502672441,15265327.56067809,15265327.619763223,15265327.679947924,15265327.741252664,15265327.803698286,15265327.86730601,15265327.932097478,15265327.99809471,15265328.065320149,15265328.133796647,15265328.20354749,15265328.274596397,15265328.34696751,15265328.42068545,15265328.495775277,15265328.572262516,15265328.650173176,15265328.729533747,15265328.810371207,15265328.892713055,15265328.976587264,15265329.062022373,15265329.149047414,15265329.237691985,15265329.327986214,15265329.41996081,15265329.513647042,15265329.609076757,15265329.706282413,15265329.805297043,15265329.906154325,15265330.008888545,15265330.113534622,15265330.220128154,15265330.328705372,15265330.439303186,15265330.551959204,15265330.66671173,15265330.783599772,15265330.90266308,15265331.02394213,15265331.14747815,15265331.273313146,15265331.401489906,15265331.532051997,15265331.665043814,15265331.800510572,15265331.938498333,15265332.079054004,15265332.222225372,15265332.368061118,15265332.516610822,15265332.667924985,15265332.822055059,15265332.979053436,15265333.1389735,15265333.301869614,15265333.46779717,15265333.636812573,15265333.808973284,15265333.984337838,15265334.162965855,15265334.344918063,15265334.530256322,15265334.719043644,15265334.91134421,15265335.107223397,15265335.306747809,15265335.50998527,15265335.717004871,15265335.927877003,15265336.142673358,15265336.361466954,15265336.584332168,15265336.811344786,15265337.042581974,15265337.278122349,15265337.518045982,15265337.762434443,15265338.01137082,15265338.26493974,15265338.523227403,15265338.786321623,15265339.054311838,15265339.327289162,15265339.605346391,15265339.888578057,15265340.17708044,15265340.470951635,15265340.77029153,15265341.075201888,15265341.385786375,15265341.702150574,15265342.02440203,15265342.352650294,15265342.687006958,15265343.02758568,15265343.374502249,15265343.72787459,15265344.087822838,15265344.454469351,15265344.82793877,15265345.208358057,15265345.595856532,15265345.99056592,15265346.392620394,15265346.802156636,15265347.219313854,15265347.644233871,15265348.077061117,15265348.517942725,15265348.967028566,15265349.424471308,15265349.890426438,15265350.365052344,15265350.84851038,15265351.340964867,15265351.842583219,15265352.353535928,15265352.873996686,15265353.404142408,15265353.944153296,15265354.494212912,15265355.054508228,15265355.62522968,15265356.206571281,15265356.798730621,15265357.401908983,15265358.016311381,15265358.642146664,15265359.279627541,15265359.928970696,15265360.590396833,15265361.264130766,15265361.9504015,15265362.649442278,15265363.3614907,15265364.08678878,15265364.825583026,15265365.578124542,15265366.344669107,15265367.125477238,15265367.920814313,15265368.730950646,15265369.55616157,15265370.396727543,15265371.252934244,15265372.125072658,15265373.013439188,15265373.91833574,15265374.84006985,15265375.778954767,15265376.735309558,15265377.70945923,15265378.701734837,15265379.71247359,15265380.742018966,15265381.790720826,15265382.858935554,15265383.947026141,15265385.055362346,15265386.184320783,15265387.334285097,15265388.505646039,15265389.698801644,15265390.914157342,15265392.15212611,15265393.413128585,15265394.697593242,15265396.00595652,15265397.338662967,15265398.696165401,15265400.07892505,15265401.487411737,15265402.922104005,15265404.38348929,15265405.872064106,15265407.388334176,15265408.932814643,15265410.506030224,15265412.108515382,15265413.740814522,15265415.40348216,15265417.09708314,15265418.822192773,15265420.579397092,15265422.369293015,15265424.192488547,15265426.049603008,15265427.941267205,15265429.868123699,15265431.830826966,15265433.830043666,15265435.866452836,15265437.940746129,15265440.053628065,15265442.205816243,15265444.398041598,15265446.631048659,15265448.90559578,15265451.222455397,15265453.582414314,15265455.98627394,15265458.434850581,15265460.92897571,15265463.46949624,15265466.057274822,15265468.693190128,15265471.37813715,15265474.113027522,15265476.898789786,15265479.736369748,15265482.626730772,15265485.57085412,15265488.569739267,15265491.624404266,15265494.735886062,15265497.905240858,15265501.133544479,15265504.42189271,15265507.771401687,15265511.183208285,15265514.658470456,15265518.198367678,15265521.804101303,15265525.476894995,15265529.217995137,15265533.028671227,15265536.910216345,15265540.86394756,15265544.891206386,15265548.993359238,15265553.17179786,15265557.427939868,15265561.763229143,15265566.17913636,15265570.677159503,15265575.258824307,15265579.925684849,15265584.679324005,15265589.521354018,15265594.453417044,15265599.47718568,15265604.594363544,15265609.806685846,15265615.115919968,15265620.523866076,15265626.032357683,15265631.643262308,15265637.35848209,15265643.17995442,15265649.109652594,15265655.149586482,15265661.301803203,15265667.568387805,15265673.951463962,15265680.453194702,15265687.075783119,15265693.82147311,15265700.692550154,15265707.691342033,15265714.820219658,15265722.081597822,15265729.477936031,15265737.011739349,15265744.685559185,15265752.501994187,15265760.463691093,15265768.573345631,15265776.833703393,15265785.247560794,15265793.817765957,15265802.547219712,15265811.43887654,15265820.49574554,15265829.72089148,15265839.117435781,15265848.688557567,15265858.437494744,15265868.367545044,15265878.482067155,15265888.78448182,15265899.278272983,15265909.966988944,15265920.854243545,15265931.943717359,15265943.239158938,15265954.744386017,15265966.463286828,15265978.399821354,15265990.558022656,15266002.941998206,15266015.555931259,15266028.404082222,15266041.490790086,15266054.82047383,15266068.397633912,15266082.226853726,15266096.312801167,15266110.660230104,15266125.273982001,15266140.158987494,15266155.320268018,15266170.762937445,15266186.49220381,15266202.51337097,15266218.831840394,15266235.453112915,15266252.382790525,15266269.626578234,15266287.19028594,15266305.079830332,15266323.301236795,15266341.860641433,15266360.764293049,15266380.01855517,15266399.629908169,15266419.60495133,15266439.950405031,15266460.673112933,15266481.780044178,15266503.278295698,15266525.175094482,15266547.477799935,15266570.193906283,15266593.33104499,15266616.896987204,15266640.899646303,15266665.34708045,15266690.247495165,15266715.609246012,15266741.440841265,15266767.750944637,15266794.54837811,15266821.842124721,15266849.64133145,15266877.955312189,15266906.793550687,15266936.165703591,15266966.081603536,15266996.551262284,15267027.584873887,15267059.192817977,15267091.385663027,15267124.174169715,15267157.569294345,15267191.582192296,15267226.224221569,15267261.506946366,15267297.442140725,15267334.041792221,15267371.318105781,15267409.283507444,15267447.95064831,15267487.332408464,15267527.441901019,15267568.292476196,15267609.897725487,15267652.27148586,15267695.427844085,15267739.381141063,15267784.14597627,15267829.737212282,15267876.169979317,15267923.459679913,15267971.621993642,15268020.672881912,15268070.628592843,15268121.50566623,15268173.320938554,15268226.09154812,15268279.834940236,15268334.568872472,15268390.311420042,15268447.08098123,15268504.896282906,15268563.776386123,15268623.740691848,15268684.8089467,15268747.00124884,15268810.338053916,15268874.840181107,15268940.528819265,15269007.425533157,15269075.552269725,15269144.93136458,15269215.585548436,15269287.537953755,15269360.812121427,15269435.432007547,15269511.421990348,15269588.806877138,15269667.611911405,15269747.862780038,15269829.585620537,15269912.80702848,15269997.554064946,15270083.854264153,15270171.735641114,15270261.226699458,15270352.356439317,15270445.154365335,15270539.650494765,15270635.87536571,15270733.860045405,15270833.636138672,15270935.235796435,15271038.691724358,15271144.037191594,15271251.306039639,15271360.532691274,15271471.752159635,15271585.000057386,15271700.312605994,15271817.726645112,15271937.279642053,15272059.009701394,15272182.95557469,15272309.156670235,15272437.653063016,15272568.485504678,15272701.695433682,15272837.324985465,15272975.417002793,15273116.015046159,15273259.163404305,15273404.907104783,15273553.291924728,15273704.364401596,15273858.171844054,15274014.762343008,15274174.1847826,15274336.488851385,15274501.725053579,15274669.944720345,15274841.200021185,15275015.543975433,15275193.03046375,15275373.714239769,15275557.650941746,15275744.897104282,15275935.510170138,15276129.548502073,15276327.071394714,15276528.139086528,15276732.81277178,15276941.154612562,15277153.227750821,15277369.09632044,15277588.825459324,15277812.481321506,15278040.131089248,15278271.842985153,15278507.686284276,15278747.731326187,15278992.04952706,15279240.713391723,15279493.796525616,15279751.373646792,15280013.520597778,15280280.31435746,15280551.833052795,15280828.155970525,15281109.363568768,15281395.53748847,15281686.760564817,15281983.11683845,15282284.691566573,15282591.571233924,15282903.843563545,15283221.597527413,15283544.923356859,15283873.912552798,15284208.657895697,15284549.25345541,15284895.794600638,15285248.378008205,15285607.101672063,15285972.064911915,15286343.368381612,15286721.11407718,15287105.405344496,15287496.34688657,15287894.04477049,15288298.606433934,15288710.140691211,15289128.757738914,15289554.569161072,15289987.687933782,15290428.22842938,15290876.30642003,15291332.039080754,15291795.544991875,15292266.944140878,15292746.357923584,15293233.909144687,15293729.722017622,15294233.922163673,15294746.636610363,15295267.9937891,15295798.12353195,15296337.157067684,15296885.227016883,15297442.46738621,15298009.013561768,15298585.00230147,15299170.571726497,15299765.861311724,15300371.01187511,15300986.165566022,15301611.465852493,15302247.05750732,15302893.08659303,15303549.700445632,15304217.047657197,15304895.278057134,15305584.542692209,15306284.993805286,15306996.784812657,15307720.070280068,15308455.00589729,15309201.748451296,15309960.455797944,15310731.286832217,15311514.401456898,15312309.960549718,15313118.125928946,15313939.060317358,15314772.92730461,15315619.891307954,15316480.117531272,15317353.771922452,15318241.021129036,15319142.032452133,15320056.973798621,15320986.013631547,15321929.320918793,15322887.065079948,15323859.415931348,15324846.543629395,15325848.618611993,15326865.811538218,15327898.293226162,15328946.234589,15330009.806569215,15331089.180071054,15332184.525891216,15333296.014647746,15334423.816707207,15335568.1021101,15336729.040494606,15337906.801018642,15339101.55228027,15340313.462236537,15341542.698120695,15342789.426357958,15344053.812479757,15345336.021036573,15346636.215509476,15347954.55822026,15349291.21024048,15350646.331299206,15352020.07968983,15353412.612175778,15354824.083895369,15356254.648265848,15357704.456886714,15359173.659442447,15360662.4036047,15362170.834934112,15363699.096781937,15365247.330191398,15366815.673799139,15368404.263736736,15370013.23353253,15371642.714013817,15373292.833209645,15374963.716254313,15376655.485291736,15378368.259380886,15380102.15440244,15381857.2829668,15383633.75432371,15385431.6742736,15387251.14508091,15389092.265389469,15390955.130140288,15392839.8304918,15394746.45374284,15396675.083258547,15398625.798399374,15400598.674453417,15402593.782572292,15404611.189710714,15406650.958570031,15408713.147545898,15410797.810680289,15412904.997618053,15415034.753568254,15417187.119270388,15419362.13096582,15421559.820374513,15423780.214677269,15426023.336503698,15428289.203926064,15430577.830459177,15432889.225066518,15435223.392172717,15437580.331682574,15439960.039006758,15442362.505094305,15444787.71647201,15447235.655290922,15449706.299379926,15452199.62230658,15454715.5934453,15457254.178052867,15459815.337351428,15462399.028618958,15465005.205287242,15467633.81704736,15470284.80996272,15472958.126589602,15475653.70610519,15478371.484443005,15481111.394435761,15483873.365965504,15486657.326120956,15489463.199361958,15492290.907690903,15495140.37083101,15498011.50641125,15500904.23015783,15503818.456091948,15506754.096733714,15509711.063311927,15512689.265979573,15515688.614034688,15518709.016146433,15521750.380585993,15524812.615462126,15527895.628960975,15530999.329589844,15534123.626424672,15537268.429360783,15540433.649366606,15543619.198740056,15546824.99136712,15550050.942982355,15553296.971430857,15556562.996931402,15559848.942340272,15563154.733415438,15566480.299080718,15569825.57168943,15573190.487287253,15576574.985873794,15579979.011662565,15583402.513338873,15586845.444315324,15590307.762984522,15593789.432968507,15597290.42336472,15600810.70898798,15604350.270608153,15607909.09518323,15611487.176087355,15615084.513333615,15618701.113791117,15622336.991396194,15625992.167357335,15629666.670353645,15633360.536726505,15637073.810664225,15640806.544379449,15644558.798279092,15648330.641126594,15652122.150196346,15655933.41142009,15659764.519525161,15663615.578164442,15667486.700037902,15671378.007005684,15675289.630192561,15679221.710083833,15683174.396612525,15687147.84923792,15691142.237015417,15695157.738657705,15699194.542587334,15703252.846980698,15707332.859803481,15711434.798837725,15715558.891700517,15719705.375854516,15723874.498610336,15728066.517121041,15732281.698368805,15736520.319143994,15740782.666016763,15745069.035301441,15749379.733013837,15753715.074821694,15758075.385988548,15762461.001311125,15766872.265050609,15771309.53085791,15775773.161693264,15780263.529740326,15784781.01631504,15789326.011769494,15793898.915391056,15798500.135296948,15803130.088324552,15807789.19991767,15812477.904008923,15817196.642898576,15821945.86712992,15826726.035361474,15831537.614236206,15836381.07824785,15841256.90960467,15846165.598090652,15851107.640924392,15856083.542615738,15861093.814820342,15866138.976192225,15871219.552234402,15876336.075147683,15881489.083677672,15886679.122959984,15891906.74436376,15897172.505333386,15902476.969228456,15907820.705161944,15913204.287836462,15918628.297378616,15924093.31917127,15929599.943683682,15935148.766299335,15940740.387141317,15946375.410895102,15952054.446628528,15957778.107608788,15963547.011116214,15969361.778254623,15975223.033757992,15981131.405793214,15987087.525758663,15993092.02807831,15999145.54999109,16005248.731335279,16011402.214327533,16017606.643336365,16023862.664649716,16030170.926236328,16036532.0775007,16042946.769031234,16049415.6523414,16055939.379603557,16062518.603375228,16069153.97631757,16075846.150905794,16082595.779131303,16089403.512195364,16096270.00019414,16103195.891794894,16110181.833903214,16117228.4713212,16124336.446396466,16131506.398661887,16138738.964466128,16146034.776594844,16153394.463882662,16160818.650815947,16168307.957126468,16175862.997376045,16183484.380532378,16191172.70953621,16198928.58086002,16206752.584058585,16214645.301311556,16222607.306958506,16230639.167026717,16238741.438752085,16246914.670093577,16255159.399241693,16263476.154121352,16271865.451889725,16280327.798429517,16288863.687838279,16297473.601914212,16306158.009639187,16314917.366659412,16323752.1147645,16332662.68136546,16341649.478972297,16350712.904671831,16359853.339606376,16369071.148453962,16378366.678910678,16387740.261175822,16397192.207440486,16406722.811380195,16416332.347652173,16426021.071397953,16435789.217751743,16445637.001355292,16455564.615879677,16465572.233554577,16475660.004705576,16485828.057299903,16496076.496501112,16506405.404233096,16516814.838753862,16527304.834239397,16537875.400377996,16548526.52197535,16559258.158570642,16570070.244063983,16580962.68635531,16591935.366995025,16602988.140846476,16614120.835760498,16625333.252262022,16636625.163249029,16647996.313703684,16659446.420415962,16670975.171719603,16682582.22724058,16694267.217657989,16706029.744477456,16717869.379817016,16729785.6662055,16741778.116393419,16753846.213176385,16765989.40923104,16778207.126963582,16790498.758370865,16802863.664914202,16815301.177405827,16827810.595908333,16840391.189646892,16853042.196934715,16865762.825111695,16878552.250496555,16891409.618352715,16904334.042868067,16917324.60714909,16930380.3632295,16943500.332093854,16956683.50371658,16969928.8371167,16983235.26042883,16996601.67099098,17010026.935449522,17023509.88988205,17037049.339938615,17050644.06100194,17064292.7983673,17077994.267442636,17091747.15396959,17105550.114266142,17119401.775491487,17133300.73593386,17147245.565321952,17161234.80516061,17175266.96909145,17189340.543279037,17203453.98682327,17217605.732198473,17231794.18571992,17246017.72803807,17260274.71466127,17274563.47650716,17288882.3204832,17303229.530096736,17317603.36609473,17332002.067133505,17346423.850478493,17360866.91273414,17375329.430603933,17389809.561680388,17404305.445264902,17418815.203217164,17433336.940833747,17447868.74775548,17462408.698903047,17476954.855440196,17491505.265763815,17506057.966520134,17520610.983646065,17535162.333434783,17549710.023624413,17564252.054508664,17578786.420068245,17593311.109121606,17607824.106493708,17622323.394201275,17636806.952653095,17651272.761863537,17665718.80267788,17680143.058007512,17694543.514073282,17708918.16165529,17723264.997347,17737582.02481206,17751867.25604168,17766118.712610714,17780334.426930543,17794512.443496585,17808650.820128713,17822747.62920243,17836800.958868824,17850808.914261587,17864769.61868888,17878681.214808356,17892541.8657834,17906349.756418798,17920103.09427408,17933800.110752773,17947439.06216591,17961018.23076827,17974535.925765723,17987990.484292235,18001380.2723552,18014703.685747713,18027959.15092662,18041145.125855096,18054260.100808825,18067302.599144667,18080271.17803103,18093164.42913909,18105980.979294155,18118719.49108665,18131378.663442034,18143957.23214939,18156453.97034826,18168867.688973505,18181197.237158034,18193441.50259334,18205599.41184788,18217669.93064338,18229652.06408927,18241544.85687554,18253347.393424336,18265058.798000783,18276678.234783433,18288204.90789505,18299638.061394237,18310976.979228664,18322220.985150732,18333369.442596328,18344421.754527673,18355377.363241166,18366235.750141107,18376996.435480423,18387658.97806934,18398222.974953163,18408688.06106018,18419053.908820935,18429320.22775995,18439486.76406109,18449553.300107785,18459519.65399932,18469385.679044392,18479151.263233133,18488816.32868889,18498380.831100978,18507844.759139538,18517208.13385384,18526471.00805518,18535633.465685576,18544695.62117344,18553657.618777443,18562519.631919652,18571281.862509172,18579944.5402573,18588507.921985365,18596972.290926285,18605337.956020877,18613605.251209952,18621774.53472321,18629846.18836576,18637820.616803415,18645698.246847436,18653479.52673972,18661164.92543926],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[9488373.953982214,9488373.963659437,9488373.973516766,9488373.983557543,9488373.99378519,9488374.004203178,9488374.014815055,9488374.025624428,9488374.036634969,9488374.047850428,9488374.059274618,9488374.070911419,9488374.082764791,9488374.094838765,9488374.107137445,9488374.119665017,9488374.13242574,9488374.145423949,9488374.158664068,9488374.172150599,9488374.185888125,9488374.199881323,9488374.214134948,9488374.228653844,9488374.243442956,9488374.258507306,9488374.273852022,9488374.28948232,9488374.305403518,9488374.321621021,9488374.338140354,9488374.35496713,9488374.37210707,9488374.389566008,9488374.407349877,9488374.425464725,9488374.443916716,9488374.462712118,9488374.481857328,9488374.501358857,9488374.521223335,9488374.541457517,9488374.562068284,9488374.58306265,9488374.60444775,9488374.626230856,9488374.648419378,9488374.671020862,9488374.694042996,9488374.717493605,9488374.741380667,9488374.765712308,9488374.790496796,9488374.815742569,9488374.841458207,9488374.867652455,9488374.894334223,9488374.921512589,9488374.949196791,9488374.977396248,9488375.006120548,9488375.035379464,9488375.065182943,9488375.095541121,9488375.126464328,9488375.157963077,9488375.190048084,9488375.222730258,9488375.256020717,9488375.289930785,9488375.324471993,9488375.359656092,9488375.395495044,9488375.432001045,9488375.469186505,9488375.507064078,9488375.545646645,9488375.584947327,9488375.624979498,9488375.665756768,9488375.70729301,9488375.74960235,9488375.792699184,9488375.836598167,9488375.881314233,9488375.926862594,9488375.97325874,9488376.020518454,9488376.068657815,9488376.117693197,9488376.167641276,9488376.218519052,9488376.270343823,9488376.323133225,9488376.376905214,9488376.431678079,9488376.487470457,9488376.544301327,9488376.602190025,9488376.66115624,9488376.721220035,9488376.782401841,9488376.844722478,9488376.908203144,9488376.972865438,9488377.038731357,9488377.105823312,9488377.17416413,9488377.243777063,9488377.314685792,9488377.386914445,9488377.460487599,9488377.535430286,9488377.611768005,9488377.689526731,9488377.768732924,9488377.849413533,9488377.931596007,9488378.015308319,9488378.10057895,9488378.187436916,9488378.275911774,9488378.366033636,9488378.457833165,9488378.551341606,9488378.646590775,9488378.743613092,9488378.842441572,9488378.943109853,9488379.045652194,9488379.150103495,9488379.256499304,9488379.364875836,9488379.475269973,9488379.587719297,9488379.702262077,9488379.818937302,9488379.937784689,9488380.05884469,9488380.182158515,9488380.30776814,9488380.435716327,9488380.56604663,9488380.69880342,9488380.834031885,9488380.971778074,9488381.112088878,9488381.255012065,9488381.4005963,9488381.54889115,9488381.699947108,9488381.853815608,9488382.010549042,9488382.17020078,9488382.332825191,9488382.49847765,9488382.667214569,9488382.839093415,9488383.014172724,9488383.19251212,9488383.374172345,9488383.559215266,9488383.74770392,9488383.9397025,9488384.135276413,9488384.334492275,9488384.537417952,9488384.744122569,9488384.95467655,9488385.169151628,9488385.387620872,9488385.610158717,9488385.83684099,9488386.067744931,9488386.302949218,9488386.542534005,9488386.786580933,9488387.035173178,9488387.288395455,9488387.546334077,9488387.809076952,9488388.076713644,9488388.349335378,9488388.627035089,9488388.909907447,9488389.198048886,9488389.491557643,9488389.790533796,9488390.095079284,9488390.405297952,9488390.721295591,9488391.043179963,9488391.371060843,9488391.70505006,9488392.045261534,9488392.391811313,9488392.744817609,9488393.104400849,9488393.470683716,9488393.843791174,9488394.223850533,9488394.610991482,9488395.005346134,9488395.407049071,9488395.816237394,9488396.233050771,9488396.657631483,9488397.090124464,9488397.530677369,9488397.979440615,9488398.436567433,9488398.902213918,9488399.37653909,9488399.859704953,9488400.351876525,9488400.853221938,9488401.363912454,9488401.884122552,9488402.414029974,9488402.9538158,9488403.503664488,9488404.063763969,9488404.634305682,9488405.215484664,9488405.807499602,9488406.410552911,9488407.0248508,9488407.650603347,9488408.288024565,9488408.937332487,9488409.59874923,9488410.27250109,9488410.958818598,9488411.657936614,9488412.370094415,9488413.09553576,9488413.834508993,9488414.587267116,9488415.35406789,9488416.135173913,9488416.930852722,9488417.741376873,9488418.56702406,9488419.408077184,9488420.264824476,9488421.137559582,9488422.02658167,9488422.932195542,9488423.854711737,9488424.794446634,9488425.751722572,9488426.726867959,9488427.720217384,9488428.732111748,9488429.762898369,9488430.812931107,9488431.882570507,9488432.972183898,9488434.082145546,9488435.21283677,9488436.36464609,9488437.537969356,9488438.733209895,9488439.950778646,9488441.191094307,9488442.454583498,9488443.741680894,9488445.052829383,9488446.388480237,9488447.74909326,9488449.135136954,9488450.54708869,9488451.985434875,9488453.45067113,9488454.94330246,9488456.463843442,9488458.012818413,9488459.590761641,9488461.198217535,9488462.835740838,9488464.503896812,9488466.20326146,9488467.934421722,9488469.697975697,9488471.494532844,9488473.324714217,9488475.18915269,9488477.088493165,9488479.023392836,9488480.994521406,9488483.002561342,9488485.048208121,9488487.132170476,9488489.255170668,9488491.417944739,9488493.621242786,9488495.865829235,9488498.152483122,9488500.481998384,9488502.855184134,9488505.27286498,9488507.73588132,9488510.245089648,9488512.801362874,9488515.405590659,9488518.05867972,9488520.76155419,9488523.515155952,9488526.32044499,9488529.178399745,9488532.09001749,9488535.05631469,9488538.078327395,9488541.157111619,9488544.293743748,9488547.489320932,9488550.744961519,9488554.061805451,9488557.441014718,9488560.88377379,9488564.391290067,9488567.96479434,9488571.605541259,9488575.314809812,9488579.093903823,9488582.944152435,9488586.866910636,9488590.863559775,9488594.9355081,9488599.08419129,9488603.311073026,9488607.617645543,9488612.00543023,9488616.47597821,9488621.030870948,9488625.671720868,9488630.40017199,9488635.217900582,9488640.126615806,9488645.1280604,9488650.224011378,9488655.416280722,9488660.706716103,9488666.09720163,9488671.589658592,9488677.186046228,9488682.888362527,9488688.698645009,9488694.618971579,9488700.651461337,9488706.798275461,9488713.061618075,9488719.443737151,9488725.946925428,9488732.573521357,9488739.325910054,9488746.206524298,9488753.217845518,9488760.362404846,9488767.642784143,9488775.0616171,9488782.621590333,9488790.3254445,9488798.175975464,9488806.17603548,9488814.328534389,9488822.636440856,9488831.102783652,9488839.730652919,9488848.523201525,9488857.483646404,9488866.615269937,9488875.9214214,9488885.405518383,9488895.071048306,9488904.921569936,9488914.96071494,9488925.1921895,9488935.619775925,9488946.247334352,9488957.07880444,9488968.118207151,9488979.369646525,9488990.837311538,9489002.525477992,9489014.438510437,9489026.580864163,9489038.957087224,9489051.571822524,9489064.429809935,9489077.535888493,9489090.89499862,9489104.51218443,9489118.392596073,9489132.541492132,9489146.964242099,9489161.666328898,9489176.65335148,9489191.93102747,9489207.505195897,9489223.381819975,9489239.56698997,9489256.06692613,9489272.887981683,9489290.03664593,9489307.519547394,9489325.343457064,9489343.515291715,9489362.042117313,9489380.931152506,9489400.189772207,9489419.82551127,9489439.84606824,9489460.259309242,9489481.073271912,9489502.296169486,9489523.93639495,9489546.002525326,9489568.503326045,9489591.44775545,9489614.844969412,9489638.704326063,9489663.035390632,9489687.84794046,9489713.151970074,9489738.957696447,9489765.275564369,9489792.116251966,9489819.49067636,9489847.409999477,9489875.885633994,9489904.929249462,9489934.552778572,9489964.76842359,9489995.588662961,9490027.026258072,9490059.094260214,9490091.8060177,9490125.175183188,9490159.215721173,9490193.941915708,9490229.368378282,9490265.510055939,9490302.38223958,9490340.000572514,9490378.381059181,9490417.54007416,9490457.494371353,9490498.261093447,9490539.857781604,9490582.302385403,9490625.613273026,9490669.809241738,9490714.909528589,9490760.933821438,9490807.902270226,9490855.835498543,9490904.754615506,9490954.681227913,9491005.637452725,9491057.64592986,9491110.729835296,9491164.912894525,9491220.21939632,9491276.674206879,9491334.302784277,9491393.131193321,9491453.186120747,9491514.494890787,9491577.085481144,9491640.986539342,9491706.22739946,9491772.838099292,9491840.849397931,9491910.292793736,9491981.200542767,9492053.605677636,9492127.542026822,9492203.0442344,9492280.147780297,9492358.889000952,9492439.30511049,9492521.434222378,9492605.315371547,9492690.988537066,9492778.494665252,9492867.875693362,9492959.174573774,9493052.435298683,9493147.702925362,9493245.023601951,9493344.444593785,9493446.014310295,9493549.782332439,9493655.799440734,9493764.117643815,9493874.790207606,9493987.871685047,9494103.417946393,9494221.486210132,9494342.135074444,9494465.424549287,9494591.416089045,9494720.17262578,9494851.75860308,9494986.240010472,9495123.684418453,9495264.16101408,9495407.74063716,9495554.495817013,9495704.500809796,9495857.831636414,9496014.566120977,9496174.783929812,9496338.566611007,9496505.997634508,9496677.162432706,9496852.148441551,9497031.045142146,9497213.944102818,9497400.939021643,9497592.125769405,9497787.602432977,9497987.469359094,9498191.829198478,9498400.786950331,9498614.45000712,9498832.928199653,9499056.333842402,9499284.78177905,9499518.389428208,9499757.276829273,9500001.566688403,9500251.38442452,9500506.858215356,9500768.119043432,9501035.300741978,9501308.54004069,9501587.976611312,9501873.753112951,9502166.015237086,9502464.911752192,9502770.594547948,9503083.218678897,9503402.942407563,9503729.927246887,9504064.33800195,9504406.342810884,9504756.1131849,9505113.824047318,9505479.653771585,9505853.784218099,9506236.40076984,9506627.692366628,9507027.851537986,9507437.074434483,9507855.56085744,9508283.514286904,9508721.14190785,9509168.65463439,9509626.267131988,9510094.19783753,9510572.668977128,9511061.906581605,9511562.140499461,9512073.604407324,9512596.535817666,9513131.176083775,9513677.770401776,9514236.56780969,9514807.821183352,9515391.787229113,9515988.726473235,9516598.903247833,9517222.585673336,9517860.045637293,9518511.558769498,9519177.404413301,9519857.865593065,9520553.228977611,9521263.784839718,9521989.827011416,9522731.652835218,9523489.563111054,9524263.862038998,9525054.85715764,9525862.85927816,9526688.182413995,9527531.143706175,9528392.063344218,9529271.264482705,9530169.073153416,9531085.8181732,9532021.831047472,9532977.44586949,9533952.999215433,9534948.830035327,9535965.279539958,9537002.691083822,9538061.410044268,9539141.783696894,9540244.161087407,9541368.89290007,9542516.331322862,9543686.829909613,9544880.74343923,9546098.427772276,9547340.239705099,9548606.536821747,9549897.67734394,9551214.019979341,9552555.9237684,9553923.747930082,9555317.851706775,9556738.594208665,9558186.33425794,9559661.43023313,9561164.239913959,9562695.120327005,9564254.427592624,9565842.516773388,9567459.74172456,9569106.45494682,9570783.00744178,9572489.748570587,9574227.025916053,9575995.185148638,9577794.569896799,9579625.521621952,9581488.379498556,9583383.480299627,9585311.158288104,9587271.745114403,9589265.569720559,9591292.958251284,9593354.23397228,9595449.717196155,9597579.72521625,9599744.572248671,9601944.569382794,9604180.024540555,9606451.242444707,9608758.524596341,9611102.169261774,9613482.471469108,9615899.723014487,9618354.212478291,9620846.225251291,9623376.043570887,9625943.946567468,9628550.210320894,9631195.107927138,9633878.909574991,9636601.882632816,9639364.291745225,9642166.398939556,9645008.463742,9647890.74330319,9650813.492533036,9653776.96424453,9656781.409306312,9659827.076803623,9662914.21420736,9666043.067550851,9669213.881613972,9672426.9001142,9675682.365904171,9678980.52117527,9682321.607666792,9685705.86688016,9689133.540297698,9692604.869605422,9696120.09691928,9699679.465014298,9703283.217556046,9706931.599333838,9710624.856495056,9714363.236780033,9718146.98975682,9721976.367055314,9725851.62260003,9729773.012840992,9733740.796982089,9737755.237206277,9741816.598897068,9745925.150855673,9750081.165513227,9754284.919137515,9758536.692033684,9762836.768738288,9767185.438206282,9771582.993990298,9776029.734411836,9780525.962723793,9785071.987263981,9789668.121599076,9794314.684658716,9799012.000859264,9803760.400216958,9808560.218450038,9813411.797069613,9818315.483458951,9823271.630940948,9828280.598833565,9833342.752493035,9838458.463344658,9843628.108901082,9848852.07276794,9854130.744636763,9859464.52026514,9864853.801444085,9870298.995952632,9875800.517499678,9881358.785653152,9886974.225756591,9892647.268833235,9898378.35147778,9904167.91573597,9910016.408972172,9915924.2837252,9921891.997552568,9927920.012863444,9934008.796740575,9940158.82075147,9946370.560749134,9952644.496662669,9958981.11227811,9965380.895009775,9971844.33566256,9978371.928185487,9984964.169416923,9991621.558821794,9998344.598221267,10005133.791515216,10011989.644397901,10018912.664067257,10025903.358928187,10032962.23829026,10040089.812060194,10047286.590429556,10054553.083557986,10061889.801252447,10069297.252642754,10076775.945853826,10084326.387675002,10091949.083226755,10099644.53562517,10107413.245644478,10115255.711378012,10123172.427897826,10131163.886913344,10139230.57642924,10147372.980402872,10155591.578401478,10163886.84525944,10172259.25073571,10180709.259171786,10189237.329150245,10197843.913154168,10206529.457227532,10215294.400636768,10224139.175533615,10233064.206619399,10242069.910810862,10251156.696907658,10260324.965261595,10269575.107447745,10278907.505937483,10288322.533773525,10297820.554247074,10307401.920577098,10317066.975591833,10326816.05141252,10336649.469139539,10346567.53854085,10356570.557742933,10366658.8129242,10376832.578010978,10387092.114376128,10397437.670540322,10407869.481876146,10418387.770314997,10428992.74405695,10439684.59728364,10450463.50987429,10461329.647124942,10472283.15947112,10483324.182213893,10494452.835249651,10505669.222803582,10516973.433167119,10528365.538439479,10539845.594273418,10551413.639625499,10563069.696510933,10574813.769763278,10586645.846799137,10598565.89738811,10610573.873428153,10622669.708726605,10634853.31878706,10647124.6006023,10659483.43245351,10671929.673715958,10684463.1646714,10697083.726327278,10709791.16024307,10722585.24836379,10735465.75286096,10748432.41598108,10761484.959901823,10774623.086595997,10787846.477703469,10801154.794411037,10814547.677340398,10828024.746444175,10841585.600910097,10855229.819073288,10868956.958336616,10882766.5550991,10896658.124692263,10910631.161324324,10924685.138032116,10938819.506640574,10953033.697729586,10967327.120608056,10981699.163294872,10996149.192506637,11010676.553651778,11025280.570830837,11039960.546842579,11054715.763195623,11069545.48012526,11084448.936615087,11099425.350423148,11114473.918112174,11129593.815083584,11144784.19561492,11160044.192900274,11175372.919093436,11190769.465353401,11206232.901891893,11221762.278022604,11237356.622211892,11253014.942130629,11268736.224706996,11284519.436180018,11300363.522153663,11316267.407651376,11332229.99717098,11348250.17473984,11364326.803970357,11380458.728115812,11396644.77012663,11412883.732707266,11429174.398373868,11445515.52951302,11461905.868441839,11478344.137469826,11494829.038962878,11511359.25540997,11527933.449493002,11544550.264160454,11561208.322705464,11577906.228849025,11594642.566829069,11611415.901496207,11628224.778416976,11645067.723985467,11661943.24554423,11678849.831515422,11695785.951543167,11712750.056648104,11729740.579395164,11746755.934075572,11763794.516904127,11780854.70623282,11797934.862781746,11815033.329888415,11832148.433776427,11849278.483844472,11866421.772976685,11883576.577875184,11900741.159415742,11917913.76302741,11935092.61909682,11952275.943398004,11969461.937548269,11986648.789490802,12003834.674004491,12021017.753241392,12038196.177292258,12055368.084780307,12072531.603483496,12089684.850985367,12106825.93535438,12123952.955851797,12141064.003667708,12158157.162685053,12175230.51027113,12192282.11809612,12209310.052978035,12226312.377753295,12243287.152172184,12260232.433818258,12277146.27905066,12294026.743968256,12310871.885394396,12327679.761880986,12344448.434730548,12361175.969034746,12377860.43472792,12394499.907654002,12411092.470645139,12427636.214610336,12444129.239632325,12460569.656070845,12476955.585670497,12493285.162671236,12509556.53491963,12525767.86497891,12541917.331235878,12558003.129002713,12574023.471611695,12589976.59150092,12605860.741289053,12621674.194837196,12637415.248295987,12653082.22113607,12668673.457160093,12684187.325494487,12699622.221559288,12714976.56801431,12730248.815680081,12745437.444431983,12760540.964066127,12775557.915135548,12790486.869755397,12805326.432375867,12820075.240521729,12834731.96549735,12849295.313056227,12863764.024034126,12878136.874945043,12892412.678539202,12906590.284322532,12920668.579037065,12934646.487101797,12948522.971013673,12962297.031708447,12975967.708881266,12989534.081266852,13002995.266879337,13016350.423211835,13029598.747395927,13042739.476321327,13055771.886716072,13068695.295187641,13081509.058225513,13094212.572165692,13106805.273117887,13119286.636855949,13131656.178672401,13143913.453197816,13156058.054185906,13168089.614265261,13180007.80465866,13191812.334870933,13203502.952346481,13215079.442097425,13226541.626303546,13237889.363885095,13249122.550049676,13260241.115814269,13271245.027503697,13282134.286226617,13292908.927330298,13303569.019835388,13314114.665851846,13324545.99997728,13334863.188678889,13345066.429660209,13355155.951213846,13365132.01156137,13374994.898181561,13384744.927128153,13394382.442338161,13403907.814931994,13413321.442506352,13422623.74842104,13431815.181080718,13440896.213212615,13449867.341141175,13458729.08406064,13467481.983306454,13476126.601626433,13484663.52245256,13493093.349174218,13501416.704413723,13509634.229304891],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[9567955.427629244,9567955.456303697,9567955.485511787,9567955.515263446,9567955.545568788,9567955.576438116,9567955.607881928,9567955.639910912,9567955.672535963,9567955.70576817,9567955.739618834,9567955.774099464,9567955.809221782,9567955.844997734,9567955.88143948,9567955.918559415,9567955.956370156,9567955.994884562,9567956.034115724,9567956.074076986,9567956.114781931,9567956.156244406,9567956.198478501,9567956.241498578,9567956.285319267,9567956.329955464,9567956.375422347,9567956.421735378,9567956.468910297,9567956.516963152,9567956.565910276,9567956.615768315,9567956.66655422,9567956.718285255,9567956.770979013,9567956.82465341,9567956.879326694,9567956.935017457,9567956.991744634,9567957.049527511,9567957.108385736,9567957.16833932,9567957.229408648,9567957.291614486,9567957.354977986,9567957.419520685,9567957.485264534,9567957.552231887,9567957.62044551,9567957.689928599,9567957.760704778,9567957.832798112,9567957.906233111,9567957.981034745,9567958.057228446,9567958.134840125,9567958.213896168,9567958.294423454,9567958.376449361,9567958.460001783,9567958.545109127,9567958.63180033,9567958.720104866,9567958.81005276,9567958.901674598,9567958.995001528,9567959.090065287,9567959.18689819,9567959.285533171,9567959.386003757,9567959.488344116,9567959.592589037,9567959.698773973,9567959.806935022,9567959.91710896,9567960.02933325,9567960.143646045,9567960.260086216,9567960.378693352,9567960.49950778,9567960.622570578,9567960.747923588,9567960.87560943,9567961.005671524,9567961.138154084,9567961.27310216,9567961.410561632,9567961.55057924,9567961.693202596,9567961.838480178,9567961.986461397,9567962.13719656,9567962.290736921,9567962.44713468,9567962.606443018,9567962.768716099,9567962.934009096,9567963.102378216,9567963.273880694,9567963.44857485,9567963.626520082,9567963.807776889,9567963.9924069,9567964.180472892,9567964.372038804,9567964.567169778,9567964.76593215,9567964.968393508,9567965.174622687,9567965.384689804,9567965.598666286,9567965.816624885,9567966.03863971,9567966.264786245,9567966.49514138,9567966.729783438,9567966.9687922,9567967.212248929,9567967.460236402,9567967.712838933,9567967.97014241,9567968.232234316,9567968.499203768,9567968.771141531,9567969.04814007,9567969.33029356,9567969.617697937,9567969.91045092,9567970.208652047,9567970.512402702,9567970.821806168,9567971.136967639,9567971.457994273,9567971.784995217,9567972.118081657,9567972.457366835,9567972.802966116,9567973.154997004,9567973.513579184,9567973.878834583,9567974.25088738,9567974.629864082,9567975.015893534,9567975.409106992,9567975.809638144,9567976.217623176,9567976.633200798,9567977.05651231,9567977.487701636,9567977.926915383,9567978.374302877,9567978.830016237,9567979.2942104,9567979.7670432,9567980.248675387,9567980.739270722,9567981.23899601,9567981.748021148,9567982.266519213,9567982.794666486,9567983.332642538,9567983.88063028,9567984.438816024,9567985.007389555,9567985.586544178,9567986.176476816,9567986.777388033,9567987.389482139,9567988.012967242,9567988.648055328,9567989.29496232,9567989.953908164,9567990.625116898,9567991.308816727,9567992.005240107,9567992.714623813,9567993.437209034,9567994.17324144,9567994.92297128,9567995.686653456,9567996.464547612,9567997.256918222,9567998.06403469,9567998.886171421,9567999.72360794,9568000.576628963,9568001.44552451,9568002.330589991,9568003.232126324,9568004.150440015,9568005.085843284,9568006.03865415,9568007.00919656,9568007.997800484,9568009.004802033,9568010.030543573,9568011.075373841,9568012.13964806,9568013.223728068,9568014.327982433,9568015.452786582,9568016.598522931,9568017.765581006,9568018.954357585,9568020.165256828,9568021.39869042,9568022.6550777,9568023.934845813,9568025.238429854,9568026.566273011,9568027.918826718,9568029.296550814,9568030.699913692,9568032.129392454,9568033.585473096,9568035.068650639,9568036.579429323,9568038.118322778,9568039.685854178,9568041.282556439,9568042.90897239,9568044.565654961,9568046.25316738,9568047.972083338,9568049.722987214,9568051.506474255,9568053.32315078,9568055.173634397,9568057.058554202,9568058.978551,9568060.934277516,9568062.926398618,9568064.955591546,9568067.022546139,9568069.12796508,9568071.272564122,9568073.457072325,9568075.68223233,9568077.948800584,9568080.257547615,9568082.609258289,9568085.004732061,9568087.444783278,9568089.930241428,9568092.461951435,9568095.040773943,9568097.667585606,9568100.343279395,9568103.068764888,9568105.844968585,9568108.672834232,9568111.553323124,9568114.487414446,9568117.4761056,9568120.520412538,9568123.62137012,9568126.78003246,9568129.997473275,9568133.274786266,9568136.613085473,9568140.013505667,9568143.477202732,9568147.005354045,9568150.5991589,9568154.259838892,9568157.988638343,9568161.786824726,9568165.655689092,9568169.596546507,9568173.610736504,9568177.699623529,9568181.864597416,9568186.10707385,9568190.428494852,9568194.83032926,9568199.314073246,9568203.88125081,9568208.533414291,9568213.272144917,9568218.099053318,9568223.015780095,9568228.023996342,9568233.125404263,9568238.32173771,9568243.614762785,9568249.00627844,9568254.498117087,9568260.092145223,9568265.790264055,9568271.594410151,9568277.506556107,9568283.528711192,9568289.66292205,9568295.911273392,9568302.275888702,9568308.75893094,9568315.362603314,9568322.089149987,9568328.940856867,9568335.920052364,9568343.0291082,9568350.270440187,9568357.646509072,9568365.159821356,9568372.812930148,9568380.608436035,9568388.548987959,9568396.637284119,9568404.876072882,9568413.268153716,9568421.816378145,9568430.523650706,9568439.392929943,9568448.427229408,9568457.629618673,9568467.003224384,9568476.551231313,9568486.27688344,9568496.18348505,9568506.27440186,9568516.553062148,9568527.022957925,9568537.687646111,9568548.550749743,9568559.615959208,9568570.88703348,9568582.367801402,9568594.062162988,9568605.974090734,9568618.107630964,9568630.466905205,9568643.056111576,9568655.87952622,9568668.941504747,9568682.246483695,9568695.798982047,9568709.603602765,9568723.66503433,9568737.988052336,9568752.577521112,9568767.438395355,9568782.575721825,9568797.99464103,9568813.700388974,9568829.698298931,9568845.993803244,9568862.59243516,9568879.499830706,9568896.721730584,9568914.263982119,9568932.13254124,9568950.333474468,9568968.872960994,9568987.757294748,9569006.992886528,9569026.586266173,9569046.544084743,9569066.873116799,9569087.580262661,9569108.672550742,9569130.157139929,9569152.04132199,9569174.332524026,9569197.038310988,9569220.166388227,9569243.724604066,9569267.720952488,9569292.163575787,9569317.060767328,9569342.420974344,9569368.252800778,9569394.565010168,9569421.3665286,9569448.666447727,9569476.474027803,9569504.79870081,9569533.650073627,9569563.03793125,9569592.972240096,9569623.463151336,9569654.521004306,9569686.15632998,9569718.379854506,9569751.202502798,9569784.635402203,9569818.689886235,9569853.377498358,9569888.709995883,9569924.699353864,9569961.357769145,9569998.697664415,9570036.731692383,9570075.472739989,9570114.933932725,9570155.128639016,9570196.070474684,9570237.773307486,9570280.25126176,9570323.51872311,9570367.590343213,9570412.481044704,9570458.206026139,9570504.780767055,9570552.22103311,9570600.54288134,9570649.76266548,9570699.897041395,9570750.962972617,9570802.977735959,9570855.958927244,9570909.924467135,9570964.892607063,9571020.881935269,9571077.911382927,9571136.000230426,9571195.168113716,9571255.43503077,9571316.821348194,9571379.347807927,9571443.03553405,9571507.906039735,9571573.981234316,9571641.283430459,9571709.835351488,9571779.660138814,9571850.781359503,9571923.223013982,9571997.009543862,9572072.16583991,9572148.717250157,9572226.689588116,9572306.10914121,9572387.00267925,9572469.39746315,9572553.321253719,9572638.802320637,9572725.869451579,9572814.55196149,9572904.879702007,9572996.883071054,9573090.593022583,9573186.041076485,9573283.259328669,9573382.280461295,9573483.137753177,9573585.865090376,9573690.496976927,9573797.068545792,9573905.615569938,9574016.174473623,9574128.782343857,9574243.476942051,9574360.296715828,9574479.280811053,9574600.469084023,9574723.90211386,9574849.621215094,9574977.66845045,9575108.086643802,9575240.919393351,9575376.211085001,9575514.006905902,9575654.352858253,9575797.295773238,9575942.88332523,9576091.164046159,9576242.187340098,9576396.00349806,9576552.66371301,9576712.220095064,9576874.725686925,9577040.234479502,9577208.801427761,9577380.482466778,9577555.334528007,9577733.41555575,9577914.784523843,9578099.501452565,9578287.627425728,9578479.224608004,9578674.356262444,9578873.086768195,9579075.481638458,9579281.607538601,9579491.532304503,9579705.324961096,9579923.055741088,9580144.796103882,9580370.6187547,9580600.597663872,9580834.808086317,9581073.326581204,9581316.231031777,9581563.600665348,9581815.51607345,9582072.059232168,9582333.313522575,9582599.363751352,9582870.296171518,9583146.19850328,9583427.159955032,9583713.271244427,9584004.624619553,9584301.31388023,9584603.434399324,9584911.083144188,9585224.358698098,9585543.361281771,9585868.19277488,9586198.9567376,9586535.758432122,9586878.704844166,9587227.904704444,9587583.46851008,9587945.508545922,9588314.138905788,9588689.475513585,9589071.636144275,9589460.740444701,9589856.909954205,9590260.268125053,9590670.940342607,9591089.053945238,9591514.738243952,9591948.12454169,9592389.346152265,9592838.538418928,9593295.838732509,9593761.386549108,9594235.323407305,9594717.79294482,9595208.940914651,9595708.915200574,9596217.865832025,9596735.944998289,9597263.307061953,9597800.108571608,9598346.508273724,9598902.66712365,9599468.74829572,9600044.917192403,9600631.3414524,9601228.190957723,9601835.637839656,9602453.856483491,9603083.023532107,9603723.31788823,9604374.920715338,9605038.015437217,9605712.787736015,9606399.425548801,9607098.119062534,9607809.060707407,9608532.445148489,9609268.469275577,9610017.332191266,9610779.235197103,9611554.38177778,9612342.977583343,9613145.230409302,9613961.350174617,9614791.548897447,9615636.04066869,9616495.04162316,9617368.769908393,9618257.44565101,9619161.290920593,9620080.529690992,9621015.387799041,9621966.092900602,9622932.874423917,9623915.963520212,9624915.593011472,9625931.997335441,9626965.412487695,9628016.075960826,9629084.22668072,9630170.10493985,9631273.952327594,9632396.011657573,9633536.526891977,9634695.743062893,9635873.906190636,9637071.263199061,9638288.061827924,9639524.550542258,9640780.978438849,9642057.595149795,9643354.65074322,9644672.395621212,9646011.080414992,9647370.95587745,9648752.27277308,9650155.281765403,9651580.233302029,9653027.377497366,9654496.964013213,9655989.241937261,9657504.459659683,9659042.864748001,9660604.703820296,9662190.222417,9663799.664871456,9665433.27417937,9667091.291867418,9668773.957861207,9670481.510352796,9672214.185668007,9673972.21813379,9675755.839945916,9677565.281037144,9679400.768946327,9681262.528688546,9683150.782626675,9685065.750344662,9687007.648522774,9688976.69081521,9690973.087730294,9692997.046513673,9695048.771034775,9697128.461676905,9699236.31523131,9701372.524795517,9703537.279676355,9705730.765297925,9707953.16311493,9710204.650531655,9712485.400826985,9714795.583085734,9717135.362136703,9719504.898497693,9721904.34832789,9724333.863387868,9726793.591007525,9729283.67406229,9731804.250957813,9734355.455623472,9736937.417514889,9739550.261625769,9742194.108509228,9744869.074308828,9747575.270799544,9750312.80543878,9753081.781427644,9755882.297782538,9758714.44941724,9761578.327235525,9764474.018234368,9767401.605617812,9770361.168921478,9773352.784147695,9776376.523911221,9779432.457595501,9782520.651519332,9785641.169113817,9788794.071109504,9791979.415733453,9795197.25891608,9798447.65450756,9801730.654503442,9805046.309279295,9808394.667833988,9811775.77804134,9815189.686909681,9818636.440849032,9822116.085945435,9825628.668242007,9829174.234026285,9832752.830123356,9836364.504194293,9840009.305039395,9843687.282905672,9847398.489798069,9851142.979793824,9854920.809359439,9858732.037669666,9862576.726927897,9866454.942687392,9870366.754172746,9874312.234600948,9878291.4615015,9882304.517034901,9886351.488308972,9890432.46769237,9894547.553124713,9898696.848422732,9902880.463581854,9907098.515072672,9911351.126131697,9915638.427045926,9919960.555430608,9924317.656499743,9928709.883328846,9933137.397109425,9937600.367394784,9942098.972336695,9946633.398912517,9951203.843142394,9955810.510296168,9960453.615089644,9965133.381869944,9969850.044789633,9974603.847969359,9979395.045648806,9984223.902325736,9989090.692882963,9993995.702703113,9998939.227771059,10003921.574763928,10008943.061128663,10014004.015147075,10019104.775988389,10024245.693749338,10029427.129481837,10034649.455208283,10039913.053924683,10045218.319591643,10050565.657113379,10055955.48230502,10061388.22184827,10066864.313235754,10072384.204704184,10077948.355156718,10083557.23407468,10089211.321418954,10094911.107521405,10100657.092966547,10106449.788463885,10112289.714711145,10118177.402248854,10124113.39130653,10130098.231640836,10136132.482366133,10142216.711777676,10148351.497167889,10154537.424636034,10160775.088891659,10167065.093052097,10173408.04843447,10179804.574342426,10186255.29784798,10192760.853568792,10199321.883441115,10205939.036488805,10212612.968588576,10219344.342231805,10226133.826283148,10232982.095736152,10239889.831466148,10246857.719980534,10253886.453166708,10260976.728037747,10268129.246476036,10275344.7149749,10282623.844378417,10289967.349619437,10297375.949455893,10304850.366205476,10312391.32547868,10319999.555910204,10327675.788888775,10335420.758285312,10343235.200179415,10351119.852584118,10359075.455168886,10367102.748980688,10375202.476163186,10383375.379673796,10391622.20299864,10399943.689865211,10408340.58395263,10416813.628599398,10425363.566508496,10433991.13944971,10442697.087959044,10451482.151035106,10460347.06583233,10469292.567350904,10478319.388123326,10487428.257897424,10496619.90331577,10505895.047591403,10515254.41017975,10524698.70644669,10534228.6473327,10543844.93901303,10553548.282553859,10563339.373564446,10573218.901845222,10583187.551031863,10593245.998235341,10603394.913677983,10613634.960325614,10623966.793515787,10634391.060582226,10644908.400475515,10655519.443380143,10666224.810328009,10677025.112808475,10687920.952375097,10698912.920249142,10710001.596919982,10721187.551742556,10732471.342531942,10743853.515155215,10755334.603120668,10766915.127164511,10778595.594835185,10790376.50007531,10802258.322801424,10814241.528481508,10826326.56771041,10838513.875783145,10850803.872266144,10863196.960566388,10875693.527498469,10888293.942849465,10900998.558941638,10913807.710192796,10926721.71267426,10939740.863666283,10952865.441210749,10966095.70366103,10979431.88922875,10992874.215527263,11006422.879111635,11020078.055014838,11033839.896279952,11047708.533488054,11061684.07428157,11075766.602882747,11089956.179607037,11104252.840371063,11118656.596194917,11133167.43269854,11147785.309591945,11162510.160159042,11177341.89073492,11192280.380176378,11207325.479325632,11222477.010467064,11237734.766777057,11253098.511766864,11268567.978718644,11284142.870114803,11299822.857060827,11315607.57870194,11331496.641633933,11347489.61930861,11363586.051434405,11379785.44337281,11396087.265531294,11412490.952753613,11428995.903708337,11445601.480276735,11462307.006941024,11479111.7701743,11496015.01783346,11513015.95855655,11530113.761166064,11547307.554079887,11564596.424731549,11581979.419001682,11599455.54066258,11617023.750837848,11634682.967479268,11652432.064863015,11670269.873107458,11688195.177714827,11706206.719139097,11724303.192382444,11742483.246622728,11760745.484874377,11779088.46368519,11797510.692871436,11816010.635293756,11834586.706676248,11853237.275471136,11871960.662771365,11890755.14227342,11909618.940292565,11928550.23583268,11947547.160712713,11966607.799751708,11985730.191014247,12004912.32611798,12024152.15060488,12043447.564377563,12062796.42220201,12082196.534277728,12101645.66687637,12121141.543049453,12140681.843405807,12160264.206959106,12179886.232045615,12199545.477312136,12219239.462773915,12238965.67094202,12258721.548019532,12278504.505165685,12298311.919826802,12318141.137132818,12337989.471357744,12357854.207442446,12377732.602577804,12397621.887846071,12417519.269918209,12437421.932804633,12457327.039656736,12477231.734616352,12497133.14471016,12517028.381785916,12536914.544487173,12556788.720263183,12576647.98741038,12596489.417141853,12616310.075681182,12636107.026376707,12655877.331832519,12675618.056052236,12695326.266591588,12714999.03671596,12734633.447558824,12754226.590277208,12773775.568200156,12793277.498966351,12812729.516647026,12832128.773850292,12851472.443803212,12870757.722407913,12889981.83026815,12909142.014682904,12928235.551603558,12947259.74755148,12966211.941492846,12985089.506667735,13003889.852370627,13022610.425679667,13041248.713132069,13059802.242343336,13078268.583568107,13096645.351200463,13114930.205211984,13133120.852525676,13151215.048324374,13169210.597292205,13187105.354787923,13204897.227949146,13222584.176726634,13240164.214847952,13257635.410710078,13274995.888200551,13292243.82744706,13309377.46549548,13326395.096916458,13343295.074340934,13360075.808924984,13376735.770744624,13393273.489121262,13409687.552878745,13425976.610532852,13442139.370414471,13458174.600727541,13474081.129543189,13489857.84473135,13505503.693831503,13521017.683863966,13536398.881083513,13551646.410676977,13566759.456406662,13581737.260201368,13596579.121696975,13611284.39772847,13625852.501775393,13640282.903362768,13654575.127419412,13668728.75359581,13682743.41554349,13696618.80015801,13710354.646787647,13723950.74640975,13737406.940776914,13750723.121534932,13763899.229314541,13776935.252799049,13789831.227769658,13802587.236130603,13815203.404915836,13827679.905279266,13840016.951470315,13852214.799796617,13864273.747575574,13876194.132076528,13887976.32945514,13899620.753681652,13911127.855464539,13922498.12117108,13933732.071746303,13944830.2616317,13955793.27768506],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[11709993.91837158,11709993.748491334,11709993.575449625,11709993.399187636,11709993.219645433,11709993.036761962,11709992.850475058,11709992.66072138,11709992.467436409,11709992.270554435,11709992.07000851,11709991.865730453,11709991.657650825,11709991.445698857,11709991.229802512,11709991.00988837,11709990.785881659,11709990.557706233,11709990.325284502,11709990.088537447,11709989.847384581,11709989.601743914,11709989.351531928,11709989.09666356,11709988.837052146,11709988.572609436,11709988.303245505,11709988.028868778,11709987.749385983,11709987.464702092,11709987.174720308,11709986.879342053,11709986.578466896,11709986.271992551,11709985.959814807,11709985.641827546,11709985.317922648,11709984.987989997,11709984.651917404,11709984.30959064,11709983.960893296,11709983.605706835,11709983.243910491,11709982.875381263,11709982.499993866,11709982.117620664,11709981.728131682,11709981.331394471,11709980.927274173,11709980.515633387,11709980.096332163,11709979.669227961,11709979.234175567,11709978.791027078,11709978.33963183,11709977.879836373,11709977.41148438,11709976.934416626,11709976.448470926,11709975.953482075,11709975.449281786,11709974.935698647,11709974.41255807,11709973.879682196,11709973.33688986,11709972.783996545,11709972.22081427,11709971.647151599,11709971.06281349,11709970.467601292,11709969.861312669,11709969.243741492,11709968.614677828,11709967.973907817,11709967.32121362,11709966.656373357,11709965.979161017,11709965.289346369,11709964.586694911,11709963.870967789,11709963.141921684,11709962.399308762,11709961.642876556,11709960.872367946,11709960.08752099,11709959.288068883,11709958.473739862,11709957.644257104,11709956.799338626,11709955.93869723,11709955.06204032,11709954.169069916,11709953.25948246,11709952.332968747,11709951.38921384,11709950.427896917,11709949.448691199,11709948.45126383,11709947.435275774,11709946.40038165,11709945.346229687,11709944.272461561,11709943.178712279,11709942.064610051,11709940.92977617,11709939.773824912,11709938.596363338,11709937.396991234,11709936.17530091,11709934.930877114,11709933.66329686,11709932.372129299,11709931.05693555,11709929.717268612,11709928.352673113,11709926.96268524,11709925.546832548,11709924.104633812,11709922.635598846,11709921.139228344,11709919.615013734,11709918.062436935,11709916.48097029,11709914.870076286,11709913.229207428,11709911.557806034,11709909.855304055,11709908.121122867,11709906.354673097,11709904.555354388,11709902.722555244,11709900.855652776,11709898.954012517,11709897.016988212,11709895.043921567,11709893.034142049,11709890.986966662,11709888.901699716,11709886.77763256,11709884.614043374,11709882.410196941,11709880.165344326,11709877.878722709,11709875.549555032,11709873.177049857,11709870.760400964,11709868.298787186,11709865.791372076,11709863.237303602,11709860.635713946,11709857.985719107,11709855.286418663,11709852.536895454,11709849.736215262,11709846.883426508,11709843.977559911,11709841.017628176,11709838.00262564,11709834.93152797,11709831.80329175,11709828.61685421,11709825.371132793,11709822.065024834,11709818.697407154,11709815.267135711,11709811.773045195,11709808.21394861,11709804.588636944,11709800.895878669,11709797.134419378,11709793.302981367,11709789.40026317,11709785.424939128,11709781.375658954,11709777.251047261,11709773.04970311,11709768.770199515,11709764.411082972,11709759.970872989,11709755.448061528,11709750.841112563,11709746.148461504,11709741.368514713,11709736.499648897,11709731.540210655,11709726.488515843,11709721.34284902,11709716.1014629,11709710.762577714,11709705.324380659,11709699.785025215,11709694.142630603,11709688.395281095,11709682.54102536,11709676.577875866,11709670.503808126,11709664.316760086,11709658.014631374,11709651.595282627,11709645.056534752,11709638.396168193,11709631.611922167,11709624.701493945,11709617.662538005,11709610.492665302,11709603.189442448,11709595.750390861,11709588.17298595,11709580.45465627,11709572.59278264,11709564.58469728,11709556.42768286,11709548.118971664,11709539.655744586,11709531.035130225,11709522.254203873,11709513.30998656,11709504.199444085,11709494.9194859,11709485.466964157,11709475.838672591,11709466.031345466,11709456.04165649,11709445.86621768,11709435.501578182,11709424.944223175,11709414.190572653,11709403.236980243,11709392.079731943,11709380.715044897,11709369.139066141,11709357.347871259,11709345.337463109,11709333.103770461,11709320.642646642,11709307.949868115,11709295.021133091,11709281.85206007,11709268.438186396,11709254.774966717,11709240.857771471,11709226.681885386,11709212.242505815,11709197.534741214,11709182.553609421,11709167.294036044,11709151.75085276,11709135.918795524,11709119.792502904,11709103.366514193,11709086.63526766,11709069.593098626,11709052.234237624,11709034.552808434,11709016.542826133,11708998.198195111,11708979.51270702,11708960.480038684,11708941.09375003,11708921.347281935,11708901.23395399,11708880.746962337,11708859.879377384,11708838.624141471,11708816.974066526,11708794.921831744,11708772.459981037,11708749.580920665,11708726.276916606,11708702.540092103,11708678.362424958,11708653.73574492,11708628.651730934,11708603.101908442,11708577.077646516,11708550.570155025,11708523.570481742,11708496.069509324,11708468.057952384,11708439.526354352,11708410.465084367,11708380.864334127,11708350.7141146,11708320.004252812,11708288.724388372,11708256.86397018,11708224.412252879,11708191.35829337,11708157.69094715,11708123.398864722,11708088.470487807,11708052.894045591,11708016.657550855,11707979.748796046,11707942.15534925,11707903.864550179,11707864.86350598,11707825.139087068,11707784.677922789,11707743.466397103,11707701.490644114,11707658.736543559,11707615.189716233,11707570.835519275,11707525.65904145,11707479.645098291,11707432.778227132,11707385.042682188,11707336.422429355,11707286.901141131,11707236.462191194,11707185.08864921,11707132.76327522,11707079.468514169,11707025.186490225,11706969.899001045,11706913.587511916,11706856.233149799,11706797.816697307,11706738.318586532,11706677.718892785,11706615.99732826,11706553.133235516,11706489.10558093,11706423.892947996,11706357.473530505,11706289.825125623,11706220.925126893,11706150.750517007,11706079.277860586,11706006.483296763,11705932.342531655,11705856.830830734,11705779.92301101,11705701.593433179,11705621.81599357,11705540.56411593,11705457.810743216,11705373.528329082,11705287.688829316,11705200.263693165,11705111.223854413,11705020.539722413,11704928.181172963,11704834.117538963,11704738.317600993,11704640.74957772,11704541.381116144,11704440.179281678,11704337.110548101,11704232.14078734,11704125.23525908,11704016.35860018,11703905.47481404,11703792.547259634,11703677.538640544,11703560.410993675,11703441.125677874,11703319.64336241,11703195.924015177,11703069.926890794,11702941.610518485,11702810.932689823,11702677.85044623,11702542.320066337,11702404.29705313,11702263.736120936,11702120.591182152,11701974.815333907,11701826.360844335,11701675.179138882,11701521.220786203,11701364.435484035,11701204.772044716,11701042.178380601,11700876.601489248,11700707.987438412,11700536.281350786,11700361.427388566,11700183.36873783,11700002.047592642,11699817.405139014,11699629.381538609,11699437.915912235,11699242.946323182,11699044.409760214,11698842.242120495,11698636.378192222,11698426.751637008,11698213.294972146,11697995.939552566,11697774.615552597,11697549.251947569,11697319.77649511,11697086.115716275,11696848.194876475,11696605.937966151,11696359.267681241,11696108.10540345,11695852.37118031,11695591.983705005,11695326.860296007,11695056.916876487,11694782.067953527,11694502.226597168,11694217.304419156,11693927.211551635,11693631.856625471,11693331.14674855,11693024.987483794,11692713.282826977,11692395.935184434,11692072.845350552,11691743.912485084,11691409.034090275,11691068.105987906,11690721.02229612,11690367.675406072,11690007.95595853,11689641.752820263,11689268.953060294,11688889.441926155,11688503.102819806,11688109.81727371,11687709.464926617,11687301.923499327,11686887.068770444,11686464.774551963,11686034.91266483,11685597.35291457,11685151.96306669,11684698.608822279,11684237.153793449,11683767.45947886,11683289.38523926,11682802.788273096,11682307.523592135,11681803.4439972,11681290.400054004,11680768.240069065,11680236.810065825,11679695.953760825,11679145.512540156,11678585.325436022,11678015.229103608,11677435.057798123,11676844.643352155,11676243.815153288,11675632.400122093,11675010.222690416,11674377.104780056,11673732.8657819,11673077.322535422,11672410.289308732,11671731.57777905,11671040.997013774,11670338.353452131,11669623.450887375,11668896.090449715,11668156.070589885,11667403.187063409,11666637.232915726,11665857.998468053,11665065.271304134,11664258.8362579,11663438.475402022,11662603.968037605,11661755.090684697,11660891.617074147,11660013.318140421,11659119.962015735,11658211.314025437,11657287.136684602,11656347.18969617,11655391.22995037,11654419.011525735,11653430.285691619,11652424.80091239,11651402.302853229,11650362.53438778,11649305.23560758,11648230.143833356,11647136.993628303,11646025.516813435,11644895.44248503,11643746.497034213,11642578.404168839,11641390.884937758,11640183.657757416,11638956.438441016,11637708.940230248,11636440.87382965,11635151.947443772,11633841.866817117,11632510.335277032,11631157.053779623,11629781.720958646,11628384.033177748,11626963.684585819,11625520.367175803,11624053.770846868,11622563.583470175,11621049.490958206,11619511.177337872,11617948.324827319,11616360.61391672,11614747.723452976,11613109.330728479,11611445.11157406,11609754.740456114,11608037.890578078,11606294.23398629,11604523.44168029,11602725.183727773,11600899.12938404,11599044.947216254,11597162.305232424,11595250.871015254,11593310.311860869,11591340.294922544,11589340.48735943,11587310.55649039,11585250.169952936,11583158.995867359,11581036.703006007,11578882.960967926,11576697.440358667,11574479.812975425,11572229.751997467,11569946.932181891,11567631.03006472,11565281.724167235,11562898.695207642,11560481.626318038,11558030.203266505,11555544.114684505,11553023.052299345,11550466.711171702,11547874.789938193,11545246.99105884,11542583.021069318,11539882.590837922,11537145.415827196,11534371.216359857,11531559.717889156,11528710.651273333,11525823.753054025,11522898.765738474,11519935.4380853,11516933.525393639,11513892.789795369,11510813.00055026,11507693.93434369,11504535.375586698,11501337.116718065,11498098.958508117,11494820.710363878,11491502.190635335,11488143.226922246,11484743.656381397,11481303.326033624,11477822.093070393,11474299.825159384,11470736.400748745,11467131.709369406,11463485.651935155,11459798.141039804,11456069.101251066,11452298.46940061,11448486.194869658,11444632.2398697,11440736.57971772,11436799.203105276,11432820.112361068,11428799.32370614,11424736.867501313,11420632.78848616,11416487.146008894,11412300.014246624,11408071.482415285,11403801.65496863,11399490.6517857,11395138.60834613,11390745.675892621,11386312.021580037,11381837.828610476,11377323.296353715,11372768.64045243,11368174.092911664,11363539.902171874,11358866.33316512,11354153.667353787,11349402.202751366,11344612.253924794,11339784.151977882,11334918.24451543,11330014.895587562,11325074.48561396,11320097.411287678,11315084.085458107,11310034.936992988,11304950.41061915,11299830.966741774,11294677.081242168,11289489.24525377,11284267.964916553,11279013.761109643,11273727.169162361,11268408.738543717,11263059.032530531,11257678.627854483,11252268.114328248,11246828.094451172,11241359.182994839,11235862.006568974,11230337.203168256,11224785.421700547,11219207.321497267,11213603.57180656,11207974.85126999,11202321.847383672,11196645.25594466,11190945.780483505,11185224.131684035,11179481.026791403,11173717.189009415,11167933.3468884,11162130.233704755,11156308.586833412,11150469.147114528,11144612.658215726,11138739.8659912,11132851.517839171,11126948.362059012,11121031.147209557,11115100.621470053,11109157.53200519,11103202.624335822,11097236.641716719,11091260.324523035,11085274.409646872,11079279.629905505,11073276.713462718,11067266.38326483,11061249.356492694,11055226.344031267,11049198.049958054,11043165.17105185,11037128.396323025,11031088.406566752,11025045.873940293,11019001.461565606,11012955.82315836,11006909.602684405,11000863.434044749,10994817.940789895,10988773.73586444,10982731.421382712,10976691.588436106,10970654.816932786,10964621.675470246,10958592.721241212,10952568.499973185,10946549.545901988,10940536.3817794,10934529.51891502,10928529.457252337,10922536.685478939,10916551.681170594,10910574.910969034,10904606.830792964,10898647.886081913,10892698.512072308,10886759.1341052,10880830.167964876,10874912.020247547,10869005.088759271,10863109.762942104,10857226.424327439,10851355.447015502,10845497.198179696,10839652.038594691,10833820.323186861,10828002.401605789,10822198.618815355,10816409.315703036,10810634.829705851,10804875.495451495,10799131.645412989,10793403.61057538,10787691.721112762,10781996.307074038,10776317.699075771,10770656.229000447,10765012.230698468,10759386.040692294,10753777.99888096,10748188.44924341,10742617.740538977,10737066.227003416,10731534.269038865,10726022.23389622,10720530.496348355,10715059.439352715,10709609.454701843,10704180.943660371,10698774.317587214,10693389.998541558,10688028.419871444,10682690.026783746,10677375.276894359,10672084.640757602,10666818.602373723,10661577.659673646,10656362.324980047,10651173.12544394,10646010.60345607,10640875.317032436,10635767.840173343,10630688.763195492,10625638.693036672,10620618.253532667,10615628.085666109,10610668.847787065,10605741.215805216,10600845.883353554,10595983.561923629,10591154.980972407,10586360.888000894,10581602.048604738,10576879.246497115,10572193.283504218,10567544.979533775,10562935.172517091,10558364.718325064,10553834.490658864,10549345.3809158,10544898.29803114,10540494.168296536,10536133.935155865,10531818.558979273,10527549.01681624,10523326.302128552,10519151.42450404,10515025.40935204,10510949.297581427,10506924.145262258,10502951.023271887,10499031.016926559,10495165.22559943,10491354.762325963,10487600.753397617,10483904.337944858,10480266.66751029,10476688.90561295,10473172.227304516,10469717.818718411,10466326.876612576,10463000.60790671,10459740.22921481,10456546.966373691,10453422.05396821,10450366.73485389,10447382.259677509,10444469.886396287,10441630.879796179,10438866.511009788,10436178.05703435,10433566.800250154,10431034.027939834,10428581.031808775,10426209.10750695,10423919.554152383,10421713.673856465,10419592.77125121,10417558.153018605,10415611.12742209,10413753.003840188,10411985.092302306,10410308.703026647,10408725.145960143,10407235.730320375,10405841.764139256,10404544.553808425,10403345.403626125,10402245.615345389,10401246.487723354,10400349.316071453,10399555.391806293,10398866.002000963,10398282.42893655,10397805.949653601,10397437.835503321,10397179.351698227,10397031.756862042,10396996.302578606,10397074.232939536,10397266.784090446,10397575.183775507,10398000.65088012,10398544.394971536,10399207.615837201,10399991.503020676,10400897.235354938,10401925.980492909,10403078.894435074,10404357.121054001,10405761.791615654,10407294.024297347,10408954.923702205,10410745.580370001,10412667.070284225,10414720.454375273,10416906.778019557,10419227.070534492,10421682.3446691,10424273.596090127,10427001.802863507,10429867.924930954,10432872.903581515,10436017.66091784,10439303.099317005,10442730.100885538,10446299.526908517,10450012.217292339,10453868.990000969,10457870.64048529,10462017.941105276,10466311.640544605,10470752.463217415,10475341.10866681,10480078.250954757,10484964.538043013,10490000.591164714,10495187.004186248,10500524.342959069,10506013.144661095,10511653.9171274,10517447.13816983,10523393.254885342,10529492.682952784,10535745.805917941,10542152.974466685,10548714.50568615,10555430.682313902,10562301.751975115,10569327.926407918,10576509.380677056,10583846.252376182,10591338.640819158,10598986.606220832,10606790.16886788,10614749.308280453,10622863.962365393,10631134.026561998,10639559.352981392,10648139.74954071,10656874.97909341,10665764.758557193,10674808.758041117,10684006.599973667,10693357.85823362,10702862.057285717,10712518.671323309,10722327.123420171,10732286.784693966,10742396.973483745,10752656.954544213,10763065.938259391,10773623.079878543,10784327.47877723,10795178.177746518,10806174.162313318,10817314.360095026,10828597.64019155,10840022.812617924,10851588.627780683,10863293.776001174,10875136.887089022,10887116.529968798,10899231.212363122,10911479.380535115,10923859.419093246,10936369.65086141,10949008.336817037,10961773.676099813,10974663.806093628,10987676.802583996,11000810.679993233,11014063.391695324,11027432.830412379,11040916.828694237,11054513.159482649,11068219.5367612,11082033.616291916,11095952.996439252,11109975.219081912,11124097.77061267,11138318.083026132,11152633.535094105,11167041.45362793,11181539.114826955,11196123.745711947,11210792.525642063,11225542.587913694,11240371.021439206,11255274.872503469,11270251.146595625,11285296.810313476,11300408.793337561,11315583.990471758,11330819.263747102,11346111.444585225,11361457.336017696,11376853.714957356,11392297.334517546,11407784.926375087,11423313.20317255,11438878.860955467,11454478.581639864,11470109.035505507,11485766.88371017,11501448.780820128,11517151.377352184,11532871.322322357,11548605.265796512,11564349.86143812,11580101.76904844,11595857.657094447,11611614.205219882,11627368.106734896,11643116.071079854,11658854.826258963,11674581.12123954,11690291.728312785,11705983.445412224,11721653.09838594,11737297.543219047,11752913.668202946,11768498.396048084,11784048.685937151,11799561.535515865,11815033.982818559,11830463.108126225,11845846.03575458,11861179.935770221,11876462.02563288,11891689.571762221,11906859.891027639,11921970.352159897,11937018.377083505,11952001.442169046,11966917.079404794,11981762.87748718,11996536.482829904,12011235.600491581,12025857.995022086,12040401.49122787,12054863.974856762,12069243.393202815,12083537.755632091,12097745.13403023,12111863.663172921,12125891.541020535,12139827.028938163,12153668.451842625,12167414.198277954,12181062.720421039,12194612.534019208,12208062.218261609,12221410.415586289,12234655.831424994,12247797.233887758,12260833.453389378,12273763.382219953,12286585.974061688,12299300.243454201,12311905.265210582,12324400.173786504,12336784.162604652,12349056.4833368,12361216.44514582,12373263.413889883,12385196.811291205,12397016.114071546,12408720.853056718,12420310.612252345,12431785.027893027,12443143.78746713,12454386.628719226,12465513.338632356,12476523.752392098,12487417.752334451,12498195.266879449,12508856.269452456,12519400.777394902,12529828.850866294,12540140.591739222,12550336.142489009,12560415.68507964],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","FigRMSEGRidRidge = visuRMSEGrid(Ridge(), 'Ridge', alphasridge, 'alpha',\n","                                GridRidge)\n","FigRMSEGRidRidge.show()\n","if write_data is True:\n","    FigRMSEGRidRidge.write_image('./Figures/ConsoGraphRMSERidge.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.3 Modèle Lasso"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      1.0\n","               R²          RMSE           MAE\n","Lasso()  0.845165  1.137523e+07  3.431214e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLasso=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[2824167.216187907,3316048.7420473173,2113847.8337943587,108460167.75870895,4616191.224198768,1052868.1359922655,976849.9156923473,8908044.010440102,958795.560883797,56590023.60887749,1116247.8590748329,1818091.1769570112,1010158.980910528,1627404.8576432506,299639.06076029805,13860439.950424667,38321450.99789306,6629257.595126908,55499508.12262711,429142.5713260246,1389297.2907435996,1007361.265298096,908133.9382291201,765680.1537686198,8271220.306680588,629584.8532458805,2417970.196139346,-278929.56411224185,7341896.314374451,4025955.5659955502,2498660.02908433,978160.0290290222,9522440.991121,2828105.776055733,5899822.310859459,1352447.329292437,2731009.732962602,138755.76291009667,4141198.827689547,547947.6563371699,3116977.0458685914,14221919.882440334,945112.699268097,9602801.953504825,151395451.71593124,2767544.154952393,35125766.06288152,639394.5037380839,120629676.00816502,5093184.343570298,1969220.5544205448,1231377.2519745657,1005216.320557957,702014.8957242228,1781267.4249711814,1502131.1597772792,4609449.432374118,3958080.5001508035,15647150.69004838,1005321.0614660934,472460.90663529513,6060132.008453425,928542.7707726946,2687821.037793938,7611809.221150653,164282.58812852483,6259507.506644646,2097047.1228945786,1195650.9062207583,6164323.046367226,939791.5514428308,847483.1668732022,5337621.5747199375,641847.4036815129,842175.5967357361,2525081.290238902,30202297.572456554,858438.4947624046,1384291.6338679467,1645646.3524997723,731489.8479672922,2123611.472755406,1113678.1579246412,542585.8961535546,3601005.570181018,9758961.606533581,1683357.0825472171,1964554.1904466092,3885404.937610123,1268343.0961390012,10436075.268651726,13099667.32473265,176749.38909666892,3003013.7916854555,16355402.25876745,9962785.199619018,1523206.9238395216,39416685.699067734,6829212.691367406,1310964.0253990795,2440345.734784672,377886.5940753638,14975184.891324954,1213709.9378969101,4286731.909169709,867517.525425151,2779761.1133183124,3328966.297148034,1583863.8885211842,1940745.4245016356,1438058.3191429377,1312781.6623889105,41564108.41910195,1103225.9687694532,139556.0664152531,5791476.586939107,12308803.052145159,1068236.1305702792,23823725.141074274,4058237.6930407393,4052310.6447783126,150068.83150596684,1392112.5499092115,1247548.8170036324,937249.2910779808,1529199.0533395382,838200.9202931114,1174314.5680990065,6471939.638385683,689804.3014414129,1115581.1143640792,8387920.2707184125,1899399.5364283114,1106821.512629056,515533.41269673477,8985595.635636497,1150761.9680663568,5764330.187326569,304328.4316963162,1931905.0017231535,977028.3022716541,756298.7499124622,3104510.0163835227,1541369.4580673575,91711221.47554404,3622692.47943521,2189425.8224175503,25728846.37505714,1043505.7405676218,4373565.191881087,59919.80293448968,556935.2296262237,209847.3076766061,160055049.37005615,837238.4284452365,4592549.393378047,8045307.3550316915,2722113.4349088967,261074.76376329223,1768847.3014242826,683139.3082358926,-1060160.3392564259,2097823.248701046,1188081.8231777144,14866401.993578093,5144619.786030898,4007866.3864048356,1075070.2809174394,7668058.831836948,7683159.209949359,367212.57220183173,3113942.098541355,4980947.522708071,11107196.706074776,10689595.851762896,2621782.5574212167,805261.2243174594,1063756.9621771618,2470652.7420392586,180947.48881466873,12529898.347871246,801618.2518381495,1738521.1069131119,3996967.5515943076,450783.21123373555,1232136.1733877533,26883019.93827278,3274370.5946537023,17958166.620501623,673142.5784343504,837746.8430909463,1824558.9653361437,1477038.9890843476,4045649.170505585,1424770.7101609954,7061500.2618909795,2039045.3929372674,4086776.812929212,2876808.2088580187,22537441.717165016,2209014.411719086,1321494.497897916,332168.39389468567,1077112.699414751,529475.6904068685,2891530.833498517,3231320.3708133614,824453.0163882514,3371034.8202267746,3317168.2833061316,1415772.510994476,2474120.074906717,1111479.7049615094,2254883.674031396,971062.9144942826,1864742.31075711,9393915.712677265,7001179.442071974,5509854.56624268,27025279.068499967,2036363.1350174933,715495.645746694,1334861.0669734185,802974.9546770798,2260764.318086848,1614420.4626430208,974640.4309442157,12323586.510832256,42948716.37354527,1102514.5160913528,1953894.4977437397,2393312.3499360997,2289273.3082941985,1921938.7061046732,21067384.447052334,27916206.65819152,3759446.9918259177,497796.21718816785,1048295.3905162364,2280831.8285989705,346638.7639958579,8135388.358097849,2814762.9535163827,703793.3335084058,242094.59311856003,928557.3316940742,5782810.995322975,1397175.541173738,1474787.7118374545,7341020.29680747,12565751.566404542,1653214.4412226314,5439002.942321874,2320908.0479850587,9118126.81415504,-650556.5838223561,4057524.4374622544,878256.419233172,1760952.1635183522,1342711.0055101814,1936714.4034211962,23081.14162783092,6076262.854886392,891618.695773789,440625.7522074119,1151070.9599194778,4220674.8402147135,3221072.547502256,475403.401022095,1771454.6040782924,2268985.9499932895,1145204.5511366273,945019.6264042039,1339714.6679431666,869158.5010126231,2140640.199778616,1927041.2625618672,6403943.435144043,1815153.6287095284,2056847.4684608448,5275280.847422746,2216373.3853827743,2301656.056293562,365924.1745645562,2119854.7876875396,10866786.546456771,4796198.991093937,462819.3198975804,1967808.927976102,1772980.0452982117,12539737.077341234,1845335.2171575036,4189490.274710472,30306010.564345513,1145809.874295313,662774.4161380783,2486848.3313564966,3439750.2520559886,2361885.5934016737,3735628.388204047,17054101.839437734,5269552.915477451,969193.1116725723,2799335.520704843,6305441.425294073,4508507.311203053,599780.8427040773,3743688.274915186,1479062.6755918856,1338887.2080174028,993779.3283931853,2285679.653552597,1886352.3307608855,1847928.3992008665,1980440.2132479213,1059796.3276311632,12501271.901402926,1351539.7205221201,325855.4574606584,1744866.2375172116,3926943.950486004,3136711.8744132314,3587630.6272766963,2428678.2755869217,1077957.893431155,8994848.68466913,634809.783706425,3830499.101241174,6281445.5296067875,5979252.957980199,1068034.3326530866,1578813.294058625,2293529.046455347,3731061.9374790695,5024884.323758223,1242755.0259549955,5106738.618207283,1279050.4269338953,873883.6471499295,2664539.699292041,497521.52153345523,22755414.702133648,3196034.41448715,745064.530163706,4678171.584797994,883195.5409872115,2047218.7784638621,1161822.9689301336,4133363.659500084,7037334.507358685,1429183.2283595363,175749.63050446613,2875787.686043935,6050984.324986918,5614864.48822227,340891.55785123375,14324960.240617719,1576045.187186565,82417.41266947519,217927.89029607014,2714437.8130420875,992639.768652759,4300665.958802356,-47578.48667010106,2022411.0404511634,2672136.8129098183,1547173.8049375485,7002218.227322472,630441.4566239752,10884679.983786302,800472.665636871,1534069.8123259256,202835.78970256262,631598.9107012663,2087117.31828663,821477.7692751107,2594188.524600268,1694898.8893261577,2068718.1003855965,159229.09210881777,8749511.14344182,900568.1338297138,8565334.897196341,1007685.2590312739,6498717.856980806,5691647.447419142,2352019.6132546184,852618.1689706824,6115106.424407987,666425.5375564243,8724414.58739929,3328446.1413733065,5510886.320903046,9185304.70556165,577841.6903158617,2457478.1188437417,710332.9351093173,2079320.329018401,4176730.174129803,2983122.1237337976,446463.6390564707,1495726.7682192074,1496604.7095127518,767241.2512683407,15824816.981520528,2068204.373384369,3960299.1824932387,16474160.64268674,2294947.884845321,2245942.2733101984,2137330.2548175063,812408.1867560223,25636026.826732993,1803342.5914179413,2000493.300175703,1744252.668264909,1511278.8604373487,43413478.166416734,9604654.007560814,2936595.836229963,1540886.2590224335,3748138.3489493364,757925.6490809168,707237.509144994,944824.5997799486,1341680.656399199,17915539.612660937,2090251.4487913644,1271942.5440288426,-419661.7752291518,2214671.904712635,1025545.5498909918,-1126012.9345164625,742273.4140893123,379594.916948742,12054246.184004758,948123.0739868795,1145874.441543351,4977917.27921195,2626781.4297574484,6801224.345831476,16392311.90222065,9891577.760322198,990290.4658536494,7623782.090142934,9691622.6640817,11353991.146925189,1705415.9098388848,1245361.7829749924,6830310.662960054,6336259.675482856,3820977.5787876467,4184219.1083282945,6735822.642656004,1037788.7592257096,2056739.4902406894,793268.1282389523,3850173.7468772233,1754376.084399931,13499259.983926944,587469.5455628498,3253248.1889392654,63292009.845345035,7920868.39763521,10661614.790225478,3775077.696269002,8666459.431729741,7997337.2754352335,709967.1704869489,3074948.087557883,1246096.170036863,35724408.48846476,2909524.2017574445,27343139.63233474,6092094.903679423,3132400.723429594,2013703.3562134055,1768712.3644547542,794951.9787197653,20929577.789317496,1944461.5882091413,1643492.7385639679,7661675.361423384,5094563.093476722,1302267.8144146584,552131.366900194,4108013.6510325633,3542263.052897404,3473733.475812859,1002765.1278101089,20465761.793541715,731504.4088886718,664428.6408295454,2322201.6780855237,2460719.4143273463,1191049.0389019698,3053293.0926987673,2154492.6604447747,3357022.693297061,590096.8126703158,1679017.7718323832,2607159.707019564,1348074.2096248525,1017883.3939840544,690360.4088444514,1471721.7141164746,5965030.721136014,1005043.098849511,1349948.2060577131,-1074000.8843549457,-482732.4500221582,-806366.0886721928,894349.8790557974,1118040.6258224195,820663.9086950175,2481694.3956519403,5469508.011717949,1455982.5369831654,9288.926076875068,1724472.1334166946,48283349.84835464,3324628.6214739946,665077.4169966017,729028.5589753098,4446409.1461514635,11066741.64269727,2089497.9635494705,4039737.1898501217,1120086.2174889916,242299.734505194,1410360.3653931872,2665092.8930508555,1371772.6207562366,1515688.867798271,10273195.879631052,61684.15946576418,1045297.0010068316,7699671.788285669,639367.6459071843,1886574.0864584579,2125750.137285871,6644284.618862597,1257422.0875401178,2426467.313482298,1346663.040228244,2476214.6563169453,1282632.1309625627,5988959.789272484,1067623.938215061,5614488.545689838,2661314.30567829,2097659.561387262,838107.3017952053,3688909.452391545,1494013.0590481479,255829.3666607563,1656160.2974218654,1509798.617685985,1431332.3482150636,3933310.8139951783,3137076.9750560215,6506497.324516367,4482643.494520236,3502271.573920346,1948878.02382492,2579806.0170778143,6444329.522622099,1811523.3211821902,31202.79319892684,3396093.133971323,4497712.628638131,1092072.3440308752,5455602.538294049,6055799.038050203,2299977.4290043935,-95278.1625053226,678301.3229926741,3341817.1062946124,921708.6204255344,1372071.8390403735,10943636.608983846,14775229.795084456,-526328.7574088327,1277067.7956552487,10166945.176426062,984977.3992455644,6910170.271324446,5663903.15050156,3856227.7634927696,2527826.047406545,4579179.985313542,5169755.5967093315,3915347.3279754035,1351116.1168147018,81129.04441735568,919227.0336949914,1302469.6123318505,2680692.331281846,939445.6536599519,1041540.0945990942,16202722.02202147,1124032.209688423,788023.4100790094,-687677.6619965332,2438728.4598759413,5945564.1990131885,873322.417929355,2029841.6582996903,930563.7221534546,1143644.6811581044,899347.8147132944,1828497.3911459907,7686065.016860519,2202608.1488917656,2001713.6413932587,2467644.891639692,2749506.3403042397,12201945.541092461,590046.8585522473,12768932.485218741,1351920.536279154,-182247.122736712,5537576.670960436,2872091.909150316,7073906.4540237915,7486109.920620022,3662555.314250417,334187.5037743193,1429793.121916108,2547393.2909344826,148365.41695470503,11164132.314660499,2148549.913644883,1186366.9864064055,1733272.272219887,1520425.6195206488,1707870.6106396597,12041707.003132632,817052.6073582133,2540984.272786377,1742677.8047746567,1130383.706624672,2122989.59350909,667174.4855396745,1483142.601450764,1969220.5544205448,3958472.787956181,20986334.52340698,21554873.953856345,875411.1467260532,4495010.976861011,10995716.80634987,1130448.8677160265,1500205.8951973305,1513390.3170218365,22464761.97875661,14611318.431643974,21232839.95333275,-191474.1085764207,1359826.4462073818,2398727.661893286,2877694.4581033145,8740040.568597892,875822.8038605966,1837729.925008275,1999714.2887180266,12739692.173581732,348007.0589871267,446820.12897529453,1452406.1598989635,4365824.433345143,4780992.426467573,1177474.500026921,2236522.0315986085,8199448.821101826,2192252.351169414,-369894.5423725005,839338.9031893669,3492672.513366755,2071761.3858366935,11358219.163809717,741619.219517713,1417447.143048158,2205873.0105722873,1234689.819847687,6559904.927681783,1418583.1794678336,3534517.0820870134,32282.819147701375,1414392.3226928487,269999.1977532171,1391564.7548618792,1932874.613574841,5298527.315688014,913016.1154076145,510269.88079893123,4007747.2708968893,644841.9132938469,1080686.8451578044,1792934.0558014256,1283558.6757547774,2778171.7104998706,1254240.9793547112,747434.114454123,1726251.617606368,3541903.1398271583,694413.1918866774,1487619.86555534,4289815.99562926,1703703.9334368878,145150.78841618402,1374305.471287385,1757752.7713042293,1454145.594227429,76198542.6542351,1845140.025681978,5870130.140467769,15551615.477154166,11677069.75239122,5133410.071759149,706508.2699476888,2247936.659378088,9629055.522787096,5778890.208587369,4793906.995092295,5987787.7078520935,1359979.9462550674,4980725.834149138,801323.4092853162,2327870.9511660472,1925471.0484215284,3920705.9055951815,131006.06385428482,1148279.4006461555,1517013.5420563258,827074.879963641,1628542.294905493,1066211.998030819,475587.41488664784,566730.4518318393,5539795.96763405,1141390.0417928272,1664787.214516612,-282777.35378166055,2194855.695296116,2386059.063053848,2481481.3771334756,3533524.7205750286,2417042.9696865855,2083046.0869532302,854362.550056661,2504300.2056888365,562043.4722042885,2052072.5606200579,15686375.543726576,1362118.3834387115,4225910.662236048,848417.3995214931,7405998.39695205,3498759.5732137323,858609.7528366859,6577887.048969803,1334114.716085428,6894148.469681667,983896.7400383181,5265030.105735189,17575173.931098502,51779646.35841866,2907919.6400490277,1320754.2488138261,1348184.5611528861,313760.8687108592,16274205.546446243,1607703.7094745927,976638.6647721769,1770707.7766898766,5564279.053441888,8069582.454808267,1820417.078483289,2100858.13714796,2673310.170618461,2908026.305790944,250828.11499323766,9538798.79917162,1169068.95042802,1119898.4348591655,4628300.421625921,4482518.061910383,777995.4842662807,3656272.6672522714,996228.3991249388,1775203.9047715394,564381.3748385534,7773522.601182882,1264287.8261074587,794478.5902991418,2827169.59107667,3203846.743887496,1269720.400709049,1554840.2506034751,2964932.3372306097,1401514.3323562592,608489.7026397889,3800960.666421516,11991602.668311624,5636069.880414642,11218842.165054614,11311155.15859512,3329460.4410650074,1314966.3137626653,5125833.960269896,14548349.083177552,2902982.060314664,5571252.848748022,18207837.21931826,24649140.25989246,1692188.8581944127,5964285.283567067,1691664.8116047052,1641622.4931063766,1474948.0928471545,2281531.3178019603,2544658.9697485673,2058705.277497591,3321914.1820353754,33677402.19945118,2802210.255357816,1123067.4485006137,1317665.0197570897,3607701.8372998973,5162067.191871425,3904861.5105758845,801783.3246075595,794135.1839879889,22823679.23207455,1756538.8412872585,1244786.0567001877,974296.0456427643,1997276.4363156632,4775729.89994308,8261772.784637347,1476117.1766763628,6822449.656513328,1668522.1861761676,1592978.959560928,71612778.83379598,3171855.5853713164,792684.6724122614,439932.72364430316,4856850.140065109,1913236.1470385622,1522621.9224371654,3743261.422268225,4118187.855161005,5029767.366130661,2930618.3142127665,4407421.430126302,6006424.658131979,2842293.2989128632,905512.6202877439,4865186.0468113795,4955961.863866151,1057908.1723823114,256204.7698034104,11041644.023905305,2992268.3929852373,6329091.22039759,45263579.19784065,5056805.236305607,1930522.986100683,1648109.1066019232,1032690.223208643,1467780.5242633787,2227214.440364492,7432631.16976873,8020526.652697967,5369710.69294983,1454662.6896857577,3224604.352081809,3441889.4622204667,-315849.64539454924,4500621.055042854,856735.5246366863,624922.8755071589,1024595.3496245621,2447891.755618586,827291.7118959569,1550264.4763984112,1089653.5264897747,3162964.5586142372,945783.135308834,1729563.2964106244,2114636.3246847624,4232505.784037124,26867646.297391362,1071739.5268387194,4671269.527244715,1653977.9501272612,1034734.723607189,1907978.1806660772,936780.6529544361,4335834.295571316,1030203.7383642015,3075415.0627623512,999764.2054316942,3313897.194781853,3827936.7727736756,3914107.8850880945,2126996.358802365,4747349.479790103,1243954.9563656119,1522379.4639137578,3750752.957082853,9436374.16594432,3891451.223863323,2880687.3849753025,1478503.1194131086,519645.74812670704,791154.4986103799,522908.40823948383,744334.411898498,3976775.0778893055,173230.37121291808,4131473.259552951,742293.4291773278,76398497.75047559,6046914.098280486,4258223.517719752,1483401.9863067192,1084323.9512887835,1423611.4029885943,15022352.334236607,17930776.021811,2332524.109005984,1832798.0017742037,6033588.583446188,1936280.8584448923,339511.16265575076,-250938.79823948024,4552170.6095716655,397582.633327886,749558.2180632087,528431.8716425325,605760.2052791263,1699181.123694626,15101760.41999917,931459.5051291697,385813.061426979,-55635.47728295019,743581.1951185849,1156810.8470499597,348961.6398160453,428751.46789625986,6956630.343246074,-39582.240886185784,1467867.3942443966,4428139.523772368,2678617.848790374,1868477.2824166657,701078.3970691231,6082737.749212984,1954331.1806404288,4559226.234732199,1330243.2036871128,2902072.151719452,15442317.286007127,808852.7096603494,7963517.587800013,5783171.680583581,2086151.2823084355,-143477.38980052294,55874.2704202584,2508500.881292927,15959605.306516223,2435327.727148805,389328.7990411322,3307604.615776427,152960.41320313886,2774792.6223488944,91508463.24857062,3072936.834270134,6982087.443873737,2308292.568704798,2681436.4733739737,4511120.88772974,6481400.625847286,4442415.094092427,3470724.837853037,1130477.8707565914,5168782.042526525,967196.3475088386,-38829.67728891643,2516476.1774706393,973871.9666393944,1632834.7305218163,21262979.57848602,1400900.9780614176,5038175.660837065,4921533.712699405,2857153.197179824,-127973.87874411466,1343934.260272986,827493.9820966756,973212.8095613678,1864546.7782058134,1156999.1050233075,784212.1688439338,3448219.599631695,296108.3344052597,1780485.1170074237,2158441.3000489054,11979517.265313497,4027674.418907575,2193357.253695602,3157592.952952154,44732313.66271521,5124017.956870269,9418805.863185162,4081534.722680472,20551096.200365026,1004747.6824344127,1454022.8538867636,25463.9331546491,5297634.2985189,2214201.849124584,3903716.9440900506,3749192.8833270352,61478550.91222775,57815.251947820885,922176.167281053,2264572.3543935525,22812244.34582312,926497.1791061223,2323919.808853506,1963957.398029842,1459906.2583334805,2772863.699222268,-1020009.2826666585,3807656.933540395,721065.8768323369,19633983.946688995,5947342.930868914,1686397.2345203878,638245.8240526137,9842238.564583227,628992.1536812694,3272967.9600279843,1922375.3850209923,334605.1493069008,4934752.980840094,5046375.696793191,627780.3332849366,1906068.809586151,483710.805852066,1597130.6374142359,1691187.6309261897,1199526.0971056116,3253962.1270080972,2229450.311123296,2113608.7459042748,1869358.1880351906,44078.885318077635,753103.834344065,1198082.617360266,1318182.95905873,1970261.1746174463,1029707.6295604997,337006.3304475865,1466615.1135041665,1221501.014888136,9145229.648620628,3965898.5967661347,16002766.925780972,10120700.413210262,1858750.1812570933,183964.2578390229,4547473.986760145,1085368.3156871328,758847.7294507148,5231302.550363861,2999290.616945341,1399481.1933461092,1126264.4927167944,2738686.0062648067,62655.69378840015,1914681.2284442645,1302419.1309640703,5199231.40826245,7420142.4884596765,1433708.1854751084,2164789.596546813,19994866.62428246,1035094.6366774344,2179086.9299565842,1454920.9796857308,805715.3015196244,2756927.5028359815,5128649.947295163,4116178.9323847005,977519.4037864232,17298050.89660563,6446481.290818606,4252265.74101881,2601542.212518837,2126552.344695411,9833684.13713369,8482132.332511162,1646092.9437143398,1068473.3394248222,7545901.798624614,805729.3168069909,2537555.408690336,785846.2045875185,2682040.5316699673,1284678.7074826686,12365796.470164044,2596838.964384404,650092.549306507,4538296.257909343,3931931.817392186,9485916.489650933,7916383.684450585,751152.8055496132,1778856.0421898363,4835198.0136883985,4479289.085329384,3480447.114129776,6153844.124857636,14752192.828051716,4135738.164345115,425785.13496544724,664427.618440277,912748.7501229844,4819927.69050568,1544297.5720244113,15988057.481110815,2105298.167612171,12701226.997643422,6353799.221098134,1174609.5337373242,7780386.468403133,2932870.041960192,1968896.0161626867,2746484.801406529,22555459.605893154,780452.868006302,807730.162790776,20962145.21496735,3633664.813703633,919696103.9985956,1047438.2631136999,896366.5837016725,413295.13450514595,85142730.70864083,1629748.218156606,1256080.5077045832,5345440.600952944,3364154.4911456387,1370406.9743350546,1039480.145157651,4379224.889073044,223036.2378683288,10266515.902617104,1038962.1169343526,-217547.69752751943,2492957.1213385263,3155787.2696228996,2617435.033923791,10256372.246632168,7252875.354543872,18397728.273141075,1030724.8125843918,726885.4258756156,6828020.5957577005,1793358.309187407,3471220.628273886,654456.3851117841,290535.63890843047,1253645.7908528727,1731949.9054826556,1957475.8316398663,845611.3425490893,732273.404276601,1603656.266799811,1177848.830697181,421600.45481400937,1826812.2581836772,500350.04967801785,1466193.258614583,828947.249921767,8213019.426860765,3819229.706834376,368336.68642716855,15432418.65481762,3252945.2896879017,1402929.0491898435,728602.2354535763,10011051.255664995,588079.1334583808,1931354.9985022591,9488415.840269556,362473.863132535,1353392.8494118806,438478.51044194144,723633.8333550515,2620847.8146426175,4228120.528986199,1649203.1608179007,3185613.694753719,6255754.134290701,10859539.403221264,982164.8105479856,1750490.320308772,2297929.115856943,5913761.912590432,2621838.8105623764,56662249.71537119,742457.7731437094,492372.72908596206,878700.7963117459,3293335.8623631056,2927189.7919761804,363495.7364417028,4717185.453997799,4131886.9136326835,1207554.4707422254,9469610.010219105,2089988.4167196762,132767.08887527278,4961267.201871993,5571634.723303683,-20756.27289821487,185899.29554588022,17053451.76943359,2878738.2768676504,15135263.010540707,1243460.8368081197,5772311.930548698,4513381.281393869,4995618.080844996,1568115.226625801,1278128.2572421986,319635.98184681893,731457.2223543758,998605.960795247,2122330.48126149,9656100.940116592,4952251.249864242,601828.2283670616,2664993.230860193,5255647.442053551,7262786.705918176,2930964.8292030995,4093330.653416666,666569.0669529124,1365553.9189993194,746108.7597110891,12535614.611509822,-234288.85454359278,2983843.273807683,4957133.427042737,1884732.9865449197,2014235.9453580838,4208113.645949952,3435804.259856557,10853255.547024494,-756014.075142845,11307151.802315274,4199251.460858826,1035081.5894334246,6134365.769957509,6680506.419266112,3349418.7340137153,1589124.8033000869,13540788.38508225,3032135.8578059557,2545490.0169026665,2025751.0206005592,1513924.677259431,8581935.90191514,4826199.691475362,5085604.815695601,1774084.7700905695,10332780.75351113,12922372.877103878,9892.21143610822,904389.1983128684,402809.8627396398,864094.3532343458,18825856.650615964,1651803.3651503953,10550521.920826137,1521723.915193943,861992.8066390799,949932.3299935646,118646.24543273542,2182711.453615634,2240208.62018196,312841.50841973326,640212.3581426593,1215540.6139449333,4382661.747680912,2560825.040556793,6686379.937345484,1000095.8793818566,-186044.7947639129,2865872.236316734,-91746.28531856416,1494999.1797609779],"xaxis":"x","y":[4395510,1499128,805751,58761304,3639670.25,1327591,476069,17714332,555153.125,143423024,415453,1160989,1696978.75,1334876,4585966,11962615,13730204,2356376.75,18470340,1396691.875,2056762,659176.1875,619409,791035.5,9683979,1241355.375,29540052,1237655.25,4018570,6118300.5,1568550.75,526752.8125,7575080,2058584,2711035,965786,5135040,669217.6875,1692323,803938,3497215.5,63668488,809291.8125,21069722,291614432,10105379,32141070,1523508,94560088,3073937.5,1024866,1516831.75,553230,1394408.25,1138386,3546459,4107827,2014500.375,20675922,1115107,1396280.25,151754.2969,1127895,2615247.75,2735128,489557.0938,2311532,5227992,848497,22965152,666967,673030,3242458,433443,614044.6875,4216639.5,12549118,733249,770736.125,1848074,1051523.25,853314.5,1017787.125,501194,2298815.75,2255026,2202407,1045337,1436109,1362523,19526080,13146645,635516.375,1532390.25,6227419,274682208,1751483.875,30135280,2265487,3218824,1725558,816300.5,666392,2292169,2663520,931148.625,7520554,7613116,1232712,4970047,942962,1166069.875,9387211,5826745,1423463,2660396,4103027.5,784584,8233527.5,4034028,31803568,345399,2360656,711118.375,549438,901679,1939297,1094343,2455923.5,1992158.25,817695.6875,2986728,1008536.813,1078936,654446,2591768,555709,5401277.5,2099386.75,1983256,862945.125,1417728.25,14172606,643740,163945984,1051944,1569569,8747819,534703.6875,5032796,518107,556700.6875,1261963,114648520,9313513,7251589,2612378,2923484,754241.3125,162960,836437.6875,2547848,1563070,677476.6875,7860942,10040121,2148565,730672,3009967,9986032,505601,308400.4063,14064603,13631685,4748137,10434387,604183.1875,2139713,920068.875,1128060.75,11624548,596242.1875,3814590,2844685,1278437,1119679,21346100,2136086,4991893.5,590629,659368.625,1437569,450518,1047461,858701,9788717,1769137,2870914.75,1025659,10444297,4916451.5,641713,867184.375,688641.8125,746858.1875,15324777,1829981.375,743748.5,1260902,1307883,966812.125,3540225.75,1063960.125,922153.6875,794715.6875,1200811,4581493,6426022,4178233.5,8437401,1063480.5,739869.6875,824320.1875,8218155.5,913117.6875,764832.6875,2402535,6843855,37022180,706505.6875,1858224.625,2583451,2004777,1237252.75,10775646,16467486,2001643.75,576052,701422,6797528,270838.5,3265995,31371618,727374.6875,811900,784584,0,2545405,1631031,1536998.25,4998015,132105,1524499,1279570,6268627,1060818.25,991918,3158151,1441445.625,648022,1702639,3686962.25,2960159.75,255971.2031,488181,1219744.75,3321035.75,3043767,965651.125,282898,1645748.25,768479.625,508051,331059.5,633408,2454861,10834087,3752132.25,1108604,907617,7456330,1076693,966764,576644.125,975721,1987934.625,6962073,455798.9063,919180,3242066,6657348.5,1128179.125,2625396,10564729,1832031.5,452614,1365778,11508035,1587454.75,2009703.25,6860521,1544146.625,887022.875,3220635.25,7771933.5,2584082,536642.875,14619984,30038490,848734,2018011.875,1612167.25,3468606,1287422,785901,1287040.625,2225512,722062.875,645028,5975230,1878442.25,3655453,2322314,3073778,1425226,25631512,2946756.25,2483074.25,5662234,4934334,1123416,1826203,1187499.75,3518459.5,5917000,904059.875,4917420,601590,2204020,2559403,1140137,13609040,19991412,552015.625,1544117,2839882.25,1339220.625,2466100,4636396.5,1730431,1700057.375,1072487.875,1090545,6858533.5,2306723.5,918514,1157976.5,2156407,858932,668915.8125,1904641.75,1769998,0,530700.125,2804534,1678495.25,1003551,5572010,634561,13901935,1360847,650313,2195160,1335007.875,1107290.625,1031667.5,1234149,2114860.25,2805826,591845.3125,2251442,1898509.75,6299357,1076389,3091387.5,9230223,1544388.5,841841,1381590.125,662424,5053635,2401148,6182350.5,22078064,790671,4608963,662685.3125,1822830,3106930,1733511,362823,830286.5,495481,601818.625,21027396,952879,2337651.25,8037287,1809586.75,1657201,1167813,720170.6875,55030192,1862176.875,1509292,2323265,1072855,25970248,9128267,9029489,5976246,1934971,3316562,948676.5,755269,598857.8125,8190809,1026921,885958.125,1878742.875,3212269,811907.625,564754.1875,336461,1186835.125,6411570.5,1960538,847298.8125,2456132,2828950.5,6298131.5,12448381,4677797,753721.125,13458948,5722325.5,12931002,1482752,645055.5,2903161,6722886,2373568.25,3375211,2822377.25,7768057.5,1831722.875,640374,1485945.875,1224045.25,5216213,730292,455153,37797084,9375125,624180,1860084,1015060.188,16016644,670779,1427294.375,776248.6875,39403320,2664566.5,7182436,41399504,786253.6875,1208168.75,974179,1325597.625,26941110,463371,938833.625,5155090.5,1811506.375,876339,1083467.125,5804784.5,1044642,3652708.75,588856.125,15088676,552822.125,648879.3125,967432.8125,892227,507451,405050.4063,1750419,405062,593042,1116040,1827424.125,222600.0938,568033.375,622051,2003447.625,2654755,2233268.5,878927.8125,560334.5,919746.3125,1247972,587903,1732954.25,880115.5,7592451,1527348,742565,4133811,2442220,131373880,1190733.5,2133797.75,1581036.75,1041652.813,1912690,2534671,2483320,2698077,1191926,1239315.375,6782435,403800,604591,3060149,1826107,1035554.688,4784731,6901554,1712796,608199.8125,5820965,550767,1332912.625,3245772,304248,1003176.875,6057734,726124.625,7802205.5,2108926,2827818,2014245,8354235,888561,527438,335477,1288669.5,1456808,3835202,1775433,2843033.5,4285049,8422861,1651120,8188381,5671256,1191803,531563,8688676,2964206,1580597,2133372,6917478.5,5447076.5,862826,3999505.25,3721129,593824,929688,13214416,602245,4865551.5,632157,14431245,4729846.5,11208498,1920352.125,3053785,2147062,3322250,1702992,2759674,791052.875,2683383,737570,695059,6471427,951675.5,16808.90039,4222682,1324534.625,1167887.75,1391714.25,12204922,6577130,751292,1761048,552391,3039626,669117,95544,4338616,1012559,9997511,57133.19922,4247457,26416770,842511,8819864,712149,1960558,3298570,1670929,4983934,4217623,2996502,493000,4801472,2235765.5,635698.6875,3467698,1310481,470666.6875,1213869.75,1093627,1372706,61762380,842923,1820292,961491.3125,1467029.75,3042875.5,718362,1453543,1222509,2594119,7791957,15571642,896172,2763597,4039667,4475652,776237.375,811021,6352846.5,6387710,12271749,783651.5,626612,1255834,2527198,3894142.5,1972156,1691963,699382,6083307,619870,586827.5,1452804,2058955.25,13962749,24126,10591549,3240830,2953338,1260416,704799,1617421,1224642.5,3822102.5,3863772,1162226.375,921786.375,2100766,2019254,5134446,1536287,782494.3125,713326.875,2892779,317794.4063,456397,3182301,919048.6875,1976663,3065502,593953.375,767297.8125,764906,642825,3397709,1084553.75,736838,1403820,1084048,6983736,1171380,1985171,851439,1677877,1107870,990361.5,847514.5,80469216,1321065,2411402.25,17750994,5436057,4757299.5,509171.4063,3258819.5,7015692,1271021,3435687,1987218.75,1034941.313,340270,559234.6875,2158629,966677.125,3637973,689373.8125,858780.625,1705515.25,512166,97690.39844,675772.875,815800,2184336.75,2751694,1987073,1169948.875,938908,1280245,1375978.875,2845946,1194835.5,606111,753784,892634,3113412,1762174,2921368,18868768,5348309,6376820,2058532.875,7782092,1212551,1224276,1958352,544172.375,8393893,767361,6522551.5,10939331,39523852,4519115,10020111,2601945,620104,8800292,968698.3125,1756668,2065156,2773779,9343428,219483,3303553,6917280,1091162.75,1306087.75,2849048,1429855.75,410433.0938,8873485,9049598,696522,3046543,1433504.375,1503823,551219,3789270,958725,960987.8125,1438479,1988784,43943,738983,1516377,1181963,806398.1875,2326565,49102164,1983509,9273278,6931901,1784796.5,1963054,2125323.5,3833052,4103800.75,318962.8125,7286313.5,25747022,1324965,5086863,1422406.25,1758581,1486177,1162493.625,1192100,970118,2646523.5,12263687,4767227,571644,559805,12250422,13410041,2726369,447947,1988760.875,13300859,1348497.125,661380,1625308.625,4236396.5,1906597,5450441.5,106918,2270531,41680640,771512.6875,35685224,0,1852926.625,851053.8125,1739286.75,1748629.875,863703,882896.5,2837589,571309,2237482,4765753.5,10513817,1234095.5,1372475,552729,3886929,809257,728167,6583940,2203758.5,1165598.875,59124384,1727783,1048813.75,838571,948157,1966424,3786217,5328224,3869754,1637199,1054581.375,1108310.875,2063212,1608224,1907697,1126157.25,502667.6875,662280,2439061,1019487.188,1558348,1162120.375,5762597,611997,4467292,1530744,2082754.75,16020642,514471,3751417,207572,2245568.25,4520838,855665.125,1688612.5,2473302,6136327.5,1423438.5,288191,2585187,2145603.5,10403123,2053611,1337968,839115,4369513.5,4498367,2291581,22530168,694479,508014.5938,1620721,607373,520011.8125,3234787.75,452744,930611.875,723424,85357952,2733003.75,3625434,2228675.5,524053,717056.375,10249433,18829136,3040924.25,3030041,2057457,1115504.375,1419453,602618,1539478.5,635697.3125,323739.4063,569115,1257144.75,630040,8767617,544043,619545.125,917783.5,755856.875,677536.3125,385719,558893.375,11521702,1382309,1823782,1602030,2147013,32579658,574650.375,6668568.5,1198291,3986851,2470566,4207110.5,6576563,962577,4734631,1049748.625,1623657,824516,647792.875,1225404,6481807.5,3064039.5,510517,1698074.625,1135172.25,1813718,74130576,2330029,6366084.5,342726.0938,2938366,1045413,6832991,13133225,1183042.125,1286882,4025787,557193,1475475.5,3147042,956958.875,916864,12720069,942712.5,2246747.5,2150120,14084606,2214469.5,1913558.375,1103762,684941.375,1216543,3542708.5,780410,4742869,1117658.75,806615,1149140.25,7539963.5,3202213,1366000.125,6807227,19827916,13253979,5149928.5,1706301,7811542.5,945245.625,209698.7031,1926134.875,17320768,1182575,10557910,10343027,157606480,1646088.5,116486.6016,2261586,13969552,1652833.25,1121259,7492939,928584.875,1856230.625,10018321,13010578,540465,13239050,11924933,3494929.5,2014475.875,19036416,1161814.625,5326222,1430356.75,607579.6875,5968153.5,3528086.75,1093757.75,8631444,770082.125,2520919,729096.875,1294615.25,4107443,485971,1210798.375,1858029,786979,955223,1118490,747326,1943899.25,867456.875,628609,2447861,1101536,12719042,3366528,4545148,9941308,998633.5,1210229,1833381,2057836,1534068.25,1813285.25,3303989,1380283,209644.9063,1210262,954352.6875,1594733.625,851585.1875,2266570,3591653,696709,1111858,48729588,969682.8125,4270584,1094940.75,984178,533742.8125,6014078,2331952.75,44293.5,11195917,2910252,29195468,3104171.5,2485521,14465798,3590731,1921890,1509775,4785692,188745.7031,7240274,1014405.375,3095882,1293390,5499097,226375,974940.375,1889880,4224872.5,4028943,3574542.25,4602793.5,8518313,3312125,5948007,3377209.75,2581741.75,22325364,3241917,1658751,577033.875,1649327.625,1588933,729397.1875,6505995,526059,1738376,2434208,1545485,16402563,2023258,537141,2503690,13567774,481206,1139980.625,8320719,1777005,873923712,666930,541950.875,863728.8125,61576184,5462958,2874181,3897738,2093011,666687.5,845961.375,3548858.25,3395651,20725996,1022114.5,2217620,360045,2184156.5,1355558,5525072.5,4089677.5,20560622,758875,621751,2188452,824483,1042652.625,698326.625,533226.8125,1045640,2217046,7801590,525668,789630.125,841821,866343,1481017,2798967,1359579,850615.8125,1860901,3091348.25,1207092.75,2094410,6125692,4039168,1130960,803533.3125,5121586,1049088,735178,8168547,524144.6875,2001936.5,383524,2772124.75,10642754,2759071.5,1353079.5,3245111.5,7380088,6836900,1807738.25,1296959,1604129.75,3605749.75,1551294,25210246,2159932,862744,1862752,3888945,1443367.75,1676426.5,5444253.5,4099807,2233262,1358022,760826,678663.3125,0,6609358,1028277.313,2307938,61827040,0,4321141.5,572139,1997181.5,1594422.5,2755578,523133,650205.8125,519317,108200.3984,844076.8125,1350190,465533.3125,2189030,464155.1875,637246,2131817,11026945,5424718,2401890,501741,1008665,730765,7564643,1784385.25,1383950,227566.7031,702385,1641667,816846,448676,1083729.625,801645.875,13315803,11702959,609921.6875,3195230,2506927.5,2249592.5,2619088.5,3780078,1387773,1072500,1075019.375,1908612,2992693,5134607.5,11458215,1897180,6579348,15590743,1261634.5,1392015,541542.6875,1962435,20781390,1795462.75,0,819378,400354.3125,5047936,1961581.5,2422767,1913034.125,622572,561368,1787119,3494383,1244824,2549649.75,866096.6875,1539176.625,2267094,1444429,1666221],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","SiteEnergyUse_predLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","print(ScoresLasso)\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[10859986.5929849,10859986.463571664,10859986.340541149,10859986.217036864,10859986.091380712]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[13325636.705243777,13325636.688301262,13325636.679380381,13325636.670852173,13325636.662453415]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[8394336.480726022,8394336.238842066,8394336.001701916,8394335.763221554,8394335.520308008]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[8268278.250111802,8268277.719186623,8268277.191471303,8268276.655824868,8268276.108697672],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[15265324.46647478,15265324.648043681,15265324.82961262,15265325.011181602,15265325.192750616],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[9488373.471290331,9488373.554112913,9488373.641532956,9488373.727779554,9488373.813368322],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[9567953.958875773,9567954.123320786,9567954.28107196,9567954.446857046,9567954.614371968],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[11710002.818171816,11710002.273194315,11710001.759016905,11710001.243541256,11710000.727714982],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","FigRMSEGRidLasso = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso, 'alpha',\n","                                GridLasso, None, None)\n","FigRMSEGRidLasso.show()\n","if write_data is True:\n","    FigRMSEGRidLasso.write_image('./Figures/ConsoGraphRMSELasso.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.263e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.272e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.272e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.274e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.274e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.272e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.275e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.656e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.770e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.274e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.272e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.770e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.472e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.275e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.265e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.279e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.472e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.770e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.658e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.280e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.473e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.770e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.487e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.282e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.473e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.274e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.656e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.660e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.488e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.283e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.474e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.275e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.279e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.656e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.661e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.489e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.284e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.772e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.474e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.656e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.280e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.662e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.286e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.772e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.475e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.656e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.281e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.272e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.664e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.287e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.476e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.772e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.282e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.665e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.491e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.289e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.773e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.476e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.282e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.666e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.492e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.463e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.773e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.485e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.279e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.282e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.274e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.493e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.668e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.293e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.738e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.452e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.615e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.774e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.280e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.476e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.280e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.669e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.495e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.275e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.694e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.427e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.295e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.774e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.461e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.554e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.280e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.761e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.269e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.496e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.297e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.367e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.577e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.635e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.405e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.254e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.279e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.459e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.753e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.480e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.672e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.497e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.299e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.226e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.473e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.239e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.609e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.202e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.447e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.051e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.469e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.335e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.674e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.498e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.901e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.219e+16, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.301e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.550e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.359e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.691e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.053e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.420e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.111e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.676e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.261e+16, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.304e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.500e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.276e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.420e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.251e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.608e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.415e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.777e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.576e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.360e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.290e+16, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.585e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.678e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.501e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.270e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.307e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.778e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.196e+16, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.329e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.121e+16, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.770e+15, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.204e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.229e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.357e+15, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.461e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.641e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.300e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.272e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.681e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.503e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.309e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.527e+14, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.779e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+16, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.522e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.486e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.465e+14, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.146e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.080e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.747e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+16, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.453e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.400e+16, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.944e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.272e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.476e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.627e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.505e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.312e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.091e+15, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.779e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.268e+16, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.728e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.487e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.324e+15, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.725e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.596e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.403e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.438e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.376e+16, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.685e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.506e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.840e+16, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.023e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.315e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.780e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.489e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.696e+16, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.974e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.849e+16, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.587e+14, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.676e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.405e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.274e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.531e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.448e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.688e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.319e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.508e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.422e+16, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+15, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.781e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.829e+16, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.827e+14, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.388e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.562e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.338e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.274e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.405e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.691e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.510e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.243e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.322e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.782e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.103e+14, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.678e+15, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.492e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.270e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.637e+15, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.196e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.651e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.086e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.307e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.275e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.193e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.693e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.512e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.326e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.373e+14, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.784e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.493e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.428e+16, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.908e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.420e+14, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.524e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.066e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.697e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.514e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.329e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.495e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.785e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.670e+16, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.743e+16, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.389e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.723e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.700e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.464e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.517e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.333e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.786e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.497e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.894e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.124e+16, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.276e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.642e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e+16, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.064e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.811e+16, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.759e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.461e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.703e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.337e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.519e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.787e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.499e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.593e+15, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e+16, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.228e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.476e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.275e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.993e+16, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.522e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.342e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.707e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.501e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.454e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.751e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.789e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.653e+15, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.109e+14, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.470e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.611e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.948e+15, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+14, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.442e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.524e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.346e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.503e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.026e+14, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.572e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e+14, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.707e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.714e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.527e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.420e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.351e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.792e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.506e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.431e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.492e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.255e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.377e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.645e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.530e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.793e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.508e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.356e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.328e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.228e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.377e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.294e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.508e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.361e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.795e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.511e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.258e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.008e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.168e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.727e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.202e+17, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.536e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.366e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.513e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.797e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.978e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.025e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.457e+17, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.539e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.819e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.943e+16, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.372e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.516e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.799e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.602e+16, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.661e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.659e+17, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.306e+17, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.600e+16, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.737e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.543e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.377e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.519e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.801e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.420e+16, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.427e+16, tolerance: 9.089e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.039e+17, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.832e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.546e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.742e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.805e+16, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.383e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.522e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.558e+15, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.195e+16, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.748e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.638e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.550e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.217e+16, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.806e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.389e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.526e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.713e+15, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.668e+14, tolerance: 9.978e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.852e+14, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.388e+15, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.753e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.554e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.396e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.808e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.368e+15, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.529e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.558e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.811e+14, tolerance: 8.670e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.811e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.402e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.532e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.344e+14, tolerance: 8.859e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.766e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.562e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.409e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.536e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.567e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.772e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.416e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.540e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.816e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.423e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.571e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.779e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.544e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.819e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.576e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.431e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.786e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.548e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.581e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.439e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.826e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.553e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.801e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.446e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.586e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.829e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.557e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.809e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.591e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.455e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.833e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.562e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.597e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.463e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.817e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.567e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.837e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.825e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.472e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.841e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.572e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.834e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.846e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.578e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.614e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.844e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.850e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.583e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.499e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.621e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.855e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.589e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.863e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.509e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.861e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.634e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.874e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.627e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.866e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.884e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.595e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.519e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.529e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.872e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.896e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.642e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.615e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.540e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.551e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.665e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.884e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.630e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.919e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.563e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.932e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.574e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.891e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.623e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.673e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.682e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.945e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.638e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.958e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.905e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.599e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.586e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.913e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.700e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.691e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.973e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.664e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.655e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.921e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.612e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.987e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.625e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.720e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.673e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.938e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.639e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.948e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.742e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.731e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.034e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.704e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.682e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.693e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.958e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.968e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.667e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.753e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.765e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.715e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.069e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.714e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.087e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.979e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.698e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.990e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.791e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.778e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.125e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.751e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.002e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.748e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.738e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.731e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.804e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.818e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.145e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.777e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.764e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.784e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.166e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.027e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.765e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.041e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.848e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.833e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.791e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.806e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.822e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.054e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.802e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.069e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.863e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.879e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.821e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.232e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.837e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.084e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.862e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.255e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.842e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.100e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.913e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.280e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.896e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.870e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.304e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.883e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.950e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.330e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.931e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.356e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.887e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.951e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.151e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.928e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.169e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.969e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.383e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.924e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.943e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.187e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.207e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.974e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.009e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.438e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.963e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.467e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.049e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.023e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.247e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.073e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.526e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.101e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.005e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.290e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.096e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.075e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.119e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.049e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.557e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.072e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.156e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.587e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.312e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.128e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.335e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.143e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.619e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.096e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.651e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.358e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.120e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.382e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.184e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.193e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.144e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.683e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.715e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.272e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.406e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.431e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.244e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.270e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.748e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.195e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.332e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.456e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.781e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.324e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.302e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.273e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.297e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.814e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.363e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.394e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.848e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.247e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.507e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.380e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.882e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.328e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.560e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.915e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.456e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.352e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.425e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.301e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.586e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.436e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.949e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.408e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.613e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.355e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.383e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.982e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.520e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.488e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.494e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.465e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.016e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.439e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.049e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.411e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.583e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.552e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.667e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.695e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.522e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.551e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.082e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.467e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.495e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.115e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.722e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.615e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.749e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.148e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.580e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.552e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.709e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.776e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.180e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.524e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.678e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.803e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.212e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.666e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.830e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.243e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.637e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.771e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.607e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.579e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.856e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.722e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.740e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.273e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.661e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.634e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.304e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.882e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.831e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.801e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.908e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.333e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.777e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.750e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.362e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.889e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.715e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.959e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.688e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.860e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.830e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.804e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.390e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.766e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.740e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.418e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.945e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.918e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.881e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.856e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.470e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.444e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.815e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.791e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.999e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.054e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.031e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.972e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.906e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.930e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.839e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.496e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.862e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.520e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.049e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.024e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.977e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.954e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.906e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.884e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.544e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.566e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.097e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.120e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.073e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.140e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.588e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.020e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.999e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.160e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.927e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.141e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.609e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.948e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.119e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.179e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.629e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.061e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.041e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.198e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.649e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.987e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.968e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.182e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.162e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.099e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.080e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.023e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.668e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.005e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.220e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.201e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.685e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.133e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.702e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.249e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.056e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.265e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.255e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.719e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.116e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.238e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.280e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.039e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.150e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.165e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.734e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.071e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.749e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.086e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.287e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.271e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.308e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.194e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.763e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.180e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.113e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.776e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.316e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.321e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.100e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.302e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.334e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.221e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.789e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.138e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.801e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.208e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.342e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.346e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.329e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.126e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.357e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.812e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.233e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.245e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.823e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.150e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.368e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.365e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.161e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.378e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.256e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.266e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.833e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.842e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.181e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.171e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.388e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.386e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.397e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.376e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.276e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.285e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.851e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.190e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.860e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.405e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.199e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.406e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.414e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.302e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.396e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.294e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.868e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.215e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.875e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.208e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.422e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.414e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.429e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.318e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.883e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.310e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.230e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.223e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.436e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.437e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.889e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.429e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.442e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.896e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.331e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.325e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.450e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.901e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.243e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.448e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.236e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.443e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.454e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.343e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.907e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.338e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.254e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.459e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.912e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.461e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.248e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.456e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.354e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.917e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.264e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.349e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.922e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.472e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.469e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.259e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.473e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.363e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.926e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.478e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.273e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.930e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.481e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.268e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.476e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.482e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.934e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.938e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.368e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.485e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.489e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.280e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.485e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.277e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.379e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.375e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.287e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.941e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.944e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.492e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.496e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.865e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.284e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.495e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.492e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.866e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.382e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.385e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.947e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.290e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.866e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.293e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.950e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.498e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.502e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.499e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.501e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.866e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.388e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.391e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.955e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.866e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.952e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.299e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.505e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.296e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.507e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.505e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.503e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.393e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.396e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.866e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.303e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.512e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.301e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.959e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.508e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.510e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.866e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.400e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.398e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.866e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.307e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.961e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.305e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.867e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.514e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.516e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.963e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.512e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.513e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.404e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.867e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.402e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.965e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.311e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.867e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.520e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.515e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.966e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.309e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.518e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.517e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.968e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.407e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.867e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.518e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.867e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.406e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.523e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.314e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.519e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.521e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.409e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.312e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.868e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.410e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.315e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.970e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.317e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.526e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.972e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.521e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.868e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.522e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.524e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.411e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.868e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.973e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.319e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.523e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.318e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.868e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.974e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.527e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.524e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.528e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.415e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.975e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.414e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.869e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.321e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.320e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.525e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.529e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.976e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.530e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.869e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.869e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.417e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.526e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.977e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.323e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.416e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.527e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.870e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.977e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.531e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.322e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.532e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.528e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.418e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.418e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.870e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.871e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.978e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.979e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.324e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.534e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.324e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.528e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.533e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.529e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.871e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.420e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.980e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.419e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.872e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.326e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.325e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.535e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.980e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.530e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.534e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.530e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.421e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.981e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.873e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.872e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.327e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.981e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.420e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.536e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.531e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.536e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.531e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.326e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.422e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.422e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.982e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.327e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.874e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.328e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.532e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.982e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.874e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.537e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.537e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.532e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.423e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.875e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.423e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.983e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.876e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.329e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.983e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.328e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.533e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.538e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.538e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.533e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.876e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.424e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.424e+17, tolerance: 8.859e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.983e+17, tolerance: 9.978e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.330e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.877e+15, tolerance: 7.078e+13\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.329e+17, tolerance: 8.670e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+17, tolerance: 9.089e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.534e+17, tolerance: 7.078e+13 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning:\n","\n","Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.358e+17, tolerance: 1.092e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha      0.276829\n","1  elasticnet__l1_ratio      0.000000\n","                    R²          RMSE           MAE\n","ElasticNet()  0.819662  1.227637e+07  3.230115e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predEN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4799313.37651941,2646012.334372718,2308587.182095859,79457367.7136528,6411462.767262178,1325021.1827772015,1269744.4207346914,6866566.4333218,1287584.906610902,44435092.14229345,1371107.8427559938,1724374.5824365993,1531459.9872451972,1742796.4151108032,1513342.6520578135,21451147.868708454,25446808.891315777,4938700.4253387265,41827213.41941885,1392753.812736508,3002634.7132377946,1570631.5024563647,1219777.495440107,1158948.3561913138,9261206.006877644,1188527.485198089,2778723.756184119,1100178.2476243372,6326722.389107613,2619036.2661881307,3771695.345012235,1144586.1348507456,8176674.927489007,3087126.895937567,5320730.1464016065,1542860.6391023647,4685900.5474224,1319610.4412855303,4041944.4629427977,1460075.2620684921,2909199.388955515,13763963.428835128,1120555.719824876,7042233.989391787,119878020.96123512,4485911.455429204,34838522.26132808,1810551.6240219427,106757987.82239845,5985624.1025474295,2148427.8814029843,2114111.783401461,1290371.0942554802,1653181.7368682907,2815223.491820981,2032851.4965696386,4068274.2399286865,3782678.761758673,17491228.441052545,1638551.5123782596,1880444.3468011976,4682869.950495539,1234617.808402259,2387768.68216641,6408524.30287862,962177.0699215978,5978368.308596116,1910834.4456472085,1585925.5602733525,6888656.509915009,1399877.049605643,1206643.9336200836,4420048.904957164,1209467.9737608684,1481248.4686785506,3782469.6533918204,22394961.2490306,1567316.716179851,1596985.009422735,3829080.966462426,1122299.2191241845,2291662.204326367,2001655.162105396,1050813.6833725767,3366302.342924216,7440372.184640313,1891709.2345478365,2302114.4014890944,4234775.444171747,1504236.290303807,11466931.02691491,9770709.797070438,1097734.6343523806,3465422.186008875,12039242.544402115,19928806.423373837,1540917.795702339,38806251.54593673,5053129.334349951,3278253.408236637,2951820.835319923,974316.6823398457,11338883.84367931,2814878.3428102755,3676530.0389772346,1221211.947261829,8706841.251907384,3233101.8768909425,2568253.4964436013,1844531.5379555349,2076351.818588811,1702066.100816293,25182259.346333224,2916961.185261969,1498844.702751149,4927787.0447916575,14913701.924190704,1493275.7165050567,20174176.351122744,2645623.151379768,3379959.330897426,893514.3590138885,1539779.8826013803,1277763.9262249744,1240948.7676017089,1671385.9183342108,1168925.597354202,1483776.1112577044,6339047.420030832,1344475.1915217899,1555567.3495863252,6830716.81436909,2300563.8207197133,1678412.808098112,1390022.0790652293,7595523.549466709,1264576.569163243,6618835.62362776,1162229.0888653954,2896593.861170981,1634831.4734159405,1140339.049101112,6547472.389898611,1680235.6462474205,45763104.14355982,3269047.4889714015,2465630.1433931696,19425275.162551664,1192102.3662616718,6245921.604577338,1186939.3889012113,1021823.5162119088,967047.7769181062,133669543.49248914,1083020.5802335695,2744068.1271688007,7385777.148717763,4842474.2061154395,960312.9108817442,1562455.888297782,1117416.955119611,354171.68907275377,1927782.1747802203,1611390.4271209438,12346089.809152458,6194769.477011599,5787381.44723412,1498245.1791024744,6166235.680673131,12142465.558509983,965642.9153211159,2453591.750071385,4300540.514511229,13379400.112871211,8363350.822202705,3268486.114844459,1175942.185244256,1731087.8113335348,2542505.2470266144,1282215.3641955946,10172768.542283693,1347511.1923248686,1578056.1163234557,3182636.844716182,1768914.5107421712,1772695.7118247342,29323964.09649541,3254549.349440768,10189164.19606193,1428536.0276387199,1199564.1512895157,2129014.253189979,1476377.9763764215,3218035.756369021,1520753.351769658,5515822.219652122,2042121.4430675977,3562101.1299660103,2337142.5744645554,16633401.494444191,3610400.467278461,1746123.781947197,1155946.494373194,1532892.3903552946,1132483.9973560933,3709529.562509117,2188683.263335365,1189897.5254150867,2853599.864162163,2531616.6014023186,2516944.0224471306,2488022.240021677,1323149.069624661,2172870.237899218,1616120.4457492256,2072456.3710095838,11895714.316721715,6435742.970899368,5313708.367964538,26811694.057959806,2463008.868064283,1267748.6847679992,1561041.4991290832,3377297.5282620643,2392445.5647180914,1921403.2034036915,1792000.0493987594,9895087.374848014,30275705.30536924,1549170.0324080691,1879702.8819627054,2456806.8617632934,1897829.2014891757,2585331.725303182,15974931.92429727,21518025.205905154,3525980.586457481,1020372.8895365379,1478775.777693413,5630438.861813437,999533.8541676423,6475086.304454563,5617025.461928371,1259239.3310052976,1445354.1545868888,1391708.0699934494,4763941.296036834,1344817.4526073118,1493072.0765159435,5614269.775945814,9602507.457841283,1447404.5450364677,4671485.309459083,1993260.2579484219,9893970.46881174,927053.78671104,2653709.059059557,1549698.7222680387,2455307.9554460766,1535780.8567717967,2360674.6109431256,2111413.34027577,7238192.200476252,1581190.9491480337,963895.3883659192,1346289.6820526244,4350899.544983174,2901634.2479305686,989498.856456822,1533383.0554548053,3030177.8689361536,1580212.1549344049,1246598.9785001427,1407491.1287031765,1255774.6176941409,2100946.5164314467,2216522.334064258,7394146.028680148,2350557.2342381096,2212145.922378095,5687452.787822336,2372031.379015739,2218572.01337139,1013557.2691685748,1793299.980465319,8178035.952402679,6684926.431559529,1084014.7183237444,2122911.1837216276,2278179.3473047107,12466697.343914427,1823777.7355820553,3522146.72387229,22517314.7135217,1162036.5337844798,1198443.6273349687,2418449.344811994,2777455.4559513787,2416797.56399564,3463842.703846049,16485097.116739808,4422158.533469794,2195088.5008435464,2626111.6887907656,5584778.731093794,5149706.531272594,1033894.0732212397,9355890.767935298,1980057.260667949,1690080.0503288212,1612817.300281707,2566589.9968234506,1782009.5142064171,2252535.4561736523,1999506.5994239876,2086491.7235681275,9366554.411146747,1730249.0804783609,984421.241885084,1866658.678224072,4890892.925045481,3920715.115725447,4125110.7758700885,4232154.19605496,1343265.237244434,8439627.963071674,1140070.5567876934,4958336.902737657,5789679.417786378,8467928.05194452,1336049.3052536629,1387707.3439873392,2415218.020960795,3698835.306230763,4532545.554358043,1651146.127900286,4090443.4609372756,1489489.9723026995,1256522.7150229455,2811109.906158858,1258795.6783071165,17106060.222103126,9041742.08934262,1132170.0694889184,3593047.5071904943,1478875.6628754612,2236113.1429634504,1642201.3247101733,5610150.027924068,7805759.928383419,1961944.438989647,1503590.2607668582,2038815.3970496028,5771054.606003439,5277276.617092298,1120150.8122987058,10692652.101526177,1579383.10895679,1013574.7117623684,1063021.2243691143,2721282.4570976496,1882920.1391192293,3403471.593565816,888359.1512157565,2218775.8025150965,2847602.895482911,1684456.2857137204,5758354.156196473,1344397.72697637,12924354.930521445,1141491.4408232518,2656895.551993018,1245015.7028703946,1049663.3754057633,2183542.557439989,1369424.569785722,2288714.9239789746,2072610.2743554781,1970721.4667332682,1020338.3058954026,6764566.928329958,2217784.0616821484,9081508.594375785,1242316.0227490943,7021559.606639408,4413627.813587727,2932006.993776682,1179409.1211898504,5328324.88797487,1201098.545708932,7562685.035074398,3596385.3015895355,4144923.3027384705,6955343.9289476,1088745.59135107,5613178.33993943,1537916.1069171233,3267989.6906089573,3596542.1136078387,3528882.8300299244,1180087.733455279,1992174.5804260361,1364494.2823622902,1198182.734874498,16811995.41589421,2033188.8648070553,3800297.6093899403,12381557.309674285,2573329.40500351,2192566.817592163,1956509.753083101,1196327.8987902151,19127692.284355976,2058778.2105487473,2146340.544027024,1425476.7447665667,1696108.6527339593,34407437.34949419,7461811.97596968,3293713.994457656,1396693.6770003536,3451890.04522245,1330641.9286400767,1306298.1099533935,1611701.8570330315,1408920.7001353104,13617706.583595738,2387484.072707038,1885178.2495880239,762450.0931954982,3595574.1070711864,1336122.2604733524,470345.2651814257,1082701.3867748487,1502590.8672292193,23568443.900840025,1405619.5891968284,1737778.9403363066,4179126.509617447,3620180.572103854,6321314.061888143,16298784.258222157,7752102.042330731,1446920.988696021,12172793.266457893,7637673.133319506,12197603.032270052,1462664.7639838045,1495961.9615519408,5151456.127122728,7028361.2843228895,3062383.548939461,3475876.7665105993,5661224.009419326,2130423.296342515,2691840.814859208,1456324.0334940124,3075895.511116852,2302917.2752974792,11440999.411947418,1178202.7432681741,2610871.4663031427,48276178.706854425,6461808.612095851,8030432.510859944,2990320.569289536,6966143.654967928,8187666.855824001,1484948.6983213918,2826348.366227772,1339416.298601563,30699378.19483866,2706060.1414382216,19907620.297307942,7847821.970174493,2553965.7887277817,2211742.353787073,2188329.865074558,1872139.1832243858,18664592.66056579,1659185.341482587,2043096.748069355,6002795.231290026,3141624.9235811573,1506372.5301678998,1148958.1062406837,3420463.8548078863,2978108.728418014,2801990.996896121,1208926.5471289554,17293024.35098834,1279389.480715375,1711451.302823347,3220451.1204533894,2506874.4737293157,1425499.632007183,2496442.5572919184,1804989.753773443,2736104.025507331,1217620.3211488812,2094486.1696791733,2874255.955287297,1225407.315705311,1487630.3859962106,1196957.3715656311,2331997.3571451437,4896907.5182088595,1321213.872681099,1729091.8083666335,756599.5525249275,1070699.3488192458,1254499.874583421,1534586.8982663946,1403380.1926137465,1644944.184557636,6433259.537961665,4536587.442481019,1430282.843495453,1387739.918723175,1530188.418829617,36013864.34547928,3007903.9381087753,1263066.4578219918,1068676.2915762225,4294847.3371598795,8292464.861413904,1904800.5978963636,3182768.1139867976,1373898.9107901598,914201.5836553497,2128648.442359207,2558765.8468225677,1397220.45097624,1347402.6196171972,8343755.573007287,1536588.6454721615,2068477.400509107,9425008.621079281,1188221.87272206,1711112.543648479,2136137.6638275133,7518022.580967188,1344977.6723257455,2511884.2797050867,3175732.3257130766,2045850.3764597513,1698509.0260619628,5506053.8115306925,1837958.6439625486,4813026.967807271,2014136.813336725,2528333.468618186,1168857.5225241003,5071108.149925214,1802880.0438071946,1059612.6024744455,1920785.649543189,1723329.7581875883,2228540.692412685,3138664.1756913783,3257246.8467248245,5002814.309531445,4653636.4215485975,5890592.498129688,5904656.1894878,8592412.34289616,7403593.671955964,1562519.0827382957,859478.1926239752,4341185.338675812,4458263.677560932,2109390.5387347518,4997715.077569449,7942304.801436018,2172929.9585862677,1103514.3212502962,1435269.8132568144,3105335.019081956,1229648.345804841,2185449.324958364,10465483.816221591,11224454.934668086,3087439.929137208,1647321.2993665189,18581975.68175284,2190693.4677607464,12634773.212464666,5023070.241040172,3808067.6561997617,3525325.1781792254,2849209.9649904226,4349590.764581475,3406477.187964134,1552022.5805681213,1195380.8373676934,1577419.627973849,1663598.9414192934,4122301.240934802,1587673.9399622492,1452051.975183024,16175666.996885002,1407736.9817402498,1028868.9881477724,883210.6186428904,2051639.6263848315,4533405.655337466,1351543.9202334974,1721269.5865352585,1224997.43009623,2217424.8122784924,1244357.3894963777,1889021.150169069,7897783.331618588,2240708.9626712664,2331434.908459524,2384746.9433586057,2989860.209986156,41008561.35256999,1176528.4124318373,11582894.691728199,1563526.8759789695,1043825.0441140325,4534477.813968388,2962031.804494135,6331546.852297131,7783354.422607364,3962626.4337039553,990479.9017641277,1483516.4581895361,2285656.43701399,974638.4342097719,8676424.59808553,1807588.4711041043,1295633.3122324469,1536587.4528591689,1674529.0334410383,2203961.991439712,10131938.82648616,1272125.2664200105,2006837.8491979772,2014665.720642902,1255275.808595969,2079620.5731569831,1201643.1443497448,1548349.3826281554,2148427.8814029843,4470925.145884495,16007719.347349953,17489722.61877946,1667222.089678114,3540580.0036853226,10067071.802552406,3085862.9566678433,1838351.9106797231,1913136.5439235435,17765842.328023538,10901254.366452154,23806715.397417895,1247229.3321985914,1354398.8319882187,2303664.9403179963,2809026.032709041,6474489.775393135,1548209.7724968262,2319121.5572739653,2156531.3307845285,12581126.25292565,1292212.553269331,1042719.6095139813,1529174.9605995924,3450851.6753165387,4186111.6055000043,2318678.7321117637,2192524.197265107,7151197.686173588,2668829.706537784,1275692.950861825,1326832.7569066118,4030753.5237038024,2411039.010041264,8849541.258788427,2056736.5080025536,1778173.760869897,2037319.4568033041,1614312.7644257254,5455766.954157596,2407391.457490715,2930728.465399539,1266887.7051217337,2022544.9369734076,1464116.2002720973,1288114.3229882799,1812154.6347419063,7026195.303524658,1254296.314691213,1016685.1506296266,8162350.040565329,1141525.6243688408,1376218.3354032026,1630041.65726825,1649847.8257387676,4812774.140008254,1345338.8088204034,1448052.46342384,2310869.7277703313,3134926.690688798,2461962.9806751325,1773451.2133502443,6024838.682832433,1798277.4016436182,1121041.2250441303,1869663.944191481,2294753.63818758,1804859.0942697292,59718861.765321836,1705970.2416402248,5015948.830162834,16653671.667897884,9050425.202028701,6006629.153127968,1261213.5010782443,4387831.2358523775,7254977.827216309,3376442.7626177445,4516562.178411246,4997047.269822309,1893466.0767786966,4181168.754520495,1282429.8857480153,3410896.269168001,1676345.0790577813,4540900.13767398,1029811.1345512464,1425368.3627365676,1379334.595324442,1317914.9541633003,1774592.2411578447,1147237.7104224395,1332636.9696083607,1818609.110621328,6106319.909153724,2654721.318384102,1958027.4619983595,1185128.25783047,2469578.4835390635,2898610.8442111844,3907326.5727573717,3179834.9601062573,4088550.6984279808,2074116.613215356,1779139.2463594605,2066272.8254902354,1225182.50739324,3767636.5749720633,11965797.875960398,3254979.541620769,2733465.175199355,1841361.7321996775,8315553.035575831,3260634.455603145,1183765.9103163537,5247544.423545327,2542466.6429817937,14163979.473181497,1382489.9967589634,4104710.418442536,15726404.275553927,45065008.726817444,3366915.881489611,6453063.5561576,2010999.9816912618,1642117.494033032,12267128.40066306,2474201.9497560794,1480855.5688076601,2267776.1407273235,4919659.693726232,7877774.2269223705,1568986.191597949,3011686.256622698,2377217.08350066,2704970.9441565955,1654485.601730947,7593703.093418697,1260515.4245162853,1247651.4276245881,6834649.08474854,8962099.297510216,1393330.1498578717,3693638.7471885374,1157724.5770603567,3712252.899814643,1126896.980898173,8094617.20980642,1792914.6723290258,1507016.2312305905,3086446.147636551,2889108.479191872,2630593.8614372825,1819229.7342080069,2715381.5127725545,1845085.2678538193,1189939.153961855,3480731.25193544,14689557.953727718,4814782.826822979,11709638.532220177,8526355.665655408,4132641.8102469575,1721082.4661841572,4160573.4076726274,11138280.241245186,3857734.442614208,4473655.110556068,17864031.525397062,20093447.565547157,1789904.1975411198,6733264.532638986,2060488.9205746022,2642061.9917790047,2253941.1574828727,2249414.181116598,2334753.836219747,2056417.1573889363,4050919.6223033434,24890920.014316667,4424429.020260188,1218987.043762221,1486045.387769177,2899581.701153673,6178217.255751846,3272741.473487385,1142444.4884446743,1324931.5420533544,19381134.228041228,3100328.6965532536,1464574.584485509,1239199.5881988513,1728558.4737457188,4220154.0662749745,6440875.991586372,1415024.9682794441,5663286.071137495,3233212.937235938,1515612.7482570256,49680411.50373943,2551221.8361505554,1768491.230108005,596133.6750053791,5824420.078008994,1981607.799496851,1378150.413666235,2998154.0551408096,5572147.533514015,4059749.70390255,4077273.4703704757,3908889.4525563642,5084086.854704963,2457801.3638030565,1217871.400197262,3940074.152583914,3590463.589342485,1488730.8605800732,991098.8879438634,7493811.087325815,2609148.0296568573,5299104.89998626,40585692.52182475,5938848.987020219,2821064.430038377,1914931.2141544502,1781587.8042488247,2336489.344735328,4496876.106100939,9923288.84875779,7655905.062280329,4464019.673592699,2004080.7763277544,2614626.841667849,2748042.248957894,1287758.59935632,3517900.5025770403,1329546.2179699007,730649.5392861674,1304462.5840865145,4502260.144863602,1640609.1813576703,1898776.7048841463,1666867.490452379,2876922.0166518125,1404233.8387321462,1758670.6654140349,2029818.154195757,4154868.2018878274,29316265.18521632,1279472.9274895247,5770586.193676568,1605039.4052684708,1499883.855414501,3161733.6393357655,1114497.0599458325,4644621.483223714,1212933.0406839186,5099127.8372425875,1939497.0643503054,2568020.659082609,3657075.8198316083,2892246.354024446,2330951.243075482,4168548.4646683633,1640187.9316256056,1823506.7173279836,5045637.292574212,8132663.503505543,4123088.6402519387,6114264.282131569,1920632.1244441418,1125336.1401954237,1605954.742586662,1061325.1041664658,1615475.4460015572,3482113.2045966145,812323.1562559335,3322366.050074385,1381181.3575094235,59833290.67433306,6631244.350383544,3887815.284209016,1777280.3255366122,1347894.3256913435,1625576.4380654127,16693112.053444564,15527027.218905494,3390965.649872206,2261117.3670652895,5260918.8170256205,2155444.4692930384,1613273.6117623732,798676.3410998159,3649132.4058298217,1497346.8365165372,1135437.661333796,1100756.2461391534,1030874.7222977178,1480829.2866163598,12345256.768827312,1393818.3897265992,996761.2994822543,935439.4759278111,1288171.1337984833,1438904.1878494108,969809.2335305414,1029405.5449222266,8772788.181652175,1341197.2183715366,3533499.387712012,4250593.817204243,2193028.1591394395,3347641.846247162,1257265.1609323507,5170546.7693235995,2417346.1843087035,3560515.3462206502,1299094.4051046683,4114358.434520705,11473837.23104907,3166701.742686851,7317220.085986005,4156107.6634396086,1565061.5217270222,1276767.2075797748,945183.6934632212,1877722.78682458,15891596.131557668,1996885.5528518127,1030575.9766939781,3152604.580621472,1730701.1052613878,2451010.199330809,80941403.67553777,3685895.109565828,7845832.056017941,2874863.2567715296,4021755.481768596,3682615.1451396113,5904108.326797603,3738753.4965007594,3180530.3127505193,1381455.216931439,5814601.9735681955,1312611.6438857224,863778.3970352027,2863516.376621308,1823200.0366913837,1746744.7552566968,21056468.89887223,2062972.3368545594,3252388.1364001995,3918180.213374972,2777269.604683241,1478918.0738720996,1549923.4140669827,1943356.7263193666,1298068.4304465505,1862576.1954688262,1644393.8038365822,1443826.943567952,3254852.9754340956,997996.2151369322,1885077.6904127633,1705035.20588542,11015378.671816887,3204965.388989511,2342377.952752069,3797953.0078387163,33939633.41213115,4555071.350030123,11072469.739012938,3684400.2729856884,15398910.990301978,1163919.3865996036,1490610.4962572465,1828907.5738115825,8181816.062793918,2301098.1309777177,5418129.0373743065,5076607.208822757,45505603.00054115,917092.3693048665,1103877.3864499805,2206113.708782384,18109997.799075875,1264099.0902258453,2437393.050477307,1687324.4477623417,1337808.9489624575,2711360.8820174225,3024465.9280771627,3014010.6101648975,1801249.775713536,21691026.928102683,11434579.921754261,1667580.6051951929,1054496.6883429778,10631291.055220423,1204847.5417541084,4993378.769241467,2616572.1043814695,1037811.9188651643,4074672.812965314,4955601.191299122,1221541.142567521,5216830.265496403,1256285.6670275948,1459246.361618536,1977224.5640870144,1848587.3925050532,4889392.66766251,1897384.0523977203,3189488.783631877,1852867.0783866774,1062717.5983757866,1642442.6715867738,2244000.614066079,1517945.251285174,2112924.6062866286,1739032.0166315795,1131079.2532805258,3284156.854778865,1324344.5647194842,13398741.976705976,4196257.304859296,16061238.087873776,10795364.049436381,1941988.248377712,1090800.7715259572,4023208.7024014187,1505733.410413652,1098585.4201430944,3847289.0068544443,2740540.59780199,1963016.3015162775,1252280.516071498,3055610.0224358654,1016772.30070099,1915389.2451845326,2008690.317857395,4340055.407977661,5828883.268972868,1601949.591630565,2290636.201485255,18988892.988306288,1343065.893143717,2193313.4524046783,1637626.3357284577,1145303.631308942,2525111.701813495,5514747.34808385,3121134.539491049,2204249.8231005394,14448759.965313692,6752418.2479441855,3494388.2399086505,3738747.968303336,2246265.7224147827,10544035.496797455,11380438.8699446,2073528.1990658848,1736401.5757091132,6360599.622487085,1176282.5593947638,2498834.971829482,1514171.4934861013,2341501.870292211,1786431.436495273,9488078.548830058,2604801.542404201,1686939.2303176704,3809919.4215730596,3449505.3609779417,9295467.24038724,6813153.98571443,2097848.315479935,1564646.8460364947,6534034.781187147,3658556.466517917,3051833.310048296,4751012.855427254,16295863.64215755,4719155.19905087,1191792.704919944,1336791.3377418201,2772074.3718439136,3614928.706058304,1311047.8357553424,12530294.429816,1776137.8995971587,9480983.320157971,4865441.764438478,1287759.233292501,6090835.215203876,4426902.6600499265,1676952.8721391081,3249259.1740584546,16991631.313091904,1284013.688772697,1127887.11373787,17477851.021021586,3789231.19414747,724421258.0842395,1321072.842631308,1399269.2565243165,1048003.1331999912,76425857.45510772,1597945.3672007604,1438232.8187315841,4348337.807796141,2954633.339038231,1272729.4113853155,1189175.1485673022,2734781.0559791983,2225842.2492869943,8565708.964041902,1660037.4843602641,866067.139552616,2218139.656978182,4465719.507038893,2619778.005026556,9253975.02851885,6021414.070763666,14402141.294301327,1414896.0372620556,1551342.8193285353,5681511.563676722,1886099.0904334087,2957418.8604567116,1223363.895541747,999714.3445753856,2000563.4952987353,2782164.952159757,1761282.4160884977,1331393.77052342,2096554.68047937,2161484.768432915,1415901.0809628554,1519171.5492309849,1887795.8032272398,1421194.7611092236,1499460.2241704564,1319276.4507653327,9135357.173495326,3524633.5271267896,1128091.2472136822,14146030.79366193,4594875.722941751,2406396.6432993356,1277279.160982225,8717846.801979724,1249973.0977250803,1649654.865268361,10188216.942184042,1336838.1447012767,4161139.9364161356,1066315.262497325,2793425.4934649994,2112631.901983145,5305159.901298754,2125280.531013368,3729286.856347369,8056733.7104472425,8670390.624551049,1932896.585670966,2100535.3590550385,2418417.537975571,8152439.662736928,3407218.090732613,39466083.04301061,1287354.235837264,1348570.5682935638,1401851.662754203,3275579.589442502,3414496.510798011,944329.8193297018,3917590.891054882,3563934.269989166,1625549.9917820795,7131073.135403948,2236244.4122340656,1317009.8759804617,4197988.117945181,5766724.051837159,1013829.3159755478,1247112.0386564764,15402038.345787216,2840753.7839259813,14094426.82443634,1306531.275272896,4575755.666441198,3714946.8733364427,4139725.2574580032,1385524.524474101,1519788.1520875054,1419784.5048013504,1497654.593100469,1146386.0123012809,2731001.013392694,7585657.569614381,4003383.744578414,1028015.57943345,2654360.0187181756,4883286.168558225,8147343.223050689,4800329.456433625,3692977.701578492,1383534.6505979414,1552391.1153165903,1402798.1962692658,12232933.273505865,808882.4447208424,2729132.6284530805,4037902.56382952,1615753.5998777575,1897970.2795853014,3166277.956342186,2743617.385001289,8639435.886811515,1163022.4129685098,13493829.021882435,3314256.658460716,2005078.9053068343,5805438.118988529,5762294.821113592,2922736.6274588807,1708024.1728619356,12110956.479790969,2607168.9791943226,2094820.7716777977,1906343.4836878,1503199.4594364571,7183237.8671943415,4523704.998376106,9200109.461463176,2446027.3955031903,9434384.78548152,19508382.54456188,852618.867906882,1217054.5022360426,1040378.7522286102,1615791.4635579973,19784518.625796936,2262745.583796616,7822329.202459201,1635923.0029002745,1646932.8505303042,1502762.800631597,1291276.1821766163,3314804.10355545,3655047.4060494984,1121154.2938897237,1182037.59328048,1408411.3930428312,3432126.2166490075,2038438.501519333,6450054.60659872,1474696.1682769042,805934.1246045735,6045571.906435744,1950114.6773870257,1332358.0821647407],"xaxis":"x","y":[4395510,1499128,805751,58761304,3639670.25,1327591,476069,17714332,555153.125,143423024,415453,1160989,1696978.75,1334876,4585966,11962615,13730204,2356376.75,18470340,1396691.875,2056762,659176.1875,619409,791035.5,9683979,1241355.375,29540052,1237655.25,4018570,6118300.5,1568550.75,526752.8125,7575080,2058584,2711035,965786,5135040,669217.6875,1692323,803938,3497215.5,63668488,809291.8125,21069722,291614432,10105379,32141070,1523508,94560088,3073937.5,1024866,1516831.75,553230,1394408.25,1138386,3546459,4107827,2014500.375,20675922,1115107,1396280.25,151754.2969,1127895,2615247.75,2735128,489557.0938,2311532,5227992,848497,22965152,666967,673030,3242458,433443,614044.6875,4216639.5,12549118,733249,770736.125,1848074,1051523.25,853314.5,1017787.125,501194,2298815.75,2255026,2202407,1045337,1436109,1362523,19526080,13146645,635516.375,1532390.25,6227419,274682208,1751483.875,30135280,2265487,3218824,1725558,816300.5,666392,2292169,2663520,931148.625,7520554,7613116,1232712,4970047,942962,1166069.875,9387211,5826745,1423463,2660396,4103027.5,784584,8233527.5,4034028,31803568,345399,2360656,711118.375,549438,901679,1939297,1094343,2455923.5,1992158.25,817695.6875,2986728,1008536.813,1078936,654446,2591768,555709,5401277.5,2099386.75,1983256,862945.125,1417728.25,14172606,643740,163945984,1051944,1569569,8747819,534703.6875,5032796,518107,556700.6875,1261963,114648520,9313513,7251589,2612378,2923484,754241.3125,162960,836437.6875,2547848,1563070,677476.6875,7860942,10040121,2148565,730672,3009967,9986032,505601,308400.4063,14064603,13631685,4748137,10434387,604183.1875,2139713,920068.875,1128060.75,11624548,596242.1875,3814590,2844685,1278437,1119679,21346100,2136086,4991893.5,590629,659368.625,1437569,450518,1047461,858701,9788717,1769137,2870914.75,1025659,10444297,4916451.5,641713,867184.375,688641.8125,746858.1875,15324777,1829981.375,743748.5,1260902,1307883,966812.125,3540225.75,1063960.125,922153.6875,794715.6875,1200811,4581493,6426022,4178233.5,8437401,1063480.5,739869.6875,824320.1875,8218155.5,913117.6875,764832.6875,2402535,6843855,37022180,706505.6875,1858224.625,2583451,2004777,1237252.75,10775646,16467486,2001643.75,576052,701422,6797528,270838.5,3265995,31371618,727374.6875,811900,784584,0,2545405,1631031,1536998.25,4998015,132105,1524499,1279570,6268627,1060818.25,991918,3158151,1441445.625,648022,1702639,3686962.25,2960159.75,255971.2031,488181,1219744.75,3321035.75,3043767,965651.125,282898,1645748.25,768479.625,508051,331059.5,633408,2454861,10834087,3752132.25,1108604,907617,7456330,1076693,966764,576644.125,975721,1987934.625,6962073,455798.9063,919180,3242066,6657348.5,1128179.125,2625396,10564729,1832031.5,452614,1365778,11508035,1587454.75,2009703.25,6860521,1544146.625,887022.875,3220635.25,7771933.5,2584082,536642.875,14619984,30038490,848734,2018011.875,1612167.25,3468606,1287422,785901,1287040.625,2225512,722062.875,645028,5975230,1878442.25,3655453,2322314,3073778,1425226,25631512,2946756.25,2483074.25,5662234,4934334,1123416,1826203,1187499.75,3518459.5,5917000,904059.875,4917420,601590,2204020,2559403,1140137,13609040,19991412,552015.625,1544117,2839882.25,1339220.625,2466100,4636396.5,1730431,1700057.375,1072487.875,1090545,6858533.5,2306723.5,918514,1157976.5,2156407,858932,668915.8125,1904641.75,1769998,0,530700.125,2804534,1678495.25,1003551,5572010,634561,13901935,1360847,650313,2195160,1335007.875,1107290.625,1031667.5,1234149,2114860.25,2805826,591845.3125,2251442,1898509.75,6299357,1076389,3091387.5,9230223,1544388.5,841841,1381590.125,662424,5053635,2401148,6182350.5,22078064,790671,4608963,662685.3125,1822830,3106930,1733511,362823,830286.5,495481,601818.625,21027396,952879,2337651.25,8037287,1809586.75,1657201,1167813,720170.6875,55030192,1862176.875,1509292,2323265,1072855,25970248,9128267,9029489,5976246,1934971,3316562,948676.5,755269,598857.8125,8190809,1026921,885958.125,1878742.875,3212269,811907.625,564754.1875,336461,1186835.125,6411570.5,1960538,847298.8125,2456132,2828950.5,6298131.5,12448381,4677797,753721.125,13458948,5722325.5,12931002,1482752,645055.5,2903161,6722886,2373568.25,3375211,2822377.25,7768057.5,1831722.875,640374,1485945.875,1224045.25,5216213,730292,455153,37797084,9375125,624180,1860084,1015060.188,16016644,670779,1427294.375,776248.6875,39403320,2664566.5,7182436,41399504,786253.6875,1208168.75,974179,1325597.625,26941110,463371,938833.625,5155090.5,1811506.375,876339,1083467.125,5804784.5,1044642,3652708.75,588856.125,15088676,552822.125,648879.3125,967432.8125,892227,507451,405050.4063,1750419,405062,593042,1116040,1827424.125,222600.0938,568033.375,622051,2003447.625,2654755,2233268.5,878927.8125,560334.5,919746.3125,1247972,587903,1732954.25,880115.5,7592451,1527348,742565,4133811,2442220,131373880,1190733.5,2133797.75,1581036.75,1041652.813,1912690,2534671,2483320,2698077,1191926,1239315.375,6782435,403800,604591,3060149,1826107,1035554.688,4784731,6901554,1712796,608199.8125,5820965,550767,1332912.625,3245772,304248,1003176.875,6057734,726124.625,7802205.5,2108926,2827818,2014245,8354235,888561,527438,335477,1288669.5,1456808,3835202,1775433,2843033.5,4285049,8422861,1651120,8188381,5671256,1191803,531563,8688676,2964206,1580597,2133372,6917478.5,5447076.5,862826,3999505.25,3721129,593824,929688,13214416,602245,4865551.5,632157,14431245,4729846.5,11208498,1920352.125,3053785,2147062,3322250,1702992,2759674,791052.875,2683383,737570,695059,6471427,951675.5,16808.90039,4222682,1324534.625,1167887.75,1391714.25,12204922,6577130,751292,1761048,552391,3039626,669117,95544,4338616,1012559,9997511,57133.19922,4247457,26416770,842511,8819864,712149,1960558,3298570,1670929,4983934,4217623,2996502,493000,4801472,2235765.5,635698.6875,3467698,1310481,470666.6875,1213869.75,1093627,1372706,61762380,842923,1820292,961491.3125,1467029.75,3042875.5,718362,1453543,1222509,2594119,7791957,15571642,896172,2763597,4039667,4475652,776237.375,811021,6352846.5,6387710,12271749,783651.5,626612,1255834,2527198,3894142.5,1972156,1691963,699382,6083307,619870,586827.5,1452804,2058955.25,13962749,24126,10591549,3240830,2953338,1260416,704799,1617421,1224642.5,3822102.5,3863772,1162226.375,921786.375,2100766,2019254,5134446,1536287,782494.3125,713326.875,2892779,317794.4063,456397,3182301,919048.6875,1976663,3065502,593953.375,767297.8125,764906,642825,3397709,1084553.75,736838,1403820,1084048,6983736,1171380,1985171,851439,1677877,1107870,990361.5,847514.5,80469216,1321065,2411402.25,17750994,5436057,4757299.5,509171.4063,3258819.5,7015692,1271021,3435687,1987218.75,1034941.313,340270,559234.6875,2158629,966677.125,3637973,689373.8125,858780.625,1705515.25,512166,97690.39844,675772.875,815800,2184336.75,2751694,1987073,1169948.875,938908,1280245,1375978.875,2845946,1194835.5,606111,753784,892634,3113412,1762174,2921368,18868768,5348309,6376820,2058532.875,7782092,1212551,1224276,1958352,544172.375,8393893,767361,6522551.5,10939331,39523852,4519115,10020111,2601945,620104,8800292,968698.3125,1756668,2065156,2773779,9343428,219483,3303553,6917280,1091162.75,1306087.75,2849048,1429855.75,410433.0938,8873485,9049598,696522,3046543,1433504.375,1503823,551219,3789270,958725,960987.8125,1438479,1988784,43943,738983,1516377,1181963,806398.1875,2326565,49102164,1983509,9273278,6931901,1784796.5,1963054,2125323.5,3833052,4103800.75,318962.8125,7286313.5,25747022,1324965,5086863,1422406.25,1758581,1486177,1162493.625,1192100,970118,2646523.5,12263687,4767227,571644,559805,12250422,13410041,2726369,447947,1988760.875,13300859,1348497.125,661380,1625308.625,4236396.5,1906597,5450441.5,106918,2270531,41680640,771512.6875,35685224,0,1852926.625,851053.8125,1739286.75,1748629.875,863703,882896.5,2837589,571309,2237482,4765753.5,10513817,1234095.5,1372475,552729,3886929,809257,728167,6583940,2203758.5,1165598.875,59124384,1727783,1048813.75,838571,948157,1966424,3786217,5328224,3869754,1637199,1054581.375,1108310.875,2063212,1608224,1907697,1126157.25,502667.6875,662280,2439061,1019487.188,1558348,1162120.375,5762597,611997,4467292,1530744,2082754.75,16020642,514471,3751417,207572,2245568.25,4520838,855665.125,1688612.5,2473302,6136327.5,1423438.5,288191,2585187,2145603.5,10403123,2053611,1337968,839115,4369513.5,4498367,2291581,22530168,694479,508014.5938,1620721,607373,520011.8125,3234787.75,452744,930611.875,723424,85357952,2733003.75,3625434,2228675.5,524053,717056.375,10249433,18829136,3040924.25,3030041,2057457,1115504.375,1419453,602618,1539478.5,635697.3125,323739.4063,569115,1257144.75,630040,8767617,544043,619545.125,917783.5,755856.875,677536.3125,385719,558893.375,11521702,1382309,1823782,1602030,2147013,32579658,574650.375,6668568.5,1198291,3986851,2470566,4207110.5,6576563,962577,4734631,1049748.625,1623657,824516,647792.875,1225404,6481807.5,3064039.5,510517,1698074.625,1135172.25,1813718,74130576,2330029,6366084.5,342726.0938,2938366,1045413,6832991,13133225,1183042.125,1286882,4025787,557193,1475475.5,3147042,956958.875,916864,12720069,942712.5,2246747.5,2150120,14084606,2214469.5,1913558.375,1103762,684941.375,1216543,3542708.5,780410,4742869,1117658.75,806615,1149140.25,7539963.5,3202213,1366000.125,6807227,19827916,13253979,5149928.5,1706301,7811542.5,945245.625,209698.7031,1926134.875,17320768,1182575,10557910,10343027,157606480,1646088.5,116486.6016,2261586,13969552,1652833.25,1121259,7492939,928584.875,1856230.625,10018321,13010578,540465,13239050,11924933,3494929.5,2014475.875,19036416,1161814.625,5326222,1430356.75,607579.6875,5968153.5,3528086.75,1093757.75,8631444,770082.125,2520919,729096.875,1294615.25,4107443,485971,1210798.375,1858029,786979,955223,1118490,747326,1943899.25,867456.875,628609,2447861,1101536,12719042,3366528,4545148,9941308,998633.5,1210229,1833381,2057836,1534068.25,1813285.25,3303989,1380283,209644.9063,1210262,954352.6875,1594733.625,851585.1875,2266570,3591653,696709,1111858,48729588,969682.8125,4270584,1094940.75,984178,533742.8125,6014078,2331952.75,44293.5,11195917,2910252,29195468,3104171.5,2485521,14465798,3590731,1921890,1509775,4785692,188745.7031,7240274,1014405.375,3095882,1293390,5499097,226375,974940.375,1889880,4224872.5,4028943,3574542.25,4602793.5,8518313,3312125,5948007,3377209.75,2581741.75,22325364,3241917,1658751,577033.875,1649327.625,1588933,729397.1875,6505995,526059,1738376,2434208,1545485,16402563,2023258,537141,2503690,13567774,481206,1139980.625,8320719,1777005,873923712,666930,541950.875,863728.8125,61576184,5462958,2874181,3897738,2093011,666687.5,845961.375,3548858.25,3395651,20725996,1022114.5,2217620,360045,2184156.5,1355558,5525072.5,4089677.5,20560622,758875,621751,2188452,824483,1042652.625,698326.625,533226.8125,1045640,2217046,7801590,525668,789630.125,841821,866343,1481017,2798967,1359579,850615.8125,1860901,3091348.25,1207092.75,2094410,6125692,4039168,1130960,803533.3125,5121586,1049088,735178,8168547,524144.6875,2001936.5,383524,2772124.75,10642754,2759071.5,1353079.5,3245111.5,7380088,6836900,1807738.25,1296959,1604129.75,3605749.75,1551294,25210246,2159932,862744,1862752,3888945,1443367.75,1676426.5,5444253.5,4099807,2233262,1358022,760826,678663.3125,0,6609358,1028277.313,2307938,61827040,0,4321141.5,572139,1997181.5,1594422.5,2755578,523133,650205.8125,519317,108200.3984,844076.8125,1350190,465533.3125,2189030,464155.1875,637246,2131817,11026945,5424718,2401890,501741,1008665,730765,7564643,1784385.25,1383950,227566.7031,702385,1641667,816846,448676,1083729.625,801645.875,13315803,11702959,609921.6875,3195230,2506927.5,2249592.5,2619088.5,3780078,1387773,1072500,1075019.375,1908612,2992693,5134607.5,11458215,1897180,6579348,15590743,1261634.5,1392015,541542.6875,1962435,20781390,1795462.75,0,819378,400354.3125,5047936,1961581.5,2422767,1913034.125,622572,561368,1787119,3494383,1244824,2549649.75,866096.6875,1539176.625,2267094,1444429,1666221],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN = np.logspace(-3, 3, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","SiteEnergyUse_predEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","print(ScoresEN)\n","figEN.show()\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[10853093.81214378,10852604.664622445,10852083.779613975,10851529.413320567,10850939.735640328,10850312.825298568,10849646.666115979,10848939.145009372,10848188.052321615,10847391.085020814,10846545.85319288,10845649.89008284,10844700.665737096,10843695.604088312,10842632.103140388,10841507.557786053,10840319.384748789,10839065.049193293,10837742.092682835,10836348.162346609,10834881.04130895,10833338.680575874,10831719.232632827,10830021.08696025,10828242.907525288,10826383.672084382,10824442.712870123,10822419.757972693,10820314.972488448,10818128.998306016,10815862.991234206,10813518.65403917,10811098.263846744,10808604.692280572,10806041.41665585,10803412.520544924,10800722.682089342,10797977.148568472,10795181.695959207,10792342.572544824,10789466.426057955,10786560.214372726,10783631.100387516,10780686.332447778,10777733.112425871,10774778.454371553,10771829.037435448,10768891.057504874,10765970.082630778,10763070.917818712,10760197.485061679,10757352.72457061,10754538.522982378,10751755.673881408,10749003.8752619,10746281.767602334,10743587.015058812,10740916.430960268,10738266.147370398,10735631.827039963,10733008.914683953,10730392.923253473,10727779.749797886,10725166.014682744,10722549.417382559,10719929.101826955,10717306.02434836,10714683.31764629,10712066.644817973,10709464.538364522,10706888.720111262,10704354.39911636,10701880.545814682,10699490.141780887,10697210.405524513,10695072.995579401,10693114.192755267,10691375.06372242,10689901.608055856,10688744.890438898,10687961.158903796,10687611.94876954,10687764.170349399,10688490.176586425,10689867.80459964,10691980.382772246,10694916.692588724,10698770.872050686,10703642.245307155,10709635.061276231,10716858.12268182,10725424.286272127,10735449.81523973,10747053.566261508,10760355.996368855,10775477.979286527,10792539.427141279,10811657.72164159,10832945.968903702,10856511.103740338,10882451.881822746,10910856.810721919,10941802.082173355,10975349.5765444,11011545.014980284,11050416.333909985,11091972.349910373,11136201.770533258,11183072.589626001,11232531.885703292,11284506.021306958,11338901.222339537,11395604.501038205,11454484.875836099,11515394.836244553,11578172.00065084,11642640.918487528,11708614.974165322,11775898.357002929,11844288.067880679,11913575.938610952,11983550.643614965,12053999.685383458,12124711.33565554,12195476.513748571,12266090.582564931,12336355.042018887,12406079.09940227,12475081.096838947,12543189.777609037,12610245.375755059,12676100.516893227,12740620.922334706,12803685.913212528,12865188.716029689,12925036.575623028,12983150.685739705,13039465.951060193,13093930.597442854,13146505.649335489,13197164.294680126,13245891.158254772,13292681.504314791,13337540.388707329,13380481.779442336,13421527.66312762,13460707.152826253,13498055.610879619,13533613.79815344,13567427.059088469,13599544.549938604,13630018.515707057,13658903.619582167,13686256.327151602,13712134.346347976,13736596.122953082,13759700.390556386,13781505.773115363,13802070.437686184,13821451.794465715,13839706.240992095,13856888.947171817,13873053.677719325,13888252.648592478,13902536.414069092,13915953.781221274,13928551.74869361,13940375.466867367,13951468.21668661,13961871.404626155,13971624.57148928,13980765.412930006,13989329.809797024,13997351.866590744,14004863.956509724,14011896.771736126,14018479.377771378,14024639.270781988,14030402.437051993,14035793.413762307,14040835.350429479,14045550.070436832,14049958.132180918,14054078.889435977,14057930.550609777,14061530.236625988,14064894.037222764,14068037.065504074,14070973.510621388,14073716.688498225,14076279.090540236,14078672.430298787,14080907.688077677,14082995.1534904,14084944.465990309,14086764.65340812,14088464.168541055,14090050.92384561,14091532.32429198,14092915.298442692]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[13322668.68379696,13322453.11265642,13322225.810042406,13321986.54452163,13321735.094227182,13321471.24487988,13321194.78954411,13320905.531014226,13320603.287711618,13320287.903861491,13319959.264510248,13319617.315651964,13319262.089386992,13318893.73368396,13318512.54601353,13318119.00992276,13317713.833562007,13317297.98927554,13316872.753601208,13316439.747341087,13316000.975688225,13315558.868645422,13315116.322088849,13314676.739783473,13314244.076461788,13313822.881777458,13313418.344604386,13313036.33682836,13312683.455513421,13312367.062133314,13312095.31743602,13311877.210441967,13311722.580053296,13311642.12776813,13311647.420056392,13311750.879073206,13311965.760575242,13312306.118175272,13312786.753427297,13313423.151680041,13314231.404165585,13315228.117390947,13316430.31155483,13317855.310394384,13319520.625544798,13321443.839130875,13323642.488862809,13326133.960335221,13328935.391488694,13332063.594250236,13335534.998195414,13339365.620653795,13343571.067008266,13348166.564029241,13353167.02796535,13358587.167824434,13364441.622877546,13370745.131968014,13377512.730775468,13384759.971840879,13392503.160966855,13400759.602624921,13409547.846271925,13418887.925031673,13428801.578050084,13439312.447981644,13450446.245496973,13462230.873387553,13474696.503746985,13487875.602782916,13501802.899011398,13516515.29185463,13532051.698955473,13548452.841792053,13565760.970385157,13584019.529009411,13603272.765827458,13623565.29025655,13644941.582655896,13667445.461609792,13691119.514710085,13716004.499356408,13742138.72074839,13769557.394998282,13798292.006200923,13828369.667406108,13859812.496771168,13892637.021723844,13926853.62569126,13962466.053752452,13999470.99529315,14037857.76316241,14077608.089669,14118696.05967615,14161088.199689787,14204743.738823388,14249615.052551672,14295648.29304355,14342784.200597305,14390959.079565447,14440105.909767857,14490155.551723272,14541037.992377387,14592683.56887371,14645024.102812879,14697993.877640087,14751530.398031963,14805574.882425755,14860072.457305633,14914972.0429108,14970225.94248075,15025789.168618282,15081618.55862611,15137671.744067805,15193906.047355853,15250277.37969324,15306739.210691307,15363241.671450634,15419730.841087038,15476148.252933513,15532430.642172761,15588509.9424841,15644313.526219685,15699764.671229526,15754783.228098754,15809286.454441171,15863189.978068179,15916408.848265078,15968858.633910352,16020456.528544657,16071122.425454432,16120779.930059928,16169357.282045344,16216788.16539355,16263012.390457222,16307976.43811022,16351633.861619184,16393945.546948157,16434879.836608557,16474412.525797155,16512526.742385348,16549212.724337304,16584467.509386348,16618294.552359182,16650703.285498496,16681708.636598678,16711330.518843053,16739593.305017866,16766525.29737454,16792158.20290249,16816526.622233365,16839667.558883093,16861619.954096433,16882424.251224205,16902121.99235764,16920755.448880702,16938367.28668446,16955000.266016044,16970696.975302298,16985499.597784776,16999449.70941711,17012588.106193602,17024954.658887073,17036588.19305944,17047526.392159093,17057805.721521385,17067461.371133126,17076527.215098232,17085035.785841897,17093018.261207648,17100504.46272931,17107522.86349334,17114100.60414242,17120263.51570525,17126036.14806799,17131441.803027593,17136502.570985734,17141239.370452553,17145671.98963208,17149819.129455645,17153698.447515752,17157326.602430828,17160719.298241813,17163891.328504287,17166856.61979616,17169628.274410333,17172218.612045627,17174639.210347444,17176900.944183048,17179014.023565147,17180988.030162398,17182831.95235666,17184554.21882496,17186162.73063938,17187664.891890734,17189067.638852596,17190377.467710756,17191600.460890263,17192742.312017597,17193808.34955992]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[8383518.9404906,8382756.216588469,8381941.749185545,8381072.282119504,8380144.377053475,8379154.405717256,8378098.542687848,8376972.759004518,8375772.816931611,8374494.266180137,8373132.441875514,8371682.464513715,8370139.242087199,8368497.4744926635,8366751.660267244,8364896.1056493465,8362924.93593557,8360832.109111046,8358611.431764461,8356256.577352131,8353761.106929677,8351118.492506325,8348322.143176805,8345365.434137028,8342241.738588788,8338944.462391306,8335467.08113586,8331803.179117026,8327946.489463475,8323890.934478717,8319630.665032391,8315160.097636374,8310473.947640192,8305567.256793015,8300435.413255308,8295074.162016641,8289479.60360344,8283648.178961671,8277576.6384911155,8271261.993409608,8264701.447950326,8257892.311354505,8250831.8892202005,8243517.354501173,8235945.599306946,8228113.069612231,8220015.586008087,8211648.154674526,8203004.773772862,8194078.2413871875,8184859.971927945,8175339.828487424,8165505.978956489,8155344.783733575,8144840.722558448,8133976.3673802335,8122732.407240077,8111087.729952522,8099019.563965328,8086503.682239046,8073514.668401051,8060026.243882025,8046011.653323848,8031444.104333813,8016297.256715033,8000545.755672265,7984165.803199746,7967135.761905026,7949436.785888961,7931053.473946127,7911974.541211126,7892193.50637809,7871709.392673892,7850527.441769721,7828659.840663869,7806126.462149391,7782955.619683075,7759184.83718829,7734861.633455817,7710044.319268003,7684802.803097507,7659219.398182671,7633389.619950408,7607422.958174568,7581443.602998359,7555591.098138384,7530020.88840628,7504904.7223775275,7480430.864923051,7456804.06880001,7434245.250070492,7412990.8093818445,7393291.540810459,7375411.072846865,7359623.793047924,7346212.219749666,7335463.801730886,7327667.150239631,7323107.737210099,7322063.12791523,7324797.853877636,7331558.069720566,7342566.1719693225,7358015.58421509,7378065.92714769,7402838.790179882,7432414.301788783,7466828.658640761,7506072.72194637,7550091.728495784,7598786.100133166,7652013.276060792,7709590.4434503,7771298.007604392,7836883.625133253,7906066.621608441,7978542.62628375,8053988.276880012,8132065.87291882,8212427.8828278445,8294721.2350491425,8378591.34474583,8463685.84454723,8549658.000081554,8636169.799398389,8722894.710688692,8809520.105969595,8895749.35053946,8981303.559767542,9065923.026673418,9149368.326055685,9231421.103726525,9311884.562624069,9390583.661031507,9467365.041602155,9542096.713135837,9614667.509860227,9684986.355172228,9752981.358277151,9818598.772873823,9881801.846974904,9942569.59217224,10000895.499243235,10056786.225055475,10110260.273386177,10161346.689656561,10210083.786809454,10256517.916741371,10300702.29893234,10342695.915274449,10382562.477643844,10420369.472531019,10456187.285067901,10490088.403079,10522146.700338311,10552436.79702546,10581033.49442831,10608011.280214682,10633443.900070071,10657403.991146654,10679962.772567082,10701189.788150031,10721152.696551576,10739917.104125513,10757546.435979092,10774101.840921164,10789642.126254093,10804223.718636502,10817900.647531323,10830724.548044665,10842744.680249251,10854007.962366674,10864559.01545163,10874440.217476238,10883691.764951458,10892351.740444662,10900456.184557019,10908039.171111422,10915132.884471904,10921767.698068969,10927972.253343206,10933773.538442835,10939196.966120023,10944266.450367669,10949004.481423393,10953432.198841643,10957569.4623999,10961434.920660704,10965046.077059727,10968419.353431303,10971570.150918074,10974512.908240916,10977261.157330396,10979827.576341422,10982224.040089883,10984461.667963643,10986550.869371355,10988501.386800956,10990322.336566363,10992022.247325461]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[8251304.727246591,8250108.350264777,8248828.959754545,8247461.002751342,8245998.590661895,8244435.484205869,8242765.078122332,8240980.385638018,8239074.022708816,8237038.192070315,8234864.667170418,8232544.776103017,8230069.385714546,8227428.886105924,8224613.175796383,8221611.6478423765,8218413.177210134,8215006.109680391,8211378.252519019,8207516.8670847975,8203408.663469966,8199039.797191917,8194395.867879259,8189461.9198255595,8184222.444215998,8178661.382759271,8172762.132372805,8166507.550465272,8159879.960236873,8152861.155274223,8145432.40256066,8137574.442862006,8129267.487292503,8120491.208726703,8111224.726612062,8101446.5836672615,8091134.712934776,8080266.393710733,8068818.195011809,8056765.9054748705,8044084.448930198,8030747.785352373,8016728.797478348,8001999.1640849365,7986529.221729511,7970287.81765717,7953242.157537201,7935357.652675245,7916597.772309769,7896923.907493149,7876295.253825871,7854668.720905034,7831998.8767185975,7808237.935325249,7783335.7959813345,7757240.141399854,7729896.602059946,7701248.992454825,7671239.623915757,7639809.697238046,7606899.776835263,7572450.346636273,7536402.446497228,7498698.386602739,7459282.536242132,7418102.182520749,7375108.454033642,7330257.304303848,7283510.549855964,7234836.958126765,7184213.380948331,7131625.929998565,7077071.191300104,7020557.476446879,6962106.108618467,6901752.741468276,6839548.708499416,6775562.399428938,6709880.658152691,6642610.194138185,6573878.995293085,6503837.725513196,6432661.084177795,6360549.097849971,6287728.306434156,6214452.797192327,6141005.030561278,6067696.391989998,5994867.394501404,5922887.448007928,5852154.104392902,5783091.683035702,5716149.181055525,5651797.377520923,5590525.052794799,5532834.264621918,5479234.652844457,5430236.785520756,5386344.6105379015,5348047.136996364,5315809.536431315,5290063.920182509,5271200.109116616,5259556.757577463,5255413.217062197,5258982.520318764,5270405.829889863,5289748.627240442,5316998.824709024,5352066.871910153,5394787.8128559375,5444925.1425555255,5502176.223220277,5566178.958113938,5636519.388861427,5712739.87863107,5794347.564474312,5880822.800515951,5971627.3622147,6066212.23370634,6164024.849863793,6264515.708516365,6367144.304388927,6471384.3643462295,6576728.3840955645,6682691.48085163,6788814.586064315,6894667.008564497,6999848.402586011,7103990.177967839,7206756.392019847,7307844.1643727785,7406983.6577477325,7503937.668953164,7598500.875451728,7690498.78341338,7779786.423173042,7866246.8373637255,7949789.405667122,8030348.04812864,8107879.346380887,8182360.618999732,8253787.9836955,8322174.435244517,8387547.964121616,8449949.736824207,8509432.354992434,8566058.20672007,8619897.919991037,8671028.925020305,8719534.129461518,8765500.70798446,8809019.00562799,8850181.552589146,8889082.186699346,8925815.278738834,8960475.054921806,8993155.010314591,9023947.406595502,9052942.847394804,9080229.924435994,9105894.92780626,9130021.61388857,9152691.024767091,9173981.353250956,9193967.848031659,9212722.753881712,9230315.282204574,9246811.607648538,9262274.886892444,9276765.29609307,9290340.08384826,9303053.636873389,9314957.555909766,9326100.739680808,9336529.474985,9346287.53126419,9355416.258211879,9363954.685190326,9371939.621408125,9379405.75597321,9386385.75708109,9392910.369726002,9399008.51143524,9404707.365625119,9410032.47226271,9415007.81559157,9419655.908743463,9423997.875112712,9428053.526416324,9431841.437402297,9435379.01720145,9438682.577345492,9441767.396496564,9444647.781951804,9447337.128001025,9449847.971227163,9452192.042847827,9454380.318202715,9456423.063496197],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[15275860.551743777,15276592.469227755,15277375.604804844,15278213.476082344,15279109.797537021,15280068.484299727,15281093.656300427,15282189.643114693,15283360.989810314,15284612.463995593,15285949.064119922,15287376.028883606,15288898.847398415,15290523.269532107,15292255.315701997,15294101.28528395,15296067.762794217,15298161.621084712,15300390.020951597,15302760.406758685,15305280.497878117,15307958.27590891,15310801.96772152,15313820.0243852,15317021.0959808,15320414.002215281,15324007.698673395,15327811.238496525,15331833.729286658,15336084.285096183,15340571.973473564,15345305.757677458,15350294.434337245,15355546.567018077,15361070.416340832,15366873.867509859,15372964.356311705,15379348.7948622,15386033.498591578,15393024.116158552,15400325.564165592,15407941.968697226,15415876.615809485,15424131.913149633,15432709.36486946,15441609.561903125,15450832.189503457,15460376.053664997,15470239.127706517,15480418.619845314,15490911.062078888,15501712.42011148,15512818.223443018,15524223.714099739,15535924.01185668,15547914.293210886,15560189.98083943,15572746.93984618,15585581.676788224,15598691.537296638,15612074.8980779,15625731.349206883,15639661.862896007,15653868.94533712,15668356.768744247,15683131.281352554,15698200.293821912,15713573.541220594,15729262.720491128,15745281.503992934,15761645.53034192,15778372.374296255,15795481.497844292,15812994.184914455,15830933.4622328,15849324.008794816,15868192.056193253,15887565.281662341,15907472.695178304,15927944.521323228,15949012.075910874,15970707.63663231,15993064.306258678,16016115.866291735,16039896.618437374,16064441.2109454,16089784.446755754,16115961.070549898,16143005.532241803,16170951.725151597,16199832.698058922,16229680.341482686,16260525.049808387,16292395.36219579,16325317.586451527,16359315.41114433,16394409.512083676,16430617.159803739,16467951.834846675,16506422.85741118,16546035.037348798,16586788.349615991,16628677.639221044,16671692.358563779,16715816.338988367,16761027.597487744,16807298.178927977,16854594.033985052,16902874.93324228,16952094.418568462,17002199.793914035,17053132.158902794,17104826.48989742,17157211.77438952,17210211.205416236,17263742.443057347,17317717.949778542,17372045.405370302,17426628.20546761,17481366.045174133,17536155.586276267,17590891.203099933,17645465.798460517,17699771.67763489,17753701.465094455,17807149.046108335,17860010.513445336,17912185.098401193,17963576.065320436,18014091.549664836,18063645.32143442,18112157.458248597,18159554.91547768,18205771.98428739,18250750.63211973,18294440.723787546,18336800.124831665,18377794.69193127,18417398.15785927,18455591.920660205,18492364.748362698,18527712.411618408,18561637.257211424,18594147.7354538,18625257.894139647,18654986.851044912,18683358.256010637,18710399.752509393,18736142.4473396,18760620.395782985,18783870.10825066,18805930.08317743,18826840.36973435,18846642.162841737,18865377.431993157,18883088.584553994,18899818.16347762,18915608.578785084,18930501.87167427,18944539.50975163,18957762.211604413,18970209.798740864,18981921.072809316,18992933.715952355,19003284.2121489,19013007.78743458,19022138.366960548,19030708.54694488,19038749.5796818,19046291.369896647,19053362.48086375,19059990.1488362,19066200.30446767,19072017.60003489,19077465.441392604,19082566.023709938,19087340.370147318,19091808.372735154,19095988.834810022,19099899.514450233,19103557.168431375,19106977.596292872,19110175.684170477,19113165.4481061,19115960.07659663,19118571.97218795,19121012.79195905,19123293.486775234,19125424.339218885,19127415.00013165,19129274.52372369,19131011.4012243,19132633.593063768,19134148.559589647,19135563.29033141,19136884.3318365,19138117.81410809,19139269.475680575,19140344.687373333,19141348.474766824],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[9490707.526000915,9490880.58908915,9491067.644639788,9491269.931452457,9491488.796968022,9491725.706074834,9491982.251020288,9492260.162661908,9492561.323292837,9492887.781258047,9493241.767537886,9493625.714416217,9494042.276276946,9494494.352492847,9494985.112294843,9495518.021449266,9496096.87053267,9496725.8045828,9497409.353916507,9498152.465931369,9498960.537732663,9499839.449434673,9500795.597961651,9501835.931111557,9502967.981546687,9504199.900247633,9505540.488825087,9506999.229941562,9508586.314964518,9510312.667861192,9512189.964257896,9514230.644525047,9516447.919717424,9518855.769201891,9521468.928849563,9524302.868764821,9527373.759677917,9530698.427348258,9534294.294616293,9538179.31110405,9542371.870994091,9546890.719805498,9551754.851618774,9556983.39875914,9562595.516503531,9568610.265899757,9575046.498242494,9581922.745103922,9589257.118032185,9597067.222076751,9605370.087152429,9614182.120897958,9623519.086117998,9633396.105130263,9643827.6923968,9654827.815738821,9666409.985268362,9678587.367975293,9691372.924749149,9704779.56555435,9718820.317574669,9733508.500447536,9748857.902259985,9764882.949798191,9781598.866639348,9799021.813039113,9817169.002176771,9836058.788137255,9855710.72198885,9876145.57340532,9897385.316426428,9919453.0790956,9942373.057805972,9966170.398178631,9990871.04514951,10016501.565623054,10043088.947539752,10070660.379490184,10099243.01509131,10128863.726233114,10159548.849028774,10191323.925892886,10224213.446671784,10258240.591206219,10293426.975170132,10329792.400550166,10367354.611752255,10406129.058078175,10446128.663224565,10487363.6025212,10529841.088825658,10573565.168290747,10618536.527565377,10664752.314314488,10712205.973179525,10760887.099383263,10810781.312059911,10861870.1490328,10914130.98416682,10967536.967621386,11022056.988386476,11077655.657492168,11134293.309356267,11191926.01799986,11250505.624438971,11309979.771554768,11370291.943224858,11431381.50548908,11493183.748996109,11555629.933847347,11618647.340084743,11682159.329277465,11746085.424740141,11810341.41964246,11874839.52344067,11939488.557502065,12004194.21038446,12068859.361921774,12133384.483078202,12197668.115568386,12261607.431671334,12325098.870718885,12388038.844680795,12450324.501375694,12511854.530377736,12572529.993897213,12632255.162970725,12690938.338319018,12748492.635261826,12804836.713090448,12859895.431197496,12913600.416901898,12965890.533102026,13016712.23743535,13066019.828310786,13113775.576812806,13159949.746881235,13204520.509207198,13247473.756851437,13288802.832622329,13328508.179721845,13366596.928084837,13403082.429235209,13437983.75241678,13471325.154295594,13503135.5337511,13533447.88225372,13562298.739141766,13589727.65983074,13615776.703673184,13640489.946889091,13663913.024745112,13686092.706006605,13707076.501640793,13726912.308824511,13745648.09051239,13763331.5901503,13780010.080570526,13795730.145671047,13810537.493152142,13824476.79634741,13837591.563031752,13849924.029003765,13861515.074212966,13872404.159222549,13882629.279856376,13892226.937965408,13901232.126356933,13909678.326052375,13917597.514171185,13925020.180874724,13931975.353941096,13938490.629677285,13944592.209005738,13950304.937687846,13955652.34976474,13960656.713406108,13965339.078459995,13969719.325090066,13973816.212972421,13977647.430601304,13981229.644322578,13984578.546776067,13987708.90448308,13990634.604364317,13993368.699016294,13995923.45061186,13998310.373323178,14000540.274193456,14002623.292408157,14004568.936936837,14006386.122534351,14008083.204104705,14009668.009443114,14011147.870381732,14012529.652372766,14013819.78254898,14015024.276306864,14016148.762461355,14017198.507023966],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[9574374.775738223,9574838.300658666,9575334.887841826,9575866.85365749,9576436.677702336,9577047.014794245,9577700.70758599,9578400.799713865,9579150.549383154,9579953.443287853,9580813.210768418,9581733.838128595,9582719.583058568,9583774.989142591,9584904.900457723,9586114.476288874,9587409.205986029,9588794.923966575,9590277.824816452,9591864.478371715,9593561.844573122,9595377.287791504,9597318.590231627,9599393.963946087,9601612.060932947,9603981.980750956,9606513.275060162,9609215.948476877,9612100.455116514,9615177.690183895,9618458.975961894,9621956.041551925,9625680.995741948,9629646.292428022,9633864.6881007,9638349.191034114,9643113.00198706,9648169.446445249,9653531.898702241,9659213.698392706,9665228.060450861,9671587.979861101,9678306.132985821,9685394.777680354,9692865.654816853,9700729.894214762,9708997.928287672,9717679.416938249,9726783.187336998,9736317.192182235,9746288.489838827,9756703.249379288,9767566.782999832,9778883.607564088,9790657.5361574,9802891.799546303,9815589.196372055,9828752.269813761,9842383.507390752,9856485.559591785,9871061.472175216,9886114.926327165,9901650.480432695,9917673.807033733,9934191.918628551,9951213.376307411,9968748.475799358,9986809.406294506,10005410.378361974,10024567.718355082,10044299.927826751,10064627.707611145,10085573.947305955,10107163.681860857,10129424.017795209,10152384.032195272,10176074.648051633,10200528.4896771,10225779.72189378,10251863.876408847,10278817.668339206,10306678.805236746,10335485.790259523,10365277.720390003,10396094.079882937,10427974.528494548,10460958.68355686,10495085.894657966,10530395.00959488,10566924.130381575,10604710.35839752,10643789.528204864,10684195.930077089,10725962.021787278,10769118.13061358,10813692.146753037,10859709.209331661,10907191.385926824,10956157.345986689,11006622.027793182,11058596.297769738,11112086.600122318,11167094.594191011,11223616.776660724,11281644.08610582,11341161.48836288,11402147.54302264,11464573.953912867,11528405.10972647,11593597.624757018,11660099.893770916,11727851.679034246,11796783.751038713,11866817.607141644,11937865.29378446,12009829.357886974,12082602.951241551,12156070.108193748,12230106.211684445,12304578.656079633,12379347.707496164,12454267.554015772,12529187.5297971,12603953.489193363,12678409.300074765,12752398.420076175,12825765.515766142,12898358.08295248,12970028.026548563,13040633.160530139,13110038.592301829,13178117.96095553,13244754.505069414,13309841.942464644,13373285.151310835,13435000.648784686,13494916.869825901,13552974.254153373,13609125.154430248,13663333.582197424,13715574.810900569,13765834.8570411,13814109.861260606,13860405.391127972,13904735.686667357,13947122.868382964,13987596.125841457,14026190.902898518,14062948.093520993,14097913.259964652,14131135.882905219,14162668.651054416,14192566.795873353,14220887.475256734,14247689.208523199,14273031.363717997,14296973.69711333,14319575.943871146,14340897.458099332,14360996.899968665,14379931.967146283,14397759.167521276,14414533.630031396,14430308.950327074,14445137.068013461,14459068.17227691,14472150.632815635,14484430.953142593,14495953.74350193,14506761.710829468,14516895.663385754,14526394.527890906,14535295.37718974,14543633.46666928,14551442.277836587,14558753.567640742,14565597.422287526,14572002.314448256,14577995.162904961,14583601.393802483,14588845.00279484,14593748.61747812,14598333.559596496,14602619.90659177,14606626.5521413,14610371.265394658,14613870.748676997,14617140.693477,14620195.834581105,14623050.002252908,14625716.172388999,14628206.514609825,14630532.438267307,14632704.636370398,14634733.127445916,14636627.295365227,14638395.927178187,14640047.249004222,14641588.96003733,14643028.264726695],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[11673221.479989395,11670603.613871882,11667811.801028868,11664835.802659199,11661664.815332362,11658287.437118173,11654691.637550855,11650864.733918378,11646793.376412952,11642463.54449226,11637860.556367766,11632969.092882767,11627773.236237003,11622256.523168094,11616402.011450993,11610192.358065795,11603609.90722089,11596636.786651984,11589255.011210592,11581446.593586477,11573193.66289089,11564478.592552355,11555284.139370074,11545593.595532844,11535390.954950003,11524661.09444877,11513389.96941916,11501564.822483229,11489174.402837679,11476209.19311459,11462661.639917016,11448526.38357942,11433800.482144598,11418483.62402817,11402578.3233761,11386090.091748564,11369027.579535251,11351402.680475924,11333230.592874117,11314529.83159395,11295322.185749043,11275632.618147431,11255489.104045143,11234922.40856484,11213965.804209998,11192654.73218294,11171026.413606413,11149119.41914196,11126973.207768414,11104627.647496114,11082122.532412384,11059497.111559292,11036789.645632444,11014037.007287707,10991274.33991729,10968534.788115809,10945849.310754271,10923246.584711274,10900753.004008105,10878392.775518995,10856188.10875672,10834159.493649509,10812326.056903515,10790705.984641941,10769316.996658515,10748176.855914943,10727303.895910105,10706717.548275243,10686438.853391947,10666490.93794251,10646899.445012873,10627692.904580237,10608903.034817083,10590564.967503607,10572717.39382657,10555402.629815584,10538666.603492279,10522558.76835354,10507131.9499632,10492442.134091118,10478548.205947047,10465511.650572557,10453396.224379214,10442267.6071942,10432193.043073611,10423240.976678792,10415480.690317472,10408981.944977397,10403814.626973122,10400048.40031885,10397752.363734104,10396994.710346637,10397842.387692263,10400360.755489059,10404613.238804845,10410660.974530093,10418562.449386692,10428373.127923835,10440145.068980427,10453926.528879572,10469761.549177404,10487689.526196608,10507744.75898183,10529955.971920174,10554345.808306066,10580930.291825773,10609718.254486524,10640710.732038852,10673900.331456117,10709270.579433488,10746795.265909169,10786437.801927654,10828150.616294464,10871874.619892927,10917538.76971997,10965059.766176736,11014341.91655877,11065277.194824835,11117745.522569682,11171615.28887489,11226744.117747206,11282979.881723875,11340161.949589947,11398122.645727526,11456688.889100332,11515683.971891299,11574929.431847908,11634246.968774151,11693460.354477908,11752397.28679193,11810891.141821697,11868782.583987331,11925921.000276683,11982165.732922098,12037387.092955364,12091467.14531673,12144300.26398668,12195793.462645398,12245866.51240619,12294451.863068849,12341494.388034634,12386950.975529794,12430789.990171228,12472990.629293574,12513542.197987469,12552443.325634921,12589701.145033015,12625330.453128345,12659352.870084822,12691796.011001213,12722692.682186546,12752080.111573873,12779999.220668541,12806493.943429604,12831610.595699668,12855397.297242196,12877903.447118873,12899179.252035469,12919275.306390757,12938242.222061338,12956130.305426378,12972989.27875894,12988868.042863572,13003814.477702906,13017875.277709601,13031095.818506842,13043520.051844744,13055190.425687857,13066147.826548409,13076431.54134103,13086079.236229092,13095126.950133568,13103609.10077705,13111558.501334064,13119006.38595078,13125982.442580216,13132514.851751741,13138630.330054661,13144354.177264601,13149710.326178275,13154721.394346666,13159408.73700951,13163792.500635542,13167891.676563703,13171724.15432151,13175306.774268327,13178655.379274338,13181784.865201494,13184709.230000768,13187441.621282091,13189994.382249357,13192379.095924009,13194606.62760711,13196687.16555228,13198630.25984068,13200444.859465083,13202139.347642854,13203721.575388564,13205198.893385177,13206578.182199774],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=0.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN = visuRMSEGrid(ElasticNet(), 'EN', alphasEN, 'alpha', GridEN,\n","                             BestParametresEN, 'elasticnet__l1_ratio')\n","FigRMSEGRidEN.show()\n","if write_data is True:\n","    FigRMSEGRidEN.write_image('./Figures/ConsoGraphRMSEEN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                     15\n","                             R²          RMSE           MAE\n","KNeighborsRegressor()  0.304837  2.410289e+07  3.513263e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predkNN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4419671.866666666,2325879.2,1382114.7333333334,106116582.8,3648631.2625,1373169.8,763409.2666666667,6002996.6,1002894.25,70243377.13333334,1092257.8,1686047.0666666667,1673467.8833333333,875994.2666666667,2463905.8333333335,35262194.266666666,13720963.3,4333262.154166667,39051144.28333333,768911.5552086666,2326816.2,765120.8458333333,925172.7333333333,832901.3916733334,23605139.233333334,986755.3916666667,2579305,1116715.52712,4312788.691666666,1697671.03334,2169629.691666667,874714.1625066666,2401311.220833333,2011738.1333333333,4038278.8,1069383.2,6488656.616666666,1358383.575,3013823.6,1098099.6166666667,2862364.4,12556883.4,731140.7151133333,6532912,125073873.6,4186088.254166667,29192720.333333332,1547777.4,54049077.86666667,3871881.8916666666,1069118.4,1869107.3208333333,964212.4666666667,1368068.6354533334,2197644.933333333,1435555.8,2413808.933333333,2356283.525,11531825.133333333,1031817.2666666667,1342113.0521200001,4333262.154166667,705029.6666666666,1859116.170316,4706937.466666667,800511.9708400001,6051823.466666667,2180641,883970.2666666667,9284923.533333333,886291.4,1366045.06042,3180229.308333333,496394.4,791500.9333333333,3226512.466666667,14044011.75,1097798.8,973647.0416666666,3230092,786473.8270866667,1732134.0708666667,1640775.7458333333,1291760.8,2091095.1166666667,6675538.933333334,1053168.9333333333,1446039.4,1562097.1093733334,1775650.8417066669,12538076.3,8364489.733333333,860575.7333400002,2606846.3833333333,8488486.933333334,35402354.166666664,1364711.1083733332,36069367.86666667,4230475.4,4469172.333333333,2570441.1708333334,702035.5041733334,7696898.466666667,2031101.5333333334,2612471.4,1206957.9,11798724.866666667,2645062.466666667,1414076.3333333333,2112523.0562533336,1676293.8666666667,817564.8583333333,27814206.466666665,2280687.5166666666,1677877.125,2459007.1333333333,12207010.866666667,874106.9333333333,16295114.833333334,3902664.7333333334,2679792.9166666665,1552678.6,1300741.5333333334,970326.0572933333,709239.8,954724.0666666667,798762.9333333333,1076354.3333333333,3316884.5416666665,1769899.72292,951929.8187533334,7790484.733333333,1893752.525,1074237.9333333333,1304234.9333333333,3935478.7666666666,837913.6,4345006.5,1285295.7416666667,2446132.6666666665,1675719.4083333334,788734.8250066667,10238204.75,896480.4,84830727.2,2581670.8666666667,1442578.4,18614582.733333334,796482.09168,14090923.133333333,982318.4,816396.0312600001,1594357.6,124534982.13333334,833863.1401066666,2967787.9916666667,5617026.666666667,5988950.733333333,919541.7791666667,1977449.9583333333,727323.9979200001,3991900.691666667,2114992,763348.1916666667,6908142.4,3860704.625,4444287.55,851741.8666666667,5740734.108333333,16327456.866666667,702035.5041733334,2944792.153649333,4615969.6,10093215.833333334,6675366.65,2316904.3333333335,986329.2500066668,1994016.6,1460624.68962,1226122.8354200001,9800351.95,1503668.7291666667,1915006.6,3008891.17604,1555767.6,1428565.2666666666,24541519.533333335,2170765,6744245.45,819674.4666666667,1371110.22292,2638488.1333333333,965731.1333333333,3008891.17604,1733373.2625,6547207.566666666,1766216.4666666666,2540355.4,2455336.066666667,11031353.333333334,4076424.316666667,1135342.5333333334,1200783.1041666667,960792.4687533334,690427.3083333333,7816821.8,3450444.8625,1192506.9166733334,2947822.8,2452308.066666667,3420483.1,2537651.4875,1339038.81146,1192454.4042,940184.9833333333,1658605.5333333334,17003990.2,5211034.933333334,4503848.541666667,17405880.366666667,2874021.1989599997,949345.6250333334,986302.8708333333,6169889.166666667,1629990.3437533334,1544469.0291666666,1281663.6,8734322.383333333,25262760.866666667,805024.4833333333,2438562.8583333334,1533567.8,2260206.066666667,3315824.029166667,13494807.966666667,17032404.233333334,2667131.6333333333,742672.3333333334,861233.9333333333,6368212.6,570928.8145933333,3126930.7333333334,10172829.916666666,902752.6292000001,1313822.8666666667,906384.1333333333,4159451.4833333334,858795.1687533334,1096320.3333333333,4019321.975,7225995.4,937074.6666666666,2600490.6666666665,1823097.2666666666,8859794.133333333,982913.6458333334,1826637.2145866668,2591596.7333333334,1929090.8041666667,950808.7333333333,1507818.525,1405178.7687533335,4073058.75,935327.0541666667,762496.6,1346140.8989600001,4120309.933333333,1953321.9333333333,1213152.56042,2150502.066666667,3017884.4916666667,788221.7208333333,700245.1333333333,1206444.0687533333,760889.8,2742071.6,1830987.2875333333,3741125.7333333334,1475523.0666666667,1151028.1333333333,3766430.7333333334,1024463.1333333333,1069821.5333333334,570928.8145933333,1661605,4151548.629166667,6369603.2,563468.9875066667,873412.3333333334,1267180.4,16895914.1,2058005.5145866668,2327148.6666666665,13651566.466666667,1007277.7083333334,529794.4666666667,1137852.6,2881936.125,1735032.0427066667,2013885.7083333333,13452278.733333332,3180229.308333333,1299773.1375333334,1989137.8,4412604.958333333,6711410.733333333,816396.0312600001,8575643.533333333,1371732.0250333333,916540.6666666666,907283.2,1809112.6833333333,1449592.0666666667,1371459.8,1565365.5333333334,1585862.2541666667,8242649.35,765981.5333333333,570928.8145933333,1008673.4,3064244.441666667,2173383.066666667,3208557.066666667,7107680.4,1289181.2,9204181.866666667,1696110.7312533334,3918872.5833333335,4036017.7916666665,9475102.333333334,1264116.4,704232.5333333333,1629944.06042,2799316.7333333334,7248662.266666667,828356.2125,4655989.866666666,1048977.0666666667,1164735.7333333334,1877403.1333333333,1214158.2,16232559.733333332,22124667.666666668,709540.8625066668,2053556.2666666666,2633619.8187533333,1512331.9042,1011278.7395866667,8413796.433333334,3742192.0166666666,2699190.4875,1435054.8,2273527.466666667,3426802.9791666665,4350171.541666667,691020.2,9543334.45,1302225.4,1056810.6,792833.0208400001,2710276.5875,1576890.1333333333,2411869.7270866665,687570.15,1384868.4958666668,2340972.283333333,896480.4,4343754.704166667,1327385.0666666667,11399936.266666668,780764.5333333333,3960047.066666667,1011927.9791666666,831469.5645933334,2363497.3625,1468571.3708333333,2522442.2666666666,1069828.1583333334,2027190.0958333334,792833.02084,7064289.266666667,2411535.775,8681233.333333334,1216990.8,4444287.55,4529219.845833333,2122862.875,951290.9333333333,4012020.558333333,511839.73333333334,4317915.25,2952610.6,3195001.7583333333,5408865.504166666,738606.4666666667,8011009.8,1148374.6145866667,2435744.6,2569990.1333333333,2800580.6666666665,744881.3333333334,1371732.0250333333,789762.7229266668,1306071.6770866667,18728184.966666665,1768336.4,3127831.85,8745996,1809112.6833333333,1494743.4666666666,2119123.8666666667,1680907.1958333333,15562143.966666667,986618.7166666667,1925446.3447933334,888259.7333333333,1167518.2666666666,30302572.466666665,5698700.333333333,2765734.7542,1729610.6572933334,2013885.7083333333,2062828,1019888.0291666667,918930.8,1206444.0687533333,6955248.033333333,1392897,1656460.0583333333,930711.5625,2206440.7083333335,811033.95,927074.0354200001,1383289.8,1602051.4937866668,14201812.2,923695.4666666667,1465854.6625333333,2210556.2666666666,2234528.725,4548346.516666667,19808113.133333333,5245092,1157478.1958333333,19175716.166666668,7133010.233333333,9328976.8,1259753.8666666667,1083811.3833333333,3929237.7333333334,4059917.533333333,4194089.1052066665,2382179.091666667,4135738.525,2007820.7291666667,1944849.2,828586.1333333333,4194089.1052066665,1337643.2166666666,13685129,928097.5333333333,2325879.2,90346967.6,6086841.533333333,6191733.2,4460658.466666667,5575953.095833333,7961308.816666666,1427948.7333333334,1966913.6416666666,875146.530212,29441509.8,2309880.7333333334,13284210.3,8300998.091666667,1470839.6416733335,1384868.4958666668,1434892.4666666666,2151815.3291666666,22704147.133333333,2704894.1333333333,1160292.6833333333,3842359.316666667,3167889.6333333333,1064150,703998.7375,2407232.1083333334,2731405,3586012.4208333334,1025465.4291733335,18116631,944303.4917,1268093.46254,2173500.4,1555334.3333333333,915226.4,2305265.7416733336,3389465.5833333335,2608516.3625,774139.3333333334,1373844.7333333334,3601950.808333333,1347667.6562533334,885902.8208333333,752189.0666666667,1659749.0083333333,4734574.333333333,752548.7708333334,765981.5333333333,1384400.5687866667,1045464.5520866667,2186997.1333333333,878216.2666666667,1059448.3333333333,1512812.3145866666,8289937.4,3829166.8,1084473.4666666666,1150478.3812533333,2377645.627086667,51631439.6,1984360,1599112.4333400002,783513.0067733333,3117064.2333333334,5152420.8,1033587.2,3111508.2,1096693.0666666667,1468984.3333333333,1282694.2916666667,3258258.0833333335,761639.5333333333,1324222.6666666667,6788132.933333334,1693519.2666666666,2045236.9010466665,7287782.4,1341489.9687866669,2098528.066666667,1650230.7125333333,5999747.266666667,1250540.4,1438692.2146200002,3226414.75,1668220.2,1478753.9062533334,4888901.666666667,1814930.0583333333,6646873.55,1026176.7333333333,1196965.9333333333,798762.9333333333,4092416.4,1122072.4,694867.5333333333,1698704.9333333333,1391304.525,1711007.4666666666,3070970.971873333,1571875.0666666667,4436437.641666667,3647546.7333333334,13039635.4,8811653.133333333,11796887.533333333,7630120.4,2023686.7333333334,724830.9333333333,5004054.533333333,3140520.1333333333,1405057.8,3345251.4,9744662.433333334,2139542.957816,1266788.2666666666,2993040.6750333332,1824177.1333333333,746741.3333333334,1698077.8,9936009.783333333,8551028.35,5595895.458333333,1077381.9333333333,18716102.866666667,1638696.4375,14355904.533333333,4137242.254166667,3424061.4,2612922.3333333335,2941320.6666666665,3201956.775,2466195.066666667,915009.8666666667,1376911.2,820481.2666666667,895540.7333333333,5004054.533333333,1525200.5792,869952.9,13637069.266666668,1009734.2375,1419577.2645866668,902943.9958400001,1740258.2,4204865.933333334,716681.4666666667,2758837.533333333,715353.1333333333,2165455.933333333,1095696.3625,885960.5333333333,6481571.333333333,1780754.7333333334,2558205.2666666666,1669739.025,3746767.7333333334,37160844.4,769428.1791666667,9145050.766666668,993412.2,1288516.5333333334,3987302.8,1931737.4,5057606.666666667,8527608.633333333,2945702.6,570928.8145933333,1670749.5083733336,2484686.878649333,620319.4500066667,5562505.733333333,1741939.4666666666,777109.2364586666,2377645.627086667,1326482.6666666667,1165540.9333333333,9402053.233333332,836572,2149342.4,984241.9416666667,742882.467712,2267538.995833333,511839.73333333334,946473.8666666667,1069118.4,3654544.1083333334,14578091.233333332,15026339.933333334,1275579.5333333334,2024879.8187533333,5612204.316666666,2998250.8666666667,1389647.9875,884599.8,15211089.633333333,7906848.2,30817602.666666668,1046465.025,1231758.8666666667,1265887.5333333334,2706762.533333333,5810116.925,2591596.7333333334,1506839.2666666666,2661698.533333333,18215049,1159125.6,871062.3458333333,1177692.0666666667,1985507.9187533334,4845681.029166667,2483766.4,2006035.6,5682844.333333333,2046477.2666666666,1500934.5625,748539,3841864.6,1669921.6,5572053.75,1336215.9333333333,823371.6125,1922112.1145866667,858525.6666666666,5618296.466666667,3447714.4583333335,2984192.3333333335,730428.863542,1310516.5625,1007696.0666666667,1084292.9343866669,1449592.0666666667,8718976.25,1070849.0416666667,1291760.8,10293422.4,946557.8145866668,1110622.2958333334,2241426.933333333,973047.8666666667,5821312.333333333,875146.530212,1326016.4166666667,2059190.2625,1794058,2064341.0541666667,826608.4,5317538.6,993529,1339082.8,1053168.9333333333,1687039.4593733333,1401148.2,42409828.13333333,2467951.062506667,2612352.004166667,18546590.066666666,5419859.666666667,5323529.683333334,900186.2625333334,3225586.2,6532912,3066302.3333333335,3205993.7333333334,7270356.833333333,1397202.7042,2210556.2666666666,949166.1375,2250596.25,1579416.0958333334,4393671.008333334,792833.02084,1030145.075,1057161.047926667,725783.2,1390837.2416666667,1018744.8,983574.0666666667,1952134,7306420,1998252.5333333334,1540809.3708333333,1366342.6,1442578.4,3504615.4375,8537347.525,2783801,2648063.2333333334,1783003.7333333334,1586823.6,1740258.2,1811604.5333333334,2376475.8666666667,7133402.4,2511261.2416666667,1837791.4,1528473.8208333333,18380193.466666665,2915326.6666666665,996506.2666666667,5701458,4188430.6041666665,20559478.6,758001.8666666667,4677474.8375,21048585.533333335,36960821.2,1562097.1093733334,18304616.2,1480553.8666666667,1611349.1125400001,8051072.733333333,1979725.2666666666,1019798.4,1729278.2,3542510.8,6368255.783333333,1831795.4666666666,2305104.1333333333,1859116.170316,2309880.7333333334,1464484.2166666666,4346007.666666667,993149.8635466668,781849.1375066668,12267457.566666666,22124667.666666668,682954.4,2394116.65,891048.9145933334,2313050.1333333333,550485.0666666667,6937973,998074.4666666667,2048671.8833333333,2011738.1333333333,1953321.9333333333,2748013.8666666667,1646200.5208666667,1935841.0666666667,1827066.4,726857.6291666667,2501797.3333333335,12782853.333333334,2622649.066666667,11563052.2,5529879.6,2296629.3,1253573.3333333333,3243778.8916666666,6617349.666666667,3693365.175,3966812.6416666666,15121186.1,17922315.866666667,966766.5333333333,5442037.666666667,1614566.1708333334,1971914.6,1712586.4,2320121.566666667,1661773.5333333334,1783003.7333333334,3359411.775,26443443.733333334,3400172.7333333334,589757.9333333333,903262.3333333334,5311700.520833333,9221068.666666666,2344439.5833333335,780764.5333333333,1005600.05,16535697.733333332,2238673.3333333335,923254.1333333333,811968.4479200001,2481552.025,2403546.3666666667,4589007.366666666,1925246.0208333333,4205150.4,3857079.2083333335,2126651.225,32259155.866666667,1388018.8750066669,1684369.2708333333,1444616.675,4346884.983333333,2086040.4917000001,1057161.047926667,5004249.4625,4420403.8,3050729.4,2472127.8666666667,2007686.5583333333,2448369.533333333,3362488.9166666665,925172.7333333333,2625897.2,2293479.066666667,1019020.8666666667,603814.6395933334,3148675.8666666667,2205308.3432293334,2686853.9458333333,33465865.333333332,5608275.933333334,2390764.1416666666,1698704.9333333333,1281663.6,1437125.4,2979009.533333333,11518402,6627174.933333334,3575598.1333333333,1188880.6208333333,1532539.84792,2216933.533333333,1755450.8,2445306.7333333334,2037343.65,1343624.9708333334,985873.2,2884928.6666666665,1195099.4395866666,1249359.8666666667,1476441.6958666667,1953321.9333333333,935762.2666666667,1444242.3333333333,1659213.4666666666,3252181.216666667,25218798,867416,3162810.691666667,1225364.1333333333,845182.95,2721196.533333333,731763.3630266667,2607057.966666667,976099.95626,4792586.083333333,1980201.4083333334,2600754.8666666667,1981013.0666666667,3660236.595833333,1746792.6666666667,2476679.8666666667,1156934.6666666667,1121981.4,5043596.783333333,6295484.2,2433836.8,10596842.966666667,1141122.2666666666,646917.2895866667,1159105.3333333333,819345.4666666667,2576063.5083333333,2281597.9,709733.2,2323317.6427066666,792014.4,42828959.333333336,3829631.566666667,2368464.3333333335,1494207.5208666667,1347181.2666666666,930195.0458333333,19104359.166666668,12175692.6,2224783.65,1519647.9333333333,3717495.3333333335,1111270.8583666668,942678.8666666667,700362.9333333333,2023388.525,1228970.1812866668,709540.8625066667,530214.6,816396.0312600001,1446817,12354900.433333334,906384.1333333333,553550.11876,678479.7520933334,920316.7875333333,965987.4125,787864.2,959396.1520866667,20316588.333333332,1319986.4666666666,2686040.8,3009705.2666666666,3247016.7333333334,2790832.533333333,861865.9458666667,2611827.2375,1494124.6666666667,2356391.933333333,1039240.8114666668,4128062.308333333,7805035.866666666,2843599.1333333333,4119363.3333333335,3053130.5166666666,1882310.5750066668,1123408.0666666667,793142.0916733333,903763.6666666666,14884179.016666668,2580510.8041666667,557878.8791799999,2283397.404166667,1544959.0583333333,1905898.7036493334,107422450.8,2823868.6666666665,7580320.333333333,2190431.3,5755881.533333333,2717652.6666666665,4662134.266666667,3948808.91042,2274599.779166667,1154589.7333333334,5913819.933333334,1384482.1333333333,669939.5145866667,2156549.95,1496293.8792,875994.2666666667,17254401.1,1396402.3041666667,2749158.36354,2672018.066666667,1742646.4666666666,1421245.825,893920.3,1425138.2666666666,797691.6708333333,920887.4,811206.5416666666,968030.7333333333,2719238.9,1045849.2,1288622.4542,2146683.8208333333,5631495.166666667,3008891.17604,1594048.0229200001,4732278.6,24018333.866666667,3812051.1333333333,6044678.116666666,2281467.8,14045987.633333333,921328.3687600001,1643660.8667066668,2240173.745833333,25178157.733333334,2698643.3333333335,4082888.816666667,4210022.658333333,35188061,850546.175,731763.3630266667,1494743.4666666666,16055132.666666666,1108122.9,2218484.533333333,2703840.7333333334,655210.2947999999,1735032.0427066667,2384481.466666667,5247701.4,1356686.2666666666,18702034.366666667,16922706.966666665,1948541.2792066669,869699.4312600001,8581037.5,809315.85,6124527.066666666,1974367.4041666666,1108843.5458333334,3243778.8916666666,3217249.3010400003,975735.0916666667,15713396.466666667,955534.1875,1491021.8,1534195.8416666666,1890029.3417,6091956.666666667,2417154.575,2737990.375,1998094.4666666666,983574.0666666667,1075533.2666666666,1373913.3167,1054798.4666666666,1764362.9750333333,1110174.5916666666,1813043.4666666666,3085261,2718140.4,17974579.733333334,3777976.466666667,18271863.6,25728987.8,1924616.6708333334,1474478.6666666667,2368464.3333333335,851741.8666666667,1168407.2000333334,2269775.35,1935841.0666666667,1181631.1333333333,766702.5177120001,2440599.1333333333,1175266.5208333333,2084256.3125,1997583.0833333333,3757379.3333333335,4412094.683333334,1006862.8,1218175.0666666667,14563744.8,810693.9166666666,1382433.2666666666,1523789.9625400002,780764.5333333333,1506054.7770866668,3397188.2,3594904.6802066662,1638696.4375,12573004.933333334,4391369.666666667,2925330.933333333,3957685.0166666666,2707771.345316,22848524.533333335,5689034.166666667,1505697.8,1195977.1208333333,4716694.4,986329.2500066668,2044477.9791666667,887269.6,1893118.1119826667,1160563.1333333333,7878331.883333334,1790455.8,1681865.9125,2575223.1333333333,2303948.283333333,9252048.733333332,6839811.95,1718438.6125,2463749.441666667,6369747.2,2717652.6666666665,2504993.1083333334,4333262.154166667,17698754.2,3452840.5,1336092.2666666666,1447703.4208333334,1712870.5052066667,2429004.933333333,996496.8468866668,9663748.483333332,2091308.3333333333,6857808.533333333,4161417.7333333334,939038.9333333333,3996213.558333333,5954822.733333333,2704894.1333333333,2191751.5833333335,12851088,708638,1375042.5500066667,14871202.533333333,2439937.6,100401983.5,1373169.8,896939.4770866667,561263.5041799999,46577084.93333333,1225364.1333333333,1393652.1333333333,5970585.225,3739332,1165666.6125066667,796482.09168,2769620.7958333334,2063390.5333333334,4686238.366666666,1499252.4250333335,1633340.825,1998382.4666666666,3586132.7583333333,1801421.5333333334,7703389.333333333,4047682.8375,9101165.466666667,903262.3333333334,831050.4,2934250.8,1444996.2666666666,2783594.2083333335,822805.475,603814.6395933332,976908.1333333333,2344680.066666667,1466415.2,762890.9333333333,1291574.5000333334,2191561.6333333333,934127.3333333334,931481.5333333333,885960.5333333333,1239900.0666666667,1643660.8667066668,725783.2,8738645.266666668,2177607.1166666667,691869.4,7798041.683333334,3872886.6,1732505.5333333334,929230.8708666668,5131236.3,1774579.4,2570602.4,9734379.8,853332.8666666667,2219836.8125333334,532242.8770933333,2534772.5,2394941.281253333,6047464.366666666,1464403.9917000001,6216030.716666667,6969779.466666667,9512661.866666667,1707280.2541666667,1838315.6,1629944.06042,4392885.383333334,2176330.2666666666,28800227.6,920316.7875333334,883602.2,1084591.2,1842892.5333333334,2581460.4166666665,1257557.1395866668,2321716.066666667,2369775.933333333,797856.5291666667,6532912,1245633.6666666667,1343202.3458333334,1806403.0666666667,21816192.133333333,1472142.3166666667,3042415.8,15060172.2,2589018.1333333333,7798041.683333334,718698.2,5835525.816666666,2063117.625,2210556.2666666666,982811.4,952155.4458333333,820706.1333333333,1596308.7125,828266.6062533334,1917022.4,7840139.366666666,3066390.8,816396.0312600001,1610255.9333333333,4132150.6,5994943.733333333,5304442.933333334,2281467.8,810082.4,1069383.2,847068.4916666667,6186080.266666667,971799.1125,1935841.0666666667,3215762.8333333335,3408867.4,2497290.941666667,3743201.2,2216933.533333333,7276349.666666667,1194638.3708333333,11033190.766666668,2323317.6427066666,1651748.82604,4819137.8,4064848.408333333,2535212.441666667,1867023.5166733335,6901991.333333333,2645927.466666667,2231913.6,2497290.941666667,811239.8666666667,4204903.133333334,3825402.35,8420152.266666668,1348164.3333333333,7929980.066666666,18935167.733333334,1585658.8875066668,925172.7333333333,561263.5041799999,1646700.4666666666,35331139.8,1469752.8333333333,5343731.1625,1044907.8666666667,1195099.4395866666,1045756.6666666666,1626147.5687866667,4551999.066666666,2976298.0833333335,749897.2,512021.4666666667,1361557.42292,2739026.466666667,2219661.4,4844608.441666666,936738.8583333333,798336.1375,7370793.866666666,1769724.4666666666,1374449.8666666667],"xaxis":"x","y":[4395510,1499128,805751,58761304,3639670.25,1327591,476069,17714332,555153.125,143423024,415453,1160989,1696978.75,1334876,4585966,11962615,13730204,2356376.75,18470340,1396691.875,2056762,659176.1875,619409,791035.5,9683979,1241355.375,29540052,1237655.25,4018570,6118300.5,1568550.75,526752.8125,7575080,2058584,2711035,965786,5135040,669217.6875,1692323,803938,3497215.5,63668488,809291.8125,21069722,291614432,10105379,32141070,1523508,94560088,3073937.5,1024866,1516831.75,553230,1394408.25,1138386,3546459,4107827,2014500.375,20675922,1115107,1396280.25,151754.2969,1127895,2615247.75,2735128,489557.0938,2311532,5227992,848497,22965152,666967,673030,3242458,433443,614044.6875,4216639.5,12549118,733249,770736.125,1848074,1051523.25,853314.5,1017787.125,501194,2298815.75,2255026,2202407,1045337,1436109,1362523,19526080,13146645,635516.375,1532390.25,6227419,274682208,1751483.875,30135280,2265487,3218824,1725558,816300.5,666392,2292169,2663520,931148.625,7520554,7613116,1232712,4970047,942962,1166069.875,9387211,5826745,1423463,2660396,4103027.5,784584,8233527.5,4034028,31803568,345399,2360656,711118.375,549438,901679,1939297,1094343,2455923.5,1992158.25,817695.6875,2986728,1008536.813,1078936,654446,2591768,555709,5401277.5,2099386.75,1983256,862945.125,1417728.25,14172606,643740,163945984,1051944,1569569,8747819,534703.6875,5032796,518107,556700.6875,1261963,114648520,9313513,7251589,2612378,2923484,754241.3125,162960,836437.6875,2547848,1563070,677476.6875,7860942,10040121,2148565,730672,3009967,9986032,505601,308400.4063,14064603,13631685,4748137,10434387,604183.1875,2139713,920068.875,1128060.75,11624548,596242.1875,3814590,2844685,1278437,1119679,21346100,2136086,4991893.5,590629,659368.625,1437569,450518,1047461,858701,9788717,1769137,2870914.75,1025659,10444297,4916451.5,641713,867184.375,688641.8125,746858.1875,15324777,1829981.375,743748.5,1260902,1307883,966812.125,3540225.75,1063960.125,922153.6875,794715.6875,1200811,4581493,6426022,4178233.5,8437401,1063480.5,739869.6875,824320.1875,8218155.5,913117.6875,764832.6875,2402535,6843855,37022180,706505.6875,1858224.625,2583451,2004777,1237252.75,10775646,16467486,2001643.75,576052,701422,6797528,270838.5,3265995,31371618,727374.6875,811900,784584,0,2545405,1631031,1536998.25,4998015,132105,1524499,1279570,6268627,1060818.25,991918,3158151,1441445.625,648022,1702639,3686962.25,2960159.75,255971.2031,488181,1219744.75,3321035.75,3043767,965651.125,282898,1645748.25,768479.625,508051,331059.5,633408,2454861,10834087,3752132.25,1108604,907617,7456330,1076693,966764,576644.125,975721,1987934.625,6962073,455798.9063,919180,3242066,6657348.5,1128179.125,2625396,10564729,1832031.5,452614,1365778,11508035,1587454.75,2009703.25,6860521,1544146.625,887022.875,3220635.25,7771933.5,2584082,536642.875,14619984,30038490,848734,2018011.875,1612167.25,3468606,1287422,785901,1287040.625,2225512,722062.875,645028,5975230,1878442.25,3655453,2322314,3073778,1425226,25631512,2946756.25,2483074.25,5662234,4934334,1123416,1826203,1187499.75,3518459.5,5917000,904059.875,4917420,601590,2204020,2559403,1140137,13609040,19991412,552015.625,1544117,2839882.25,1339220.625,2466100,4636396.5,1730431,1700057.375,1072487.875,1090545,6858533.5,2306723.5,918514,1157976.5,2156407,858932,668915.8125,1904641.75,1769998,0,530700.125,2804534,1678495.25,1003551,5572010,634561,13901935,1360847,650313,2195160,1335007.875,1107290.625,1031667.5,1234149,2114860.25,2805826,591845.3125,2251442,1898509.75,6299357,1076389,3091387.5,9230223,1544388.5,841841,1381590.125,662424,5053635,2401148,6182350.5,22078064,790671,4608963,662685.3125,1822830,3106930,1733511,362823,830286.5,495481,601818.625,21027396,952879,2337651.25,8037287,1809586.75,1657201,1167813,720170.6875,55030192,1862176.875,1509292,2323265,1072855,25970248,9128267,9029489,5976246,1934971,3316562,948676.5,755269,598857.8125,8190809,1026921,885958.125,1878742.875,3212269,811907.625,564754.1875,336461,1186835.125,6411570.5,1960538,847298.8125,2456132,2828950.5,6298131.5,12448381,4677797,753721.125,13458948,5722325.5,12931002,1482752,645055.5,2903161,6722886,2373568.25,3375211,2822377.25,7768057.5,1831722.875,640374,1485945.875,1224045.25,5216213,730292,455153,37797084,9375125,624180,1860084,1015060.188,16016644,670779,1427294.375,776248.6875,39403320,2664566.5,7182436,41399504,786253.6875,1208168.75,974179,1325597.625,26941110,463371,938833.625,5155090.5,1811506.375,876339,1083467.125,5804784.5,1044642,3652708.75,588856.125,15088676,552822.125,648879.3125,967432.8125,892227,507451,405050.4063,1750419,405062,593042,1116040,1827424.125,222600.0938,568033.375,622051,2003447.625,2654755,2233268.5,878927.8125,560334.5,919746.3125,1247972,587903,1732954.25,880115.5,7592451,1527348,742565,4133811,2442220,131373880,1190733.5,2133797.75,1581036.75,1041652.813,1912690,2534671,2483320,2698077,1191926,1239315.375,6782435,403800,604591,3060149,1826107,1035554.688,4784731,6901554,1712796,608199.8125,5820965,550767,1332912.625,3245772,304248,1003176.875,6057734,726124.625,7802205.5,2108926,2827818,2014245,8354235,888561,527438,335477,1288669.5,1456808,3835202,1775433,2843033.5,4285049,8422861,1651120,8188381,5671256,1191803,531563,8688676,2964206,1580597,2133372,6917478.5,5447076.5,862826,3999505.25,3721129,593824,929688,13214416,602245,4865551.5,632157,14431245,4729846.5,11208498,1920352.125,3053785,2147062,3322250,1702992,2759674,791052.875,2683383,737570,695059,6471427,951675.5,16808.90039,4222682,1324534.625,1167887.75,1391714.25,12204922,6577130,751292,1761048,552391,3039626,669117,95544,4338616,1012559,9997511,57133.19922,4247457,26416770,842511,8819864,712149,1960558,3298570,1670929,4983934,4217623,2996502,493000,4801472,2235765.5,635698.6875,3467698,1310481,470666.6875,1213869.75,1093627,1372706,61762380,842923,1820292,961491.3125,1467029.75,3042875.5,718362,1453543,1222509,2594119,7791957,15571642,896172,2763597,4039667,4475652,776237.375,811021,6352846.5,6387710,12271749,783651.5,626612,1255834,2527198,3894142.5,1972156,1691963,699382,6083307,619870,586827.5,1452804,2058955.25,13962749,24126,10591549,3240830,2953338,1260416,704799,1617421,1224642.5,3822102.5,3863772,1162226.375,921786.375,2100766,2019254,5134446,1536287,782494.3125,713326.875,2892779,317794.4063,456397,3182301,919048.6875,1976663,3065502,593953.375,767297.8125,764906,642825,3397709,1084553.75,736838,1403820,1084048,6983736,1171380,1985171,851439,1677877,1107870,990361.5,847514.5,80469216,1321065,2411402.25,17750994,5436057,4757299.5,509171.4063,3258819.5,7015692,1271021,3435687,1987218.75,1034941.313,340270,559234.6875,2158629,966677.125,3637973,689373.8125,858780.625,1705515.25,512166,97690.39844,675772.875,815800,2184336.75,2751694,1987073,1169948.875,938908,1280245,1375978.875,2845946,1194835.5,606111,753784,892634,3113412,1762174,2921368,18868768,5348309,6376820,2058532.875,7782092,1212551,1224276,1958352,544172.375,8393893,767361,6522551.5,10939331,39523852,4519115,10020111,2601945,620104,8800292,968698.3125,1756668,2065156,2773779,9343428,219483,3303553,6917280,1091162.75,1306087.75,2849048,1429855.75,410433.0938,8873485,9049598,696522,3046543,1433504.375,1503823,551219,3789270,958725,960987.8125,1438479,1988784,43943,738983,1516377,1181963,806398.1875,2326565,49102164,1983509,9273278,6931901,1784796.5,1963054,2125323.5,3833052,4103800.75,318962.8125,7286313.5,25747022,1324965,5086863,1422406.25,1758581,1486177,1162493.625,1192100,970118,2646523.5,12263687,4767227,571644,559805,12250422,13410041,2726369,447947,1988760.875,13300859,1348497.125,661380,1625308.625,4236396.5,1906597,5450441.5,106918,2270531,41680640,771512.6875,35685224,0,1852926.625,851053.8125,1739286.75,1748629.875,863703,882896.5,2837589,571309,2237482,4765753.5,10513817,1234095.5,1372475,552729,3886929,809257,728167,6583940,2203758.5,1165598.875,59124384,1727783,1048813.75,838571,948157,1966424,3786217,5328224,3869754,1637199,1054581.375,1108310.875,2063212,1608224,1907697,1126157.25,502667.6875,662280,2439061,1019487.188,1558348,1162120.375,5762597,611997,4467292,1530744,2082754.75,16020642,514471,3751417,207572,2245568.25,4520838,855665.125,1688612.5,2473302,6136327.5,1423438.5,288191,2585187,2145603.5,10403123,2053611,1337968,839115,4369513.5,4498367,2291581,22530168,694479,508014.5938,1620721,607373,520011.8125,3234787.75,452744,930611.875,723424,85357952,2733003.75,3625434,2228675.5,524053,717056.375,10249433,18829136,3040924.25,3030041,2057457,1115504.375,1419453,602618,1539478.5,635697.3125,323739.4063,569115,1257144.75,630040,8767617,544043,619545.125,917783.5,755856.875,677536.3125,385719,558893.375,11521702,1382309,1823782,1602030,2147013,32579658,574650.375,6668568.5,1198291,3986851,2470566,4207110.5,6576563,962577,4734631,1049748.625,1623657,824516,647792.875,1225404,6481807.5,3064039.5,510517,1698074.625,1135172.25,1813718,74130576,2330029,6366084.5,342726.0938,2938366,1045413,6832991,13133225,1183042.125,1286882,4025787,557193,1475475.5,3147042,956958.875,916864,12720069,942712.5,2246747.5,2150120,14084606,2214469.5,1913558.375,1103762,684941.375,1216543,3542708.5,780410,4742869,1117658.75,806615,1149140.25,7539963.5,3202213,1366000.125,6807227,19827916,13253979,5149928.5,1706301,7811542.5,945245.625,209698.7031,1926134.875,17320768,1182575,10557910,10343027,157606480,1646088.5,116486.6016,2261586,13969552,1652833.25,1121259,7492939,928584.875,1856230.625,10018321,13010578,540465,13239050,11924933,3494929.5,2014475.875,19036416,1161814.625,5326222,1430356.75,607579.6875,5968153.5,3528086.75,1093757.75,8631444,770082.125,2520919,729096.875,1294615.25,4107443,485971,1210798.375,1858029,786979,955223,1118490,747326,1943899.25,867456.875,628609,2447861,1101536,12719042,3366528,4545148,9941308,998633.5,1210229,1833381,2057836,1534068.25,1813285.25,3303989,1380283,209644.9063,1210262,954352.6875,1594733.625,851585.1875,2266570,3591653,696709,1111858,48729588,969682.8125,4270584,1094940.75,984178,533742.8125,6014078,2331952.75,44293.5,11195917,2910252,29195468,3104171.5,2485521,14465798,3590731,1921890,1509775,4785692,188745.7031,7240274,1014405.375,3095882,1293390,5499097,226375,974940.375,1889880,4224872.5,4028943,3574542.25,4602793.5,8518313,3312125,5948007,3377209.75,2581741.75,22325364,3241917,1658751,577033.875,1649327.625,1588933,729397.1875,6505995,526059,1738376,2434208,1545485,16402563,2023258,537141,2503690,13567774,481206,1139980.625,8320719,1777005,873923712,666930,541950.875,863728.8125,61576184,5462958,2874181,3897738,2093011,666687.5,845961.375,3548858.25,3395651,20725996,1022114.5,2217620,360045,2184156.5,1355558,5525072.5,4089677.5,20560622,758875,621751,2188452,824483,1042652.625,698326.625,533226.8125,1045640,2217046,7801590,525668,789630.125,841821,866343,1481017,2798967,1359579,850615.8125,1860901,3091348.25,1207092.75,2094410,6125692,4039168,1130960,803533.3125,5121586,1049088,735178,8168547,524144.6875,2001936.5,383524,2772124.75,10642754,2759071.5,1353079.5,3245111.5,7380088,6836900,1807738.25,1296959,1604129.75,3605749.75,1551294,25210246,2159932,862744,1862752,3888945,1443367.75,1676426.5,5444253.5,4099807,2233262,1358022,760826,678663.3125,0,6609358,1028277.313,2307938,61827040,0,4321141.5,572139,1997181.5,1594422.5,2755578,523133,650205.8125,519317,108200.3984,844076.8125,1350190,465533.3125,2189030,464155.1875,637246,2131817,11026945,5424718,2401890,501741,1008665,730765,7564643,1784385.25,1383950,227566.7031,702385,1641667,816846,448676,1083729.625,801645.875,13315803,11702959,609921.6875,3195230,2506927.5,2249592.5,2619088.5,3780078,1387773,1072500,1075019.375,1908612,2992693,5134607.5,11458215,1897180,6579348,15590743,1261634.5,1392015,541542.6875,1962435,20781390,1795462.75,0,819378,400354.3125,5047936,1961581.5,2422767,1913034.125,622572,561368,1787119,3494383,1244824,2549649.75,866096.6875,1539176.625,2267094,1444429,1666221],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors = np.linspace(1, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","SiteEnergyUse_predkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","print(ScoreskNN)\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[12631260.609177724,10722273.385929497,10618738.148864666,10512792.148508172,10308672.112572763,10276089.36061855,10295554.13591782,10247150.309842385,10319341.828195065,10256915.993864119,10337501.247107115,10344105.29321233,10370831.936937843,10450787.844116673,10504366.142831732,10535960.340344839,10534864.326022977,10574915.240856092,10610333.314039372,10646983.136999588,10686071.01708436,10727499.052197596,10717862.856427241,10753302.385845061,10785534.535695076,10819704.541695518,10851683.851246122,10883482.749025667,10899380.518467322,10924224.824233737,10943155.533278346,10968104.56732122,10992995.198448837,11015435.902373467,11047101.22142286,11073479.63995345,11101307.034474986,11120930.760128487,11144995.604031133,11162126.452988457,11183093.66879437,11204340.715011675,11226383.272821616,11244082.280465558,11265341.097437738,11283355.0996611,11301962.307447381,11319684.462979423,11331763.942811592,11353340.626592597]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[14024091.268345108,12764034.984616058,13071895.704806333,13060454.8641048,12978122.960600285,13057003.954063546,13188401.377078185,13222814.554235276,13412267.687602146,13452279.461959425,13549781.400470462,13589405.492231585,13637091.496623736,13631719.23935638,13723792.657656938,13801949.97658465,13821541.371405274,13889856.237791475,13961139.636672448,14008666.67145382,14087643.204008693,14146954.64911203,14111125.006086776,14160507.58475098,14184572.715232518,14240194.466620423,14283407.860576238,14318429.166034495,14362872.35816613,14392168.195583746,14415775.000233952,14446130.891344916,14471726.28077862,14508827.923351511,14540462.749646254,14567254.112932617,14601201.466632511,14623797.007915683,14649012.470523903,14665214.665186703,14681714.105869494,14700516.556795359,14723571.453107953,14735608.770817867,14755561.216582656,14766541.414128628,14784409.08893147,14804010.016156338,14816816.380004954,14841220.246448394]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[11238429.950010339,8680511.787242936,8165580.592922999,7965129.432911544,7639221.264545241,7495174.767173556,7402706.894757455,7271486.065449495,7226415.968787985,7061552.525768813,7125221.093743768,7098805.094193074,7104572.377251949,7269856.448876967,7284939.628006526,7269970.704105027,7248187.28064068,7259974.24392071,7259526.9914062945,7285299.602545356,7284498.830160027,7308043.455283162,7324600.706767706,7346097.186939143,7386496.356157635,7399214.616770614,7419959.841916006,7448536.332016839,7435888.678768514,7456281.452883728,7470536.06632274,7490078.243297525,7514264.116119053,7522043.881395422,7553739.693199466,7579705.166974282,7601412.60231746,7618064.512341292,7640978.737538362,7659038.240790211,7684473.231719244,7708164.873227992,7729195.092535279,7752555.79011325,7775120.978292819,7800168.785193574,7819515.525963293,7835358.909802508,7846711.5056182295,7865461.006736799]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[10909149.743526723,8060113.289153256,7505438.529335152,7263776.856489262,6765906.904103184,6475091.223540894,6461780.908591111,6242173.006627346,5976569.793875922,5844207.9304090245,5839848.936330217,5751202.121219369,5770332.258905123,6116219.9926290745,6053777.626230738,5970924.652389702,5920720.251481972,5875457.48932355,5814009.764461876,5817919.286180339,5744408.254528944,5735676.570498127,5718785.368912381,5729873.828806714,5751043.010323037,5716804.8731633965,5721761.213158471,5722648.79687114,5668834.3856083155,5661636.091008899,5655739.395619911,5651882.643466645,5679043.533635451,5672127.341879455,5694595.669592584,5714978.916892587,5731287.588407983,5730095.633289541,5753584.336283979,5767902.948440005,5794025.229769578,5819897.879390348,5837743.219067623,5856856.642245994,5877283.118339831,5903458.692235843,5919679.831995821,5930705.425042064,5936714.393266324,5953546.888213077],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[14780624.645013139,13883464.585746637,14711503.51518132,15072343.000423223,15024277.716389436,15113510.560738115,15431389.060697097,15489128.252907889,15661541.605625706,15823063.52632528,15898516.916592037,15921152.033939699,16001439.787733953,16027789.297342611,16113870.186748061,16194228.672189785,16219076.52750481,16276902.590792712,16341002.492306089,16383303.030795446,16447938.451859817,16499828.612607064,16407169.644818306,16460918.770636952,16459194.770627102,16497178.531140523,16537644.759208566,16550118.218014898,16585238.427310577,16594216.510697888,16603948.030516475,16617307.304561723,16644744.72418742,16683000.725548267,16704608.243509838,16725773.869750425,16760198.478720728,16768250.676479813,16793805.355112314,16805229.912650857,16814770.320567235,16830506.480240993,16851316.66298431,16851822.185346767,16867350.08351566,16870491.52013768,16883635.082055505,16899181.347879928,16905360.106211502,16929405.459285356],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[11537765.140856018,9133304.471805967,8832552.22582866,9484287.985759242,9455209.292998714,9376460.189379197,9693433.569110569,9460689.948529642,9740332.51936186,9809440.219140178,9962610.051908376,10089525.77367127,10080555.662678368,10194811.868461652,10270726.572251756,10322727.169425234,10205155.470107034,10254497.8868676,10325008.008628866,10400653.286620606,10484479.81617727,10556026.656085338,10614271.339138499,10673229.248391125,10713429.093067179,10791781.345296305,10830501.629111608,10894139.584225835,10951234.457052395,10987965.00714533,11017574.207682097,11058539.55685896,11074028.172157653,11103042.774835182,11162743.44414177,11196595.10110874,11232255.782851672,11258499.074101526,11292471.50992057,11323491.354322676,11359043.372711666,11391563.815557677,11411433.427552411,11435513.052755041,11468130.797662742,11493260.263520434,11516740.724351624,11541601.773176564,11561493.04318688,11597125.863588773],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[12374134.820815273,11835098.576380022,11277723.190435821,10302024.90531488,9972464.466294374,10191719.866695303,9662122.55022441,9890162.004153484,10043722.958692102,9631610.146680633,9816620.089896755,9723672.696052352,9760105.310232027,9685413.836801277,9859236.504399145,9945590.641314836,10046654.490239507,10158691.536505446,10252641.71105675,10304961.090973103,10410963.651342636,10495225.218005203,10545683.297488907,10600623.899281042,10681999.818960637,10740372.888841638,10800942.789357804,10865921.015406892,10928629.986788653,10984000.08323291,11026088.887049904,11080286.681509657,11125060.923604788,11166022.972801078,11196868.784057006,11231273.924220495,11266144.341657203,11311505.925269276,11336635.282050755,11347952.989612134,11377132.575099332,11404121.171662115,11434817.623875732,11464339.832743892,11485076.375171266,11506343.945825232,11531481.138789348,11558687.936613433,11586046.540218418,11606235.173211591],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[13554628.695677469,10699386.0065616,10766473.283542372,10441527.994554259,10325502.1830781,10223664.96273925,10229044.590965914,10153598.336993568,10174542.26341973,10176258.146765482,10169910.2408082,10234973.84117896,10241726.665139731,10229704.225348752,10224219.824528959,10246330.566404631,10282714.890781563,10309026.700791158,10319004.59374327,10328078.990428453,10342564.911513131,10350738.20379225,10303404.631778117,10301866.182109473,10322005.985497424,10352385.070035733,10367568.865394162,10384586.130609574,10362965.335576665,10393306.429083664,10412427.145523336,10432506.65020911,10442098.638658877,10452985.696803343,10476689.9658131,10498776.387795001,10516648.980737338,10536302.491502278,10548481.53678804,10566055.059916615,10570496.845824033,10575614.228207238,10596605.430628,10611879.689236099,10628865.11249919,10643221.076586306,10658274.760044614,10668245.832185125,10669205.63117483,10680389.74866419],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","FigRMSEGRidkNN = visuRMSEGrid(KNeighborsRegressor(), 'kNN', n_neighbors,\n","                              'n neighbors', GridkNN)\n","FigRMSEGRidkNN.show()\n","if write_data is True:\n","    FigRMSEGRidkNN.write_image('./Figures/ConsoGraphRMSEkNN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                     215\n","1  randomforestregressor__max_features                    sqrt\n","                               R²          RMSE           MAE\n","RandomForestRegressor()  0.459721  2.124882e+07  2.905579e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRF=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[5451281.576453488,1964414.8191860465,1911624.450872093,91357675.47674419,5428525.691569768,1718546.4674418604,833091.5260303874,11161118.225581395,831990.4172237209,94679116.43023255,1012368.4918604651,932928.1093023255,1896416.1766968993,932387.3232558139,2388064.031397674,28515155.706976745,16066743.486046512,4284631.781395349,90867144.79534884,1023736.8819767442,3261660.714534884,793770.9959311626,705489.2857558139,842580.1041689921,9975876.046511628,824591.0413759691,15503139.944186047,1240462.456548372,4904682.133953488,6715970.134374884,2663400.2558139535,723414.3251469767,3982279.430232558,2367627.045348837,5146625.48372093,752934.0418604651,3981421.1360465116,786013.2614827907,2460380.2337209303,1045313.413372093,2124198.985183721,36689138.95581395,1111180.034448372,12300135.440988373,227865438.00930232,7728189.195639535,32192406.54418605,1629315.7610465116,107073555.18139535,3925147.7148255813,1280890.196395349,1624886.534883721,260538.7841572093,1324744.0197327132,2716367.184595349,2332487.9383720933,2993601.3639534884,2410021.4677348835,15781317.053488372,899155.9584302326,1638298.6527744185,2660361.6127906977,860030.4656976744,2017670.5439337683,3682795.405232558,600643.8953553485,4936588.269186047,2626050.4383720933,811286.736627907,17561311.34651163,1569353.8755813953,2691498.406831628,4480182.820930232,569947.1122097673,826848.4896804652,2861223.7959302324,14447097.025581395,1015293.8648982791,733761.0861925582,2419502.8156976746,659212.5110469767,1966870.2306688372,1530940.973111628,1309894.4556776749,1453369.2372093024,4467509.402325582,1362210.4593023255,1137376.9643418598,2227788.5610465114,1709817.466757209,9827397.610465117,8615993.374418605,758133.4654097673,2661009.4421534883,7736437.43255814,155262723.5488372,2292521.8219762794,36590286.10232558,3778444.827906977,3748985.913953488,2469173.383139535,835524.4937530231,7922832.637209303,3081499.2337209303,1401377.4075581396,985316.0534883721,24780617.323255815,3565132.4744186047,1840141.0188953488,2572512.0633720933,1593006.8697674419,1081150.3476744187,27207480.54418605,3413973.8639534884,1512675.4950767441,3794276.9418604653,13103484.897674419,1022517.1171511628,23946847.541860465,4991322.941860465,2446209.673255814,1297195.7857665124,2463831.538662791,923168.8838250701,544542.9444767443,832871.6947674418,1178266.5302334884,1064286.3938953488,4625599.34127907,2141530.3591572093,985965.0129413954,6802182.7290697675,1404214.2931716281,907638.0453488372,879722.833576279,3696423.045348837,887939.632571163,5600935.747093023,1681698.5500004652,2482768.2308139536,1413847.9947697672,962947.1349008194,8268573.680232558,793631.0514534884,91901299.57674418,2884415.4755813954,1372095.5023255814,16671080.644186046,635880.8850511617,7688182.774418605,834233.9200590698,844975.6792906979,1841764.656395349,147958566.427907,3366371.004144186,8817157.76947721,4305151.140116279,4936384.268604651,811497.5868934882,1835289.2319776746,931267.8798417496,3838424.5678786044,2926992.413953488,969323.2802325581,8368043.443023256,5000352.689534884,2053057.1674413956,1442549.0162790697,3604697.2273255815,14035305.038372094,691521.8036418601,1838707.7192964654,4406745.644767442,43794348.57906977,4055628.3941860464,5108157.44244186,492717.3772018596,1465161.7674418604,1231240.9891086037,1019848.6956404652,13699780.490697674,1229281.6501455815,3313931.413953488,1715496.5785525606,1901274.7877906978,1622742.6729651163,25159039.59534884,1809551.3331395348,8128274.831976744,956645.2718046511,741869.665043721,2351282.7104651164,838672.2793023254,6109138.911772558,1468976.6018920932,6268652.229941861,1468925.313488372,1395105.9895348838,698432.5244186047,11702767.026744187,3839275.0959302327,2030807.4646804652,1024769.3752448063,938884.9343041863,810273.9168637209,10335995.504651163,3671228.950586046,843337.8341655812,2712693.248255814,2213278.196511628,2514846.5834302325,2864160.107462791,1646517.1704223256,1065934.1302372091,961705.2148255814,1174053.003488372,18542349.130232558,5717495.276744186,4707401.953488372,19424254.01162791,2827652.986627907,709098.3658874414,932328.9808139534,6924979.634011628,2055780.5970930234,1330490.586627907,1578789.7438953489,9765284.856976744,56479574.92093023,804546.683575814,2707960.8123544184,1452879.9354651163,2115702.209665581,2318052.2764534885,12413702.11860465,16383863.659302326,1950020.7581395349,628415.584447907,977308.3026162791,6996359.023255814,581086.0827106972,2715468.7523255814,16356976.094767442,754634.1158548836,1192266.078788372,958554.352044186,3303100.2360465117,886092.2420083721,1243065.0363372094,6232915.886627907,8732791.231395349,1001295.7922265118,2158756.4639534885,1492275.018604651,7693189.25,940342.3633753491,10673776.151526047,2154068.050872093,2969281.656397674,821148.7104651163,1612351.1359609303,5044802.0177330235,3925460.4534883723,694507.6021804651,573986.3555232558,1900581.260828837,2682051.5616279067,2131858.7726744185,1384808.945359071,1056729.953340465,2548679.194497675,869511.9299418605,427440.0085772092,1650099.4311046512,810194.1617248063,3353579.4229651163,1791517.6328511627,4258217.16627907,1530904.2735465115,1110199.764244186,5805381.753197675,1706842.9270348838,1241314.1389534883,539493.0761744181,1718572.4915720928,8951789.988953488,8944685.86511628,564836.9357567441,1673710.549127907,1881259.391860465,11500673.873255813,1892934.975363256,2737223.773837209,15437977.672093023,1193659.423886124,513013.10843023256,1360534.2078488371,4628317.57877907,1600140.2148260465,1829643.0319767443,10921382.253488373,2187045.0674418607,1300340.075646511,3667993.733139535,3574167.6104651163,8747493.178488372,551201.8674195341,24421211.6872093,1232249.7407945737,1020689.4895348838,896849.000872093,1342553.6296511628,1287215.9010181397,1391953.6674418605,1359208.460488372,1949389.1279069767,10165186.520930232,912766.7477469767,632184.8434595349,2192525.1433139537,2823327.2197720935,2555817.1941860467,2671271.6988372095,6044183.836046511,1374843.3255813953,12154299.66627907,2591559.5212213956,4236725.030818604,3403375.1918604653,8647674.045348838,942967.4162790697,1334497.3002906977,1297742.9645348836,2462999.4392465116,7288166.006395349,801424.674127907,4560485.552906977,767249.9563953489,1784263.8729651163,4028513.6395348837,1448575.126744186,17467736.297674417,18964061.027906977,523271.7296502313,2900319.8854651162,3444082.6143902326,926977.159023721,1186551.8501455814,5003844.068604651,4134887.9066860466,2288122.977764558,1099187.5511627907,1984374.198256279,4325133.031395349,3740757.3799418607,881284.1998062016,3552191.2255813954,1536310.3632269767,1262248.9902618604,730881.7476753489,1811428.9421511628,1646884.4941934887,1890117.0524711628,826113.2877972092,1419759.6657069765,1925434.1497093022,862406.8325581396,4236438.190697675,1519238.124127907,13789295.439534884,832734.4142928682,4192784.125,1313132.550872093,738353.4712223256,1561569.4072697673,1108242.3329944187,1787508.5569767442,1532747.2558813954,1925469.0367480624,702786.2672972092,7830372.3581395345,2071019.9959302326,8930366.495348837,1615488.2250586047,3546980.4174418603,7133858.370348837,2129775.9566883724,726886.6738372094,4710504.370348837,570904.24782,5671214.257848837,2519452.9872093024,4269563.798837209,9335687.427034883,881105.750872093,14302424.546511628,1233130.492587907,2689636.4029069766,1697408.8052325582,2490336.810465116,521657.7053888367,1226633.598255814,827976.5715863259,913889.9618703877,17273409.56976744,1579076.396223256,3353500.859593023,8621555.704651162,1630329.796511628,1384313.7444767442,3573556.168604651,903214.0859023255,17248536.48372093,1346032.050872093,2645419.0815423243,1124830.3861920931,1351313.2959302326,33410297.162790697,5525191.208139535,4238892.9491348835,1103088.2870655814,2324557.701162791,2036043.6723860463,779685.2440530232,985241.6598837209,1712101.7095976744,12164953.910465116,1547696.6302325581,1105635.141860465,1115178.2296530232,2704209.430232558,724858.8839170542,849321.1668613955,607831.0925511628,1218528.5862432553,18519337.272093024,1521666.6988372093,1050949.38432093,1872013.1430232557,2532002.6883720933,5257613.647674419,16714785.293023257,4221239.769476744,855725.2465116279,19529285.825581394,3835118.8002906977,10266796.65116279,1305765.789447907,797750.765116279,3324080.8558139536,5876098.8162790695,3396908.54084279,2238333.9795060465,3795883.86627907,3434949.9497093023,3002539.084886046,766503.4354651163,2541122.032194419,1807098.7218106976,6857716.587209302,1385077.3703488372,620187.6574167439,38398267.21395349,5907383.666860465,3539150.8029069765,4503658.493023256,4765132.293313953,9578288.404651163,1381185.5200581395,2398805.860755814,937633.0719915348,39439163.8372093,1575759.1459302325,15564811.589534884,18959387.23139535,2427413.926454419,1489177.8587325579,2684898.3046511626,1720736.7095930232,26548302.944186047,1153162.558139535,1668457.622095349,4568949.684302325,2712407.8589399997,827875.9656976744,826815.8666572093,4226723.704069767,3264065.5424418603,3432006.861337209,851451.0921562791,18903354.169767443,895060.127476743,1526117.4464136285,1609722.721511628,2101709.062790698,561352.9753875969,634949.5550478122,4039078.9571227906,1411731.7165697673,648964.1014534883,1560677.141860465,2324386.8316860464,952291.1280544187,1049907.4279069768,660328.0537790698,1536858.5950581396,2847887.5255813953,920387.909207752,900283.415988372,1002576.2101837209,1503818.1113372094,1713277.9505813953,1250578.5020418605,1492843.634379845,1539070.993171163,7892552.684595348,1940689.1959302325,825948.4202046511,2711273.5921816747,1964784.106831628,64893301.29767442,2110051.8191860467,2085819.955094884,1052705.4340297675,2810214.7209302327,7633630.284593023,1673944.1738372094,2528429.8476744187,1503491.780959535,1376480.3072674419,1303679.1734013953,4789698.811627907,966124.3976781396,1189379.1183139535,4789837.913953489,1811366.6581418605,2992297.6196223255,8401218.936046511,3228754.6074134884,1619622.811627907,1519298.2231106977,6209583.347674418,968451.3308144187,1357521.4005818604,3175026.3302325583,2560169.3299418604,833748.9732883718,4414072.847674418,1825266.6988395348,4803447.277906977,1923656.703488372,1856771.5622093023,1178266.5302334884,6442143.996511628,1344430.2790697673,554089.6321241858,1048962.2244186047,1204494.1812502325,1741450.1970930232,1942470.0159860472,2359576.7438976746,3372821.6941860467,3698758.218023256,8492984.774418605,5733661.758139535,23757621.876744185,8152584.631395349,2087385.3890260467,533395.3052325582,4551383.265116279,3144469.4686046513,1611716.801744186,4032002.7,8137761.044186046,2205660.281403722,849605.3828488372,2020362.024127907,2786303.3930232557,733505.1924418604,1436302.3244186046,13462018.679069767,8188262.937209303,6330551.943897674,1046105.8777623257,22549722.674418606,3790490.1625,17172455.330232557,2907633.120348837,4953418.985465116,2882198.545348837,5666791.467296745,3935378.4215116277,1811757.4470930232,1272150.2781976743,1489972.470639535,1107710.2672967443,781566.5502906977,3991667.768895349,1089404.407267442,719495.7052325582,13151705.206976743,1032363.9921511627,793547.8417153489,1328444.8164260464,4153528.423255814,6853005.762790698,894015.3828488372,1331605.848255814,749729.599515504,2157771.414244186,863122.0502180465,1392713.0688953488,6587548.83372093,1721607.6180232558,4671313.168023256,1903845.9854651163,3682664.9656976745,31347825.339534882,629722.541137209,7174647.591860465,851208.4415697674,1611710.7505813953,4002190.9308139533,2184714.605232558,5891723.886046512,7669541.345348837,2602319.0651162793,540751.7718032558,1683076.672245117,2417544.980894792,909489.0417158139,4980972.796511628,1255810.9976744186,465003.17718251236,923153.7947674418,1272360.2377906977,1432849.3001455814,36648271.740697674,1227488.0543604651,2670004.301671628,925746.0507475082,1171238.3886627906,2838915.0994232562,566898.8989827908,1358946.0055813952,1280890.196395349,2504624.009883721,10463453.11627907,16637793.279069768,977553.7441860465,2245481.050299536,4476358.703488372,3858997.0639534886,1079053.0118984496,1579658.0107581397,17394456.076744188,6273613.588372093,27500321.15348837,914273.9796516278,1392404.7862646512,1212722.0140990699,1630485.5104651162,5005265.172093024,2158181.3531976743,1506559.9011627906,3931353.104651163,10453163.825581396,708756.5252916277,944949.747680465,1279951.2180232557,3594874.5690409304,3738787.655523256,3655113.1188953486,5169598.747674419,3821388.6372093023,1763417.1247093023,1123959.4285610698,654945.8825581395,3347370.9406976746,1265953.733873024,4631729.789534884,2235879.8610465117,792162.0973837209,3623251.884011628,822415.0497093024,16487689.126744187,2758912.5441860463,3365532.1779069765,816407.2372097676,1127909.6093279067,1308543.279215349,975017.145931163,1815990.1126520936,11643495.06744186,1017833.2707851164,1287448.5154074419,8905766.326744186,1105925.9598902324,826440.1135176745,1468911.754360465,738941.2989827908,7527128.4941860465,887989.8411798838,817015.2601744186,1777879.1500069771,2282975.136627907,2477996.5377906975,1023163.0764534883,5212068.247674419,929222.5595930233,1006084.4578488372,1316126.721516279,976988.1114097674,680593.9071984495,46125478.50232558,2611552.6260246513,4645651.404360465,17407811.327906977,3401660.2674418604,6405779.160465117,686473.3077153488,4729390.3444767445,4881969.975581395,2818105.9814683716,2976097.8755813953,3228779.9994186047,1085889.8148255814,1638644.7796511627,1433226.0697674418,2496470.9081395348,1174163.705232558,3984736.3773255814,603163.6763181387,960572.2813962791,682473.674131907,1222965.0489827907,2130663.834593023,825741.4677390697,915047.724127907,1814370.4683209304,6889586.972093023,2028633.9953488372,1216167.019476744,1737141.5061046511,1319687.690988372,1770674.0248548838,6689605.263953488,1760173.0825581395,1520989.3114827906,1370735.1482558139,1280814.3206395349,2099071.230232558,1456645.8593023256,3262620.6627906975,9939220.030232558,3337037.5459302324,7073193.162790698,1396408.8721139533,10110940.681395348,1791543.1430232557,1206257.1575581396,4029966.4558139537,4159202.581395349,15859563.720930232,687028.3441860465,4021049.687790698,20748155.888372093,40348486.576744184,2265291.438464419,20173423.28139535,2789607.76007752,1196987.6071758142,8113058.455813954,1809038.7659906975,1004202.188372093,2470848.318023256,3739760.530232558,7814892.170930233,2703978.5994186047,2671441.0290697673,3110779.4608312557,1579778.9936046512,1921218.5642534886,3703391.7674418604,851772.7143902326,824588.9216576745,13757519.563953489,29707390.669767443,834253.8143897675,4274049.625872093,905924.8293711629,3322440.295348837,569063.9684637206,6954264.001162791,1068887.0296511627,1776016.8358331628,2246886.550581395,1877430.438372093,2142449.746511628,1275639.9810100775,1431145.5715116279,2717587.0145348837,926011.9818372094,1871796.3529069768,32402099.08139535,3220364.9110465115,9360982.979069768,5766252.0058139535,2079858.6052325582,1472016.1619186047,2644459.132267442,7366716.702325582,3233491.9348837207,5996419.316860465,13605600.074418604,21633374.586046513,1185233.7020348837,6744545.909302326,1409329.3881950697,2077280.075,2008105.598255814,1722052.0441860466,2099940.3290697676,2711847.3002906977,2935659.3889534883,16064254.037209302,3793982.1372093023,534232.0284883721,739513.5049418604,2742828.6258720933,9648589.283720931,1615265.2944767443,608719.0566865116,678637.2415697675,15412973.476744186,3109072.9674418606,1328010.0828488371,1060628.587655628,3078465.7222432555,2352901.291860465,4037029.284883721,1080388.1586511624,3776503.888372093,4794790.231976744,1550273.5863372092,49305460.493023254,2396861.363155814,1766406.6010133028,1420269.7968023256,3627982.482848837,2319283.2947674417,1054223.6965124654,4815806.415697674,3531659.0709302328,3561651.8139534886,3244117.2406976745,2950642.734883721,3548747.746511628,2395562.42725414,810131.9046511628,3198700.4488372095,3202370.7593023255,1095774.3502906978,674585.8161437206,5826550.820930232,1608402.9502911628,2629728.7305232557,42517243.153488375,4746424.840988372,1827202.1029069768,882627.1537860464,1006033.0851953487,1669402.7697674418,4875492.98372093,13137400.122093024,10613296.997674419,3820021.1924418607,1495006.0224023256,1594046.5165697674,1745087.6770348838,1221183.4125,1678090.761627907,1170469.5402618605,1636027.3024711628,868749.9545060466,4384043.008720931,1751258.6591674434,1315006.9296511628,1277668.9924418605,2718603.958139535,807270.7046511628,2388086.7947674417,986983.9348832559,2471305.1389534883,26675968.739534885,903023.1726744187,4578173.543313953,1035563.6898269767,813197.5799418605,3688214.5104651162,1007754.1590116279,2536039.6662790696,1315098.5058702945,5119421.287209302,1408308.4116302324,2237591.184011628,2252341.3941860464,2541039.0360465115,1681720.9110465117,2887322.8906976744,1103836.4441860465,1042843.4764534883,4724668.716860465,6168137.459302326,2180861.5924418606,15269896.646511627,1699057.2177348838,627190.8997691469,1094994.421511628,757774.6470930233,1179897.2578493024,1862857.4203488373,605603.5052821705,1667407.934883721,865753.3526167442,47076717.97209302,5272198.915988372,2883373.1122093024,1675558.8625,1215793.5399709768,1416428.170135659,18450674.14883721,17933415.09534884,2643805.3011627905,2317528.9180232557,4685053.737209302,1327018.629397674,1467915.4177325582,610816.0622093023,1924545.2087209302,1450562.9205027907,613053.7642660452,634884.0822674419,669338.4386981393,1231900.742543628,6990886.496511628,1008814.9221023255,670994.8420083721,891307.0652618604,772748.4284930232,1212338.3674418605,543767.0593023256,888543.6436130232,13021088.213953488,1487160.4372093023,3433635.839827907,2066745.3656976745,2664799.56627907,4360739.5587209305,999803.7591665116,3349081.6255813953,1852609.2588665118,2381858.4023255813,1010796.039755814,3784152.3476744187,7195805.525581395,3360824.995646512,5633944.088372093,2415341.761627907,1836858.6839400001,1135724.4142441861,539516.521963255,1679603.527616279,15087654.061627908,2386938.744476744,659955.6257395345,1584007.2075581395,1239748.4714390698,1685881.7627920932,51171011.70697674,3465098.1680232557,6314861.997674419,3014353.958139535,6310053.302906977,1900331.7918604652,3982208.473255814,6435938.085466047,2205679.229651163,965592.2273255814,4545312.865116279,999976.7127906977,807038.9377934884,2476666.8860465116,1219514.8943337675,910319.4218023255,16100550.372093024,1301312.6978227908,2544737.3014544183,2230050.9523255816,1501261.465116279,1584258.009388977,1296223.9011627906,1688265.4107581393,1051360.2337209303,1176591.5412790698,1499754.2161339535,893572.1656976744,2366575.413953488,941061.1988376745,1119613.7558496112,2652238.706831628,7023602.9337209305,3626385.2058083722,2223013.673692093,4269757.599418605,34870058.37209302,4999717.055813953,7436936.318604651,2098174.3930232557,11536870.105813954,780848.5482958138,1000903.0056513189,1808088.2697674418,20291592.002325583,2303836.746511628,5307099.651744186,3525607.653488372,122620409.3232558,1331028.902616279,524285.35628558137,1440825.3430232557,17621706.655813955,1354164.3084302326,2067053.7709302325,2847828.3299418604,1345306.84746093,2508971.979289769,5200512.404651163,3739012.1662790696,1154180.0677325581,21281981.720930234,14537686.018604651,1503581.3812641867,2040153.44753721,11879966.3,1437595.9614827908,5983008.511046512,1950335.9197697672,1007562.2281981396,2891967.6924418607,1938816.6026162792,1194205.5497579845,8291010.588372093,681433.9258720931,1005150.6694334884,908397.6714558138,2071979.7837209303,6897358.485465116,2941536.204360465,2685775.0558139533,2539298.991860465,729214.014244186,1210920.4593023255,1165979.959888372,1535593.8097480622,1979210.6177325582,852791.0889534884,1592428.2683139534,2649880.0395348836,1459286.8825581395,19001944.873255815,3552581.920348837,12324489.29767442,16605103.11860465,2724779.672093023,1360785.0761627906,2203552.795348837,1034967.4377906977,987345.4136499226,3677576.7796511627,3744946.925581395,1650851.650872093,991056.7373590694,1848573.9587209302,1352680.1912800001,1868757.0240526984,1509394.2604651162,1939978.403488372,4818383.4,775169.7415697675,1468481.483432558,34795963.095348835,788108.1057217054,2289943.4081395348,2705811.539826512,571039.5110102326,1524045.9348846513,4655214.887790698,2821422.576016744,2472441.215116279,6647469.8430232555,4805345.381395349,3344113.615116279,3546228.0360465115,1936262.4851744187,14143107.302325582,8834555.45232558,1350954.0351744187,1549887.3066883723,3703715.656976744,499368.7498762782,4031056.462790698,1050455.1838665116,3182171.9545876277,1245991.3383744187,10118682.776744187,1115058.5276251163,1532114.051165116,1924241.5337209303,1607073.2828493025,11662568.19883721,7105811.980523256,3577080.2851455817,4360825.027620463,8898583.686046511,2779521.326744186,2671107.233792558,3947877.0023255814,20678298.5627907,3920031.9726744187,1368367.7912790698,1507132.2427627908,1643273.7898279068,2469888.890116279,964128.5994952561,6366313.039534884,1199655.751744186,9208167.36860465,3137738.798837209,1127332.8552325582,7313071.35,4343223.29127907,1687449.313372093,2231382.3773255814,15013730.382558139,705783.977616279,1325054.8873613956,19167742.04883721,3508147.5575581393,148476242.56744185,2780677.1624031006,690883.1776162791,472737.4129683709,51285459.488372095,1461101.2133725581,2262863.317732558,5381846.240406977,1724303.8296511627,3052434.2675939533,922093.4354753492,5226820.503852094,5001266.348837209,11610838.512790699,913824.7415697675,1967907.629215349,2192029.443895349,2886801.3029069765,1294134.524127907,9307240.586046511,3055179.863372093,15132500.177906977,757520.0255813954,1494952.3715139534,2897818.075,2939144.7811046513,3836968.786627907,1012245.938372093,596962.6927404647,1181590.1622093024,2665552.615409302,2667566.1918604653,671426.1480376744,1294310.5587465113,1724698.4053781396,1214256.1,1331576.5011651162,1277863.9328488372,1190564.954360465,1144192.6526395353,1172744.7359013953,6300355.35116279,1900000.2959302326,989955.9229655815,5516049.845348837,3430217.148837209,1745468.2430232558,929067.9768999987,6142795.9,1566603.6651186044,1177679.0831395348,10468737.595348837,791133.0373793645,4484894.129746512,492503.11017488374,2407466.583139535,7020305.141571628,4610193.815988372,1368882.8500255812,4868819.225581395,7354619.230232558,8635805.518604651,1720782.0877906976,2433829.025290698,1513922.9610465115,3425444.5576306977,1975302.4726744187,36168787.423255816,1079359.6587279069,881631.6712209302,1462427.494651163,2433376.8814,1855861.4869186047,895919.016019535,2412193.3723837207,1984889.2441860465,1270080.688372093,6844282.169186046,1021805.5968023256,788572.7363372094,1804103.3662790698,9559133.020930232,1025033.2992834109,2246749.391860465,41837763.62093023,1419547.544767442,4873100.268604651,757620.6795069766,6533345.2063953485,2149135.3058139533,2465659.75872093,2958937.495937674,1107217.285755814,868729.674127907,881840.897383721,1654793.252911163,1891705.9127906978,3995034.369767442,5683163.988372093,712882.1764590696,989002.3812186046,4900062.719186046,9435741.320930233,4159076.819767442,2316999.8209302328,903084.4957851163,886655.9639534884,766015.8725293024,12932094.45116279,1266627.9540697674,2452079.5279069766,1989013.0854651162,1005649.4412800001,3034155.931976744,2451381.459883721,1672082.911627907,6428141.018604651,2319413.6475325585,44527877.9744186,3773619.6575581394,909906.1779116278,4821023.326744186,3309883.0406976743,3916708.111337209,2348414.089244651,6925270.955813954,2487078.8273255816,1204149.9058139534,2112296.7410609303,1261419.0575581396,3729334.676744186,3147603.052325581,9065766.412790697,1830034.731395349,8313314.460465116,23479281.76744186,1806229.991571628,798229.9127906977,508661.01143209275,2159438.687064186,25991097.162790697,1531078.263372093,3792025.77877907,882671.5847868216,1488400.4556043262,1483684.6422674418,2662662.2736972095,3801301.7494186047,3231092.0593023254,790971.6381796901,488000.4318703876,1809592.5273269769,2467616.2813953487,1662279.4619186046,4192196.3540697675,810736.9145348837,1229859.8610965891,6229327.7125,2113552.148255814,1191842.7308139536],"xaxis":"x","y":[4395510,1499128,805751,58761304,3639670.25,1327591,476069,17714332,555153.125,143423024,415453,1160989,1696978.75,1334876,4585966,11962615,13730204,2356376.75,18470340,1396691.875,2056762,659176.1875,619409,791035.5,9683979,1241355.375,29540052,1237655.25,4018570,6118300.5,1568550.75,526752.8125,7575080,2058584,2711035,965786,5135040,669217.6875,1692323,803938,3497215.5,63668488,809291.8125,21069722,291614432,10105379,32141070,1523508,94560088,3073937.5,1024866,1516831.75,553230,1394408.25,1138386,3546459,4107827,2014500.375,20675922,1115107,1396280.25,151754.2969,1127895,2615247.75,2735128,489557.0938,2311532,5227992,848497,22965152,666967,673030,3242458,433443,614044.6875,4216639.5,12549118,733249,770736.125,1848074,1051523.25,853314.5,1017787.125,501194,2298815.75,2255026,2202407,1045337,1436109,1362523,19526080,13146645,635516.375,1532390.25,6227419,274682208,1751483.875,30135280,2265487,3218824,1725558,816300.5,666392,2292169,2663520,931148.625,7520554,7613116,1232712,4970047,942962,1166069.875,9387211,5826745,1423463,2660396,4103027.5,784584,8233527.5,4034028,31803568,345399,2360656,711118.375,549438,901679,1939297,1094343,2455923.5,1992158.25,817695.6875,2986728,1008536.813,1078936,654446,2591768,555709,5401277.5,2099386.75,1983256,862945.125,1417728.25,14172606,643740,163945984,1051944,1569569,8747819,534703.6875,5032796,518107,556700.6875,1261963,114648520,9313513,7251589,2612378,2923484,754241.3125,162960,836437.6875,2547848,1563070,677476.6875,7860942,10040121,2148565,730672,3009967,9986032,505601,308400.4063,14064603,13631685,4748137,10434387,604183.1875,2139713,920068.875,1128060.75,11624548,596242.1875,3814590,2844685,1278437,1119679,21346100,2136086,4991893.5,590629,659368.625,1437569,450518,1047461,858701,9788717,1769137,2870914.75,1025659,10444297,4916451.5,641713,867184.375,688641.8125,746858.1875,15324777,1829981.375,743748.5,1260902,1307883,966812.125,3540225.75,1063960.125,922153.6875,794715.6875,1200811,4581493,6426022,4178233.5,8437401,1063480.5,739869.6875,824320.1875,8218155.5,913117.6875,764832.6875,2402535,6843855,37022180,706505.6875,1858224.625,2583451,2004777,1237252.75,10775646,16467486,2001643.75,576052,701422,6797528,270838.5,3265995,31371618,727374.6875,811900,784584,0,2545405,1631031,1536998.25,4998015,132105,1524499,1279570,6268627,1060818.25,991918,3158151,1441445.625,648022,1702639,3686962.25,2960159.75,255971.2031,488181,1219744.75,3321035.75,3043767,965651.125,282898,1645748.25,768479.625,508051,331059.5,633408,2454861,10834087,3752132.25,1108604,907617,7456330,1076693,966764,576644.125,975721,1987934.625,6962073,455798.9063,919180,3242066,6657348.5,1128179.125,2625396,10564729,1832031.5,452614,1365778,11508035,1587454.75,2009703.25,6860521,1544146.625,887022.875,3220635.25,7771933.5,2584082,536642.875,14619984,30038490,848734,2018011.875,1612167.25,3468606,1287422,785901,1287040.625,2225512,722062.875,645028,5975230,1878442.25,3655453,2322314,3073778,1425226,25631512,2946756.25,2483074.25,5662234,4934334,1123416,1826203,1187499.75,3518459.5,5917000,904059.875,4917420,601590,2204020,2559403,1140137,13609040,19991412,552015.625,1544117,2839882.25,1339220.625,2466100,4636396.5,1730431,1700057.375,1072487.875,1090545,6858533.5,2306723.5,918514,1157976.5,2156407,858932,668915.8125,1904641.75,1769998,0,530700.125,2804534,1678495.25,1003551,5572010,634561,13901935,1360847,650313,2195160,1335007.875,1107290.625,1031667.5,1234149,2114860.25,2805826,591845.3125,2251442,1898509.75,6299357,1076389,3091387.5,9230223,1544388.5,841841,1381590.125,662424,5053635,2401148,6182350.5,22078064,790671,4608963,662685.3125,1822830,3106930,1733511,362823,830286.5,495481,601818.625,21027396,952879,2337651.25,8037287,1809586.75,1657201,1167813,720170.6875,55030192,1862176.875,1509292,2323265,1072855,25970248,9128267,9029489,5976246,1934971,3316562,948676.5,755269,598857.8125,8190809,1026921,885958.125,1878742.875,3212269,811907.625,564754.1875,336461,1186835.125,6411570.5,1960538,847298.8125,2456132,2828950.5,6298131.5,12448381,4677797,753721.125,13458948,5722325.5,12931002,1482752,645055.5,2903161,6722886,2373568.25,3375211,2822377.25,7768057.5,1831722.875,640374,1485945.875,1224045.25,5216213,730292,455153,37797084,9375125,624180,1860084,1015060.188,16016644,670779,1427294.375,776248.6875,39403320,2664566.5,7182436,41399504,786253.6875,1208168.75,974179,1325597.625,26941110,463371,938833.625,5155090.5,1811506.375,876339,1083467.125,5804784.5,1044642,3652708.75,588856.125,15088676,552822.125,648879.3125,967432.8125,892227,507451,405050.4063,1750419,405062,593042,1116040,1827424.125,222600.0938,568033.375,622051,2003447.625,2654755,2233268.5,878927.8125,560334.5,919746.3125,1247972,587903,1732954.25,880115.5,7592451,1527348,742565,4133811,2442220,131373880,1190733.5,2133797.75,1581036.75,1041652.813,1912690,2534671,2483320,2698077,1191926,1239315.375,6782435,403800,604591,3060149,1826107,1035554.688,4784731,6901554,1712796,608199.8125,5820965,550767,1332912.625,3245772,304248,1003176.875,6057734,726124.625,7802205.5,2108926,2827818,2014245,8354235,888561,527438,335477,1288669.5,1456808,3835202,1775433,2843033.5,4285049,8422861,1651120,8188381,5671256,1191803,531563,8688676,2964206,1580597,2133372,6917478.5,5447076.5,862826,3999505.25,3721129,593824,929688,13214416,602245,4865551.5,632157,14431245,4729846.5,11208498,1920352.125,3053785,2147062,3322250,1702992,2759674,791052.875,2683383,737570,695059,6471427,951675.5,16808.90039,4222682,1324534.625,1167887.75,1391714.25,12204922,6577130,751292,1761048,552391,3039626,669117,95544,4338616,1012559,9997511,57133.19922,4247457,26416770,842511,8819864,712149,1960558,3298570,1670929,4983934,4217623,2996502,493000,4801472,2235765.5,635698.6875,3467698,1310481,470666.6875,1213869.75,1093627,1372706,61762380,842923,1820292,961491.3125,1467029.75,3042875.5,718362,1453543,1222509,2594119,7791957,15571642,896172,2763597,4039667,4475652,776237.375,811021,6352846.5,6387710,12271749,783651.5,626612,1255834,2527198,3894142.5,1972156,1691963,699382,6083307,619870,586827.5,1452804,2058955.25,13962749,24126,10591549,3240830,2953338,1260416,704799,1617421,1224642.5,3822102.5,3863772,1162226.375,921786.375,2100766,2019254,5134446,1536287,782494.3125,713326.875,2892779,317794.4063,456397,3182301,919048.6875,1976663,3065502,593953.375,767297.8125,764906,642825,3397709,1084553.75,736838,1403820,1084048,6983736,1171380,1985171,851439,1677877,1107870,990361.5,847514.5,80469216,1321065,2411402.25,17750994,5436057,4757299.5,509171.4063,3258819.5,7015692,1271021,3435687,1987218.75,1034941.313,340270,559234.6875,2158629,966677.125,3637973,689373.8125,858780.625,1705515.25,512166,97690.39844,675772.875,815800,2184336.75,2751694,1987073,1169948.875,938908,1280245,1375978.875,2845946,1194835.5,606111,753784,892634,3113412,1762174,2921368,18868768,5348309,6376820,2058532.875,7782092,1212551,1224276,1958352,544172.375,8393893,767361,6522551.5,10939331,39523852,4519115,10020111,2601945,620104,8800292,968698.3125,1756668,2065156,2773779,9343428,219483,3303553,6917280,1091162.75,1306087.75,2849048,1429855.75,410433.0938,8873485,9049598,696522,3046543,1433504.375,1503823,551219,3789270,958725,960987.8125,1438479,1988784,43943,738983,1516377,1181963,806398.1875,2326565,49102164,1983509,9273278,6931901,1784796.5,1963054,2125323.5,3833052,4103800.75,318962.8125,7286313.5,25747022,1324965,5086863,1422406.25,1758581,1486177,1162493.625,1192100,970118,2646523.5,12263687,4767227,571644,559805,12250422,13410041,2726369,447947,1988760.875,13300859,1348497.125,661380,1625308.625,4236396.5,1906597,5450441.5,106918,2270531,41680640,771512.6875,35685224,0,1852926.625,851053.8125,1739286.75,1748629.875,863703,882896.5,2837589,571309,2237482,4765753.5,10513817,1234095.5,1372475,552729,3886929,809257,728167,6583940,2203758.5,1165598.875,59124384,1727783,1048813.75,838571,948157,1966424,3786217,5328224,3869754,1637199,1054581.375,1108310.875,2063212,1608224,1907697,1126157.25,502667.6875,662280,2439061,1019487.188,1558348,1162120.375,5762597,611997,4467292,1530744,2082754.75,16020642,514471,3751417,207572,2245568.25,4520838,855665.125,1688612.5,2473302,6136327.5,1423438.5,288191,2585187,2145603.5,10403123,2053611,1337968,839115,4369513.5,4498367,2291581,22530168,694479,508014.5938,1620721,607373,520011.8125,3234787.75,452744,930611.875,723424,85357952,2733003.75,3625434,2228675.5,524053,717056.375,10249433,18829136,3040924.25,3030041,2057457,1115504.375,1419453,602618,1539478.5,635697.3125,323739.4063,569115,1257144.75,630040,8767617,544043,619545.125,917783.5,755856.875,677536.3125,385719,558893.375,11521702,1382309,1823782,1602030,2147013,32579658,574650.375,6668568.5,1198291,3986851,2470566,4207110.5,6576563,962577,4734631,1049748.625,1623657,824516,647792.875,1225404,6481807.5,3064039.5,510517,1698074.625,1135172.25,1813718,74130576,2330029,6366084.5,342726.0938,2938366,1045413,6832991,13133225,1183042.125,1286882,4025787,557193,1475475.5,3147042,956958.875,916864,12720069,942712.5,2246747.5,2150120,14084606,2214469.5,1913558.375,1103762,684941.375,1216543,3542708.5,780410,4742869,1117658.75,806615,1149140.25,7539963.5,3202213,1366000.125,6807227,19827916,13253979,5149928.5,1706301,7811542.5,945245.625,209698.7031,1926134.875,17320768,1182575,10557910,10343027,157606480,1646088.5,116486.6016,2261586,13969552,1652833.25,1121259,7492939,928584.875,1856230.625,10018321,13010578,540465,13239050,11924933,3494929.5,2014475.875,19036416,1161814.625,5326222,1430356.75,607579.6875,5968153.5,3528086.75,1093757.75,8631444,770082.125,2520919,729096.875,1294615.25,4107443,485971,1210798.375,1858029,786979,955223,1118490,747326,1943899.25,867456.875,628609,2447861,1101536,12719042,3366528,4545148,9941308,998633.5,1210229,1833381,2057836,1534068.25,1813285.25,3303989,1380283,209644.9063,1210262,954352.6875,1594733.625,851585.1875,2266570,3591653,696709,1111858,48729588,969682.8125,4270584,1094940.75,984178,533742.8125,6014078,2331952.75,44293.5,11195917,2910252,29195468,3104171.5,2485521,14465798,3590731,1921890,1509775,4785692,188745.7031,7240274,1014405.375,3095882,1293390,5499097,226375,974940.375,1889880,4224872.5,4028943,3574542.25,4602793.5,8518313,3312125,5948007,3377209.75,2581741.75,22325364,3241917,1658751,577033.875,1649327.625,1588933,729397.1875,6505995,526059,1738376,2434208,1545485,16402563,2023258,537141,2503690,13567774,481206,1139980.625,8320719,1777005,873923712,666930,541950.875,863728.8125,61576184,5462958,2874181,3897738,2093011,666687.5,845961.375,3548858.25,3395651,20725996,1022114.5,2217620,360045,2184156.5,1355558,5525072.5,4089677.5,20560622,758875,621751,2188452,824483,1042652.625,698326.625,533226.8125,1045640,2217046,7801590,525668,789630.125,841821,866343,1481017,2798967,1359579,850615.8125,1860901,3091348.25,1207092.75,2094410,6125692,4039168,1130960,803533.3125,5121586,1049088,735178,8168547,524144.6875,2001936.5,383524,2772124.75,10642754,2759071.5,1353079.5,3245111.5,7380088,6836900,1807738.25,1296959,1604129.75,3605749.75,1551294,25210246,2159932,862744,1862752,3888945,1443367.75,1676426.5,5444253.5,4099807,2233262,1358022,760826,678663.3125,0,6609358,1028277.313,2307938,61827040,0,4321141.5,572139,1997181.5,1594422.5,2755578,523133,650205.8125,519317,108200.3984,844076.8125,1350190,465533.3125,2189030,464155.1875,637246,2131817,11026945,5424718,2401890,501741,1008665,730765,7564643,1784385.25,1383950,227566.7031,702385,1641667,816846,448676,1083729.625,801645.875,13315803,11702959,609921.6875,3195230,2506927.5,2249592.5,2619088.5,3780078,1387773,1072500,1075019.375,1908612,2992693,5134607.5,11458215,1897180,6579348,15590743,1261634.5,1392015,541542.6875,1962435,20781390,1795462.75,0,819378,400354.3125,5047936,1961581.5,2422767,1913034.125,622572,561368,1787119,3494383,1244824,2549649.75,866096.6875,1539176.625,2267094,1444429,1666221],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","SiteEnergyUse_predRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train.ravel(),\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","print(ScoresRF)\n","figRF.show()\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[15224974.054300034,11581795.975585224,10573027.119173728,9827423.131987756,9547083.616019081,9743411.701045183,9566085.418839049,9375038.91234776,9404841.452191215,9493926.327243028]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[17195140.13418084,13845262.03759868,13588214.917789476,12595402.700285558,12359196.733984502,12480439.366042567,12293616.677007237,12215382.448006708,12309269.712009024,12292728.972342499]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[13254807.974419229,9318329.913571768,7557839.32055798,7059443.563689954,6734970.498053661,7006384.0360477995,6838554.16067086,6534695.376688811,6500413.192373406,6695123.682143558]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[18791102.436767403,7802818.987984711,6757469.572195232,6756810.82237927,5690456.461827845,6547067.2609594185,6445371.084868265,5783234.072597662,5853505.851591688,6077711.785250126],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[15464156.340544568,13654164.85948265,15041548.4199552,14626904.851429384,14196871.197881587,14647564.770267818,14511608.996143082,14332867.151481718,14540464.362204084,14413780.10543923],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[12859507.049685787,11243101.93358448,7896127.665263009,7614500.397327058,7970716.3751965165,8638198.45508669,8293070.315612649,8013766.982394437,8058586.442659942,7913938.443249975],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[14479723.712304583,14159225.025830671,12511460.497277968,9424977.575768167,9582201.928757554,8497071.636909632,8506840.860779423,8664298.302839205,8463883.0531763,8902925.80908315],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[14530380.732197827,11049669.071043601,10658529.441177225,10713922.013034903,10295172.1164319,10387156.382002361,10073535.836791828,10081028.052425783,10107767.551324053,10161275.493192656],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=sqrt<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF = visuRMSEGrid(RandomForestRegressor(), 'RF', n_estimatorsRF,\n","                             'n estimators', GridRF, BestParametresRF,\n","                             'randomforestregressor__max_features')\n","FigRMSEGRidRF.show()\n","if write_data is True:\n","    FigRMSEGRidRF.write_image('./Figures/ConsoGraphRMSERF.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.1.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                  28\n","1          adaboostregressor__loss              square\n","                           R²          RMSE           MAE\n","AdaBoostRegressor()  0.475406  2.093811e+07  4.403702e+06\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predAB=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[5267609.915404684,2901972.428856302,2901972.428856302,109160710.45454545,4072276.816666667,2901972.428856302,2901972.428856302,4099544.6727152066,2901972.428856302,33490100.492135763,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,58220263.360427804,17831144.427540075,3206568.7603573254,41996167.07692308,4021620.3531928062,5267609.915404684,4021620.3531928062,2901972.428856302,2901972.428856302,13229184.007668711,2901972.428856302,4021620.3531928062,4021620.3531928062,4099544.6727152066,2901972.428856302,4825295.621868596,2901972.428856302,5055455.672904192,4021620.3531928062,5055455.672904192,2901972.428856302,5267609.915404684,4021620.3531928062,5055455.672904192,4021620.3531928062,4021620.3531928062,13949552.214285715,2901972.428856302,4099544.6727152066,215038888.98816568,4825295.621868596,50057688.70704078,4021620.3531928062,64248923.57522124,5055455.672904192,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,5055455.672904192,5055455.672904192,12399906.665605096,4021620.3531928062,4021620.3531928062,3206568.7603573254,2901972.428856302,2901972.428856302,5055455.672904192,4021620.3531928062,5055455.672904192,2901972.428856302,4021620.3531928062,11268352.732317073,4021620.3531928062,2901972.428856302,3206568.7603573254,4021620.3531928062,4021620.3531928062,4021620.3531928062,16555707.143382354,4021620.3531928062,2901972.428856302,4825295.621868596,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,4099544.6727152066,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,12399906.665605096,4390302.626738889,4021620.3531928062,4021620.3531928062,8173348.469921189,276613041.2121212,2901972.428856302,50057688.70704078,3206568.7603573254,5267609.915404684,2901972.428856302,2901972.428856302,8173348.469921189,4021620.3531928062,2901972.428856302,2901972.428856302,58089158.3878327,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,20666322.75755287,4021620.3531928062,4021620.3531928062,5055455.672904192,11396490.699475065,4021620.3531928062,17149451.607209522,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4099544.6727152066,2901972.428856302,2901972.428856302,5055455.672904192,4021620.3531928062,4021620.3531928062,4021620.3531928062,5055455.672904192,2901972.428856302,5055455.672904192,2901972.428856302,5267609.915404684,4021620.3531928062,2901972.428856302,12082258.207482994,2901972.428856302,72295590.5539284,2901972.428856302,4021620.3531928062,17149451.607209522,2901972.428856302,11396490.699475065,4021620.3531928062,2901972.428856302,2901972.428856302,185914465.11163896,2901972.428856302,3206568.7603573254,5055455.672904192,5055455.672904192,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,12399906.665605096,5055455.672904192,2901972.428856302,4021620.3531928062,4099544.6727152066,15008107.923533786,2901972.428856302,2901972.428856302,2901972.428856302,58089158.3878327,4099544.6727152066,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,10735794.827885952,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,36469368.43790497,4021620.3531928062,4390302.626738889,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4099544.6727152066,2901972.428856302,2901972.428856302,2901972.428856302,11268352.732317073,5267609.915404684,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4099544.6727152066,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,15430128.032636883,5267609.915404684,5055455.672904192,21551120.730290458,2901972.428856302,4021620.3531928062,2901972.428856302,4825295.621868596,4021620.3531928062,4021620.3531928062,4021620.3531928062,5248294.845963207,40364178.646464646,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,15430128.032636883,17149451.607209522,2901972.428856302,2901972.428856302,4021620.3531928062,5267609.915404684,4021620.3531928062,4099544.6727152066,11268352.732317073,4021620.3531928062,4021620.3531928062,4021620.3531928062,3206568.7603573254,2901972.428856302,2901972.428856302,4099544.6727152066,4390302.626738889,2901972.428856302,5055455.672904192,2901972.428856302,12082258.207482994,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,5055455.672904192,4021620.3531928062,2901972.428856302,2901972.428856302,4072276.816666667,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,5055455.672904192,4021620.3531928062,4021620.3531928062,5055455.672904192,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,4099544.6727152066,5267609.915404684,4021620.3531928062,4021620.3531928062,4021620.3531928062,15008107.923533786,2901972.428856302,5055455.672904192,17831144.427540075,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,15219678.179971848,3206568.7603573254,4021620.3531928062,2901972.428856302,5055455.672904192,5055455.672904192,2901972.428856302,16555707.143382354,4021620.3531928062,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,4099544.6727152066,4021620.3531928062,4021620.3531928062,2901972.428856302,5055455.672904192,4021620.3531928062,5055455.672904192,5055455.672904192,2901972.428856302,5267609.915404684,2901972.428856302,5055455.672904192,5055455.672904192,12082258.207482994,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4099544.6727152066,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,11268352.732317073,15008107.923533786,2901972.428856302,5055455.672904192,2901972.428856302,4021620.3531928062,2901972.428856302,8327529.868774673,5055455.672904192,2901972.428856302,5267609.915404684,2901972.428856302,3206568.7603573254,5055455.672904192,4021620.3531928062,7680929.1565385675,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,5055455.672904192,2901972.428856302,13229184.007668711,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,4099544.6727152066,4021620.3531928062,13229184.007668711,2901972.428856302,2901972.428856302,3206568.7603573254,4021620.3531928062,2901972.428856302,5055455.672904192,4021620.3531928062,5055455.672904192,2901972.428856302,5267609.915404684,4099544.6727152066,2901972.428856302,11268352.732317073,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,20666322.75755287,2901972.428856302,5055455.672904192,8173348.469921189,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,21551120.730290458,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,50057688.70704078,4099544.6727152066,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,8173348.469921189,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,17831144.427540075,4021620.3531928062,4021620.3531928062,3206568.7603573254,2901972.428856302,5267609.915404684,20666322.75755287,4099544.6727152066,2901972.428856302,15430128.032636883,4099544.6727152066,13229184.007668711,2901972.428856302,2901972.428856302,4099544.6727152066,5267609.915404684,2901972.428856302,2901972.428856302,5055455.672904192,5267609.915404684,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,12399906.665605096,2901972.428856302,2901972.428856302,50057688.70704078,5055455.672904192,4099544.6727152066,2901972.428856302,4099544.6727152066,10645668.033819629,2901972.428856302,2901972.428856302,2901972.428856302,36469368.43790497,2901972.428856302,13949552.214285715,11396490.699475065,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,20666322.75755287,2901972.428856302,4021620.3531928062,4099544.6727152066,3206568.7603573254,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,16555707.143382354,4021620.3531928062,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,4099544.6727152066,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,5267609.915404684,4021620.3531928062,2901972.428856302,4021620.3531928062,5267609.915404684,3206568.7603573254,2901972.428856302,4021620.3531928062,2901972.428856302,33490100.492135763,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,4099544.6727152066,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,2901972.428856302,2901972.428856302,11396490.699475065,2901972.428856302,2901972.428856302,2901972.428856302,11396490.699475065,2901972.428856302,4021620.3531928062,5267609.915404684,2901972.428856302,2901972.428856302,5055455.672904192,4021620.3531928062,4099544.6727152066,4021620.3531928062,4021620.3531928062,2901972.428856302,5267609.915404684,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,4099544.6727152066,5055455.672904192,11396490.699475065,10735794.827885952,58089158.3878327,11396490.699475065,2901972.428856302,4021620.3531928062,4072276.816666667,5055455.672904192,4021620.3531928062,5055455.672904192,11396490.699475065,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,12561813.817547724,8173348.469921189,4825295.621868596,2901972.428856302,30233484.86783217,2901972.428856302,15008107.923533786,5055455.672904192,2901972.428856302,2901972.428856302,2901972.428856302,3206568.7603573254,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4072276.816666667,4021620.3531928062,2901972.428856302,15008107.923533786,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,3206568.7603573254,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,11268352.732317073,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,62822935.82641509,4021620.3531928062,12399906.665605096,2901972.428856302,2901972.428856302,3206568.7603573254,4021620.3531928062,5055455.672904192,11268352.732317073,5055455.672904192,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4099544.6727152066,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,12399906.665605096,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,5055455.672904192,15430128.032636883,16555707.143382354,4021620.3531928062,2901972.428856302,5055455.672904192,4021620.3531928062,4021620.3531928062,4021620.3531928062,15430128.032636883,7680929.1565385675,36469368.43790497,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4099544.6727152066,2901972.428856302,4021620.3531928062,2901972.428856302,15008107.923533786,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,4021620.3531928062,4021620.3531928062,4021620.3531928062,4072276.816666667,4021620.3531928062,4099544.6727152066,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,4099544.6727152066,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,11396490.699475065,2901972.428856302,2901972.428856302,11396490.699475065,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,5055455.672904192,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,4825295.621868596,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,49159569.34918919,2901972.428856302,5055455.672904192,20666322.75755287,4099544.6727152066,5267609.915404684,4021620.3531928062,5055455.672904192,4099544.6727152066,3206568.7603573254,5055455.672904192,4099544.6727152066,4021620.3531928062,3206568.7603573254,2901972.428856302,2901972.428856302,2901972.428856302,5267609.915404684,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,5055455.672904192,4021620.3531928062,4021620.3531928062,4021620.3531928062,4021620.3531928062,4021620.3531928062,5055455.672904192,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,4825295.621868596,8173348.469921189,4021620.3531928062,2901972.428856302,4021620.3531928062,15008107.923533786,4021620.3531928062,2901972.428856302,4099544.6727152066,2901972.428856302,15008107.923533786,4021620.3531928062,3206568.7603573254,16555707.143382354,50057688.70704078,4021620.3531928062,15008107.923533786,4021620.3531928062,4021620.3531928062,8173348.469921189,4021620.3531928062,2901972.428856302,2901972.428856302,5055455.672904192,7680929.1565385675,2901972.428856302,4825295.621868596,2901972.428856302,2901972.428856302,4021620.3531928062,4099544.6727152066,2901972.428856302,2901972.428856302,11396490.699475065,13229184.007668711,4021620.3531928062,2901972.428856302,2901972.428856302,4825295.621868596,4021620.3531928062,11396490.699475065,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,15219678.179971848,5055455.672904192,12399906.665605096,4099544.6727152066,4825295.621868596,2901972.428856302,3206568.7603573254,8173348.469921189,4072276.816666667,3206568.7603573254,15219678.179971848,20666322.75755287,2901972.428856302,5055455.672904192,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,5055455.672904192,20666322.75755287,4825295.621868596,2901972.428856302,2901972.428856302,2901972.428856302,5267609.915404684,2901972.428856302,2901972.428856302,4021620.3531928062,17149451.607209522,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,4099544.6727152066,2901972.428856302,5055455.672904192,4825295.621868596,2901972.428856302,50057688.70704078,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,2901972.428856302,2901972.428856302,2901972.428856302,5267609.915404684,3206568.7603573254,5055455.672904192,2901972.428856302,5055455.672904192,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4099544.6727152066,2901972.428856302,5055455.672904192,50057688.70704078,5055455.672904192,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,4825295.621868596,13229184.007668711,11396490.699475065,3206568.7603573254,4021620.3531928062,2901972.428856302,2901972.428856302,5267609.915404684,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,36469368.43790497,2901972.428856302,5055455.672904192,2901972.428856302,4021620.3531928062,4825295.621868596,2901972.428856302,5055455.672904192,2901972.428856302,5267609.915404684,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,5055455.672904192,2901972.428856302,4021620.3531928062,5267609.915404684,7680929.1565385675,4021620.3531928062,7680929.1565385675,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,49159569.34918919,5055455.672904192,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,20666322.75755287,15430128.032636883,4021620.3531928062,2901972.428856302,5055455.672904192,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,12399906.665605096,4021620.3531928062,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,10735794.827885952,4021620.3531928062,4825295.621868596,5055455.672904192,2901972.428856302,4825295.621868596,4021620.3531928062,5055455.672904192,4021620.3531928062,2901972.428856302,2901972.428856302,5267609.915404684,8173348.469921189,4825295.621868596,4099544.6727152066,3206568.7603573254,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,10735794.827885952,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,50057688.70704078,5267609.915404684,5267609.915404684,4021620.3531928062,5055455.672904192,2901972.428856302,5055455.672904192,2901972.428856302,4021620.3531928062,2901972.428856302,5055455.672904192,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,21551120.730290458,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,12399906.665605096,2901972.428856302,4021620.3531928062,5267609.915404684,64146355.22580645,5055455.672904192,11396490.699475065,4021620.3531928062,15430128.032636883,2901972.428856302,2901972.428856302,5267609.915404684,13229184.007668711,2901972.428856302,4825295.621868596,5267609.915404684,105378075.7926296,2901972.428856302,2901972.428856302,2901972.428856302,16555707.143382354,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4825295.621868596,2901972.428856302,4021620.3531928062,30233484.86783217,15430128.032636883,2901972.428856302,2901972.428856302,12399906.665605096,4021620.3531928062,5267609.915404684,4021620.3531928062,2901972.428856302,3206568.7603573254,2901972.428856302,4021620.3531928062,11396490.699475065,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,4099544.6727152066,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,20666322.75755287,4072276.816666667,15008107.923533786,13229184.007668711,2901972.428856302,2901972.428856302,5055455.672904192,4021620.3531928062,2901972.428856302,5055455.672904192,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,3206568.7603573254,4099544.6727152066,2901972.428856302,4021620.3531928062,17149451.607209522,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,2901972.428856302,2901972.428856302,8549568.320549242,5055455.672904192,2901972.428856302,5267609.915404684,2901972.428856302,10645668.033819629,13229184.007668711,2901972.428856302,2901972.428856302,5055455.672904192,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,4390302.626738889,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,11396490.699475065,4099544.6727152066,2901972.428856302,2901972.428856302,10735794.827885952,2901972.428856302,2901972.428856302,3206568.7603573254,30233484.86783217,5055455.672904192,2901972.428856302,2901972.428856302,4021620.3531928062,5055455.672904192,2901972.428856302,12399906.665605096,2901972.428856302,4099544.6727152066,3206568.7603573254,2901972.428856302,4099544.6727152066,5055455.672904192,2901972.428856302,4021620.3531928062,11268352.732317073,4021620.3531928062,2901972.428856302,17149451.607209522,4825295.621868596,185914465.11163896,2901972.428856302,4021620.3531928062,4021620.3531928062,58911584.90909091,2901972.428856302,2901972.428856302,5055455.672904192,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4099544.6727152066,4021620.3531928062,2901972.428856302,2901972.428856302,4825295.621868596,4021620.3531928062,8173348.469921189,5055455.672904192,10735794.827885952,2901972.428856302,4021620.3531928062,5055455.672904192,2901972.428856302,2901972.428856302,4021620.3531928062,4021620.3531928062,4021620.3531928062,5267609.915404684,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,2901972.428856302,4021620.3531928062,11396490.699475065,4021620.3531928062,4021620.3531928062,4825295.621868596,5267609.915404684,4021620.3531928062,4021620.3531928062,4390302.626738889,2901972.428856302,2901972.428856302,13229184.007668711,4021620.3531928062,4825295.621868596,4021620.3531928062,4021620.3531928062,2901972.428856302,5055455.672904192,4021620.3531928062,4099544.6727152066,11396490.699475065,10645668.033819629,4021620.3531928062,4021620.3531928062,4021620.3531928062,4072276.816666667,4825295.621868596,50057688.70704078,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,4021620.3531928062,4099544.6727152066,4021620.3531928062,4021620.3531928062,3206568.7603573254,13229184.007668711,2901972.428856302,2901972.428856302,20666322.75755287,4021620.3531928062,4825295.621868596,2901972.428856302,4099544.6727152066,2901972.428856302,3206568.7603573254,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,4021620.3531928062,5055455.672904192,2901972.428856302,2901972.428856302,4021620.3531928062,5055455.672904192,12082258.207482994,5267609.915404684,4021620.3531928062,4021620.3531928062,2901972.428856302,4021620.3531928062,13229184.007668711,2901972.428856302,2901972.428856302,3206568.7603573254,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,4021620.3531928062,58089158.3878327,2901972.428856302,4021620.3531928062,5267609.915404684,5055455.672904192,2901972.428856302,2901972.428856302,12399906.665605096,2901972.428856302,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,5055455.672904192,12082258.207482994,4021620.3531928062,12399906.665605096,30233484.86783217,2901972.428856302,2901972.428856302,4021620.3531928062,2901972.428856302,33490100.492135763,4021620.3531928062,4099544.6727152066,2901972.428856302,4021620.3531928062,2901972.428856302,2901972.428856302,5267609.915404684,4825295.621868596,4021620.3531928062,4021620.3531928062,2901972.428856302,2901972.428856302,2901972.428856302,5055455.672904192,4021620.3531928062,2901972.428856302,5267609.915404684,4021620.3531928062,2901972.428856302],"xaxis":"x","y":[4395510,1499128,805751,58761304,3639670.25,1327591,476069,17714332,555153.125,143423024,415453,1160989,1696978.75,1334876,4585966,11962615,13730204,2356376.75,18470340,1396691.875,2056762,659176.1875,619409,791035.5,9683979,1241355.375,29540052,1237655.25,4018570,6118300.5,1568550.75,526752.8125,7575080,2058584,2711035,965786,5135040,669217.6875,1692323,803938,3497215.5,63668488,809291.8125,21069722,291614432,10105379,32141070,1523508,94560088,3073937.5,1024866,1516831.75,553230,1394408.25,1138386,3546459,4107827,2014500.375,20675922,1115107,1396280.25,151754.2969,1127895,2615247.75,2735128,489557.0938,2311532,5227992,848497,22965152,666967,673030,3242458,433443,614044.6875,4216639.5,12549118,733249,770736.125,1848074,1051523.25,853314.5,1017787.125,501194,2298815.75,2255026,2202407,1045337,1436109,1362523,19526080,13146645,635516.375,1532390.25,6227419,274682208,1751483.875,30135280,2265487,3218824,1725558,816300.5,666392,2292169,2663520,931148.625,7520554,7613116,1232712,4970047,942962,1166069.875,9387211,5826745,1423463,2660396,4103027.5,784584,8233527.5,4034028,31803568,345399,2360656,711118.375,549438,901679,1939297,1094343,2455923.5,1992158.25,817695.6875,2986728,1008536.813,1078936,654446,2591768,555709,5401277.5,2099386.75,1983256,862945.125,1417728.25,14172606,643740,163945984,1051944,1569569,8747819,534703.6875,5032796,518107,556700.6875,1261963,114648520,9313513,7251589,2612378,2923484,754241.3125,162960,836437.6875,2547848,1563070,677476.6875,7860942,10040121,2148565,730672,3009967,9986032,505601,308400.4063,14064603,13631685,4748137,10434387,604183.1875,2139713,920068.875,1128060.75,11624548,596242.1875,3814590,2844685,1278437,1119679,21346100,2136086,4991893.5,590629,659368.625,1437569,450518,1047461,858701,9788717,1769137,2870914.75,1025659,10444297,4916451.5,641713,867184.375,688641.8125,746858.1875,15324777,1829981.375,743748.5,1260902,1307883,966812.125,3540225.75,1063960.125,922153.6875,794715.6875,1200811,4581493,6426022,4178233.5,8437401,1063480.5,739869.6875,824320.1875,8218155.5,913117.6875,764832.6875,2402535,6843855,37022180,706505.6875,1858224.625,2583451,2004777,1237252.75,10775646,16467486,2001643.75,576052,701422,6797528,270838.5,3265995,31371618,727374.6875,811900,784584,0,2545405,1631031,1536998.25,4998015,132105,1524499,1279570,6268627,1060818.25,991918,3158151,1441445.625,648022,1702639,3686962.25,2960159.75,255971.2031,488181,1219744.75,3321035.75,3043767,965651.125,282898,1645748.25,768479.625,508051,331059.5,633408,2454861,10834087,3752132.25,1108604,907617,7456330,1076693,966764,576644.125,975721,1987934.625,6962073,455798.9063,919180,3242066,6657348.5,1128179.125,2625396,10564729,1832031.5,452614,1365778,11508035,1587454.75,2009703.25,6860521,1544146.625,887022.875,3220635.25,7771933.5,2584082,536642.875,14619984,30038490,848734,2018011.875,1612167.25,3468606,1287422,785901,1287040.625,2225512,722062.875,645028,5975230,1878442.25,3655453,2322314,3073778,1425226,25631512,2946756.25,2483074.25,5662234,4934334,1123416,1826203,1187499.75,3518459.5,5917000,904059.875,4917420,601590,2204020,2559403,1140137,13609040,19991412,552015.625,1544117,2839882.25,1339220.625,2466100,4636396.5,1730431,1700057.375,1072487.875,1090545,6858533.5,2306723.5,918514,1157976.5,2156407,858932,668915.8125,1904641.75,1769998,0,530700.125,2804534,1678495.25,1003551,5572010,634561,13901935,1360847,650313,2195160,1335007.875,1107290.625,1031667.5,1234149,2114860.25,2805826,591845.3125,2251442,1898509.75,6299357,1076389,3091387.5,9230223,1544388.5,841841,1381590.125,662424,5053635,2401148,6182350.5,22078064,790671,4608963,662685.3125,1822830,3106930,1733511,362823,830286.5,495481,601818.625,21027396,952879,2337651.25,8037287,1809586.75,1657201,1167813,720170.6875,55030192,1862176.875,1509292,2323265,1072855,25970248,9128267,9029489,5976246,1934971,3316562,948676.5,755269,598857.8125,8190809,1026921,885958.125,1878742.875,3212269,811907.625,564754.1875,336461,1186835.125,6411570.5,1960538,847298.8125,2456132,2828950.5,6298131.5,12448381,4677797,753721.125,13458948,5722325.5,12931002,1482752,645055.5,2903161,6722886,2373568.25,3375211,2822377.25,7768057.5,1831722.875,640374,1485945.875,1224045.25,5216213,730292,455153,37797084,9375125,624180,1860084,1015060.188,16016644,670779,1427294.375,776248.6875,39403320,2664566.5,7182436,41399504,786253.6875,1208168.75,974179,1325597.625,26941110,463371,938833.625,5155090.5,1811506.375,876339,1083467.125,5804784.5,1044642,3652708.75,588856.125,15088676,552822.125,648879.3125,967432.8125,892227,507451,405050.4063,1750419,405062,593042,1116040,1827424.125,222600.0938,568033.375,622051,2003447.625,2654755,2233268.5,878927.8125,560334.5,919746.3125,1247972,587903,1732954.25,880115.5,7592451,1527348,742565,4133811,2442220,131373880,1190733.5,2133797.75,1581036.75,1041652.813,1912690,2534671,2483320,2698077,1191926,1239315.375,6782435,403800,604591,3060149,1826107,1035554.688,4784731,6901554,1712796,608199.8125,5820965,550767,1332912.625,3245772,304248,1003176.875,6057734,726124.625,7802205.5,2108926,2827818,2014245,8354235,888561,527438,335477,1288669.5,1456808,3835202,1775433,2843033.5,4285049,8422861,1651120,8188381,5671256,1191803,531563,8688676,2964206,1580597,2133372,6917478.5,5447076.5,862826,3999505.25,3721129,593824,929688,13214416,602245,4865551.5,632157,14431245,4729846.5,11208498,1920352.125,3053785,2147062,3322250,1702992,2759674,791052.875,2683383,737570,695059,6471427,951675.5,16808.90039,4222682,1324534.625,1167887.75,1391714.25,12204922,6577130,751292,1761048,552391,3039626,669117,95544,4338616,1012559,9997511,57133.19922,4247457,26416770,842511,8819864,712149,1960558,3298570,1670929,4983934,4217623,2996502,493000,4801472,2235765.5,635698.6875,3467698,1310481,470666.6875,1213869.75,1093627,1372706,61762380,842923,1820292,961491.3125,1467029.75,3042875.5,718362,1453543,1222509,2594119,7791957,15571642,896172,2763597,4039667,4475652,776237.375,811021,6352846.5,6387710,12271749,783651.5,626612,1255834,2527198,3894142.5,1972156,1691963,699382,6083307,619870,586827.5,1452804,2058955.25,13962749,24126,10591549,3240830,2953338,1260416,704799,1617421,1224642.5,3822102.5,3863772,1162226.375,921786.375,2100766,2019254,5134446,1536287,782494.3125,713326.875,2892779,317794.4063,456397,3182301,919048.6875,1976663,3065502,593953.375,767297.8125,764906,642825,3397709,1084553.75,736838,1403820,1084048,6983736,1171380,1985171,851439,1677877,1107870,990361.5,847514.5,80469216,1321065,2411402.25,17750994,5436057,4757299.5,509171.4063,3258819.5,7015692,1271021,3435687,1987218.75,1034941.313,340270,559234.6875,2158629,966677.125,3637973,689373.8125,858780.625,1705515.25,512166,97690.39844,675772.875,815800,2184336.75,2751694,1987073,1169948.875,938908,1280245,1375978.875,2845946,1194835.5,606111,753784,892634,3113412,1762174,2921368,18868768,5348309,6376820,2058532.875,7782092,1212551,1224276,1958352,544172.375,8393893,767361,6522551.5,10939331,39523852,4519115,10020111,2601945,620104,8800292,968698.3125,1756668,2065156,2773779,9343428,219483,3303553,6917280,1091162.75,1306087.75,2849048,1429855.75,410433.0938,8873485,9049598,696522,3046543,1433504.375,1503823,551219,3789270,958725,960987.8125,1438479,1988784,43943,738983,1516377,1181963,806398.1875,2326565,49102164,1983509,9273278,6931901,1784796.5,1963054,2125323.5,3833052,4103800.75,318962.8125,7286313.5,25747022,1324965,5086863,1422406.25,1758581,1486177,1162493.625,1192100,970118,2646523.5,12263687,4767227,571644,559805,12250422,13410041,2726369,447947,1988760.875,13300859,1348497.125,661380,1625308.625,4236396.5,1906597,5450441.5,106918,2270531,41680640,771512.6875,35685224,0,1852926.625,851053.8125,1739286.75,1748629.875,863703,882896.5,2837589,571309,2237482,4765753.5,10513817,1234095.5,1372475,552729,3886929,809257,728167,6583940,2203758.5,1165598.875,59124384,1727783,1048813.75,838571,948157,1966424,3786217,5328224,3869754,1637199,1054581.375,1108310.875,2063212,1608224,1907697,1126157.25,502667.6875,662280,2439061,1019487.188,1558348,1162120.375,5762597,611997,4467292,1530744,2082754.75,16020642,514471,3751417,207572,2245568.25,4520838,855665.125,1688612.5,2473302,6136327.5,1423438.5,288191,2585187,2145603.5,10403123,2053611,1337968,839115,4369513.5,4498367,2291581,22530168,694479,508014.5938,1620721,607373,520011.8125,3234787.75,452744,930611.875,723424,85357952,2733003.75,3625434,2228675.5,524053,717056.375,10249433,18829136,3040924.25,3030041,2057457,1115504.375,1419453,602618,1539478.5,635697.3125,323739.4063,569115,1257144.75,630040,8767617,544043,619545.125,917783.5,755856.875,677536.3125,385719,558893.375,11521702,1382309,1823782,1602030,2147013,32579658,574650.375,6668568.5,1198291,3986851,2470566,4207110.5,6576563,962577,4734631,1049748.625,1623657,824516,647792.875,1225404,6481807.5,3064039.5,510517,1698074.625,1135172.25,1813718,74130576,2330029,6366084.5,342726.0938,2938366,1045413,6832991,13133225,1183042.125,1286882,4025787,557193,1475475.5,3147042,956958.875,916864,12720069,942712.5,2246747.5,2150120,14084606,2214469.5,1913558.375,1103762,684941.375,1216543,3542708.5,780410,4742869,1117658.75,806615,1149140.25,7539963.5,3202213,1366000.125,6807227,19827916,13253979,5149928.5,1706301,7811542.5,945245.625,209698.7031,1926134.875,17320768,1182575,10557910,10343027,157606480,1646088.5,116486.6016,2261586,13969552,1652833.25,1121259,7492939,928584.875,1856230.625,10018321,13010578,540465,13239050,11924933,3494929.5,2014475.875,19036416,1161814.625,5326222,1430356.75,607579.6875,5968153.5,3528086.75,1093757.75,8631444,770082.125,2520919,729096.875,1294615.25,4107443,485971,1210798.375,1858029,786979,955223,1118490,747326,1943899.25,867456.875,628609,2447861,1101536,12719042,3366528,4545148,9941308,998633.5,1210229,1833381,2057836,1534068.25,1813285.25,3303989,1380283,209644.9063,1210262,954352.6875,1594733.625,851585.1875,2266570,3591653,696709,1111858,48729588,969682.8125,4270584,1094940.75,984178,533742.8125,6014078,2331952.75,44293.5,11195917,2910252,29195468,3104171.5,2485521,14465798,3590731,1921890,1509775,4785692,188745.7031,7240274,1014405.375,3095882,1293390,5499097,226375,974940.375,1889880,4224872.5,4028943,3574542.25,4602793.5,8518313,3312125,5948007,3377209.75,2581741.75,22325364,3241917,1658751,577033.875,1649327.625,1588933,729397.1875,6505995,526059,1738376,2434208,1545485,16402563,2023258,537141,2503690,13567774,481206,1139980.625,8320719,1777005,873923712,666930,541950.875,863728.8125,61576184,5462958,2874181,3897738,2093011,666687.5,845961.375,3548858.25,3395651,20725996,1022114.5,2217620,360045,2184156.5,1355558,5525072.5,4089677.5,20560622,758875,621751,2188452,824483,1042652.625,698326.625,533226.8125,1045640,2217046,7801590,525668,789630.125,841821,866343,1481017,2798967,1359579,850615.8125,1860901,3091348.25,1207092.75,2094410,6125692,4039168,1130960,803533.3125,5121586,1049088,735178,8168547,524144.6875,2001936.5,383524,2772124.75,10642754,2759071.5,1353079.5,3245111.5,7380088,6836900,1807738.25,1296959,1604129.75,3605749.75,1551294,25210246,2159932,862744,1862752,3888945,1443367.75,1676426.5,5444253.5,4099807,2233262,1358022,760826,678663.3125,0,6609358,1028277.313,2307938,61827040,0,4321141.5,572139,1997181.5,1594422.5,2755578,523133,650205.8125,519317,108200.3984,844076.8125,1350190,465533.3125,2189030,464155.1875,637246,2131817,11026945,5424718,2401890,501741,1008665,730765,7564643,1784385.25,1383950,227566.7031,702385,1641667,816846,448676,1083729.625,801645.875,13315803,11702959,609921.6875,3195230,2506927.5,2249592.5,2619088.5,3780078,1387773,1072500,1075019.375,1908612,2992693,5134607.5,11458215,1897180,6579348,15590743,1261634.5,1392015,541542.6875,1962435,20781390,1795462.75,0,819378,400354.3125,5047936,1961581.5,2422767,1913034.125,622572,561368,1787119,3494383,1244824,2549649.75,866096.6875,1539176.625,2267094,1444429,1666221],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predAB"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB, \\\n","BestParametresAB, \\\n","ScoresAB, \\\n","SiteEnergyUse_predAB, \\\n","figAB = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train.ravel(),\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predAB',\n","                         score=score,\n","                         param_grid=param_gridAB)\n","\n","print(BestParametresAB)\n","print(ScoresAB)\n","figAB.show()\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[11713666.846553776,12529086.692775715,12572861.792232055,12951399.323779168,12604664.577561732,13222372.686492026,12128992.930020608,10049023.062433288,12219109.26658617,10677677.176004676,11704077.566948194,11779667.870503647,11247446.484364297,10784950.427070716,10299833.691795658,9930725.369089112,11010085.837187074,10966962.111842081,10150623.552612077,10170122.57462084,10167413.916472688,9789209.721316984,10876207.562824074,11379623.563296938,10879477.934713557,11490004.190474331,11240620.5863405,11491351.381676694,15266166.793064496,12013639.273584224]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[14232497.871177077,14807075.670035481,16483939.728499943,16082281.531136665,16816274.513629586,17353327.79196062,14572113.711490592,12985990.038111595,15085602.774934825,13656474.645246964,14900646.132339114,16637652.8721267,13973997.88699031,13243344.206072122,12915455.589759734,13104381.332223577,13242595.515057735,13489788.743074432,12349222.355430529,13534491.627314344,12607624.410495201,12860056.695213482,14328698.939146366,15872901.661511887,14853996.408401838,15919867.207707994,13777494.882119363,15856598.67173971,18728548.468721766,15639389.326976972]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[9194835.821930476,10251097.715515949,8661783.855964167,9820517.11642167,8393054.64149388,9091417.58102343,9685872.148550624,7112056.08675498,9352615.758237515,7698879.706762387,8507509.001557274,6921682.868880594,8520895.081738284,8326556.648069311,7684211.793831584,6757069.405954646,8777576.159316413,8444135.48060973,7952024.749793626,6805753.521927334,7727203.4224501755,6718362.747420486,7423716.186501782,6886345.46508199,6904959.461025277,7060141.17324067,8703746.290561637,7126104.091613678,11803785.117407225,8387889.220191477]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[8431441.900831148,9636340.689208148,7669153.804264752,10020357.880544588,8143715.905619619,8955556.626338959,9497756.856429603,7373110.449018471,8424812.97126683,6728982.518033754,6816445.940636245,5763406.436636502,8736102.035104705,8232253.578593048,6297712.870163945,6661922.56553989,9303606.508571824,7039567.3667625915,8775227.801107166,6211546.65926932,7924061.185577359,6132927.503462826,7491929.422778928,8051523.767495265,7062535.088453174,8687317.01289184,8322622.754136874,7727463.001628157,19423780.013007548,10479531.571418703],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[15986896.432257557,16526124.216028666,16703234.96313126,16788736.94110155,16057940.77406795,15418340.017195912,16703596.781908916,15531645.266332734,16333609.246041149,15910372.937800454,16511928.969476925,16275768.054674568,16549954.160560735,15034048.164828898,14361954.166083518,15866099.757099507,15175027.087449575,14913659.66158841,14290458.398862215,16235514.459328748,14421323.196240831,15185995.39566297,17432091.944211736,20206236.893795412,17939357.067510255,20187520.095757272,15831748.230111115,19834330.448463153,19246994.500798456,18827907.632203646],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[10795898.551516308,12635873.790936641,14898812.705285853,16751153.012414653,9679068.739938786,8561374.371916955,12179636.683481863,9444870.302522875,11522392.008863097,9504089.971078394,10801091.786188832,7979559.214064952,10199060.89700038,11538048.671415433,9362109.150598751,8635725.139687058,8882344.445155269,10741519.98808403,8315294.494252261,8465294.528287774,7647107.709080179,9398891.122444347,9802070.085278843,8672145.859937489,10123788.435866032,9693494.162272424,9755401.82653637,10667894.22650108,10903166.055690886,9481274.19475616],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[10716820.435047073,11268901.943365548,8004393.242956565,10901996.11507981,19075113.834008984,19580968.175805755,11171901.468700377,7699026.968377423,14552340.940955363,10691299.124251254,13376048.793963546,18501594.985624146,10630247.42299683,8540341.28427127,10277682.068040842,8288621.130045394,11044373.419952903,11682546.735622007,8867783.141364787,9211382.67671785,10136898.74610637,7773788.558478718,8875615.513422318,9377058.210252404,7300391.576377112,8217930.891079007,11572083.399636108,8239800.512692961,13946314.492706101,8802009.950707607],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[12637276.913116787,12578192.824339569,15588714.245521845,10294752.669755237,10067483.634173317,13595624.241202546,11092072.859582277,10196462.32591493,10262391.165804412,10553641.328859521,11014872.344475426,10378010.661518075,10121867.906158838,10580060.436244937,11199710.204091245,10201258.253073711,10645077.724805795,10457516.807153363,10504353.92747396,10726874.549500512,10707678.745358706,10454446.026536064,10779330.848428536,10591153.085004121,11971317.505361218,10663758.790371116,10721246.72128203,10987268.719098132,12810578.903119486,12477473.018835008],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=square<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB,\n","                             'n estimators', GridAB, BestParametresAB,\n","                             'adaboostregressor__loss')\n","FigRMSEGRidAB.show()\n","if write_data is True:\n","    FigRMSEGRidAB.write_image('./Figures/ConsoGraphRMSEAB.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## 1.2 Consommation énergétique au log"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["SiteEnergyUse_train_log = np.log2(1 + SiteEnergyUse_train)\n","SiteEnergyUse_test_log = np.log2(1 + SiteEnergyUse_test)\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.8452540008574219\n","rmse : 11371966.097141473\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logLR=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.20562744140625,20.54962158203125,20.52044677734375,28.17620849609375,21.02337646484375,20.48345947265625,20.47808837890625,20.9639892578125,20.51025390625,24.61895751953125,20.4874267578125,20.4910888671875,20.56280517578125,20.524169921875,20.50677490234375,23.4381103515625,22.9371337890625,20.80218505859375,24.10150146484375,20.5848388671875,20.87225341796875,20.5701904296875,20.47296142578125,20.49774169921875,21.90106201171875,20.501220703125,20.65960693359375,20.6082763671875,21.004638671875,20.572998046875,20.85479736328125,20.46441650390625,21.179443359375,20.75079345703125,20.96929931640625,20.50421142578125,21.2935791015625,20.63970947265625,20.84405517578125,20.62139892578125,20.82855224609375,22.0849609375,20.46240234375,20.978271484375,32.25421142578125,20.968505859375,25.59918212890625,20.6585693359375,34.90924072265625,21.075927734375,20.59564208984375,20.717041015625,20.47998046875,20.58001708984375,20.83062744140625,20.6468505859375,20.7835693359375,20.817138671875,22.1473388671875,20.58636474609375,20.605224609375,20.77825927734375,20.47412109375,20.58624267578125,21.04412841796875,20.5377197265625,21.036376953125,20.509033203125,20.540283203125,21.38214111328125,20.52239990234375,20.5023193359375,20.81597900390625,20.503662109375,20.561279296875,20.886962890625,22.5457763671875,20.540283203125,20.5404052734375,20.83392333984375,20.4937744140625,20.63983154296875,20.64508056640625,20.4266357421875,20.74517822265625,21.02032470703125,20.6005859375,20.64208984375,20.8253173828125,20.50048828125,21.8341064453125,21.24761962890625,20.55487060546875,20.75885009765625,21.4976806640625,23.331787109375,20.50323486328125,26.08526611328125,20.78363037109375,20.9007568359375,20.67694091796875,20.48004150390625,21.4010009765625,20.7962646484375,20.71368408203125,20.50341796875,22.43597412109375,20.6719970703125,20.73565673828125,20.5328369140625,20.65167236328125,20.58184814453125,22.86993408203125,20.741455078125,20.71234130859375,20.8670654296875,21.81768798828125,20.5313720703125,22.46820068359375,20.5762939453125,20.6832275390625,20.41265869140625,20.44378662109375,20.446533203125,20.47540283203125,20.51715087890625,20.46783447265625,20.4990234375,21.01214599609375,20.45635986328125,20.53717041015625,21.2579345703125,20.64202880859375,20.58074951171875,20.5826416015625,21.22314453125,20.476806640625,21.1676025390625,20.50067138671875,20.91021728515625,20.61285400390625,20.4959716796875,21.85797119140625,20.51800537109375,24.86602783203125,20.6439208984375,20.65826416015625,22.2840576171875,20.46844482421875,21.545166015625,20.57586669921875,20.48431396484375,20.42041015625,33.55572509765625,20.45849609375,20.7379150390625,21.20648193359375,21.2135009765625,20.478759765625,20.473876953125,20.49365234375,20.52020263671875,20.5107421875,20.5732421875,21.6502685546875,21.12445068359375,20.77655029296875,20.53155517578125,20.9866943359375,22.35614013671875,20.478759765625,20.5909423828125,20.714599609375,22.63812255859375,21.200927734375,20.71014404296875,20.4990234375,20.465087890625,20.66436767578125,20.544189453125,21.4725341796875,20.5169677734375,20.445556640625,20.6324462890625,20.6243896484375,20.46893310546875,24.7203369140625,20.735595703125,21.32470703125,20.5728759765625,20.50152587890625,20.53338623046875,20.46661376953125,20.635986328125,20.47119140625,21.0716552734375,20.5533447265625,20.73272705078125,20.51959228515625,21.91876220703125,21.02081298828125,20.5567626953125,20.523193359375,20.53546142578125,20.52642822265625,20.98919677734375,20.56195068359375,20.5001220703125,20.6015625,20.5374755859375,20.6673583984375,20.6707763671875,20.45196533203125,20.626708984375,20.57489013671875,20.58843994140625,22.53070068359375,21.14013671875,21.00128173828125,23.0579833984375,20.5670166015625,20.5396728515625,20.536865234375,20.97613525390625,20.6497802734375,20.60369873046875,20.62396240234375,21.3250732421875,23.3359375,20.567626953125,20.53643798828125,20.62579345703125,20.506591796875,20.6717529296875,22.007568359375,22.61602783203125,20.7310791015625,20.47491455078125,20.5302734375,21.35589599609375,20.5133056640625,20.9884033203125,21.3017578125,20.538330078125,20.64227294921875,20.52154541015625,20.84967041015625,20.4532470703125,20.4381103515625,20.8707275390625,21.23065185546875,20.43255615234375,20.8431396484375,20.48626708984375,21.6158447265625,20.57159423828125,20.576416015625,20.477294921875,20.62847900390625,20.50439453125,20.6480712890625,20.63189697265625,21.228271484375,20.5718994140625,20.465087890625,20.4547119140625,20.8145751953125,20.6378173828125,20.45086669921875,20.44073486328125,20.84588623046875,20.5704345703125,20.47564697265625,20.49005126953125,20.52130126953125,20.6055908203125,20.7271728515625,21.2171630859375,20.67840576171875,20.60186767578125,21.08575439453125,20.67401123046875,20.601318359375,20.514404296875,20.496826171875,21.1243896484375,21.26983642578125,20.52142333984375,20.61065673828125,20.61016845703125,22.2203369140625,20.53131103515625,20.85235595703125,23.18621826171875,20.43499755859375,20.50238037109375,20.69769287109375,20.5927734375,20.55084228515625,20.75433349609375,22.09033203125,20.81671142578125,20.696533203125,20.64105224609375,20.99530029296875,21.1353759765625,20.48516845703125,22.2554931640625,20.64117431640625,20.55072021484375,20.57415771484375,20.69830322265625,20.4967041015625,20.64678955078125,20.549560546875,20.5933837890625,21.2369384765625,20.5848388671875,20.51165771484375,20.53717041015625,20.963623046875,20.80615234375,20.88507080078125,21.16314697265625,20.4857177734375,21.313232421875,20.5240478515625,21.00360107421875,21.07769775390625,21.819091796875,20.48443603515625,20.4571533203125,20.65191650390625,20.77960205078125,21.03564453125,20.57696533203125,20.6922607421875,20.49993896484375,20.48638916015625,20.69207763671875,20.47943115234375,22.0234375,21.92333984375,20.49468994140625,20.763671875,20.58526611328125,20.6343994140625,20.5462646484375,21.3580322265625,21.074951171875,20.54791259765625,20.6888427734375,20.5189208984375,20.84381103515625,21.0458984375,20.49603271484375,21.36712646484375,20.47705078125,20.45550537109375,20.55126953125,20.681884765625,20.54229736328125,20.6539306640625,20.50372314453125,20.63287353515625,20.72576904296875,20.5186767578125,20.97833251953125,20.48760986328125,21.7418212890625,20.46514892578125,20.59039306640625,20.60064697265625,20.4862060546875,20.5982666015625,20.51953125,20.5460205078125,20.618896484375,20.5760498046875,20.5465087890625,20.984375,20.786865234375,21.78179931640625,20.41448974609375,20.86279296875,20.75286865234375,20.736572265625,20.46929931640625,20.96875,20.50299072265625,21.25201416015625,20.71051025390625,20.93878173828125,21.0025634765625,20.46099853515625,21.2884521484375,20.56829833984375,20.6824951171875,20.706298828125,20.79644775390625,20.5179443359375,20.6419677734375,20.4549560546875,20.501708984375,22.40185546875,20.5526123046875,20.9072265625,21.52880859375,20.69903564453125,20.5682373046875,20.51348876953125,20.53448486328125,23.99261474609375,20.61700439453125,20.53387451171875,20.42962646484375,20.6356201171875,25.4444580078125,21.0545654296875,20.775390625,20.45733642578125,20.7530517578125,20.4554443359375,20.5438232421875,20.58721923828125,20.4901123046875,21.7236328125,20.651123046875,20.6016845703125,20.4825439453125,20.83697509765625,20.51434326171875,20.50628662109375,20.4290771484375,20.631591796875,22.8453369140625,20.52313232421875,20.6171875,20.76300048828125,20.746337890625,21.15966796875,22.740234375,21.112548828125,20.5263671875,22.755615234375,21.132080078125,22.14080810546875,20.43426513671875,20.53057861328125,20.79443359375,21.3731689453125,20.62078857421875,20.69256591796875,21.030517578125,20.75030517578125,20.74310302734375,20.52899169921875,20.6219482421875,20.6429443359375,21.5750732421875,20.509521484375,20.54620361328125,25.4578857421875,21.16766357421875,21.07635498046875,20.5833740234375,21.005615234375,21.5491943359375,20.5020751953125,20.66058349609375,20.483642578125,23.334228515625,20.64892578125,22.5347900390625,21.408935546875,20.57086181640625,20.6322021484375,20.6412353515625,20.71484375,23.23663330078125,20.45306396484375,20.636962890625,20.9395751953125,20.62249755859375,20.50115966796875,20.52813720703125,20.68768310546875,20.61376953125,20.59521484375,20.4720458984375,22.7757568359375,20.54119873046875,20.58636474609375,20.7666015625,20.6307373046875,20.49322509765625,20.56524658203125,20.49737548828125,20.58892822265625,20.53228759765625,20.62213134765625,20.69866943359375,20.44097900390625,20.56103515625,20.517578125,20.61712646484375,20.83343505859375,20.513671875,20.58477783203125,20.5789794921875,20.5250244140625,20.698974609375,20.57147216796875,20.52166748046875,20.6051025390625,21.10357666015625,20.79766845703125,20.431640625,20.63409423828125,20.47052001953125,23.97039794921875,20.6785888671875,20.447998046875,20.45794677734375,20.899169921875,21.1048583984375,20.53887939453125,20.602294921875,20.488525390625,20.41412353515625,20.62646484375,20.644775390625,20.42852783203125,20.4228515625,21.23223876953125,20.510986328125,20.53076171875,21.68408203125,20.47119140625,20.45855712890625,20.59326171875,21.53515625,20.42340087890625,20.661376953125,20.91949462890625,20.490966796875,20.5517578125,20.98675537109375,20.658447265625,20.85601806640625,20.578125,20.7069091796875,20.46734619140625,21.19140625,20.561767578125,20.520751953125,20.573486328125,20.55291748046875,20.697998046875,20.62811279296875,20.8060302734375,20.80999755859375,20.98162841796875,21.54388427734375,21.120361328125,22.45550537109375,21.5546875,20.4439697265625,20.494384765625,20.7281494140625,20.91693115234375,20.5960693359375,20.93756103515625,21.62615966796875,20.56573486328125,20.58837890625,20.496337890625,20.773193359375,20.473388671875,20.69378662109375,21.747314453125,21.4205322265625,21.10174560546875,20.51641845703125,23.88128662109375,20.60479736328125,22.59393310546875,20.93878173828125,20.69891357421875,20.70703125,20.564453125,20.80914306640625,20.687255859375,20.535888671875,20.4749755859375,20.58074951171875,20.548583984375,20.70831298828125,20.602783203125,20.52703857421875,22.08868408203125,20.5218505859375,20.42315673828125,20.545654296875,20.491943359375,20.73406982421875,20.517333984375,20.45916748046875,20.48138427734375,20.57659912109375,20.50592041015625,20.53863525390625,21.4908447265625,20.5899658203125,20.5537109375,20.61749267578125,20.5887451171875,26.364990234375,20.53082275390625,21.65008544921875,20.50665283203125,20.46075439453125,20.79693603515625,20.70672607421875,21.09942626953125,21.5103759765625,20.8980712890625,20.511962890625,20.497802734375,20.576171875,20.51190185546875,21.20184326171875,20.46783447265625,20.47882080078125,20.47125244140625,20.60888671875,20.602783203125,21.5352783203125,20.4788818359375,20.51654052734375,20.61260986328125,20.4755859375,20.62640380859375,20.50299072265625,20.474609375,20.59564208984375,20.889404296875,21.9873046875,22.32177734375,20.611572265625,20.667724609375,21.39044189453125,20.78936767578125,20.59539794921875,20.59332275390625,22.411376953125,21.3836669921875,24.58502197265625,20.603515625,20.4241943359375,20.5791015625,20.6602783203125,20.92388916015625,20.47723388671875,20.67510986328125,20.5953369140625,22.2047119140625,20.5789794921875,20.48760986328125,20.44232177734375,20.65869140625,20.733642578125,20.58709716796875,20.50750732421875,21.1507568359375,20.64898681640625,20.636962890625,20.51531982421875,20.7244873046875,20.68310546875,21.24951171875,20.59234619140625,20.589599609375,20.552001953125,20.54351806640625,20.88946533203125,20.6556396484375,20.6087646484375,20.57318115234375,20.6153564453125,20.593017578125,20.44720458984375,20.4996337890625,21.443115234375,20.5068359375,20.42364501953125,21.480224609375,20.496337890625,20.51898193359375,20.45068359375,20.5465087890625,21.14508056640625,20.48443603515625,20.588623046875,20.85186767578125,20.66058349609375,20.695556640625,20.576171875,20.9891357421875,20.529541015625,20.46630859375,20.60223388671875,20.60906982421875,20.59197998046875,28.037353515625,20.51934814453125,20.90716552734375,23.07025146484375,21.2396240234375,21.34149169921875,20.5389404296875,21.10821533203125,21.00250244140625,20.61358642578125,20.88922119140625,20.87371826171875,20.63226318359375,20.76263427734375,20.5101318359375,20.72607421875,20.48419189453125,21.0487060546875,20.54852294921875,20.52362060546875,20.45562744140625,20.514404296875,20.55767822265625,20.4344482421875,20.57855224609375,20.728515625,21.27020263671875,20.84283447265625,20.60748291015625,20.5059814453125,20.6585693359375,20.7021484375,21.08575439453125,20.69561767578125,20.7071533203125,20.55694580078125,20.6514892578125,20.49310302734375,20.44482421875,20.7950439453125,21.52532958984375,20.8372802734375,20.553955078125,20.6295166015625,22.15545654296875,20.704833984375,20.469482421875,20.86663818359375,20.60943603515625,22.98297119140625,20.55548095703125,20.72271728515625,22.39544677734375,26.6976318359375,20.70880126953125,21.9488525390625,20.6451416015625,20.58099365234375,21.54833984375,20.6920166015625,20.50006103515625,20.51788330078125,20.89898681640625,21.45343017578125,20.4442138671875,20.83038330078125,20.585205078125,20.64874267578125,20.6434326171875,21.09552001953125,20.44549560546875,20.47479248046875,21.6025390625,21.89044189453125,20.54296875,20.71795654296875,20.4654541015625,20.7921142578125,20.49530029296875,21.5069580078125,20.59234619140625,20.57373046875,20.75048828125,20.63653564453125,20.61968994140625,20.5628662109375,20.61956787109375,20.7744140625,20.53192138671875,20.72613525390625,21.980224609375,20.85626220703125,21.7880859375,21.18310546875,20.857666015625,20.52374267578125,20.75982666015625,21.4461669921875,20.7393798828125,20.79058837890625,22.21722412109375,22.59832763671875,20.5286865234375,21.1480712890625,20.58734130859375,20.7099609375,20.700439453125,20.60418701171875,20.6729736328125,20.55523681640625,20.91033935546875,22.75927734375,20.9246826171875,20.441162109375,20.52154541015625,20.60479736328125,21.377685546875,20.67291259765625,20.4656982421875,20.54559326171875,22.33428955078125,20.7265625,20.49737548828125,20.47454833984375,20.49041748046875,20.82843017578125,20.98309326171875,20.45965576171875,20.99957275390625,21.02874755859375,20.4697265625,26.417724609375,20.5704345703125,20.56134033203125,20.3779296875,21.0880126953125,20.57794189453125,20.45623779296875,20.61444091796875,21.3193359375,20.71917724609375,21.02166748046875,20.767333984375,20.8826904296875,20.62298583984375,20.472900390625,20.70806884765625,20.67083740234375,20.50067138671875,20.51287841796875,21.078857421875,20.6077880859375,20.97021484375,25.94549560546875,21.0684814453125,20.66607666015625,20.572509765625,20.6226806640625,20.617919921875,21.04376220703125,22.0240478515625,21.5684814453125,20.79010009765625,20.61322021484375,20.5767822265625,20.55975341796875,20.63848876953125,20.6348876953125,20.5595703125,20.392578125,20.4815673828125,21.08868408203125,20.578125,20.48028564453125,20.6097412109375,20.635498046875,20.5225830078125,20.46502685546875,20.52154541015625,20.79351806640625,23.93499755859375,20.44830322265625,21.02337646484375,20.47906494140625,20.56256103515625,20.9188232421875,20.46160888671875,20.87493896484375,20.50128173828125,21.20367431640625,20.6748046875,20.5718994140625,20.77490234375,20.6024169921875,20.7081298828125,20.7933349609375,20.515380859375,20.5640869140625,21.2242431640625,21.504150390625,20.82330322265625,21.296630859375,20.60064697265625,20.526123046875,20.60546875,20.45880126953125,20.51519775390625,20.725341796875,20.4600830078125,20.6463623046875,20.5469970703125,28.006103515625,21.0767822265625,20.76654052734375,20.61962890625,20.48590087890625,20.5430908203125,22.6636962890625,22.26068115234375,20.8157958984375,20.6536865234375,20.93243408203125,20.62646484375,20.69281005859375,20.50872802734375,20.70867919921875,20.5654296875,20.49530029296875,20.49273681640625,20.4853515625,20.43548583984375,21.71649169921875,20.52215576171875,20.51263427734375,20.5087890625,20.54156494140625,20.52532958984375,20.48223876953125,20.485595703125,22.139404296875,20.61834716796875,20.930908203125,20.8643798828125,20.50494384765625,21.00921630859375,20.53875732421875,20.9217529296875,20.6239013671875,20.63922119140625,20.44866943359375,21.0701904296875,21.44134521484375,20.96478271484375,21.07659912109375,20.755859375,20.5030517578125,20.59368896484375,20.539794921875,20.5728759765625,21.81341552734375,20.51629638671875,20.51611328125,20.72412109375,20.65191650390625,20.5924072265625,28.5098876953125,20.8731689453125,21.50177001953125,20.7611083984375,21.06622314453125,20.68267822265625,21.05816650390625,20.688232421875,20.72613525390625,20.48883056640625,21.276123046875,20.482666015625,20.50103759765625,20.72833251953125,20.6270751953125,20.52484130859375,23.68194580078125,20.61962890625,20.63433837890625,20.70550537109375,20.5657958984375,20.62713623046875,20.54840087890625,20.70758056640625,20.51116943359375,20.535888671875,20.576904296875,20.55804443359375,20.7342529296875,20.48284912109375,20.568603515625,20.48675537109375,21.5643310546875,20.634765625,20.64483642578125,20.91455078125,24.52886962890625,21.0703125,21.61376953125,20.74627685546875,22.0108642578125,20.46673583984375,20.49810791015625,20.72381591796875,21.8974609375,20.54949951171875,20.95977783203125,21.0450439453125,26.7191162109375,20.4761962890625,20.459716796875,20.569580078125,22.37738037109375,20.50823974609375,20.59814453125,20.45562744140625,20.45233154296875,20.57952880859375,20.89971923828125,20.585693359375,20.60882568359375,23.95819091796875,22.80450439453125,20.5157470703125,20.48687744140625,21.74493408203125,20.53399658203125,21.270751953125,20.76629638671875,20.48651123046875,20.7518310546875,20.73345947265625,20.59564208984375,21.5018310546875,20.57000732421875,20.4342041015625,20.60906982421875,20.6451416015625,21.05035400390625,20.5068359375,20.82550048828125,20.50421142578125,20.55218505859375,20.6263427734375,20.7005615234375,20.50262451171875,20.67010498046875,20.5875244140625,20.4361572265625,20.77850341796875,20.461669921875,22.9517822265625,20.739501953125,22.11212158203125,22.14654541015625,20.57427978515625,20.43328857421875,20.7791748046875,20.53240966796875,20.49072265625,20.84796142578125,20.62200927734375,20.6260986328125,20.4754638671875,20.686767578125,20.48614501953125,20.54058837890625,20.6751708984375,20.77783203125,20.924072265625,20.51116943359375,20.60943603515625,22.38311767578125,20.515625,20.56878662109375,20.51373291015625,20.46630859375,20.56982421875,21.15142822265625,20.6253662109375,20.60614013671875,21.7666015625,21.29095458984375,20.6641845703125,21.0029296875,20.57421875,21.98248291015625,21.59161376953125,20.5589599609375,20.55670166015625,21.03814697265625,20.4990234375,20.65655517578125,20.56536865234375,20.581298828125,20.6068115234375,21.25018310546875,20.6402587890625,20.675048828125,20.7265625,20.7216796875,21.48956298828125,20.99346923828125,20.59613037109375,20.47418212890625,21.37078857421875,20.680419921875,20.6517333984375,20.7861328125,23.23919677734375,20.94586181640625,20.4727783203125,20.5169677734375,20.7479248046875,20.73486328125,20.44866943359375,21.6663818359375,20.46453857421875,21.2174072265625,20.767578125,20.44866943359375,20.94921875,21.15118408203125,20.454833984375,20.76739501953125,22.046875,20.510986328125,20.43353271484375,22.58538818359375,20.84735107421875,87.11322021484375,20.4832763671875,20.55279541015625,20.51788330078125,30.67022705078125,20.478759765625,20.43359375,21.0426025390625,20.5819091796875,20.445068359375,20.46832275390625,20.58349609375,20.61285400390625,21.085693359375,20.609375,20.4820556640625,20.50885009765625,20.953857421875,20.64178466796875,21.63323974609375,21.00482177734375,21.70550537109375,20.5146484375,20.59423828125,20.9420166015625,20.4775390625,20.641845703125,20.53485107421875,20.51348876953125,20.65643310546875,20.92926025390625,20.49407958984375,20.5155029296875,20.68743896484375,20.59844970703125,20.4923095703125,20.59814453125,20.538818359375,20.5264892578125,20.49896240234375,20.51495361328125,21.41925048828125,20.76043701171875,20.5272216796875,21.52471923828125,20.96624755859375,20.75970458984375,20.54058837890625,21.2418212890625,20.44757080078125,20.45263671875,21.9661865234375,20.57928466796875,20.90423583984375,20.5201416015625,20.69952392578125,20.52703857421875,21.18670654296875,20.6551513671875,21.2305908203125,21.60662841796875,21.53179931640625,20.69940185546875,20.79791259765625,20.65234375,20.9754638671875,20.89117431640625,25.48858642578125,20.5419921875,20.58001708984375,20.4925537109375,20.74102783203125,20.7535400390625,20.44635009765625,20.736083984375,20.70263671875,20.57440185546875,20.9876708984375,20.6043701171875,20.6390380859375,20.79522705078125,21.78851318359375,20.48638916015625,20.518798828125,22.510009765625,20.693603515625,21.58221435546875,20.44940185546875,21.0321044921875,20.7159423828125,20.7586669921875,20.42645263671875,20.5325927734375,20.55816650390625,20.53326416015625,20.4337158203125,20.74725341796875,21.28656005859375,20.71429443359375,20.4847412109375,20.6451416015625,20.95709228515625,21.59490966796875,21.2740478515625,20.7467041015625,20.55902099609375,20.505615234375,20.5538330078125,21.6541748046875,20.4659423828125,20.62091064453125,20.74761962890625,20.4482421875,20.53839111328125,20.599609375,20.559326171875,21.26092529296875,20.5970458984375,22.61859130859375,20.6453857421875,20.57568359375,21.07940673828125,21.0120849609375,20.6390380859375,20.52069091796875,21.58404541015625,20.57745361328125,20.495849609375,20.53924560546875,20.4697265625,21.21466064453125,20.890625,21.83782958984375,20.71820068359375,21.6307373046875,24.006591796875,20.439453125,20.47283935546875,20.51702880859375,20.5147705078125,23.4212646484375,20.70013427734375,21.0889892578125,20.53460693359375,20.5787353515625,20.50262451171875,20.5816650390625,20.99359130859375,20.98797607421875,20.49658203125,20.50115966796875,20.49102783203125,20.62677001953125,20.489990234375,21.05224609375,20.55950927734375,20.46551513671875,21.10296630859375,20.77044677734375,20.42071533203125],"xaxis":"x","y":[22.06759946491166,20.51569310181126,19.61997633871636,25.808363100543477,21.795376715513594,20.34038041027431,18.86081419340691,24.078413808713293,19.082528830592434,27.09570141066156,18.66432922183982,20.146924115160264,20.694537918678005,20.34827538264879,22.1287945428413,23.512029578625103,23.710849830139047,21.16813940491307,24.138707165857692,20.413583384325577,20.971944131576173,19.330306789697467,19.240535149076692,19.593384739494034,23.207168669114285,20.243485920599927,24.816169078762734,20.239179241482244,21.93825114094642,22.544699771649725,20.581001696999003,19.006769325349232,22.852829884665343,20.97322158881004,21.37041284025584,19.8813455186455,22.291944364905724,19.352118206703615,20.690574372087426,19.616726513482636,21.73777568014623,25.92407619036082,19.62630225708032,24.328668011629535,28.119486884464127,23.268620237666717,24.937914667295647,20.538966591972148,26.49472805759287,21.551656870776874,19.967005268430295,20.53263058829885,19.077522473826907,20.411222614242394,20.118559661298907,21.75794824506378,21.969944344762656,20.941991359524017,24.30144839882211,20.08875201356742,20.413158139175476,17.21138734793915,20.10520261649224,21.318516744600927,21.383177447413818,18.901120542977083,21.140418528305776,22.317825778768146,19.694551733623157,24.452943059423166,19.34725801943171,19.360313431748278,21.62865690143549,18.725486086935668,19.22798647641259,22.00766259568964,23.581082748790354,19.483945640419044,19.555879359056263,20.81759187582515,20.004050690864457,19.7027197284411,19.957005833266606,18.935012496616167,21.132460035639845,21.104713276655342,21.0706503248077,19.995538068644624,20.45373482710394,20.377850211599817,24.218899085058613,23.648191447276112,19.27757204338643,20.547353262390168,22.570203152455143,28.033188230056272,20.740147099081785,24.844950181647548,21.11139041842472,21.618102712310883,20.71863237230781,19.638742582781422,19.346013720701006,21.12828261562749,21.344903224909334,19.828653485294133,22.842407702796528,22.86005581934068,20.23340552018178,22.244828354517683,19.8468416379525,20.153224049031966,23.16226531069458,22.47425899129698,20.440974576504654,21.34321011850724,21.96825774467718,19.581570228112746,22.97307940319244,21.943790024620068,24.922685338201692,18.397908557303648,21.170757004644493,19.439732238591404,19.06759979313172,19.78225599625029,20.88710307988011,20.06163488065717,21.227834779401228,20.925901548056537,19.64120627043042,21.51013491039007,19.943833745099443,20.041179196484592,19.31991683554051,21.305505708300736,19.083972676252298,22.36486950758078,21.00153722158492,20.91944021038121,19.71891096707218,20.43515061037962,23.756601825433552,19.29612083176196,27.288645329988054,20.004627845911102,20.581937941283943,23.06049210432163,19.0283827986455,22.26292897661663,18.98289333455679,19.08654493081137,20.26723932452398,26.772642502942784,23.150894170391226,22.789865927709045,21.316932785410458,21.47925775839002,19.52466856143405,17.31416721288844,19.67390026636621,21.280848347151327,20.575951881264743,19.36981390700933,22.906270958120995,23.259273464150183,21.034942664057727,19.478866371959384,21.521316718605924,23.251480243552532,18.947642644803835,18.234449818345436,23.74556559722114,23.70046067341344,22.17893043543761,23.314842649926593,19.204628901280135,21.028986544289005,19.811383905702034,20.105414612220844,23.470671408984025,19.185541351838207,21.863096949082852,21.439837984160558,20.28595076567891,20.09465504412721,24.347469240771108,21.026538976608304,22.25115601355335,19.17189511189754,19.330727902738385,20.455200776646638,18.7812284253763,19.9984664758684,19.71179802639097,23.222688495940556,20.75461515785591,21.453079563108336,19.968121134960665,23.31621219099147,22.229186273778243,19.29157093266521,19.725980900417238,19.393396349595932,19.510476738006748,23.86936283890287,20.803398322957783,19.50445726786541,20.266025864180154,20.318803159054653,19.88287753302219,21.755410336588376,20.021014007950576,19.81464925130666,19.600081112964823,20.195578868662224,22.127386699288984,22.615494713832,21.994462034082755,23.008367409641956,20.02036350615788,19.496913617064063,19.65284705132002,22.970383373621917,19.800442868861996,19.544786543412965,21.196126618068195,22.706377974056313,25.14188655199703,19.430343691015025,20.82549425312562,21.300868648362627,20.935011057360228,20.238709984208988,23.361271159574002,23.97311707542301,20.93275451807793,19.135842027978637,19.419925212472517,22.69657897049395,18.04707863542646,21.639091593296747,24.902956648700677,19.472341179018663,19.630944296038688,19.581570228112746,0,21.279464358226686,20.6373536565784,20.55168503037007,22.252924089212918,17.011336466938804,20.539904719671835,20.287228770242784,22.579718287600105,20.016747429777542,19.919862789535355,21.590649178257912,20.459085985918723,19.305685493406397,20.69934199839951,21.814001605688915,21.49724409211446,17.96562762596234,18.897059576214055,20.218149025767367,21.663202256858984,21.537426968242833,19.881144027962787,18.109927551497627,20.650313109616484,19.551649361459194,18.954616641321604,18.33673536238825,19.272777840955303,21.22721049508145,23.36907437848721,21.839279632705356,20.080313988880153,19.791725695449298,22.830034474576443,20.038176858751854,19.882805718288704,19.137324212876308,19.89611063211052,20.922839608419043,22.73108571789678,18.798041102461394,19.809989451392685,21.628482474975726,22.66651647870706,20.105565995835796,21.324104166055236,23.33275256138136,20.80501366619404,18.787924871827784,20.381292625866948,23.456138303517488,20.59828494694422,20.938551777966236,22.709886920964298,20.558379254361796,19.758613410846518,21.618914296187317,22.889842311610867,21.30122097904847,19.0336054859068,23.801438495307856,24.840309004310797,19.694954646749302,20.944503948247828,20.620570884389835,21.72592495902031,20.296054717472362,19.58398989769835,20.2956272827196,21.085706497425413,19.461766940454588,19.29900449887296,22.510563057499144,20.841106100432793,21.801619166438428,21.147132242697182,21.551582010518878,20.4427602890876,24.611415306054898,21.490696787354505,21.24369655260969,22.432940196049124,22.234424235191682,20.099462108962847,20.800416502964755,20.17949699394329,21.746512886013605,22.496434708234126,19.78606039431591,22.229470444639624,19.19842345841963,21.071706539354626,21.28737646300594,20.120777025175872,23.69806207100269,24.25287711008343,19.074352191519743,20.558351575498786,21.437400189861094,20.352963298343866,21.233800456337935,22.14457282995132,20.722700819009958,20.69715285443839,20.03253125195289,20.0566191944814,22.70946891023392,21.137414277628565,19.808943756196857,20.143175790739598,21.040198736231254,19.712186074557728,19.35146728073557,20.861088989018167,20.755317114550273,0,19.017540081468233,21.419330156983328,20.67873788317677,19.93668394257525,22.409766675366317,19.27540160418116,23.728782472563765,20.376074503254586,19.31077695697734,21.065895324749285,20.3484179021553,20.07860380095908,19.976548041795603,20.235086321270654,21.01213158467734,21.41999462819776,19.174863067555084,21.10241852261651,20.85643673732399,22.586773372950685,20.037769462388923,21.559823540829672,23.13793422908773,20.558605220150522,19.683189962828273,20.39789928981696,19.337397595787667,22.26889032385089,21.195293500127566,22.559724250840063,24.39611039895289,19.59271980995523,22.136011068345166,19.33796659591498,20.797749380419276,21.567058771175564,20.725266392758815,18.468910364525364,19.663251453070878,18.918473122196858,19.198971627350232,24.325766932844235,19.86193501563865,21.156628899315663,22.938277348217966,20.78722963747693,20.66031803587651,20.155379081002646,19.457981357627542,25.713720052658104,20.828559454854545,20.52544147367378,21.147722912842873,20.03302501817969,24.630356510701596,23.121909718158822,23.106213073613993,22.51080834568276,20.883881259335507,21.661257500272384,19.85555820528158,19.526632957320885,19.191856386440307,22.96557469826004,19.96989517504386,19.756880613939327,20.841336970061143,21.615161729704354,19.630957845101975,19.107266091841186,18.360084050573274,20.178689314774683,22.61224657942639,20.902818910258993,19.692513024244924,21.22795725431778,21.431836012474292,22.58649267850949,23.56945490190484,22.15739813287633,19.523673217064264,23.682062419502355,22.448170385491473,23.624330847094043,20.4998468600676,19.29906600505008,21.469193647687533,22.680649469911085,21.178626711012377,21.686546691263366,21.428479917582088,22.88912263482666,20.804770608680126,19.28855746111665,20.502951107445575,20.22322663992422,22.31457162800466,19.478115876564008,18.795995233915015,25.17177163891695,23.160406649511106,19.25160491688623,20.826937118763155,19.953135265171547,23.933068642871486,19.355480148534408,20.44485249695551,19.56616125697471,25.23181389258149,21.345469949872594,22.776042002687795,25.303110182218393,19.58463718752616,20.204391739449186,19.893828839264728,20.33821258053668,24.68330601022857,18.821811344100055,19.840511525069328,22.297566604177444,20.78875924862035,19.741131185755275,20.04722528013383,22.468811326543563,19.99457856414159,21.80053568878591,19.167558108393887,23.846962977749094,19.07645844042533,19.307592868706436,19.883803436319816,19.767052898077655,18.952911838336565,18.62774549084819,20.73926969664862,18.627786784209636,19.177777189052396,20.089958597754855,20.801380865239704,17.764101156137162,19.11561871251436,19.24667566091232,20.93405408413005,21.340147837597844,21.090725928345268,19.745386795650543,19.095931371701568,19.810878030088894,20.251155291034372,19.165221068220944,20.724802969675316,19.74733497869572,22.856134451929176,20.542598325656595,19.502159734210206,21.979041347822633,21.2197623270523,26.969103234931445,20.18342033833375,21.02499268359878,20.592440384261195,19.990444456103596,20.867172390851817,21.273367636241293,21.243839329097405,21.363500625759567,20.18486444945059,20.241113097705927,22.693372098426487,18.623284957897813,19.205602363695707,21.545170940853957,20.80034066129298,19.981973707277767,22.19000668879013,22.718490023684353,20.707922743251714,19.214188218113872,22.472827160217914,19.07108521476948,20.346151863763343,21.63013067480897,18.214892995560163,19.93614600441511,22.53034703709958,19.469859640957857,22.895450751816803,21.00807772677881,21.431258349952667,20.941808459871957,22.99407646829777,19.76111291920157,19.008644725628592,18.355858634806896,20.297451997659746,20.474380309584664,21.870871511939562,20.759740299921024,21.439000175436664,22.030880607811586,23.00587909853429,20.655014419256375,22.965146977089795,22.43523710461151,20.18471556395119,19.019883876304206,23.05070508803856,21.499214768467855,20.59203905710115,21.024704798577186,22.72181503272866,22.377050963043796,19.71871179715288,21.93139047566312,21.827309362120346,19.1796783064222,19.826388660339887,23.65560944108187,19.199993380834425,22.21417220655097,19.269925661522706,23.782692532330046,22.173362238186836,23.41808975500013,20.872940195293037,21.54216753533708,21.03393309328942,21.663729644062077,20.699641074284234,21.39606694375647,19.593416427729718,21.355622092522935,19.49242240707679,19.406777996109206,22.625652665035147,19.860111721120674,14.03702355520748,22.00972851632279,20.33705521659505,20.155471422765178,20.40843263005365,23.540959857497864,22.64902697475969,19.51901613478499,20.748003621067692,19.075332900397907,21.535462867247563,19.35190112925433,16.543892756530866,22.048803804340604,19.94957596782787,23.253137677025506,15.802066947826521,22.01816825105002,24.65495079669392,19.684337709357063,23.07232514287102,19.441821622178967,20.90283362751463,21.65340972782181,20.672219865399406,22.248853822319084,22.007999054329076,21.514848387611945,18.91123104739614,22.195045634281605,21.092338092530998,19.27798585366196,21.725547245838506,20.321666107127033,18.844349286283478,20.211183384442457,20.06069065496676,20.388592288941165,25.880225035189923,19.68504303470752,20.795739259102447,19.874915797147626,20.484467680558765,21.537004350283745,19.454353518988064,20.471143313052202,20.221414835639845,21.30681378731033,22.893554470389834,23.892417838743924,19.773417735905042,21.39811634221899,21.94580529932221,22.093666756982273,19.566140232030243,19.62938152434226,22.598971957007752,22.606867611519395,23.548837661891714,19.57985452716288,19.257215175267675,20.260215495083163,21.269107840654502,21.892874617975046,20.911342976063032,20.690267441816314,19.415723203109316,22.536424619535012,19.241608485058645,19.162579413291002,20.470409642279566,20.97348174432658,23.73507977629211,14.55836091863428,23.336410397420998,21.627932359551377,21.49391553553141,20.265469687416747,19.426854398695138,20.625264709408093,20.223930403747513,21.865935415382868,21.881578906408237,20.14846091060974,19.81407448278221,21.0024847282008,20.945391681085823,22.291777470342872,20.551017265346704,19.577722584113683,19.444205825873514,21.46402517777047,18.277738745265133,18.799932943985397,21.60163932304092,19.80978333592071,20.91463622707276,21.547692385839394,19.17999258814081,19.549428996570953,19.54492482512454,19.294068756456163,21.69613129076446,20.048671453648097,19.49098989799628,20.42092755989845,20.047998538616795,22.735567798975932,20.159778968313912,20.920832580620292,19.699545342858148,20.678206389277292,20.079358473730228,19.917597162486278,19.692880228017533,26.26193365893149,20.33327111425421,21.201441496966886,24.081396558763124,22.374129418340807,22.181711726885904,18.957794711482784,21.63591845810156,22.742154187536123,20.277557571355647,21.712167598440185,20.922319986303112,19.981118924422468,18.376324675581085,19.093096903348385,21.041684548715036,19.88267606965292,21.79470380164697,19.394929062748655,19.711931797206645,20.701777070204972,18.966254774482525,16.57594392051226,19.3661810523187,19.637857750008234,21.05876451742095,21.39188913879269,20.922214169866013,20.158015290040524,19.840625811644227,20.287989620862316,20.392027938746434,21.440477364164977,20.18838178436931,19.209224880205603,19.523793560708743,19.767710849855263,21.57006553499441,20.748925773432315,21.47821316674072,24.169496968680054,22.350651659007692,22.60440595404428,20.973185759004682,22.891726790683144,20.2096151875794,20.22349858267661,20.901209408549594,19.053706845403045,23.000908813982363,19.549547798448483,22.637005220660473,23.38302130821851,25.236220259743373,22.107609158967936,23.256395298587126,21.31115959043422,19.242152996635603,23.069120127461606,19.88568939220856,20.74441094675746,20.977820033541025,21.403421935393933,23.155520679582406,17.74375624808358,21.65558749657167,22.72177363339029,20.057436191229318,20.31682150206961,21.442049003709382,20.447439187365134,18.64679105166152,23.081069556252004,23.109422435163253,19.40981146751902,21.538742146089504,20.451115883369862,20.520204300445414,19.072268708954628,21.853488891111965,19.870759032200375,19.874160110175985,20.45611373193672,20.923455889638944,15.423378576518493,19.495183602853512,20.532198000108426,20.172754664219987,19.621134659665625,21.149770683357705,25.5492833010917,20.91962424027046,23.144648130459657,22.724819827816283,20.76732896720974,20.90466916348112,21.019251701774245,21.870062515886435,21.968529606994224,18.28303322918483,22.7967578369547,24.617902294510255,20.337523908425865,22.278345092825212,20.439903152029967,20.745981176528062,20.50317548758169,20.148792615003963,20.18507504172347,19.887802201303376,21.335667578998795,23.547889563240428,22.184719195487645,19.124759964560997,19.09456742423413,23.546328229722338,23.676810419954805,21.378549935187863,18.772971741326142,20.92343911431291,23.66501619396984,20.362922084924325,19.335122075152558,20.6322831501737,22.014406532680294,20.862569257472042,22.377941930707426,16.70615872384773,21.114598939015725,25.312874128977917,19.557332226454086,25.0888235331049,0,20.821375100396438,19.69889252664812,20.730065203035867,20.737794346856816,19.720177445507503,19.751886432327307,21.436234720492326,19.12391425698277,21.093445290401018,22.18427320470571,23.32578333060166,20.235023779555554,20.38834949087981,19.076215392899233,21.890199695637197,19.626240196846272,19.473911816076555,22.65051997686549,21.07153535807478,20.152641197123305,25.817249936117022,20.72049143867345,20.000328449472278,19.677575133698696,19.854767962698624,20.90714373186156,21.852326049162542,22.34522357506863,21.88381079939571,20.64279914121781,20.008240359915625,20.079932477068777,20.97646133780068,20.617037831360907,20.86340137149202,20.10297814136792,18.939248297157278,19.337083944174847,21.21789500066683,19.959413629413806,20.57158693763026,20.1483293247843,22.45828794986169,19.223167412657357,22.090969450288238,20.54580254002858,20.99006223015013,23.933428716720645,18.97273303540371,21.839004592901738,17.66325927218184,21.098649782686852,22.108159109401438,19.70668845124008,20.687407722542194,21.23800756191085,22.54894428694912,20.440949745295235,18.136670762341843,21.301837769324877,21.032952737462605,23.31051349089806,20.969732200565332,20.351613259246466,19.678510738089678,22.059041559037812,22.10097025846529,21.127912480118983,24.425354799557958,19.405573621751774,18.954513256380405,20.62820521809647,19.21222562648202,18.9881876444261,21.62524006847279,18.78833918294514,19.827821621310783,19.4644839297056,26.34702224299619,21.38205653661138,21.78972267747008,21.087755789009996,18.999355953219133,19.451729034530373,23.289040906826244,24.16646354231811,21.536078922759724,21.530906360636845,20.97243154975957,20.089266034468004,20.436904665975636,19.20088663487939,20.554011225634962,19.277982733151024,18.304477915148002,19.118363214083377,20.261720490850706,19.26508618967162,23.063753511610845,19.05336380976314,19.240852168960235,19.807795916443087,19.527755462443025,19.369940873414674,18.557194427644223,19.092216129486825,23.457850638636746,20.3986497634222,20.79850265219969,20.611470634120458,21.03390016791011,24.957468167964976,19.132327452939787,22.668945885967275,20.192548076466373,21.926818619899166,21.236410750491928,22.00439862331522,22.64890259783158,19.876543924901863,22.174820865681593,20.001613842126076,20.63081635106876,19.653189714082398,19.305175301710232,20.224827212369068,22.6279649673046,21.547003936026666,18.961602303420257,20.695469281090816,20.114481067581714,20.790519525351012,26.143565404950515,21.151917099556776,22.601975102609295,18.386700720994313,21.48658316808204,19.99564295418165,22.704086007044513,23.646718002570474,20.17407123394833,20.29544946282625,21.94083977054476,19.087820197169854,20.49274951238928,21.58556546053215,19.868098908707033,19.806349800221792,23.600603274171462,19.846459862589864,21.099407208256782,21.035986420389108,23.747615973251953,21.078530347858226,20.86782723578085,20.073998999202523,19.385623092000827,20.214357069981805,21.756421739524402,19.573874587511387,22.17732889464112,20.092049623606254,19.621522497700703,20.13212471121583,22.84612630027242,21.610638293924836,20.381527241115613,22.69863600121815,24.241029788998915,23.65992231174119,22.296121251970213,20.702441582535556,22.897176211133427,19.850331267520566,17.677965293255863,20.87727804782974,24.045999647847655,20.173501472804098,23.33182107390064,23.30215527163116,27.231751620760654,20.650611348398325,16.829816883719186,21.108904064764797,23.735782522109897,20.656510624448828,20.096689421256738,22.83710046840733,19.824675810238606,20.823945313943256,23.256137551486724,23.633181830866906,19.043844337217745,23.65829637497728,23.50747794569296,21.73683233585413,20.94197381362215,24.18225862695574,20.14794970691058,22.344681402352435,20.447944597167705,19.21271648764287,22.508853453023672,21.75045501313216,20.060863127783303,23.041170672161062,19.554654657325607,21.26551890511897,19.47575297115503,20.304093087106327,21.96980947524255,18.890513667640615,20.207528404900692,20.825342365179193,19.58596744655146,19.86547955920327,20.093122217042076,19.51138012046539,20.89052275919248,19.726434174485984,19.26180569731426,21.223090796665396,20.071086524480997,23.60048678836413,21.682830461889793,22.115896159241377,23.245004397316144,19.929597223002617,20.206849822052806,20.80607598372765,20.972697281096956,20.548932178877266,20.790175259909486,21.65577788956451,20.396533708238668,17.677595133772794,20.206889160267256,19.864164508631692,20.604884938737765,19.69979302428942,21.112079923463828,21.776216944868956,19.41019874513054,20.084542414131516,25.538294718669484,19.887154874828134,22.02600227823102,20.06242269100319,19.908561207609715,19.025787914622292,22.519912388376707,21.15310774495207,15.434839951572775,23.416469489833915,21.472713147192902,24.799241151178187,21.565777300167408,21.24511744232927,23.786142675104543,21.775846548503456,20.874095085342866,20.525903087310763,22.190296420947053,17.526091919071504,22.787613064091445,19.95220428650718,21.56191951932416,20.302727045932432,22.39076356657128,17.788361488629835,19.89495594402764,20.849863964556313,22.01047671458767,21.9419703223203,21.769327477492673,22.134078601621123,23.02213648046312,21.659326126254854,22.503975157123477,21.687400780796548,21.299913824117656,24.41218042654058,21.628416169590647,20.661666774415252,19.13829898996108,20.6534474504661,20.599627769624064,19.476347089125902,22.633338504709375,19.004867830543457,20.729309561171004,21.215021611787833,20.55962915472571,23.96741801447904,20.948249582299137,19.034944006994706,21.25562508835354,23.6936808146177,18.876298103633747,20.12057913956066,22.988276940898057,20.761017121909465,29.702932107840674,19.34717798381203,19.04780522125321,19.72022056098374,25.87586915107988,22.381251164844315,21.45471998887307,21.894206057556605,20.997149152437558,19.346653316091935,19.69023397394279,21.75892392666085,21.695257182632144,24.304938166770185,19.96312680068985,21.08058139373211,18.457821713581545,21.058645462338415,20.370456476417175,22.39756222611425,21.96355600303241,24.29338064398619,19.533504643608662,19.24597971762759,21.061479969798718,19.653131971320136,19.991828535916927,19.41354451956652,19.024392506006556,19.995956185326282,21.080207924079772,22.895336936087247,19.00379513256332,19.59081933226322,19.683155687698772,19.724580465382388,20.49815774428782,21.41646356230674,20.374729613814893,19.698149844649336,20.827570650601533,21.55980522344904,20.203106297950406,20.998113148879906,22.5464416348448,21.94562707940607,20.109117749689304,19.61600010557632,22.28815948936714,20.00070564420032,19.48773603183014,22.96164822427184,18.99960834234589,20.932965503614184,18.548961097016324,21.40256127224899,23.343368321662904,21.395751936133603,20.367816242708102,21.629837063073623,22.815206783859768,22.704911104222457,20.7857551664019,20.3067025549342,20.61336030755643,21.781868242254394,20.56504163001677,24.58750691743097,21.042555130756018,19.718574682047347,20.82900495573487,21.89094777167843,20.46100849412487,20.676958661968467,22.376303078727783,21.96712491711981,21.09072172933059,20.373076483151994,19.53720891947194,19.37233862557095,0,22.656078929958284,19.971799365414682,21.138173662648782,25.881734625061707,0,22.042981377352834,19.12600868541295,20.929534739770034,20.604603448464516,21.393924057871644,18.99682101244613,19.31053914628864,18.986258705756388,16.7233596191811,19.687016476370815,20.364732076794134,18.828527980887348,21.061860954738233,18.824250825082274,19.28149315085171,21.023652845270533,23.394529944648035,22.371116977331784,21.19573925103901,18.93658618322981,19.94401710262461,19.47904998638186,22.850840758637354,20.76699650557159,20.40036143331872,17.795936296212627,19.421904563712328,20.64673096490357,19.639706353282477,18.775317705171794,20.047574770414855,19.61260734468754,23.666636204423178,23.480370137373004,19.218266855752937,21.60748880425466,21.257489419039537,21.101232900314493,21.320633929890864,21.849984955018474,20.40434121243742,20.032547562218568,20.035932572983462,20.864093173595,21.51301334156099,22.29182284846925,23.44987910374218,20.855425894539348,22.64951341150128,23.894186440194584,20.26686372980413,20.408744363117133,19.046718202293015,20.904214174162476,24.308788888311135,20.77592509493985,0,19.644171392658865,18.610921424553307,22.26726247442381,20.903586582709064,21.2082248307893,20.867431932395597,19.247883485213144,19.09858987177298,20.769205079804532,21.73660672493293,20.24751150890443,21.281868210231526,19.724170230709305,20.553728301262954,21.112413416140527,20.46206885900041,20.668149201097204],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données de consommation prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBNumM_train, SiteEnergyUse_train_log)\n","\n","SiteEnergyUse_pred_logLR = pipeLR.predict(BEBNumM_test)\n","\n","LRr2_log = metrics.r2_score(SiteEnergyUse_test_log, SiteEnergyUse_pred_logLR)\n","print(\"r2 :\", LRr2)\n","LRrmse_log = metrics.mean_squared_error(SiteEnergyUse_test_log,\n","                                        SiteEnergyUse_pred_logLR,\n","                                        squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=SiteEnergyUse_pred_logLR.squeeze(),\n","    y=SiteEnergyUse_test_log.squeeze(),\n","    labels={\n","        'x': f'{SiteEnergyUse_pred_logLR=}'.partition('=')[0],\n","        'y': f'{SiteEnergyUse_test_log=}'.partition('=')[0]\n","    },\n","    title=\n","    'Visualisation des données de consommation prédites par le modèle de régression linéaire<br>vs les données test'\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.2 Modèle Ridge"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre      Ridge()\n","0  ridge__alpha  1052.015218\n","               R²      RMSE       MAE\n","Ridge() -0.169007  2.392635  1.124331\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logRidge=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.2272773129649,20.580310176422138,20.560710018533396,27.358445549679193,21.04150919266924,20.528368246442874,20.523535977291495,20.95211473990794,20.524879036897424,24.24646994042022,20.532397121092487,20.531607315600592,20.58031896761896,20.564889965386264,20.52883973842844,23.320880410684932,22.6677093941465,20.777799849691192,23.778108689613102,20.60066532476668,20.9132728558314,20.582966621816507,20.519167891630023,20.513851043271263,21.91091156611007,20.51880703658642,20.692727768217367,20.634271975610464,20.97273060242494,20.567148416362322,20.85352462964018,20.480705928309995,21.1365845140533,20.777427383524767,20.972687953876978,20.547411671669995,21.3248926648746,20.65436758096542,20.860897096723395,20.634974729454783,20.83639723701153,22.013876775707708,20.478605200464628,20.961395293358812,31.189067320968096,20.97369578998728,25.446002054265797,20.700412979389654,34.135528155408764,21.053100734504913,20.632022303513143,20.723822523410963,20.525339151617686,20.5957994113014,20.870635234912452,20.683601902645446,20.799854390467917,20.806343439086447,22.043233768606648,20.63374307446265,20.622935043407395,20.758348852849764,20.52046522497362,20.589384375653022,21.036110197822783,20.55913551973987,21.04466745814504,20.547605454282333,20.582848608934487,21.413151703003848,20.566584333576447,20.517803214211472,20.79776602804461,20.550422732206744,20.575719472374363,20.8830867164497,22.338403961356452,20.585400085112617,20.55192665179032,20.871824302477958,20.510429838006345,20.644327234765434,20.655944543833694,20.477407110062906,20.73827825102798,21.001447911001737,20.63745506347966,20.677129730150778,20.823657693209906,20.51488724627589,21.763137620336092,21.20334202307487,20.57281907204887,20.75678890506682,21.4286086371833,23.240763223984384,20.51535306008259,25.865878274146354,20.788019719840353,20.93310499315744,20.681188221973105,20.49950574270775,21.344199850396524,20.84201335254405,20.733936070699265,20.519076743273537,22.408900210429767,20.6998371442367,20.783679185636583,20.541894833992888,20.689065890470925,20.592785014174154,22.597014135736483,20.752977871625774,20.732026890794746,20.874992605129677,21.820957148798446,20.574749202142577,22.303942224559094,20.599093407705315,20.676121226492175,20.466018512148615,20.489002128018086,20.46241617808384,20.521018674519283,20.558647292554,20.514722442053564,20.54354548147682,20.98569980215236,20.476956459164022,20.55114513258887,21.22561259260971,20.6482758104913,20.62260602329738,20.63170273592455,21.171332800976657,20.520657080451155,21.139908721303634,20.52038419871262,20.950509435875365,20.629780459094004,20.512006871657967,21.862844772488227,20.55942093170385,24.16006264889014,20.67092819484526,20.691424201211888,22.14235635827614,20.48485977543766,21.612524678144943,20.629602660032962,20.502134052126618,20.473001153210213,32.37671425347251,20.476078167783882,20.735796172247394,21.19130018236697,21.26572311817338,20.499589547973038,20.485563688612014,20.51056129749958,20.563886880154488,20.549389113906418,20.584858188423365,21.596297168443105,21.103326748112753,20.805489543538524,20.57518363028057,20.951368446999574,22.379163678941,20.498730666877638,20.590638841655412,20.73313335018628,22.53210661795932,21.143439243626243,20.741840675060683,20.515119281468525,20.51365638533536,20.66622800215102,20.563515898171794,21.39663437462465,20.53333021353384,20.488409314751614,20.62719916373873,20.669758457591808,20.51580329779739,24.593826375127172,20.760391155885983,21.210483940234216,20.623024426911844,20.51718430289159,20.574355258614435,20.509927566193593,20.630293720338138,20.485762530340626,21.0537487252041,20.591056821862406,20.723716200550104,20.553502990410507,21.803275379503415,21.012988501139265,20.598119671571848,20.545313872711468,20.54926057822917,20.542992408275616,21.046909915197375,20.561022128682954,20.516339250897136,20.630323671121683,20.568185372169015,20.677666086845893,20.677838843880817,20.469045025740442,20.62999194758408,20.58826871887207,20.625380908965184,22.544858847256215,21.133506306181804,20.976385785212607,22.900617764112425,20.577308247456738,20.554817185127582,20.548784486627845,21.013086938263907,20.65315707649999,20.61195936294971,20.665178322140505,21.25490247365768,23.03052018048065,20.57941891009287,20.545441725333816,20.658980652350287,20.543651342419615,20.67811878690645,21.867596877949165,22.420103723376034,20.72428140250804,20.5257217030308,20.573481624150894,21.37053091785291,20.531369967816687,20.978584574163943,21.32492074636336,20.554073301329648,20.698135200818545,20.56587020513043,20.828200903821,20.467508169257403,20.482949177596982,20.839771397836476,21.1926951364621,20.47572250049848,20.852586825135894,20.524553604441476,21.586308939387862,20.594619417063917,20.570395556748316,20.525717762177134,20.637737364959683,20.546792760350115,20.654138743350504,20.653410480019257,21.201290624165317,20.585636322730462,20.517172086480574,20.47096381840382,20.80542212105529,20.666195036524165,20.47102960062186,20.48323870239281,20.8622456551672,20.582132598187734,20.52151261336111,20.503688962130976,20.566859489079572,20.650492602459337,20.737341654046435,21.184630333950576,20.713036801975562,20.637592505392075,21.100422618939017,20.713766127145444,20.634990339691498,20.532595888315683,20.534857590602527,21.06389465954854,21.246213210472845,20.53875524616258,20.648419483669446,20.648388853116796,22.193008182608708,20.54097713357341,20.874086075594118,23.04581193769869,20.451529545277772,20.548975116311745,20.736536729175462,20.591778392816284,20.565308926303434,20.7467987156985,22.00497631470909,20.798902023774932,20.70713422662295,20.64189566790701,20.965637846146933,21.17426890654675,20.50283734733614,22.204913640766566,20.648759095332018,20.591953746621204,20.58761446741321,20.700033517756044,20.536793225443148,20.687727656106862,20.58733145180235,20.608096111096973,21.16779439737364,20.595248757312913,20.530048830191557,20.576426937332094,20.95285091784377,20.832756399716796,20.905899083761216,21.229178047371132,20.529963133305646,21.260564602613467,20.54758562025638,20.990059391030385,21.045201706153712,21.84567559269177,20.529332319844997,20.499176398286462,20.655128427511443,20.771735974615886,21.0736036477779,20.588333613527315,20.7092061087578,20.54274603248935,20.537793386376613,20.721625883408024,20.528803851234183,21.907939431669767,22.006488125835862,20.51129274321195,20.777401821947425,20.610372013127222,20.63947116133252,20.559696083421105,21.378059257368392,21.067603329932478,20.561183110369075,20.702123233923704,20.550711912322406,20.839404014118447,21.018823851216105,20.545002979291176,21.28372124844374,20.519504077006353,20.508410824988136,20.56859222619622,20.68188443995555,20.588234301545878,20.646504436062727,20.52488686258946,20.63796847676274,20.724599536299028,20.559789898067624,20.94589406888536,20.536083373139704,21.710760845899536,20.512324160689026,20.63413104611212,20.616836088487137,20.504080045907177,20.60459534729491,20.53538365817676,20.580941689079605,20.62688689505009,20.582313064197248,20.564860905065782,20.97221861572273,20.80545128125747,21.81100092427188,20.462667413982466,20.873964707630865,20.734787894018996,20.7378266226824,20.515638906892622,20.94104198917814,20.5492072080567,21.20013429476357,20.738300860510922,20.910197366666857,20.95700748439094,20.50972561285691,21.342411622266077,20.583486002455775,20.72028573382297,20.726943562998684,20.82211265158741,20.568236833225132,20.649818385860275,20.46825797593687,20.517983415541433,22.285482516902626,20.589720262756927,20.895322702832942,21.463253335204055,20.700622673724006,20.604208687409887,20.551900465608245,20.553997967954675,23.95401015430194,20.623968622983565,20.54426707060683,20.472180598372603,20.683243073847997,25.186126377301754,21.03456226783305,20.781058930982056,20.471072832228252,20.745365686674617,20.50414208602972,20.559008706560434,20.63554979596019,20.503813934609028,21.602772611174796,20.689526370169002,20.61271661972817,20.510737511288376,20.838688397340484,20.52912215008084,20.539686033012817,20.478105268614108,20.655342994533864,22.78954330159329,20.56708052175375,20.627579235837228,20.777872823340466,20.749440218770044,21.123286436032643,22.69269720774449,21.090220703487557,20.541323873369723,22.78668088915824,21.080000833338396,22.14615845112076,20.476638004974866,20.543095263341243,20.798413752920258,21.388315621036284,20.616828991318062,20.684506284662493,20.99673543688987,20.756920690014745,20.745137866419626,20.574524346927106,20.617867885377443,20.650469436751585,21.518678020255834,20.561966058520234,20.57743224442246,24.979796727439744,21.167404313162336,21.0518080944755,20.610603510739953,20.965107471999968,21.50877701566704,20.54989563757409,20.659397031845504,20.4977378917475,23.085906719791048,20.6488814904779,22.408185217278948,21.39498834212902,20.572241028747335,20.637340678135235,20.686909811719815,20.740122043203247,23.102859056444167,20.494236280461475,20.646966458287203,20.905376638806175,20.608140994541145,20.544221897944453,20.544432567308416,20.679662113370345,20.64120817885306,20.593920044095025,20.48718701796842,22.624934853494622,20.555834818163156,20.60247171792161,20.768760770052417,20.66337694664915,20.537152026328886,20.5672123742733,20.506067248172457,20.588510201990665,20.58174277038659,20.65897896548118,20.69951642675701,20.455525913983067,20.57403914246621,20.565233158693225,20.6285618925528,20.84062090946383,20.527818865666863,20.595147589116394,20.61620568489621,20.55002377264116,20.747262327456696,20.619014733170772,20.535001807619714,20.62222717977735,21.136289646288276,20.809121893924097,20.476554527791333,20.65762683860045,20.48274288125025,23.623978880339408,20.67526853655823,20.46867017065463,20.47601059015513,20.882789202363465,21.074114529697702,20.575843729713377,20.627427186714037,20.53264111497821,20.46697179323695,20.636634155223167,20.65192288612823,20.47418368008217,20.466980378105156,21.205287226684224,20.564054438594354,20.54883196319523,21.697270285971285,20.489867724765666,20.500509393790576,20.599059129227445,21.570660392753563,20.470186549128712,20.663578947455978,20.920985991525992,20.528038360239613,20.564036924057362,20.991110925547684,20.668009121153453,20.8356539416626,20.60852673541646,20.749042945672144,20.514716490983183,21.21169955776547,20.601814670246622,20.568510793511585,20.612121924150802,20.563618540196288,20.734042393560124,20.62339778784805,20.833790362809854,20.786232852498014,21.00244165958196,21.588178282990224,21.16996666460531,22.398680340280606,21.560440522604402,20.48578576051694,20.548492899766206,20.758685885950882,20.92896375290048,20.639650769760294,20.944450124907387,21.63929062949511,20.57184206457029,20.64521432878451,20.515497892027973,20.802296173581528,20.520030796835627,20.730275366007383,21.693513508615013,21.333979980247364,21.152961560940994,20.55948044837833,23.90608032768527,20.62012996259986,22.63174601720551,20.914777892543754,20.724856944824722,20.741513335625953,20.584884210466925,20.79255818274615,20.71032817448802,20.547611805143855,20.527673875157422,20.62875556834807,20.58963878024203,20.74510726392559,20.614457125641664,20.541179871991496,22.012411615434015,20.535382676124257,20.4422298359448,20.5755434165576,20.52915382277061,20.744850471126444,20.56235907360418,20.499663656651204,20.528059463383975,20.62161986725286,20.521100107203917,20.57767286456997,21.5013394937677,20.627720697323696,20.593407951394834,20.620792438267898,20.6243057655068,26.298997903318458,20.546842750813724,21.571479407368802,20.549606437702963,20.51516174330924,20.80798589819377,20.73481940644819,21.092725307712957,21.491119623618538,20.924814598254667,20.530578475455687,20.51052912806751,20.580457770077807,20.5313930425169,21.171005797041467,20.50720961389745,20.493903934301922,20.483302281866294,20.645425027665176,20.64140528936572,21.456409502888448,20.52593059283747,20.55181589361681,20.620112329375075,20.490382368753526,20.635579816955158,20.549254816619765,20.517464533825446,20.632022303513143,20.879195105269677,21.843378650979982,22.194856795256175,20.65329995746624,20.658412763687767,21.34297875373271,20.828831769831762,20.604699057081874,20.634758702952364,22.29282013741281,21.331527483388303,24.50458670622093,20.62235737658265,20.469810811311845,20.613920834275717,20.68977149051439,20.88329921349032,20.525592773979902,20.709406353626726,20.635716190576364,22.203228052757872,20.628970382144697,20.50582252409198,20.48707107966447,20.650646381049626,20.722913480037118,20.631884889782174,20.54677890112434,21.135625346429126,20.684120970478826,20.6565385751195,20.56019883505498,20.753304794053015,20.6864353219298,21.185941788305026,20.638775796966247,20.59943831086288,20.55874826531889,20.585330205284393,20.891804943271268,20.666031299631157,20.636296420712004,20.592662436279188,20.625348106072142,20.639916853223347,20.46158087496661,20.539360574744062,21.44927833271407,20.521968963479903,20.474227602190698,21.51852189399043,20.513626908152773,20.53262733053671,20.49318286539302,20.58843666402457,21.196282909482782,20.498255634870862,20.602251480285158,20.865073572701803,20.686589354728337,20.710293726874156,20.618090266092025,21.011259609965137,20.569740087748798,20.518945962639073,20.641797304482328,20.62926097681614,20.6017711304532,27.488623487102018,20.528508792548468,20.88248312044174,23.010644966793674,21.2037195178412,21.31981654949453,20.55424588237077,21.12315556055401,20.98356408644349,20.62461057049527,20.902387959436986,20.85012886582474,20.64118933380424,20.77805135545197,20.52644473877676,20.731293465476792,20.49551982936357,21.03098791171711,20.566242105058727,20.53692400335358,20.469555309280466,20.559419244834743,20.567452994420808,20.451030218006544,20.623242069313658,20.74856131588158,21.301587728401444,20.89570213308555,20.615161038816023,20.560243642790322,20.691769363294128,20.705059150544162,21.107656900914222,20.69066804087036,20.736053518682706,20.593853824942638,20.700310849015644,20.529823681354657,20.49560469233019,20.8310946697205,21.458580058083918,20.842840404885223,20.577368286511483,20.642532725218494,22.230677677223657,20.729250891438365,20.516019775397165,20.86952165265339,20.62391117596296,22.968584946802828,20.60268342453506,20.707806412082906,22.262197319471866,26.372855872361413,20.716117587179227,22.072605397877513,20.683352862902787,20.59983392210941,21.453033465054894,20.699343816824044,20.545886779411884,20.560990675690395,20.905954337465793,21.415223779548892,20.48635111220337,20.879852908807425,20.588461959743583,20.648786273351767,20.659538587442647,21.07818288723749,20.46164254042665,20.489715848870574,21.633808733296387,21.963313803966738,20.59028236523021,20.71463707467556,20.481854484894004,20.828944110408596,20.54272054133871,21.518427536512124,20.632615723682385,20.593204562841738,20.777367872820932,20.665100039573606,20.66366700193721,20.573166471346156,20.649912907954977,20.82454332715397,20.54801511167927,20.748498121177143,21.94033291905127,20.865113828293108,21.75034670282926,21.15314838167073,20.851818717865054,20.566780567852568,20.744362150579487,21.38623884650239,20.742108259438158,20.771532367603694,22.105915830150483,22.42820806098587,20.56900810609163,21.150128591452795,20.59687131502187,20.74622186841259,20.736146450622243,20.60896171034555,20.70158257543918,20.592306546642934,20.90082046983329,22.556816077801116,20.957169129494336,20.487426569073673,20.566422218548638,20.60245461308424,21.37774401144006,20.666748290638203,20.512407475674394,20.5598160842497,22.20541228483811,20.734853442464576,20.540567940728998,20.490773402297297,20.500084300347698,20.81291510765891,20.943705155214765,20.473881792334293,20.996579806080362,21.050712924231544,20.48296299524386,25.916682245010218,20.57142154158263,20.578014431396717,20.401088244518583,21.067980461774127,20.585550199456957,20.46935475538966,20.61107176299951,21.35676882909041,20.7357647361761,21.06292542988369,20.75612754827709,20.888656262730137,20.627674917117073,20.519001261659284,20.72530275444195,20.687076271577368,20.545630776600643,20.53168960274238,21.047458105811973,20.608737256540085,20.945707417837202,25.69026032438571,21.078200331923288,20.674182274373205,20.611610132097823,20.663297755710474,20.65865363160253,21.086116370708012,22.0195502326617,21.598969724448423,20.80277805289531,20.622853562388055,20.577426256430297,20.589423651245163,20.686654568072168,20.65672430621189,20.580172684185854,20.41284755959633,20.526571023187067,21.133375430703172,20.59278876877307,20.524884574824874,20.620826452904296,20.664358148862082,20.56696520208099,20.50764905891097,20.55996508691048,20.78451185048586,23.720087818791903,20.494517742371567,21.00721837772359,20.52117508921836,20.575110335135232,20.970571945555765,20.478075555200498,20.863746887484297,20.516019031376445,21.187742355622376,20.685275013702487,20.600858711804573,20.795579835063474,20.59756977188664,20.7475615241956,20.808620317142775,20.559169299789072,20.603617844572813,21.20180068852796,21.48916155007723,20.84405977641108,21.32256358970821,20.64337836805645,20.54236754588535,20.64794399412111,20.507559465652694,20.53368886152663,20.71672369284952,20.51430977864474,20.640187258449476,20.595222628150655,27.498843357251175,21.051106000465346,20.785460505259483,20.6272351933305,20.53036780609172,20.55442610135138,22.57220271320552,22.193717598705987,20.815448617692976,20.6996192904936,20.935787053298156,20.6324191429281,20.742246760943907,20.565986662692932,20.697913376810725,20.583384783588016,20.511578394590355,20.540435330311457,20.50243755048134,20.47864447605677,21.62090328454058,20.566054688312317,20.530551214711295,20.529978757691097,20.556602506242626,20.538242498663728,20.53527548192791,20.504655380237505,22.190572967298866,20.67941412114768,20.98040948802207,20.879137127448747,20.54090457440869,21.060932794380705,20.553900720288528,20.895998001282614,20.660689306900746,20.66044967627194,20.463565420076954,21.057044275188133,21.380998274137237,21.026313559425947,21.070036297700586,20.73507574670061,20.51036085785973,20.65401033614607,20.558290923362424,20.605647306603863,21.74637763141346,20.52261563293655,20.534083655911555,20.71959042190525,20.668918890321724,20.59491292003927,27.829161721981368,20.895081116282242,21.45489977077182,20.761139544902303,21.117876771063383,20.702795806251647,21.055421576302873,20.680096380977457,20.720359469825922,20.533301683790775,21.304750086461194,20.528203285690594,20.522167468696267,20.72837123516951,20.638928039813916,20.565235127468505,23.60046258705815,20.62980859512644,20.620622303361614,20.722229572150123,20.600357839922022,20.646032246330577,20.561287499135194,20.75592200899584,20.52579550173648,20.574390079145733,20.588768646721665,20.602098634755922,20.72852892962123,20.50310892232051,20.577111581653188,20.49543323732535,21.49321163695889,20.629151114824513,20.648760782201126,20.934584140674648,24.200974204839365,21.100769510115356,21.562365354964978,20.766296304575498,21.871094040837825,20.4823960322989,20.5109552190692,20.73857597868823,21.974024047132144,20.58734746876514,20.95142334038279,21.032445296943287,26.528907629461486,20.49773960267833,20.477147188220673,20.6053929504162,22.24365939418257,20.522825917615126,20.63396136992811,20.496953649829898,20.465925156346547,20.590980595887654,20.973053668378437,20.612674483233402,20.654693095970174,23.928139513155386,22.890546934532598,20.526573355293987,20.504502571904403,21.682640024332102,20.549318396093252,21.314857368693588,20.767747373911394,20.506807594277163,20.737830855443704,20.73777523221956,20.608653390949097,21.592427481782273,20.58548731501491,20.477728039406564,20.616839240664163,20.66106070116516,21.097575105835805,20.514842954898715,20.825831556898905,20.544521980669458,20.600454452460976,20.679472420947754,20.709243024642742,20.545233579909645,20.679837785232085,20.60049367282449,20.488660362068494,20.8165397711205,20.51121895859478,22.936715173214886,20.764479749269697,22.002191745284854,22.155417130001943,20.582086676493773,20.48644853800356,20.795914781874057,20.57583824802275,20.50755270619221,20.82488727324781,20.652115538056172,20.67128057131432,20.49012052165665,20.717738414625448,20.50822669786398,20.54974521676132,20.682934405675212,20.791941153727002,20.89020503246732,20.552577200762855,20.644454089544222,22.28814654726663,20.529729159259958,20.6051841299933,20.526506095034172,20.5126574206305,20.574220961638776,21.175628915353293,20.619090203462967,20.621665019633014,21.688055064954487,21.30370772390925,20.68634109664134,20.992536359100367,20.581930604452214,22.02110459653262,21.549974045459606,20.599650258033343,20.570919650943182,21.03192064427282,20.515149036820443,20.661915106057982,20.57995993525111,20.58456413290319,20.648258219491552,21.182475266312938,20.67191827936396,20.686396493276174,20.74468350271199,20.713873130135838,21.485485012007562,20.958969280069233,20.613436636714198,20.485661422299692,21.403857548888993,20.70067578851487,20.64905382167942,20.764305874303624,23.143414278365714,20.93187143212968,20.522672848239477,20.53403489176706,20.766227259718434,20.74904166448205,20.461960728223442,21.57105598336304,20.504460219380285,21.1780142675228,20.774525744452784,20.494015626834745,20.913104751302935,21.200776711034667,20.49578950983156,20.766121514905844,21.897719561520603,20.556455611783772,20.452447543833305,22.513072956338995,20.879790317324225,81.20301465064941,20.528023084360633,20.566314653108456,20.535607129929723,30.050524992787913,20.520748998216675,20.480076413693986,21.04172166197526,20.612335413165795,20.460235933059945,20.484603879411168,20.57466434031776,20.66363035016842,21.049944568049664,20.6207831134593,20.510697189295946,20.5460522091035,20.946958131893428,20.673227514848328,21.560062058564338,20.970380430005672,21.59301929779179,20.561907959345703,20.64490778951657,20.940882856415516,20.52028310572451,20.639182932621942,20.550937087237557,20.53214131196619,20.70503353308229,20.940289565726204,20.533645447304764,20.56059755677067,20.699880204913608,20.60874264224545,20.536312925404815,20.652355948956803,20.577565745303065,20.574733086733062,20.51172885821905,20.559538266242413,21.373472306283137,20.752113021550944,20.57425134755752,21.506413188403496,20.988891123953227,20.804093646779446,20.55565033498127,21.190063767383293,20.49787990473018,20.49340313060779,21.985344395755018,20.595640062386117,20.910106645205744,20.537207967862877,20.716214360620842,20.532380924502306,21.18740227381742,20.661850989577257,21.270356036031007,21.649510499644272,21.484184710203788,20.707980774050153,20.84558792414498,20.655408127819467,20.99877865500044,20.93428836390439,25.12283506317595,20.55653109339802,20.595140342399567,20.53883668498232,20.77037941266187,20.752414632547257,20.467748533066285,20.724119526403456,20.724093000285,20.58609601106313,20.97258555150152,20.639699184307826,20.654172585165483,20.77930514900215,21.868090627594405,20.50903355506344,20.578500096941173,22.451751183941486,20.692328568478548,21.532922958203198,20.49507964558682,21.02864234880097,20.705400508498055,20.773464520116082,20.470312977519903,20.54517813797546,20.605782626490658,20.55016493929925,20.45184652840413,20.777967244060555,21.24652191687532,20.730837249898585,20.502187605525233,20.67625065860313,20.934230254758226,21.602310025664174,21.33511253502376,20.767046139443817,20.609342946577556,20.54824482152368,20.56870289790835,21.58586847450892,20.490201199594686,20.65111502417244,20.733638321748465,20.490439497556817,20.546566424243917,20.623423946269916,20.58903683167024,21.199246694813368,20.621012756258715,22.542326488108486,20.6384200295828,20.597066424157802,21.078405345501206,20.979400565837913,20.63714464846814,20.53215890304269,21.53149046121455,20.6087807963335,20.532293454159703,20.547298405901085,20.51227228792468,21.20228656587476,20.873147726465863,21.825908331318455,20.764547372898672,21.623412828974626,23.966415928947725,20.46278128306105,20.518929848814683,20.534940610046775,20.561679937364566,23.252117280798373,20.70514380034634,21.030480490467802,20.578386645558407,20.59299290812969,20.54856393420272,20.609752728309935,21.022947538693906,21.001578689175187,20.545485305172324,20.547540908349326,20.50545132130121,20.649225957528706,20.526119281645663,21.02335143710649,20.572908439093347,20.489242231676712,21.13856771682808,20.83111875991817,20.465665191550407],"xaxis":"x","y":[22.06759946491166,20.51569310181126,19.61997633871636,25.808363100543477,21.795376715513594,20.34038041027431,18.86081419340691,24.078413808713293,19.082528830592434,27.09570141066156,18.66432922183982,20.146924115160264,20.694537918678005,20.34827538264879,22.1287945428413,23.512029578625103,23.710849830139047,21.16813940491307,24.138707165857692,20.413583384325577,20.971944131576173,19.330306789697467,19.240535149076692,19.593384739494034,23.207168669114285,20.243485920599927,24.816169078762734,20.239179241482244,21.93825114094642,22.544699771649725,20.581001696999003,19.006769325349232,22.852829884665343,20.97322158881004,21.37041284025584,19.8813455186455,22.291944364905724,19.352118206703615,20.690574372087426,19.616726513482636,21.73777568014623,25.92407619036082,19.62630225708032,24.328668011629535,28.119486884464127,23.268620237666717,24.937914667295647,20.538966591972148,26.49472805759287,21.551656870776874,19.967005268430295,20.53263058829885,19.077522473826907,20.411222614242394,20.118559661298907,21.75794824506378,21.969944344762656,20.941991359524017,24.30144839882211,20.08875201356742,20.413158139175476,17.21138734793915,20.10520261649224,21.318516744600927,21.383177447413818,18.901120542977083,21.140418528305776,22.317825778768146,19.694551733623157,24.452943059423166,19.34725801943171,19.360313431748278,21.62865690143549,18.725486086935668,19.22798647641259,22.00766259568964,23.581082748790354,19.483945640419044,19.555879359056263,20.81759187582515,20.004050690864457,19.7027197284411,19.957005833266606,18.935012496616167,21.132460035639845,21.104713276655342,21.0706503248077,19.995538068644624,20.45373482710394,20.377850211599817,24.218899085058613,23.648191447276112,19.27757204338643,20.547353262390168,22.570203152455143,28.033188230056272,20.740147099081785,24.844950181647548,21.11139041842472,21.618102712310883,20.71863237230781,19.638742582781422,19.346013720701006,21.12828261562749,21.344903224909334,19.828653485294133,22.842407702796528,22.86005581934068,20.23340552018178,22.244828354517683,19.8468416379525,20.153224049031966,23.16226531069458,22.47425899129698,20.440974576504654,21.34321011850724,21.96825774467718,19.581570228112746,22.97307940319244,21.943790024620068,24.922685338201692,18.397908557303648,21.170757004644493,19.439732238591404,19.06759979313172,19.78225599625029,20.88710307988011,20.06163488065717,21.227834779401228,20.925901548056537,19.64120627043042,21.51013491039007,19.943833745099443,20.041179196484592,19.31991683554051,21.305505708300736,19.083972676252298,22.36486950758078,21.00153722158492,20.91944021038121,19.71891096707218,20.43515061037962,23.756601825433552,19.29612083176196,27.288645329988054,20.004627845911102,20.581937941283943,23.06049210432163,19.0283827986455,22.26292897661663,18.98289333455679,19.08654493081137,20.26723932452398,26.772642502942784,23.150894170391226,22.789865927709045,21.316932785410458,21.47925775839002,19.52466856143405,17.31416721288844,19.67390026636621,21.280848347151327,20.575951881264743,19.36981390700933,22.906270958120995,23.259273464150183,21.034942664057727,19.478866371959384,21.521316718605924,23.251480243552532,18.947642644803835,18.234449818345436,23.74556559722114,23.70046067341344,22.17893043543761,23.314842649926593,19.204628901280135,21.028986544289005,19.811383905702034,20.105414612220844,23.470671408984025,19.185541351838207,21.863096949082852,21.439837984160558,20.28595076567891,20.09465504412721,24.347469240771108,21.026538976608304,22.25115601355335,19.17189511189754,19.330727902738385,20.455200776646638,18.7812284253763,19.9984664758684,19.71179802639097,23.222688495940556,20.75461515785591,21.453079563108336,19.968121134960665,23.31621219099147,22.229186273778243,19.29157093266521,19.725980900417238,19.393396349595932,19.510476738006748,23.86936283890287,20.803398322957783,19.50445726786541,20.266025864180154,20.318803159054653,19.88287753302219,21.755410336588376,20.021014007950576,19.81464925130666,19.600081112964823,20.195578868662224,22.127386699288984,22.615494713832,21.994462034082755,23.008367409641956,20.02036350615788,19.496913617064063,19.65284705132002,22.970383373621917,19.800442868861996,19.544786543412965,21.196126618068195,22.706377974056313,25.14188655199703,19.430343691015025,20.82549425312562,21.300868648362627,20.935011057360228,20.238709984208988,23.361271159574002,23.97311707542301,20.93275451807793,19.135842027978637,19.419925212472517,22.69657897049395,18.04707863542646,21.639091593296747,24.902956648700677,19.472341179018663,19.630944296038688,19.581570228112746,0,21.279464358226686,20.6373536565784,20.55168503037007,22.252924089212918,17.011336466938804,20.539904719671835,20.287228770242784,22.579718287600105,20.016747429777542,19.919862789535355,21.590649178257912,20.459085985918723,19.305685493406397,20.69934199839951,21.814001605688915,21.49724409211446,17.96562762596234,18.897059576214055,20.218149025767367,21.663202256858984,21.537426968242833,19.881144027962787,18.109927551497627,20.650313109616484,19.551649361459194,18.954616641321604,18.33673536238825,19.272777840955303,21.22721049508145,23.36907437848721,21.839279632705356,20.080313988880153,19.791725695449298,22.830034474576443,20.038176858751854,19.882805718288704,19.137324212876308,19.89611063211052,20.922839608419043,22.73108571789678,18.798041102461394,19.809989451392685,21.628482474975726,22.66651647870706,20.105565995835796,21.324104166055236,23.33275256138136,20.80501366619404,18.787924871827784,20.381292625866948,23.456138303517488,20.59828494694422,20.938551777966236,22.709886920964298,20.558379254361796,19.758613410846518,21.618914296187317,22.889842311610867,21.30122097904847,19.0336054859068,23.801438495307856,24.840309004310797,19.694954646749302,20.944503948247828,20.620570884389835,21.72592495902031,20.296054717472362,19.58398989769835,20.2956272827196,21.085706497425413,19.461766940454588,19.29900449887296,22.510563057499144,20.841106100432793,21.801619166438428,21.147132242697182,21.551582010518878,20.4427602890876,24.611415306054898,21.490696787354505,21.24369655260969,22.432940196049124,22.234424235191682,20.099462108962847,20.800416502964755,20.17949699394329,21.746512886013605,22.496434708234126,19.78606039431591,22.229470444639624,19.19842345841963,21.071706539354626,21.28737646300594,20.120777025175872,23.69806207100269,24.25287711008343,19.074352191519743,20.558351575498786,21.437400189861094,20.352963298343866,21.233800456337935,22.14457282995132,20.722700819009958,20.69715285443839,20.03253125195289,20.0566191944814,22.70946891023392,21.137414277628565,19.808943756196857,20.143175790739598,21.040198736231254,19.712186074557728,19.35146728073557,20.861088989018167,20.755317114550273,0,19.017540081468233,21.419330156983328,20.67873788317677,19.93668394257525,22.409766675366317,19.27540160418116,23.728782472563765,20.376074503254586,19.31077695697734,21.065895324749285,20.3484179021553,20.07860380095908,19.976548041795603,20.235086321270654,21.01213158467734,21.41999462819776,19.174863067555084,21.10241852261651,20.85643673732399,22.586773372950685,20.037769462388923,21.559823540829672,23.13793422908773,20.558605220150522,19.683189962828273,20.39789928981696,19.337397595787667,22.26889032385089,21.195293500127566,22.559724250840063,24.39611039895289,19.59271980995523,22.136011068345166,19.33796659591498,20.797749380419276,21.567058771175564,20.725266392758815,18.468910364525364,19.663251453070878,18.918473122196858,19.198971627350232,24.325766932844235,19.86193501563865,21.156628899315663,22.938277348217966,20.78722963747693,20.66031803587651,20.155379081002646,19.457981357627542,25.713720052658104,20.828559454854545,20.52544147367378,21.147722912842873,20.03302501817969,24.630356510701596,23.121909718158822,23.106213073613993,22.51080834568276,20.883881259335507,21.661257500272384,19.85555820528158,19.526632957320885,19.191856386440307,22.96557469826004,19.96989517504386,19.756880613939327,20.841336970061143,21.615161729704354,19.630957845101975,19.107266091841186,18.360084050573274,20.178689314774683,22.61224657942639,20.902818910258993,19.692513024244924,21.22795725431778,21.431836012474292,22.58649267850949,23.56945490190484,22.15739813287633,19.523673217064264,23.682062419502355,22.448170385491473,23.624330847094043,20.4998468600676,19.29906600505008,21.469193647687533,22.680649469911085,21.178626711012377,21.686546691263366,21.428479917582088,22.88912263482666,20.804770608680126,19.28855746111665,20.502951107445575,20.22322663992422,22.31457162800466,19.478115876564008,18.795995233915015,25.17177163891695,23.160406649511106,19.25160491688623,20.826937118763155,19.953135265171547,23.933068642871486,19.355480148534408,20.44485249695551,19.56616125697471,25.23181389258149,21.345469949872594,22.776042002687795,25.303110182218393,19.58463718752616,20.204391739449186,19.893828839264728,20.33821258053668,24.68330601022857,18.821811344100055,19.840511525069328,22.297566604177444,20.78875924862035,19.741131185755275,20.04722528013383,22.468811326543563,19.99457856414159,21.80053568878591,19.167558108393887,23.846962977749094,19.07645844042533,19.307592868706436,19.883803436319816,19.767052898077655,18.952911838336565,18.62774549084819,20.73926969664862,18.627786784209636,19.177777189052396,20.089958597754855,20.801380865239704,17.764101156137162,19.11561871251436,19.24667566091232,20.93405408413005,21.340147837597844,21.090725928345268,19.745386795650543,19.095931371701568,19.810878030088894,20.251155291034372,19.165221068220944,20.724802969675316,19.74733497869572,22.856134451929176,20.542598325656595,19.502159734210206,21.979041347822633,21.2197623270523,26.969103234931445,20.18342033833375,21.02499268359878,20.592440384261195,19.990444456103596,20.867172390851817,21.273367636241293,21.243839329097405,21.363500625759567,20.18486444945059,20.241113097705927,22.693372098426487,18.623284957897813,19.205602363695707,21.545170940853957,20.80034066129298,19.981973707277767,22.19000668879013,22.718490023684353,20.707922743251714,19.214188218113872,22.472827160217914,19.07108521476948,20.346151863763343,21.63013067480897,18.214892995560163,19.93614600441511,22.53034703709958,19.469859640957857,22.895450751816803,21.00807772677881,21.431258349952667,20.941808459871957,22.99407646829777,19.76111291920157,19.008644725628592,18.355858634806896,20.297451997659746,20.474380309584664,21.870871511939562,20.759740299921024,21.439000175436664,22.030880607811586,23.00587909853429,20.655014419256375,22.965146977089795,22.43523710461151,20.18471556395119,19.019883876304206,23.05070508803856,21.499214768467855,20.59203905710115,21.024704798577186,22.72181503272866,22.377050963043796,19.71871179715288,21.93139047566312,21.827309362120346,19.1796783064222,19.826388660339887,23.65560944108187,19.199993380834425,22.21417220655097,19.269925661522706,23.782692532330046,22.173362238186836,23.41808975500013,20.872940195293037,21.54216753533708,21.03393309328942,21.663729644062077,20.699641074284234,21.39606694375647,19.593416427729718,21.355622092522935,19.49242240707679,19.406777996109206,22.625652665035147,19.860111721120674,14.03702355520748,22.00972851632279,20.33705521659505,20.155471422765178,20.40843263005365,23.540959857497864,22.64902697475969,19.51901613478499,20.748003621067692,19.075332900397907,21.535462867247563,19.35190112925433,16.543892756530866,22.048803804340604,19.94957596782787,23.253137677025506,15.802066947826521,22.01816825105002,24.65495079669392,19.684337709357063,23.07232514287102,19.441821622178967,20.90283362751463,21.65340972782181,20.672219865399406,22.248853822319084,22.007999054329076,21.514848387611945,18.91123104739614,22.195045634281605,21.092338092530998,19.27798585366196,21.725547245838506,20.321666107127033,18.844349286283478,20.211183384442457,20.06069065496676,20.388592288941165,25.880225035189923,19.68504303470752,20.795739259102447,19.874915797147626,20.484467680558765,21.537004350283745,19.454353518988064,20.471143313052202,20.221414835639845,21.30681378731033,22.893554470389834,23.892417838743924,19.773417735905042,21.39811634221899,21.94580529932221,22.093666756982273,19.566140232030243,19.62938152434226,22.598971957007752,22.606867611519395,23.548837661891714,19.57985452716288,19.257215175267675,20.260215495083163,21.269107840654502,21.892874617975046,20.911342976063032,20.690267441816314,19.415723203109316,22.536424619535012,19.241608485058645,19.162579413291002,20.470409642279566,20.97348174432658,23.73507977629211,14.55836091863428,23.336410397420998,21.627932359551377,21.49391553553141,20.265469687416747,19.426854398695138,20.625264709408093,20.223930403747513,21.865935415382868,21.881578906408237,20.14846091060974,19.81407448278221,21.0024847282008,20.945391681085823,22.291777470342872,20.551017265346704,19.577722584113683,19.444205825873514,21.46402517777047,18.277738745265133,18.799932943985397,21.60163932304092,19.80978333592071,20.91463622707276,21.547692385839394,19.17999258814081,19.549428996570953,19.54492482512454,19.294068756456163,21.69613129076446,20.048671453648097,19.49098989799628,20.42092755989845,20.047998538616795,22.735567798975932,20.159778968313912,20.920832580620292,19.699545342858148,20.678206389277292,20.079358473730228,19.917597162486278,19.692880228017533,26.26193365893149,20.33327111425421,21.201441496966886,24.081396558763124,22.374129418340807,22.181711726885904,18.957794711482784,21.63591845810156,22.742154187536123,20.277557571355647,21.712167598440185,20.922319986303112,19.981118924422468,18.376324675581085,19.093096903348385,21.041684548715036,19.88267606965292,21.79470380164697,19.394929062748655,19.711931797206645,20.701777070204972,18.966254774482525,16.57594392051226,19.3661810523187,19.637857750008234,21.05876451742095,21.39188913879269,20.922214169866013,20.158015290040524,19.840625811644227,20.287989620862316,20.392027938746434,21.440477364164977,20.18838178436931,19.209224880205603,19.523793560708743,19.767710849855263,21.57006553499441,20.748925773432315,21.47821316674072,24.169496968680054,22.350651659007692,22.60440595404428,20.973185759004682,22.891726790683144,20.2096151875794,20.22349858267661,20.901209408549594,19.053706845403045,23.000908813982363,19.549547798448483,22.637005220660473,23.38302130821851,25.236220259743373,22.107609158967936,23.256395298587126,21.31115959043422,19.242152996635603,23.069120127461606,19.88568939220856,20.74441094675746,20.977820033541025,21.403421935393933,23.155520679582406,17.74375624808358,21.65558749657167,22.72177363339029,20.057436191229318,20.31682150206961,21.442049003709382,20.447439187365134,18.64679105166152,23.081069556252004,23.109422435163253,19.40981146751902,21.538742146089504,20.451115883369862,20.520204300445414,19.072268708954628,21.853488891111965,19.870759032200375,19.874160110175985,20.45611373193672,20.923455889638944,15.423378576518493,19.495183602853512,20.532198000108426,20.172754664219987,19.621134659665625,21.149770683357705,25.5492833010917,20.91962424027046,23.144648130459657,22.724819827816283,20.76732896720974,20.90466916348112,21.019251701774245,21.870062515886435,21.968529606994224,18.28303322918483,22.7967578369547,24.617902294510255,20.337523908425865,22.278345092825212,20.439903152029967,20.745981176528062,20.50317548758169,20.148792615003963,20.18507504172347,19.887802201303376,21.335667578998795,23.547889563240428,22.184719195487645,19.124759964560997,19.09456742423413,23.546328229722338,23.676810419954805,21.378549935187863,18.772971741326142,20.92343911431291,23.66501619396984,20.362922084924325,19.335122075152558,20.6322831501737,22.014406532680294,20.862569257472042,22.377941930707426,16.70615872384773,21.114598939015725,25.312874128977917,19.557332226454086,25.0888235331049,0,20.821375100396438,19.69889252664812,20.730065203035867,20.737794346856816,19.720177445507503,19.751886432327307,21.436234720492326,19.12391425698277,21.093445290401018,22.18427320470571,23.32578333060166,20.235023779555554,20.38834949087981,19.076215392899233,21.890199695637197,19.626240196846272,19.473911816076555,22.65051997686549,21.07153535807478,20.152641197123305,25.817249936117022,20.72049143867345,20.000328449472278,19.677575133698696,19.854767962698624,20.90714373186156,21.852326049162542,22.34522357506863,21.88381079939571,20.64279914121781,20.008240359915625,20.079932477068777,20.97646133780068,20.617037831360907,20.86340137149202,20.10297814136792,18.939248297157278,19.337083944174847,21.21789500066683,19.959413629413806,20.57158693763026,20.1483293247843,22.45828794986169,19.223167412657357,22.090969450288238,20.54580254002858,20.99006223015013,23.933428716720645,18.97273303540371,21.839004592901738,17.66325927218184,21.098649782686852,22.108159109401438,19.70668845124008,20.687407722542194,21.23800756191085,22.54894428694912,20.440949745295235,18.136670762341843,21.301837769324877,21.032952737462605,23.31051349089806,20.969732200565332,20.351613259246466,19.678510738089678,22.059041559037812,22.10097025846529,21.127912480118983,24.425354799557958,19.405573621751774,18.954513256380405,20.62820521809647,19.21222562648202,18.9881876444261,21.62524006847279,18.78833918294514,19.827821621310783,19.4644839297056,26.34702224299619,21.38205653661138,21.78972267747008,21.087755789009996,18.999355953219133,19.451729034530373,23.289040906826244,24.16646354231811,21.536078922759724,21.530906360636845,20.97243154975957,20.089266034468004,20.436904665975636,19.20088663487939,20.554011225634962,19.277982733151024,18.304477915148002,19.118363214083377,20.261720490850706,19.26508618967162,23.063753511610845,19.05336380976314,19.240852168960235,19.807795916443087,19.527755462443025,19.369940873414674,18.557194427644223,19.092216129486825,23.457850638636746,20.3986497634222,20.79850265219969,20.611470634120458,21.03390016791011,24.957468167964976,19.132327452939787,22.668945885967275,20.192548076466373,21.926818619899166,21.236410750491928,22.00439862331522,22.64890259783158,19.876543924901863,22.174820865681593,20.001613842126076,20.63081635106876,19.653189714082398,19.305175301710232,20.224827212369068,22.6279649673046,21.547003936026666,18.961602303420257,20.695469281090816,20.114481067581714,20.790519525351012,26.143565404950515,21.151917099556776,22.601975102609295,18.386700720994313,21.48658316808204,19.99564295418165,22.704086007044513,23.646718002570474,20.17407123394833,20.29544946282625,21.94083977054476,19.087820197169854,20.49274951238928,21.58556546053215,19.868098908707033,19.806349800221792,23.600603274171462,19.846459862589864,21.099407208256782,21.035986420389108,23.747615973251953,21.078530347858226,20.86782723578085,20.073998999202523,19.385623092000827,20.214357069981805,21.756421739524402,19.573874587511387,22.17732889464112,20.092049623606254,19.621522497700703,20.13212471121583,22.84612630027242,21.610638293924836,20.381527241115613,22.69863600121815,24.241029788998915,23.65992231174119,22.296121251970213,20.702441582535556,22.897176211133427,19.850331267520566,17.677965293255863,20.87727804782974,24.045999647847655,20.173501472804098,23.33182107390064,23.30215527163116,27.231751620760654,20.650611348398325,16.829816883719186,21.108904064764797,23.735782522109897,20.656510624448828,20.096689421256738,22.83710046840733,19.824675810238606,20.823945313943256,23.256137551486724,23.633181830866906,19.043844337217745,23.65829637497728,23.50747794569296,21.73683233585413,20.94197381362215,24.18225862695574,20.14794970691058,22.344681402352435,20.447944597167705,19.21271648764287,22.508853453023672,21.75045501313216,20.060863127783303,23.041170672161062,19.554654657325607,21.26551890511897,19.47575297115503,20.304093087106327,21.96980947524255,18.890513667640615,20.207528404900692,20.825342365179193,19.58596744655146,19.86547955920327,20.093122217042076,19.51138012046539,20.89052275919248,19.726434174485984,19.26180569731426,21.223090796665396,20.071086524480997,23.60048678836413,21.682830461889793,22.115896159241377,23.245004397316144,19.929597223002617,20.206849822052806,20.80607598372765,20.972697281096956,20.548932178877266,20.790175259909486,21.65577788956451,20.396533708238668,17.677595133772794,20.206889160267256,19.864164508631692,20.604884938737765,19.69979302428942,21.112079923463828,21.776216944868956,19.41019874513054,20.084542414131516,25.538294718669484,19.887154874828134,22.02600227823102,20.06242269100319,19.908561207609715,19.025787914622292,22.519912388376707,21.15310774495207,15.434839951572775,23.416469489833915,21.472713147192902,24.799241151178187,21.565777300167408,21.24511744232927,23.786142675104543,21.775846548503456,20.874095085342866,20.525903087310763,22.190296420947053,17.526091919071504,22.787613064091445,19.95220428650718,21.56191951932416,20.302727045932432,22.39076356657128,17.788361488629835,19.89495594402764,20.849863964556313,22.01047671458767,21.9419703223203,21.769327477492673,22.134078601621123,23.02213648046312,21.659326126254854,22.503975157123477,21.687400780796548,21.299913824117656,24.41218042654058,21.628416169590647,20.661666774415252,19.13829898996108,20.6534474504661,20.599627769624064,19.476347089125902,22.633338504709375,19.004867830543457,20.729309561171004,21.215021611787833,20.55962915472571,23.96741801447904,20.948249582299137,19.034944006994706,21.25562508835354,23.6936808146177,18.876298103633747,20.12057913956066,22.988276940898057,20.761017121909465,29.702932107840674,19.34717798381203,19.04780522125321,19.72022056098374,25.87586915107988,22.381251164844315,21.45471998887307,21.894206057556605,20.997149152437558,19.346653316091935,19.69023397394279,21.75892392666085,21.695257182632144,24.304938166770185,19.96312680068985,21.08058139373211,18.457821713581545,21.058645462338415,20.370456476417175,22.39756222611425,21.96355600303241,24.29338064398619,19.533504643608662,19.24597971762759,21.061479969798718,19.653131971320136,19.991828535916927,19.41354451956652,19.024392506006556,19.995956185326282,21.080207924079772,22.895336936087247,19.00379513256332,19.59081933226322,19.683155687698772,19.724580465382388,20.49815774428782,21.41646356230674,20.374729613814893,19.698149844649336,20.827570650601533,21.55980522344904,20.203106297950406,20.998113148879906,22.5464416348448,21.94562707940607,20.109117749689304,19.61600010557632,22.28815948936714,20.00070564420032,19.48773603183014,22.96164822427184,18.99960834234589,20.932965503614184,18.548961097016324,21.40256127224899,23.343368321662904,21.395751936133603,20.367816242708102,21.629837063073623,22.815206783859768,22.704911104222457,20.7857551664019,20.3067025549342,20.61336030755643,21.781868242254394,20.56504163001677,24.58750691743097,21.042555130756018,19.718574682047347,20.82900495573487,21.89094777167843,20.46100849412487,20.676958661968467,22.376303078727783,21.96712491711981,21.09072172933059,20.373076483151994,19.53720891947194,19.37233862557095,0,22.656078929958284,19.971799365414682,21.138173662648782,25.881734625061707,0,22.042981377352834,19.12600868541295,20.929534739770034,20.604603448464516,21.393924057871644,18.99682101244613,19.31053914628864,18.986258705756388,16.7233596191811,19.687016476370815,20.364732076794134,18.828527980887348,21.061860954738233,18.824250825082274,19.28149315085171,21.023652845270533,23.394529944648035,22.371116977331784,21.19573925103901,18.93658618322981,19.94401710262461,19.47904998638186,22.850840758637354,20.76699650557159,20.40036143331872,17.795936296212627,19.421904563712328,20.64673096490357,19.639706353282477,18.775317705171794,20.047574770414855,19.61260734468754,23.666636204423178,23.480370137373004,19.218266855752937,21.60748880425466,21.257489419039537,21.101232900314493,21.320633929890864,21.849984955018474,20.40434121243742,20.032547562218568,20.035932572983462,20.864093173595,21.51301334156099,22.29182284846925,23.44987910374218,20.855425894539348,22.64951341150128,23.894186440194584,20.26686372980413,20.408744363117133,19.046718202293015,20.904214174162476,24.308788888311135,20.77592509493985,0,19.644171392658865,18.610921424553307,22.26726247442381,20.903586582709064,21.2082248307893,20.867431932395597,19.247883485213144,19.09858987177298,20.769205079804532,21.73660672493293,20.24751150890443,21.281868210231526,19.724170230709305,20.553728301262954,21.112413416140527,20.46206885900041,20.668149201097204],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge_log = np.logspace(-3, 5, 1000)\n","param_gridRidge_log = {'ridge__alpha': alphasridge_log}\n","\n","GridRidge_log, \\\n","BestParametresRidge_log, \\\n","ScoresRidge_log, \\\n","SiteEnergyUse_pred_logRidge_log, \\\n","figRidge_log = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=SiteEnergyUse_train_log,\n","                            y_test=SiteEnergyUse_test_log,\n","                            y_test_name='SiteEnergyUse_test_log',\n","                            y_pred_name='SiteEnergyUse_pred_logRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge_log)\n","\n","print(BestParametresRidge_log)\n","print(ScoresRidge_log)\n","figRidge_log.show()\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.7037873147466898,1.7037873144896931,1.7037873142279132,1.703787313961262,1.7037873136896486,1.70378731341298,1.7037873131311627,1.703787312844101,1.7037873125516971,1.7037873122538514,1.7037873119504627,1.7037873116414282,1.7037873113266424,1.7037873110059985,1.703787310679387,1.7037873103466978,1.703787310007817,1.7037873096626295,1.7037873093110185,1.703787308952864,1.7037873085880435,1.7037873082164345,1.7037873078379095,1.70378730745234,1.7037873070595952,1.7037873066595417,1.7037873062520432,1.7037873058369608,1.7037873054141541,1.7037873049834786,1.7037873045447889,1.7037873040979346,1.7037873036427644,1.7037873031791235,1.7037873027068546,1.7037873022257966,1.703787301735786,1.7037873012366567,1.7037873007282383,1.7037873002103585,1.703787299682841,1.703787299145506,1.7037872985981721,1.7037872980406519,1.7037872974727564,1.703787296894292,1.7037872963050629,1.703787295704868,1.7037872950935036,1.7037872944707622,1.7037872938364313,1.7037872931902958,1.7037872925321356,1.7037872918617274,1.7037872911788425,1.7037872904832496,1.7037872897747122,1.7037872890529886,1.7037872883178338,1.703787287568998,1.703787286806227,1.7037872860292602,1.7037872852378346,1.7037872844316808,1.7037872836105243,1.703787282774087,1.7037872819220834,1.7037872810542243,1.7037872801702143,1.7037872792697535,1.7037872783525347,1.7037872774182474,1.7037872764665731,1.7037872754971886,1.7037872745097637,1.7037872735039634,1.7037872724794458,1.7037872714358617,1.7037872703728572,1.7037872692900706,1.7037872681871336,1.7037872670636713,1.703787265919302,1.7037872647536365,1.7037872635662779,1.7037872623568238,1.7037872611248617,1.7037872598699735,1.7037872585917326,1.703787257289704,1.7037872559634457,1.703787254612506,1.7037872532364262,1.7037872518347377,1.703787250406965,1.7037872489526222,1.7037872474712148,1.7037872459622396,1.703787244425183,1.7037872428595222,1.7037872412647261,1.7037872396402516,1.7037872379855468,1.7037872363000488,1.703787234583185,1.7037872328343713,1.7037872310530138,1.7037872292385061,1.703787227390232,1.7037872255075626,1.7037872235898583,1.7037872216364671,1.7037872196467252,1.7037872176199553,1.7037872155554692,1.7037872134525647,1.703787211310527,1.703787209128628,1.7037872069061255,1.7037872046422649,1.7037872023362755,1.703787199987374,1.703787197594762,1.7037871951576258,1.7037871926751371,1.703787190146452,1.7037871875707107,1.7037871849470378,1.7037871822745412,1.7037871795523127,1.7037871767794268,1.7037871739549406,1.7037871710778945,1.70378716814731,1.703787165162191,1.703787162121523,1.7037871590242724,1.7037871558693862,1.7037871526557917,1.703787149382397,1.7037871460480893,1.7037871426517353,1.7037871391921797,1.7037871356682477,1.7037871320787406,1.7037871284224386,1.7037871246980991,1.703787120904456,1.7037871170402195,1.7037871131040763,1.7037871090946886,1.7037871050106936,1.7037871008507026,1.7037870966133029,1.703787092297053,1.7037870879004864,1.703787083422108,1.7037870788603968,1.7037870742138015,1.703787069480743,1.703787064659613,1.703787059748772,1.703787054746552,1.7037870496512517,1.7037870444611403,1.7037870391744534,1.7037870337893946,1.7037870283041332,1.703787022716805,1.7037870170255112,1.7037870112283175,1.7037870053232542,1.703786999308314,1.7037869931814527,1.7037869869405877,1.7037869805835992,1.703786974108326,1.7037869675125676,1.7037869607940828,1.703786953950588,1.7037869469797589,1.703786939879225,1.7037869326465742,1.7037869252793485,1.7037869177750444,1.7037869101311123,1.7037869023449546,1.7037868944139247,1.703786886335329,1.7037868781064216,1.7037868697244067,1.7037868611864362,1.7037868524896083,1.703786843630969,1.7037868346075082,1.70378682541616,1.703786816053801,1.7037868065172508,1.7037867968032685,1.7037867869085552,1.703786776829748,1.703786766563423,1.7037867561060929,1.7037867454542042,1.7037867346041389,1.7037867235522104,1.7037867122946646,1.7037867008276772,1.7037866891473528,1.7037866772497234,1.7037866651307478,1.703786652786309,1.7037866402122144,1.7037866274041922,1.7037866143578924,1.7037866010688838,1.703786587532652,1.7037865737446005,1.7037865597000454,1.7037865453942174,1.703786530822257,1.7037865159792163,1.7037865008600535,1.7037864854596343,1.703786469772729,1.7037864537940106,1.7037864375180525,1.7037864209393283,1.7037864040522082,1.703786386850958,1.7037863693297368,1.7037863514825962,1.7037863333034753,1.703786314786203,1.7037862959244918,1.7037862767119385,1.70378625714202,1.703786237208093,1.70378621690339,1.7037861962210186,1.7037861751539585,1.7037861536950571,1.7037861318370318,1.703786109572463,1.7037860868937937,1.703786063793327,1.7037860402632223,1.7037860162954943,1.7037859918820093,1.703785967014482,1.7037859416844738,1.7037859158833897,1.7037858896024747,1.7037858628328113,1.7037858355653177,1.7037858077907413,1.7037857794996598,1.7037857506824754,1.7037857213294116,1.703785691430512,1.7037856609756343,1.703785629954448,1.7037855983564316,1.7037855661708683,1.7037855333868421,1.7037854999932356,1.7037854659787235,1.7037854313317724,1.703785396040633,1.7037853600933406,1.7037853234777063,1.7037852861813174,1.7037852481915288,1.7037852094954633,1.7037851700800029,1.7037851299317879,1.7037850890372113,1.7037850473824125,1.7037850049532755,1.7037849617354215,1.7037849177142061,1.703784872874714,1.7037848272017526,1.703784780679849,1.703784733293243,1.7037846850258827,1.7037846358614188,1.7037845857832,1.7037845347742664,1.7037844828173434,1.703784429894838,1.7037843759888307,1.703784321081071,1.7037842651529698,1.703784208185595,1.7037841501596638,1.7037840910555375,1.7037840308532126,1.7037839695323176,1.7037839070721028,1.7037838434514352,1.703783778648792,1.7037837126422517,1.7037836454094886,1.7037835769277627,1.7037835071739156,1.7037834361243605,1.703783363755074,1.7037832900415908,1.7037832149589929,1.7037831384819018,1.703783060584471,1.7037829812403769,1.70378290042281,1.7037828181044659,1.7037827342575373,1.703782648853703,1.7037825618641194,1.7037824732594118,1.703782383009664,1.7037822910844067,1.7037821974526117,1.7037821020826764,1.7037820049424177,1.7037819059990589,1.7037818052192197,1.7037817025689055,1.7037815980134958,1.7037814915177325,1.7037813830457087,1.7037812725608572,1.7037811600259385,1.7037810454030275,1.7037809286535015,1.70378080973803,1.703780688616557,1.7037805652482914,1.703780439591694,1.7037803116044608,1.7037801812435123,1.703780048464979,1.703779913224184,1.7037797754756334,1.7037796351729966,1.7037794922690945,1.7037793467158835,1.7037791984644393,1.7037790474649408,1.7037788936666547,1.7037787370179192,1.7037785774661263,1.7037784149577053,1.7037782494381055,1.703778080851779,1.703777909142163,1.7037777342516591,1.7037775561216182,1.70377737469232,1.7037771899029532,1.7037770016915974,1.7037768099952018,1.7037766147495668,1.7037764158893212,1.7037762133479035,1.703776007057541,1.7037757969492233,1.7037755829526873,1.7037753649963907,1.7037751430074894,1.7037749169118164,1.7037746866338555,1.7037744520967206,1.7037742132221294,1.7037739699303782,1.703773722140319,1.7037734697693323,1.7037732127333016,1.7037729509465873,1.7037726843219994,1.70377241277077,1.7037721362025278,1.7037718545252662,1.703771567645319,1.7037712754673269,1.7037709778942118,1.703770674827144,1.703770366165514,1.7037700518068994,1.7037697316470346,1.703769405579779,1.7037690734970827,1.7037687352889557,1.7037683908434336,1.7037680400465423,1.703767682782265,1.7037673189325062,1.7037669483770563,1.7037665709935546,1.7037661866574527,1.7037657952419782,1.703765396618095,1.7037649906544654,1.703764577217412,1.7037641561708754,1.7037637273763757,1.7037632906929727,1.7037628459772194,1.7037623930831252,1.7037619318621107,1.7037614621629635,1.703760983831795,1.7037604967119955,1.7037600006441873,1.7037594954661823,1.70375898101293,1.7037584571164743,1.7037579236059028,1.7037573803072998,1.703756827043695,1.7037562636350139,1.703755689898027,1.7037551056462994,1.703754510690135,1.7037539048365271,1.7037532878891024,1.703752659648067,1.7037520199101515,1.7037513684685528,1.7037507051128826,1.703750029629104,1.703749341799475,1.7037486414024925,1.7037479282128287,1.703747202001272,1.7037464625346672,1.7037457095758497,1.7037449428835871,1.7037441622125122,1.7037433673130615,1.7037425579314065,1.7037417338093916,1.7037408946844654,1.7037400402896132,1.703739170353289,1.7037382845993485,1.703737382746975,1.7037364645106146,1.7037355295998986,1.7037345777195785,1.7037336085694466,1.7037326218442677,1.7037316172337014,1.7037305944222285,1.7037295530890766,1.7037284929081402,1.7037274135479072,1.7037263146713788,1.703725195935992,1.7037240569935403,1.703722897490092,1.7037217170659127,1.703720515355382,1.7037192919869113,1.7037180465828634,1.7037167787594654,1.7037154881267302,1.7037141742883684,1.7037128368417034,1.7037114753775882,1.7037100894803179,1.7037086787275435,1.7037072426901845,1.7037057809323417,1.7037042930112105,1.70370277847699,1.7037012368727964,1.7036996677345726,1.7036980705910008,1.7036964449634096,1.7036947903656867,1.703693106304187,1.703691392277643,1.7036896477770742,1.703687872285694,1.7036860652788224,1.703684226223792,1.7036823545798576,1.7036804497981066,1.7036785113213657,1.7036765385841115,1.703674531012379,1.7036724880236704,1.703670409026867,1.7036682934221354,1.7036661406008407,1.703663949945455,1.7036617208294687,1.7036594526173043,1.7036571446642246,1.7036547963162465,1.7036524069100565,1.70364997577292,1.7036475022226,1.7036449855672704,1.7036424251054327,1.7036398201258334,1.7036371699073818,1.7036344737190696,1.703631730819892,1.7036289404587681,1.7036261018744647,1.7036232142955192,1.7036202769401683,1.7036172890162713,1.7036142497212428,1.70361115824198,1.703608013754795,1.7036048154253514,1.7036015624085976,1.703598253848704,1.703594888879006,1.703591466621941,1.7035879861889967,1.703584446680655,1.70358084718634,1.703577186784372,1.7035734645419165,1.7035696795149433,1.7035658307481842,1.7035619172750938,1.7035579381178134,1.7035538922871385,1.7035497787824891,1.7035455965918807,1.7035413446919012,1.70353702204769,1.7035326276129208,1.7035281603297847,1.7035236191289815,1.703519002929712,1.703514310639671,1.70350954115505,1.7035046933605376,1.7034997661293272,1.703494758323128,1.703489668792176,1.7034844963752558,1.7034792398997212,1.7034738981815187,1.7034684700252203,1.7034629542240567,1.7034573495599532,1.7034516548035743,1.7034458687143694,1.703439990040622,1.7034340175195066,1.7034279498771454,1.703421785828675,1.7034155240783115,1.7034091633194233,1.70340270223461,1.7033961394957822,1.7033894737642463,1.703382703690799,1.7033758279158178,1.7033688450693645,1.7033617537712877,1.703354552631335,1.7033472402492642,1.7033398152149641,1.7033322761085787,1.7033246215006386,1.7033168499521902,1.703308960014941,1.7033009502314003,1.7032928191350323,1.7032845652504112,1.7032761870933808,1.7032676831712252,1.703259051982839,1.703250292018906,1.703241401762088,1.7032323796872126,1.7032232242614742,1.7032139339446382,1.7032045071892525,1.7031949424408694,1.70318523813827,1.7031753927137014,1.7031654045931202,1.7031552721964431,1.7031449939378103,1.703134568225855,1.703123993463984,1.7031132680506698,1.7031023903797533,1.7030913588407564,1.7030801718192083,1.7030688276969865,1.7030573248526668,1.7030456616618914,1.7030338364977524,1.7030218477311867,1.7030096937313945,1.7029973728662697,1.7029848835028532,1.702972224007803,1.7029593927478892,1.7029463880905056,1.7029332084042124,1.702919852059295,1.7029063174283567,1.7028926028869336,1.7028787068141427,1.7028646275933557,1.7028503636129109,1.702835913266854,1.7028212749557199,1.702806447087345,1.7027914280777268,1.7027762163519191,1.7027608103449758,1.702745208502936,1.7027294092838585,1.7027134111589095,1.7026972126134976,1.7026808121484691,1.7026642082813566,1.7026473995476916,1.7026303845023758,1.7026131617211213,1.702595729801957,1.7025780873668068,1.7025602330631426,1.7025421655657111,1.7025238835783465,1.702505385835859,1.702486671106015,1.702467738191605,1.7024485859326035,1.7024292132084256,1.702409618940282,1.7023898020936392,1.7023697616807822,1.7023494967634902,1.7023290064558236,1.7023082899270257,1.7022873464045514,1.702266175177212,1.7022447755984509,1.702223147089752,1.7022012891441787,1.7021792013300558,1.70215688329479,1.702134334768838,1.702111555569822,1.7020885456068018,1.7020653048847003,1.7020418335088905,1.702018131689946,1.7019941997485617,1.7019700381206408,1.7019456473625625,1.7019210281566213,1.701896181316655,1.7018711077938538,1.7018458086827564,1.7018202852274438,1.7017945388279236,1.7017685710467156,1.7017423836156353,1.70171597844279,1.7016893576197725,1.701662523429076,1.7016354783517158,1.7016082250750713,1.7015807665009461,1.701553105753851,1.7015252461895123,1.7014971914036063,1.701468945240725,1.7014405118035711,1.7014118954623918,1.7013831008646456,1.7013541329449056,1.7013249969350113,1.7012956983744505,1.7012662431209953,1.701236637361577,1.7012068876234092,1.7011770007853582,1.701146984089562,1.7011168451533016,1.701086591981115,1.7010562329771701,1.7010257769578854,1.7009952331648002,1.7009646112777026,1.700933921427999,1.7009031742123437,1.7008723807065145,1.7008415524795393,1.7008107016080707,1.7007798406910077,1.700748982864366,1.7007181418163893,1.7006873318029072,1.7006565676629293,1.70062586483448,1.700595239370669,1.7005647079559907,1.7005342879228587,1.7005039972683584,1.7004738546712275,1.7004438795090508,1.7004140918756683,1.700384512598791,1.7003551632578222,1.70032606620187,1.700297244567961,1.7002687222994257,1.700240524164475,1.7002126757749387,1.7001852036051748,1.7001581350111241,1.7001314982495241,1.700105322497253,1.7000796378708052,1.7000544754458846,1.7000298672771095,1.7000058464178127,1.6999824469399325,1.6999597039539716,1.6999376536290263,1.699916333212864,1.6998957810520328,1.699876036611999,1.699857140497291,1.6998391344716381,1.6998220614780835,1.6998059656590623,1.6997908923764242,1.6997768882313824,1.6997640010843742,1.6997522800748093,1.6997417756406972,1.6997325395381204,1.6997246248605424,1.699718086057932,1.6997129789556698,1.699709360773228,1.6997072901425962,1.6997068271264275,1.6997080332358838,1.699710971448157,1.6997157062236377,1.699722303522708,1.6997308308221346,1.6997413571310283,1.699753953006352,1.699768690567944,1.6997856435130252,1.6998048871301705,1.699826498312707,1.6998505555715107,1.6998771390471756,1.6999063305215185,1.6999382134283891,1.6999728728637582,1.7000103955950436,1.7000508700696493,1.7000943864226745,1.7001410364837692,1.7001909137830906,1.700244113556335,1.7003007327488036,1.700360870018465,1.7004246257379858,1.7004921019956794,1.7005634025953473,1.7006386330549674,1.7007179006041901,1.7008013141806066,1.700888984424747,1.7009810236737657,1.701077545953775,1.7011786669707845,1.7012845041002016,1.7013951763748523,1.701510804471474,1.7016315106956408,1.7017574189650713,1.7018886547912728,1.702025345259477,1.7021676190068171,1.7023156061986984,1.7024694385033137,1.7026292490642472,1.7027951724711297,1.7029673447282732,1.7031459032212513,1.7033309866813622,1.7035227351479176,1.7037212899283136,1.7039267935558189,1.7041393897450248,1.7043592233449107,1.7045864402894562,1.70482118754575,1.7050636130595378,1.705313865698154,1.7055720951907722,1.7058384520659355,1.7061130875862875,1.7063961536804704,1.7066878028721253,1.7069881882059412,1.7072974631707076,1.7076157816193138,1.7079432976856552,1.7082801656983917,1.7086265400915253,1.7089825753117502,1.7093484257225449,1.7097242455049648,1.7101101885551238,1.7105064083783172,1.710913057979796,1.7113302897521514,1.711758255359327,1.7121971056172398,1.7126469903710295,1.7131080583689382,1.7135804571328586,1.7140643328255603,1.7145598301146516,1.7150670920333162,1.715586259837884,1.7161174728623063,1.7166608683696143,1.717216581400454,1.7177847446187933,1.7183654881549224,1.7189589394458735,1.7195652230733927,1.7201844605996228,1.7208167704006634,1.7214622674981794,1.722121063389259,1.7227932658747185,1.7234789788860856,1.7241783023114792,1.7248913318206462,1.7256181586894062,1.7263588696237853,1.7271135465841176,1.7278822666094182,1.728665101642342,1.7294621183550412,1.730273377976268,1.7310989361200542,1.7319388426163365,1.7327931413438737,1.7336618700658466,1.734545060268498,1.735442737003217,1.7363549187324445,1.7372816171797978,1.7382228371848143,1.7391785765627035,1.7401488259695141,1.741133568773114,1.7421327809303637,1.7431464308708933,1.7441744793878584,1.7452168795360463,1.7462735765377198,1.7473445076965433,1.7484296023199495,1.7495287816502763,1.7506419588050033,1.7517690387263856,1.7529099181407812,1.7540644855279375,1.7552326211004903,1.7564141967939058,1.757609076267069,1.7588171149137053,1.7600381598847967,1.761272050122122,1.7625186164030324,1.763777681396542,1.7650490597307875,1.7663325580718763,1.7676279752141273,1.768935102181667,1.7702537223413124,1.7715836115266619,1.772924538173258,1.7742762634646811,1.7756385414893932,1.7770111194081124,1.7783937376314995,1.7797861300078757,1.7811880240206868,1.7825991409953983,1.7840191963154766,1.7854478996470884,1.7868849551721318,1.7883300618291866,1.7897829135619525,1.7912431995747273,1.7927106045944523,1.7941848091388484,1.7956654897901383,1.797152319473851,1.7986449677421785,1.8001431010613662,1.8016463831025848,1.8031544750357535,1.8046670358257533,1.8061837225304913,1.8077041906002596,1.809228094177837,1.8107550863987947,1.8122848196914525,1.813816946075962,1.8153511174619763,1.8168869859443961,1.8184242040966798,1.8199624252612172,1.821501303836294,1.8230404955591695,1.824579657784815,1.8261184497598841,1.827656532891489,1.8291935710103906,1.8307292306282115,1.8322631811883288,1.8337950953100908,1.8353246490260537,1.836851522011937,1.8383753978090236,1.839895964038757,1.8414129126093073,1.8429259399139017,1.844434747020734,1.8459390398542976,1.8474385293680027,1.848932931707966,1.8504219683678766,1.8519053663348728,1.8533828582263738,1.8548541824178435,1.8563190831614786,1.8577773106958246,1.8592286213463627,1.8606727776171024,1.8621095482732606,1.863538708415101,1.864960039543033,1.8663733296140965,1.867778373089947,1.8691749709764953,1.870562930855352,1.8719420669072508,1.87331219992762,1.8746731573345052,1.8760247731690272,1.8773668880885936,1.878699349353078,1.880022010804181,1.8813347328382093,1.8826373823725038,1.8839298328057503,1.885211963972423,1.8864836620915952,1.887744819710371,1.8889953356421827,1.8902351149002041,1.891464068626128,1.8926821140145589,1.8938891742332644,1.8950851783395397,1.896270061192918,1.8974437633644805,1.8986062310429976,1.8997574159381387,1.900897275180983,1.9020257712220565,1.9031428717271262,1.9042485494709538,1.905342782229242,1.9064255526689657,1.9074968482372963,1.908556661049321,1.9096049877747325,1.9106418295236942,1.9116671917320414,1.9126810840460007]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.7909253668566332,1.790925366323029,1.7909253657794937,1.790925365225844,1.7909253646618903,1.7909253640874412,1.7909253635023017,1.7909253629062727,1.790925362299152,1.790925361680732,1.7909253610508034,1.7909253604091517,1.7909253597555588,1.7909253590898022,1.7909253584116558,1.7909253577208892,1.790925357017267,1.7909253563005505,1.790925355570496,1.790925354826855,1.7909253540693744,1.7909253532977976,1.7909253525118611,1.790925351711298,1.7909253508958367,1.7909253500651996,1.7909253492191044,1.7909253483572625,1.7909253474793823,1.7909253465851642,1.7909253456743053,1.7909253447464943,1.790925343801417,1.7909253428387515,1.7909253418581708,1.7909253408593413,1.7909253398419234,1.7909253388055713,1.7909253377499323,1.790925336674648,1.7909253355793524,1.7909253344636729,1.7909253333272313,1.79092533216964,1.7909253309905055,1.790925329789427,1.7909253285659965,1.7909253273197978,1.7909253260504068,1.7909253247573929,1.7909253234403155,1.7909253220987271,1.7909253207321716,1.7909253193401844,1.7909253179222917,1.7909253164780123,1.790925315006855,1.7909253135083187,1.7909253119818944,1.7909253104270633,1.7909253088432968,1.790925307230056,1.7909253055867924,1.7909253039129478,1.7909253022079523,1.7909253004712273,1.7909252987021813,1.7909252969002132,1.79092529506471,1.7909252931950481,1.790925291290591,1.7909252893506926,1.7909252873746921,1.790925285361918,1.7909252833116858,1.7909252812232985,1.7909252790960466,1.7909252769292057,1.7909252747220399,1.7909252724737985,1.7909252701837168,1.7909252678510168,1.7909252654749048,1.7909252630545733,1.7909252605891985,1.790925258077944,1.7909252555199537,1.7909252529143593,1.7909252502602748,1.7909252475567972,1.7909252448030082,1.7909252419979702,1.7909252391407307,1.790925236230317,1.790925233265741,1.7909252302459935,1.7909252271700482,1.7909252240368598,1.7909252208453623,1.79092521759447,1.7909252142830794,1.7909252109100628,1.7909252074742747,1.790925203974546,1.7909252004096872,1.7909251967784856,1.7909251930797079,1.790925189312095,1.7909251854743669,1.790925181565218,1.79092517758332,1.7909251735273184,1.7909251693958348,1.7909251651874636,1.7909251609007746,1.7909251565343098,1.790925152086585,1.790925147556088,1.7909251429412776,1.7909251382405862,1.790925133452414,1.790925128575134,1.7909251236070878,1.790925118546586,1.790925113391908,1.7909251081413013,1.7909251027929805,1.7909250973451276,1.7909250917958897,1.7909250861433808,1.7909250803856782,1.7909250745208245,1.790925068546826,1.7909250624616515,1.7909250562632315,1.7909250499494591,1.7909250435181874,1.7909250369672298,1.7909250302943587,1.7909250234973055,1.7909250165737596,1.7909250095213665,1.790925002337728,1.7909249950204025,1.7909249875669015,1.7909249799746914,1.7909249722411902,1.7909249643637692,1.7909249563397496,1.7909249481664031,1.790924939840951,1.790924931360563,1.790924922722355,1.7909249139233911,1.7909249049606788,1.790924895831171,1.790924886531763,1.7909248770592945,1.7909248674105438,1.7909248575822307,1.790924847571014,1.7909248373734892,1.7909248269861895,1.790924816405583,1.7909248056280729,1.7909247946499947,1.7909247834676159,1.7909247720771342,1.7909247604746772,1.7909247486562998,1.7909247366179841,1.7909247243556377,1.7909247118650906,1.7909246991420966,1.7909246861823294,1.7909246729813837,1.7909246595347705,1.7909246458379184,1.7909246318861705,1.790924617674783,1.7909246031989254,1.7909245884536746,1.790924573434018,1.7909245581348492,1.7909245425509661,1.790924526677071,1.790924510507767,1.7909244940375557,1.7909244772608388,1.7909244601719116,1.7909244427649647,1.7909244250340794,1.790924406973227,1.7909243885762685,1.7909243698369484,1.7909243507488954,1.7909243313056205,1.790924311500513,1.7909242913268386,1.7909242707777406,1.7909242498462312,1.7909242285251943,1.7909242068073816,1.7909241846854091,1.790924162151756,1.7909241391987614,1.7909241158186213,1.7909240920033875,1.7909240677449634,1.7909240430351014,1.7909240178654011,1.790923992227305,1.7909239661120973,1.7909239395108987,1.7909239124146665,1.7909238848141884,1.7909238567000807,1.7909238280627857,1.7909237988925677,1.7909237691795097,1.7909237389135098,1.7909237080842793,1.7909236766813361,1.7909236446940053,1.790923612111412,1.7909235789224793,1.7909235451159244,1.7909235106802541,1.7909234756037615,1.7909234398745224,1.7909234034803896,1.7909233664089914,1.7909233286477242,1.7909232901837513,1.790923251003997,1.7909232110951414,1.7909231704436175,1.790923129035606,1.7909230868570294,1.79092304389355,1.7909230001305625,1.7909229555531885,1.790922910146275,1.790922863894386,1.7909228167817983,1.7909227687924965,1.7909227199101665,1.7909226701181915,1.7909226193996457,1.790922567737287,1.7909225151135544,1.7909224615105586,1.7909224069100782,1.7909223512935528,1.790922294642077,1.7909222369363929,1.7909221781568845,1.7909221182835713,1.7909220572961007,1.7909219951737418,1.7909219318953784,1.7909218674394998,1.7909218017841968,1.790921734907152,1.790921666785633,1.7909215973964847,1.79092152671612,1.790921454720515,1.790921381385197,1.7909213066852396,1.7909212305952513,1.7909211530893698,1.7909210741412498,1.7909209937240584,1.7909209118104605,1.7909208283726152,1.790920743382163,1.790920656810216,1.79092056862735,1.7909204788035922,1.7909203873084134,1.7909202941107167,1.7909201991788257,1.790920102480476,1.7909200039828024,1.7909199036523284,1.7909198014549548,1.7909196973559487,1.790919591319931,1.7909194833108635,1.7909193732920394,1.7909192612260683,1.7909191470748649,1.7909190307996345,1.790918912360862,1.790918791718297,1.790918668830941,1.7909185436570318,1.7909184161540324,1.7909182862786128,1.7909181539866381,1.790918019233153,1.7909178819723663,1.7909177421576348,1.7909175997414473,1.79091745467541,1.7909173069102295,1.7909171563956947,1.790917003080662,1.7909168469130365,1.7909166878397542,1.7909165258067647,1.7909163607590128,1.7909161926404196,1.790916021393863,1.7909158469611604,1.7909156692830455,1.7909154882991511,1.7909153039479881,1.7909151161669243,1.7909149248921614,1.7909147300587178,1.7909145316004016,1.7909143294497918,1.7909141235382133,1.790913913795715,1.7909137001510453,1.7909134825316286,1.7909132608635392,1.7909130350714786,1.7909128050787475,1.7909125708072229,1.7909123321773284,1.790912089108008,1.7909118415167022,1.7909115893193142,1.7909113324301853,1.7909110707620661,1.7909108042260842,1.7909105327317163,1.7909102561867587,1.7909099744972916,1.7909096875676527,1.7909093953004003,1.790909097596283,1.790908794354206,1.7909084854711963,1.790908170842367,1.7909078503608833,1.7909075239179262,1.7909071914026544,1.7909068527021683,1.7909065077014708,1.7909061562834292,1.7909057983287362,1.7909054337158667,1.7909050623210399,1.7909046840181766,1.7909042986788557,1.790903906172271,1.790903506365188,1.7909030991218984,1.7909026843041735,1.790902261771219,1.7909018313796274,1.7909013929833262,1.7909009464335335,1.7909004915787052,1.7909000282644838,1.7908995563336476,1.7908990756260552,1.7908985859785946,1.7908980872251268,1.7908975791964281,1.7908970617201363,1.7908965346206904,1.7908959977192718,1.790895450833745,1.7908948937785942,1.790894326364863,1.79089374840009,1.7908931596882431,1.7908925600296555,1.7908919492209563,1.7908913270550033,1.7908906933208124,1.7908900478034895,1.7908893902841538,1.790888720539867,1.7908880383435588,1.790887343463948,1.7908866356654674,1.790885914708184,1.7908851803477168,1.7908844323351572,1.7908836704169837,1.790882894334978,1.7908821038261367,1.7908812986225848,1.7908804784514851,1.790879643034947,1.7908787920899325,1.7908779253281635,1.790877042456022,1.7908761431744544,1.790875227178872,1.7908742941590443,1.7908733437990012,1.7908723757769243,1.7908713897650388,1.7908703854295052,1.7908693624303076,1.7908683204211386,1.7908672590492871,1.7908661779555164,1.7908650767739474,1.7908639551319343,1.7908628126499437,1.7908616489414237,1.7908604636126786,1.7908592562627357,1.7908580264832135,1.7908567738581844,1.790855497964037,1.7908541983693353,1.790852874634675,1.7908515263125384,1.7908501529471428,1.790848754074295,1.790847329221232,1.7908458779064649,1.7908443996396235,1.790842893921289,1.7908413602428308,1.790839798086239,1.7908382069239503,1.790836586218678,1.790834935423229,1.7908332539803293,1.7908315413224343,1.7908297968715459,1.7908280200390203,1.7908262102253738,1.7908243668200856,1.7908224892013993,1.790820576736114,1.7908186287793817,1.7908166446744895,1.7908146237526514,1.7908125653327824,1.7908104687212796,1.790808333211794,1.7908061580849979,1.790803942608354,1.7908016860358709,1.7907993876078638,1.7907970465507055,1.790794662076574,1.7907922333831974,1.7907897596535902,1.790787240055793,1.7907846737425974,1.7907820598512743,1.790779397503294,1.79077668580404,1.7907739238425235,1.7907711106910869,1.790768245405103,1.7907653270226753,1.7907623545643239,1.7907593270326738,1.7907562434121325,1.790753102668565,1.7907499037489638,1.7907466455811103,1.7907433270732327,1.7907399471136576,1.7907365045704573,1.7907329982910873,1.790729427102021,1.790725789808378,1.7907220851935441,1.7907183120187882,1.7907144690228682,1.7907105549216364,1.790706568407633,1.7907025081496755,1.7906983727924413,1.7906941609560425,1.7906898712355943,1.7906855022007773,1.7906810523953898,1.7906765203368977,1.7906719045159711,1.7906672033960183,1.7906624154127104,1.7906575389734976,1.7906525724571218,1.7906475142131142,1.7906423625612908,1.7906371157912417,1.790631772161802,1.790626329900529,1.7906207872031585,1.7906151422330612,1.7906093931206861,1.7906035379629965,1.7905975748228995,1.7905915017286644,1.790585316673332,1.7905790176141183,1.790572602471805,1.7905660691301253,1.7905594154351367,1.7905526391945898,1.790545738177281,1.7905387101123995,1.7905315526888694,1.790524263554675,1.7905168403161777,1.790509280537431,1.7905015817394743,1.7904937413996278,1.7904857569507704,1.7904776257806103,1.7904693452309488,1.790460912596928,1.7904523251262743,1.7904435800185299,1.7904346744242738,1.7904256054443337,1.7904163701289875,1.7904069654771573,1.790397388435589,1.7903876358980255,1.7903777047043687,1.790367591639834,1.7903572934340888,1.7903468067603892,1.7903361282347023,1.790325254414818,1.7903141817994552,1.7903029068273546,1.7902914258763643,1.7902797352625157,1.7902678312390878,1.7902557099956689,1.7902433676572005,1.7902308002830192,1.7902180038658861,1.7902049743310122,1.7901917075350682,1.7901781992651935,1.7901644452379943,1.7901504410985318,1.790136182419309,1.7901216646992422,1.7901068833626355,1.790091833758138,1.790076511157704,1.7900609107555432,1.790045027667065,1.7900288569278178,1.790012393492428,1.7899956322335255,1.7899785679406774,1.7899611953193069,1.7899435089896185,1.7899255034855146,1.7899071732535128,1.789888512651664,1.789869515948469,1.7898501773217899,1.7898304908577714,1.7898104505497576,1.7897900502972124,1.7897692839046453,1.789748145080535,1.7897266274362673,1.7897047244850695,1.7896824296409546,1.789659736217678,1.7896366374276926,1.789613126381124,1.7895891960847485,1.7895648394409873,1.789540049246912,1.7895148181932647,1.7894891388634935,1.7894630037328045,1.7894364051672331,1.7894093354227325,1.7893817866442865,1.7893537508650408,1.7893252200054635,1.7892961858725265,1.7892666401589168,1.789236574442276,1.7892059801844729,1.7891748487309065,1.7891431713098427,1.789110939031794,1.7890781428889286,1.7890447737545294,1.7890108223824894,1.7889762794068576,1.7889411353414273,1.7889053805793815,1.7888690053929803,1.7888319999333167,1.7887943542301175,1.7887560581916133,1.788717101604466,1.788677474133768,1.7886371653231026,1.788596164594683,1.7885544612495636,1.7885120444679306,1.7884689033094696,1.7884250267138255,1.7883804035011417,1.7883350223727008,1.7882888719116516,1.788241940583839,1.7881942167387381,1.7881456886104914,1.7880963443190592,1.7880461718714826,1.7879951591632692,1.787943293979895,1.7878905639984402,1.7878369567893537,1.7877824598183552,1.787727060448476,1.7876707459422478,1.7876135034640432,1.7875553200825667,1.7874961827735123,1.7874360784223822,1.7873749938274797,1.787312915703075,1.7872498306827556,1.7871857253229635,1.7871205861067228,1.7870543994475694,1.7869871516936835,1.7869188291322289,1.786849417993918,1.7867789044577882,1.7867072746562143,1.7866345146801546,1.7865606105846352,1.7864855483944868,1.7864093141103323,1.7863318937148371,1.7862532731792267,1.786173438470079,1.7860923755563987,1.7860100704169792,1.7859265090480596,1.7858416774712864,1.785755561741982,1.7856681479577339,1.7855794222673023,1.7854893708798658,1.7853979800746023,1.7853052362106143,1.7852111257372134,1.7851156352045614,1.7850187512746807,1.7849204607328417,1.7848207504993356,1.7847196076416323,1.7846170193869444,1.7845129731351912,1.7844074564723797,1.784300457184405,1.7841919632712775,1.7840819629617903,1.7839704447286244,1.7838573973039065,1.7837428096952215,1.7836266712020932,1.7835089714329326,1.7833897003224615,1.7832688481496302,1.7831464055560104,1.7830223635647018,1.7828967135997293,1.7827694475059548,1.7826405575695028,1.7825100365387068,1.7823778776455814,1.7822440746278212,1.78210862175134,1.7819715138333465,1.7818327462659633,1.781692315040397,1.781550216771651,1.7814064487238002,1.7812610088358163,1.781113895747954,1.7809651088286953,1.7808146482022547,1.780662514776646,1.7805087102723098,1.7803532372513053,1.7801960991470553,1.7800373002946543,1.7798768459617307,1.7797147423798587,1.779550996776524,1.7793856174076252,1.77921861359052,1.7790499957376,1.7788797753903893,1.7787079652541595,1.7785345792330545,1.7783596324657047,1.778183141361339,1.7780051236363534,1.777825598351355,1.7776445859486407,1.7774621082901119,1.777278188695591,1.7770928519815448,1.7769061245001703,1.7767180341788387,1.7765286105598654,1.7763378848405882,1.7761458899137215,1.775952660407969,1.775758232728848,1.7755626450997175,1.77536593760296,1.7751681522212879,1.77496933287914,1.7747695254841305,1.7745687779685075,1.774367140330579,1.7741646646760654,1.7739614052593313,1.7737574185244454,1.7735527631460233,1.7733475000697938,1.773141692552845,1.7729354062034783,1.7727287090206223,1.7725216714327463,1.7723143663361964,1.772106869132901,1.7718992577673736,1.7716916127629392,1.7714840172571118,1.7712765570360507,1.7710693205680128,1.7708623990357228,1.7706558863675812,1.7704498792676175,1.7702444772441095,1.770039782636777,1.769835900642449,1.769632939339127,1.7694310097083348,1.7692302256556578,1.7690307040293825,1.7688325646371204,1.7686359302603194,1.7684409266665597,1.7682476826195188,1.7680563298865077,1.7678670032434582,1.7676798404772593,1.7674949823853219,1.7673125727722696,1.7671327584436336,1.7669556891964395,1.7667815178065813,1.7666104000128517,1.766442494497535,1.7662779628634357,1.7661169696072359,1.765959682089074,1.7658062704982314,1.765656907814823,1.7655117697673857,1.7653710347862652,1.7652348839527003,1.7651035009435112,1.764977071971298,1.7648557857200695,1.7647398332762136,1.764629408054737,1.7645247057207025,1.7644259241058067,1.764333263120029,1.7642469246583208,1.7641671125022755,1.7640940322167686,1.7640278910415224,1.7639688977776091,1.7639172626688697,1.763873197278268,1.7638369143592036,1.7638086277218137,1.7637885520943055,1.7637769029793957,1.7637738965059115,1.7637797492756562,1.7637946782056326,1.7638189003657525,1.7638526328121507,1.763896092416271,1.76394949568987,1.7640130586061349,1.7640869964171018,1.7641715234675892,1.7642668530058778,1.764373196991373,1.7644907658995164,1.76461976852421,1.764760411778049,1.7649129004906563,1.7650774372054343,1.765254221975058,1.765443452156049,1.765645322202762,1.7658600234611572,1.7660877439626987,1.7663286682187624,1.7665829770159132,1.7668508472124294,1.7671324515364493,1.7674279583861203,1.76773753163211,1.7680613304228647,1.7683995089929725,1.7687522164749874,1.7691195967150686,1.7695017880927717,1.7698989233453242,1.7703111293966913,1.7707385271917468,1.7711812315358253,1.771639350939925,1.77211298747182,1.772602236613309,1.7731071871238189,1.7736279209105534,1.7741645129053658,1.774717030948512,1.7752855356794095,1.7758700804345282,1.7764707111524958,1.7770874662865055,1.7777203767240681,1.7783694657141547,1.7790347488017557,1.7797162337698438,1.7804139205887486,1.7811278013729024,1.7818578603449333,1.7826040738070397,1.7833664101196036,1.7841448296869575,1.7849392849502401,1.7857497203872517,1.7865760725192195,1.7874182699243888,1.7882762332583324,1.7891498752808914,1.7900391008896515,1.790943807159841,1.7918638833905758,1.7927992111573479,1.7937496643706512,1.7947151093406832,1.795695404848009,1.7966904022201189,1.7976999454137945,1.7987238711032083,1.7997620087736756,1.8008141808209992,1.801880202656325,1.8029598828164506,1.804053023079516,1.8051594185860154,1.8062788579650566,1.807411123465818,1.8085559910941182,1.809713230754045,1.81088260639456,1.812063876161013,1.8132567925514775,1.8144611025778281,1.8156765479314687,1.8169028651536057,1.8181397858099733,1.8193870366698848,1.820644339889496,1.8219114131991507,1.8231879700946565,1.8244737200323584,1.8257683686278336,1.8270716178580453,1.8283831662667733,1.8297027091731302,1.8310299388829607,1.8323645449029113,1.8337062141569542,1.8350546312051237,1.8364094784642413,1.83777043643036,1.8391371839026893,1.8405093982087242,1.8418867554303109,1.8432689306303698,1.8446555980799957,1.8460464314856424,1.8474411042161027,1.8488392895289898,1.85024066079642,1.8516448917296033,1.853051656602041,1.854460630471033,1.8558714893972008,1.8572839106617356,1.8586975729810729,1.8601121567187247,1.8615273440939741,1.8629428193871687,1.864358269141345,1.865773382359926,1.867187850700239,1.8686013686626197,1.8700136337748656,1.87142434677183,1.872833211769931,1.8742399364364006,1.8756442321530697,1.8770458141745323,1.878444401780521,1.879839718422353,1.8812314918633108,1.8826194543128436,1.8840033425544847,1.8853828980673866,1.8867578671414087,1.8881280009856851,1.8894930558306287,1.890852793023334,1.8922069791163576,1.8935553859498653,1.8948977907271527,1.8962339760835576,1.8975637301487844,1.8988868466026976,1.9002031247246167,1.9015123694361984,1.9028143913379627,1.9041090067395545,1.905396037683841,1.9066753119649382,1.9079466631402846,1.9092099305368817,1.910464959251833,1.911711600147306,1.9129497098400745,1.9141791506857742,1.9153997907580318,1.9166115038226295,1.917814169306856,1.9190076722642204,1.9201919033346926,1.9213667587006424,1.9225321400386581,1.9236879544674101,1.9248341144917442,1.9259705379431828,1.927097147917008,1.9282138727061087,1.9293206457317706,1.930417405471581,1.9315040953846345,1.9325806638342022,1.9336470640080459,1.9347032538365472,1.9357491959088144,1.9367848573869395,1.937810209918562,1.938825229547907,1.939829896625441,1.9408241957163144,1.9418081155077263,1.9427816487153593,1.9437447919890385,1.944697545817724,1.9456399144340037,1.9465719057181874,1.9474935311021395]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.6166492626367464,1.6166492626563573,1.6166492626763327,1.6166492626966802,1.616649262717407,1.6166492627385187,1.6166492627600237,1.6166492627819293,1.6166492628042424,1.6166492628269709,1.6166492628501221,1.6166492628737046,1.616649262897726,1.6166492629221947,1.6166492629471183,1.6166492629725064,1.6166492629983669,1.6166492630247085,1.616649263051541,1.6166492630788727,1.6166492631067126,1.6166492631350715,1.616649263163958,1.616649263193382,1.6166492632233538,1.6166492632538838,1.616649263284982,1.6166492633166591,1.616649263348926,1.616649263381793,1.6166492634152725,1.6166492634493748,1.6166492634841119,1.6166492635194956,1.6166492635555385,1.6166492635922518,1.6166492636296488,1.616649263667742,1.6166492637065442,1.616649263746069,1.6166492637863294,1.6166492638273393,1.616649263869113,1.6166492639116639,1.6166492639550072,1.616649263999157,1.6166492640441292,1.6166492640899384,1.6166492641366004,1.6166492641841315,1.6166492642325472,1.6166492642818644,1.6166492643320995,1.6166492643832704,1.6166492644353934,1.616649264488487,1.6166492645425694,1.6166492645976585,1.6166492646537731,1.616649264710933,1.616649264769157,1.6166492648284645,1.6166492648888768,1.6166492649504138,1.6166492650130964,1.6166492650769466,1.6166492651419855,1.6166492652082354,1.6166492652757185,1.6166492653444589,1.6166492654144784,1.6166492654858022,1.6166492655584541,1.6166492656324591,1.6166492657078417,1.6166492657846283,1.616649265862845,1.6166492659425178,1.6166492660236744,1.6166492661063427,1.6166492661905503,1.6166492662763259,1.6166492663636993,1.6166492664526997,1.6166492665433572,1.6166492666357035,1.6166492667297696,1.6166492668255876,1.6166492669231904,1.6166492670226107,1.6166492671238832,1.6166492672270416,1.6166492673321218,1.6166492674391584,1.6166492675481892,1.616649267659251,1.6166492677723814,1.6166492678876194,1.6166492680050035,1.6166492681245743,1.616649268246373,1.6166492683704403,1.6166492684968188,1.6166492686255516,1.616649268756683,1.616649268890257,1.6166492690263197,1.6166492691649172,1.616649269306097,1.616649269449907,1.6166492695963968,1.6166492697456158,1.6166492698976156,1.616649270052447,1.6166492702101638,1.6166492703708195,1.616649270534469,1.616649270701168,1.6166492708709734,1.6166492710439435,1.6166492712201368,1.616649271399614,1.6166492715824363,1.6166492717686656,1.6166492719583663,1.6166492721516028,1.616649272348441,1.616649272548948,1.6166492727531927,1.6166492729612447,1.6166492731731754,1.6166492733890567,1.6166492736089628,1.6166492738329685,1.6166492740611504,1.616649274293587,1.6166492745303573,1.6166492747715426,1.6166492750172248,1.6166492752674886,1.616649275522419,1.616649275782104,1.6166492760466316,1.616649276316093,1.6166492765905796,1.616649276870186,1.616649277155008,1.6166492774451426,1.6166492777406893,1.6166492780417494,1.616649278348426,1.6166492786608242,1.6166492789790503,1.6166492793032146,1.6166492796334273,1.6166492799698018,1.616649280312453,1.6166492806614992,1.6166492810170592,1.6166492813792555,1.6166492817482119,1.616649282124055,1.6166492825069143,1.6166492828969203,1.6166492832942077,1.6166492836989121,1.6166492841111733,1.6166492845311322,1.616649284958933,1.6166492853947225,1.616649285838651,1.6166492862908708,1.6166492867515372,1.6166492872208087,1.616649287698846,1.6166492881858148,1.6166492886818815,1.6166492891872168,1.616649289701995,1.6166492902263931,1.6166492907605923,1.6166492913047752,1.6166492918591304,1.6166492924238478,1.6166492929991227,1.6166492935851535,1.6166492941821422,1.6166492947902937,1.616649295409819,1.6166492960409315,1.6166492966838488,1.616649297338793,1.6166492980059894,1.6166492986856698,1.6166492993780681,1.6166493000834243,1.6166493008019815,1.6166493015339887,1.6166493022796984,1.6166493030393698,1.616649303813265,1.6166493046016517,1.6166493054048041,1.6166493062229992,1.6166493070565218,1.6166493079056594,1.616649308770708,1.616649309651967,1.6166493105497421,1.6166493114643454,1.6166493123960946,1.616649313345313,1.6166493143123315,1.6166493152974857,1.6166493163011182,1.6166493173235792,1.6166493183652235,1.6166493194264153,1.6166493205075232,1.616649321608925,1.6166493227310044,1.6166493238741533,1.6166493250387708,1.6166493262252633,1.6166493274340459,1.616649328665542,1.6166493299201805,1.6166493311984025,1.6166493325006548,1.6166493338273935,1.616649335179084,1.616649336556201,1.6166493379592264,1.6166493393886545,1.6166493408449867,1.6166493423287356,1.6166493438404224,1.61664934538058,1.6166493469497505,1.6166493485484872,1.6166493501773544,1.6166493518369258,1.6166493535277886,1.6166493552505399,1.6166493570057892,1.6166493587941577,1.6166493606162782,1.6166493624727971,1.616649364364373,1.6166493662916768,1.6166493682553933,1.6166493702562208,1.6166493722948712,1.6166493743720698,1.6166493764885583,1.6166493786450897,1.6166493808424351,1.6166493830813795,1.6166493853627226,1.6166493876872823,1.6166493900558903,1.6166493924693963,1.6166493949286664,1.6166493974345846,1.6166493999880513,1.6166494025899865,1.616649405241327,1.6166494079430298,1.6166494106960692,1.6166494135014415,1.6166494163601612,1.616649419273265,1.6166494222418077,1.6166494252668682,1.6166494283495452,1.6166494314909605,1.6166494346922595,1.616649437954609,1.616649441279201,1.6166494446672508,1.6166494481199989,1.6166494516387113,1.6166494552246795,1.616649458879222,1.6166494626036836,1.6166494663994369,1.6166494702678829,1.6166494742104511,1.6166494782286018,1.6166494823238233,1.6166494864976364,1.6166494907515931,1.6166494950872772,1.616649499506305,1.616649504010328,1.6166495086010306,1.616649513280134,1.6166495180493934,1.6166495229106028,1.6166495278655928,1.6166495329162323,1.6166495380644308,1.616649543312137,1.6166495486613424,1.6166495541140782,1.616649559672421,1.6166495653384914,1.6166495711144535,1.6166495770025195,1.6166495830049492,1.6166495891240493,1.6166495953621773,1.6166496017217409,1.6166496082052004,1.6166496148150686,1.6166496215539143,1.6166496284243603,1.6166496354290876,1.6166496425708354,1.6166496498524034,1.616649657276652,1.6166496648465056,1.6166496725649513,1.6166496804350436,1.6166496884599044,1.6166496966427244,1.6166497049867656,1.616649713495363,1.6166497221719258,1.6166497310199388,1.616649740042967,1.616649749244654,1.6166497586287267,1.616649768198995,1.6166497779593578,1.6166497879137998,1.6166497980663974,1.6166498084213219,1.6166498189828373,1.6166498297553082,1.6166498407431993,1.6166498519510764,1.6166498633836142,1.6166498750455929,1.6166498869419061,1.616649899077561,1.6166499114576822,1.6166499240875145,1.616649936972426,1.6166499501179121,1.616649963529598,1.6166499772132423,1.6166499911747403,1.616650005420129,1.6166500199555898,1.6166500347874515,1.6166500499221965,1.6166500653664633,1.6166500811270508,1.616650097210924,1.6166501136252156,1.6166501303772352,1.6166501474744688,1.6166501649245881,1.6166501827354545,1.6166502009151205,1.616650219471841,1.6166502384140762,1.616650257750495,1.6166502774899851,1.6166502976416557,1.6166503182148466,1.616650339219132,1.6166503606643283,1.6166503825605019,1.6166504049179742,1.6166504277473315,1.6166504510594297,1.6166504748654045,1.6166504991766772,1.6166505240049656,1.6166505493622894,1.6166505752609823,1.6166506017136975,1.6166506287334204,1.6166506563334755,1.6166506845275384,1.616650713329645,1.6166507427542023,1.6166507728159991,1.6166508035302174,1.616650834912444,1.6166508669786832,1.6166508997453677,1.6166509332293728,1.6166509674480287,1.6166510024191347,1.6166510381609724,1.6166510746923206,1.6166511120324714,1.616651150201243,1.6166511892189983,1.6166512291066604,1.6166512698857287,1.616651311578297,1.6166513542070735,1.6166513977953945,1.6166514423672491,1.616651487947297,1.6166515345608883,1.616651582234085,1.6166516309936834,1.616651680867236,1.6166517318830775,1.6166517840703438,1.6166518374590013,1.6166518920798714,1.616651947964656,1.616652005145966,1.6166520636573491,1.6166521235333184,1.6166521848093853,1.6166522475220857,1.6166523117090172,1.6166523774088695,1.616652444661459,1.6166525135077645,1.6166525839899628,1.6166526561514702,1.616652730036976,1.616652805692485,1.6166528831653615,1.6166529625043684,1.6166530437597133,1.6166531269830955,1.616653212227749,1.6166532995484963,1.6166533890017956,1.6166534806457937,1.6166535745403787,1.6166536707472374,1.6166537693299106,1.6166538703538527,1.6166539738864922,1.6166540799972977,1.616654188757836,1.6166543002418474,1.6166544145253077,1.6166545316865055,1.6166546518061107,1.6166547749672557,1.6166549012556088,1.6166550307594592,1.6166551635697992,1.6166552997804096,1.6166554394879507,1.6166555827920521,1.61665572979541,1.6166558806038833,1.6166560353265937,1.6166561940760324,1.6166563569681667,1.6166565241225483,1.6166566956624329,1.6166568717148908,1.616657052410937,1.6166572378856499,1.6166574282783037,1.616657623732501,1.616657824396312,1.6166580304224132,1.6166582419682365,1.6166584591961184,1.6166586822734572,1.6166589113728698,1.61665914667236,1.6166593883554876,1.6166596366115442,1.6166598916357318,1.6166601536293523,1.6166604227999959,1.616660699361742,1.6166609835353603,1.61666127554852,1.6166615756360083,1.6166618840399507,1.6166622010100398,1.616662526803772,1.616662861686689,1.6166632059326287,1.616663559823981,1.616663923651951,1.6166642977168364,1.6166646823282997,1.616665077805663,1.6166654844781994,1.6166659026854397,1.6166663327774868,1.616666775115335,1.6166672300712022,1.6166676980288712,1.616668179384038,1.6166686745446712,1.6166691839313823,1.6166697079778043,1.6166702471309806,1.616670801851767,1.6166713726152397,1.6166719599111197,1.6166725642442041,1.6166731861348111,1.6166738261192335,1.6166744847502112,1.6166751625974058,1.6166758602478957,1.616676578306679,1.6166773173971907,1.6166780781618333,1.6166788612625203,1.6166796673812303,1.616680497220581,1.6166813515044076,1.6166822309783655,1.6166831364105394,1.6166840685920698,1.6166850283377954,1.616686016486905,1.6166870339036123,1.6166880814778386,1.6166891601259137,1.6166902707912931,1.6166914144452895,1.616692592087821,1.6166938047481723,1.616695053485777,1.6166963393910112,1.6166976635860075,1.6166990272254806,1.616700431497574,1.6167018776247217,1.616703366864524,1.6167049005106449,1.6167064798937205,1.6167081063822901,1.61670978138374,1.616711506345264,1.6167132827548427,1.6167151121422418,1.6167169960800183,1.6167189361845544,1.6167209341171012,1.6167229915848382,1.616725110341955,1.6167272921907445,1.6167295389827123,1.6167318526197043,1.6167342350550487,1.6167366882947147,1.6167392143984851,1.6167418154811426,1.616744493713677,1.6167472513244994,1.6167500906006749,1.61675301388917,1.61675602359811,1.6167591221980515,1.6167623122232686,1.6167655962730514,1.6167689770130138,1.6167724571764155,1.6167760395654935,1.6167797270528081,1.6167835225825906,1.6167874291721105,1.616791449913043,1.6167955879728522,1.616799846596177,1.6168042291062266,1.6168087389061832,1.6168133794806085,1.6168181543968576,1.616823067306498,1.6168281219467326,1.6168333221418245,1.6168386718045278,1.6168441749375178,1.6168498356348269,1.6168556580832751,1.6168616465639094,1.6168678054534358,1.6168741392256532,1.616880652452888,1.6168873498074234,1.616894236062927,1.616901316095876,1.6169085948869801,1.616916077522596,1.6169237691961407,1.6169316752095002,1.6169398009744271,1.6169481520139402,1.6169567339637108,1.6169655525734448,1.6169746137082597,1.61698392335005,1.6169934875988488,1.6170033126741785,1.617013404916397,1.6170237707880308,1.617034416875108,1.6170453498884725,1.6170565766651002,1.617068104169401,1.6170799394945172,1.6170920898636088,1.6171045626311387,1.6171173652841444,1.6171305054435092,1.6171439908652203,1.617157829441628,1.6171720292026965,1.6171865983172509,1.6172015450942203,1.617216877983878,1.6172326055790809,1.6172487366165038,1.617265279977879,1.6172822446912305,1.617299639932114,1.6173174750248567,1.6173357594438025,1.6173545028145602,1.6173737149152585,1.6173934056778092,1.6174135851891744,1.6174342636926498,1.6174554515891513,1.6174771594385176,1.6174993979608276,1.6175221780377274,1.6175455107137762,1.6175694071978084,1.617593878864315,1.6176189372548417,1.617644594079411,1.6176708612179638,1.6176977507218224,1.6177252748151847,1.6177534458966356,1.6177822765406875,1.6178117794993494,1.6178419677037221,1.6178728542656249,1.6179044524792476,1.617936775822839,1.6179698379604175,1.6180036527435246,1.618038234213002,1.6180735966008017,1.6181097543318326,1.618146722025837,1.6181845144992997,1.618223146767391,1.6182626340459403,1.6183029917534444,1.6183442355131052,1.6183863811548984,1.6184294447176741,1.6184734424512859,1.6185183908187506,1.618564306498429,1.6186112063862443,1.6186591075979126,1.6187080274712078,1.6187579835682404,1.6188089936777628,1.6188610758174873,1.6189142482364247,1.6189685294172342,1.6190239380785882,1.6190804931775435,1.6191382139119208,1.6191971197226904,1.6192572302963586,1.6193185655673497,1.6193811457203924,1.6194449911928905,1.6195101226772888,1.6195765611234247,1.6196443277408636,1.6197134440012135,1.6197839316404175,1.6198558126610219,1.6199291093344086,1.6200038442030003,1.6200800400824242,1.620157720063637,1.6202369075150083,1.620317626084347,1.6203998997008873,1.6204837525772127,1.6205692092111246,1.620656294387446,1.6207450331797608,1.6208354509520861,1.6209275733604687,1.621021426354509,1.6211170361788032,1.6212144293743058,1.6213136327796072,1.6214146735321227,1.6215175790691934,1.6216223771290916,1.621729095751935,1.6218377632805017,1.6219484083609472,1.6220610599434226,1.62217574728259,1.6222924999380353,1.622411347774583,1.622532320962498,1.622655449977595,1.6227807656012367,1.6229082989202377,1.6230380813266572,1.6231701445175035,1.623304520494336,1.6234412415627717,1.6235803403319038,1.6237218497136308,1.623865802921904,1.624012233471896,1.6241611751790952,1.624312662158335,1.6244667288227679,1.6246234098827776,1.6247827403448578,1.6249447555104517,1.6251094909747688,1.625276982625588,1.6254472666420592,1.6256203794935171,1.6257963579383194,1.6259752390227251,1.6261570600798247,1.6263418587285494,1.6265296728727625,1.6267205407004626,1.6269145006831176,1.627111591575143,1.6273118524135552,1.627515322517819,1.6277220414899158,1.6279320492146558,1.6281453858602635,1.6283620918792625,1.6285822080096932,1.628805775276688,1.629032834994439,1.6292634287685943,1.6294975984991111,1.6297353863836015,1.629976834921214,1.630221986917079,1.6304708854873635,1.6307235740649688,1.6309800964059167,1.6312404965964589,1.6315048190609567,1.6317731085705685,1.6320454102527908,1.6323217696018908,1.6326022324902791,1.6328868451808594,1.6331756543404006,1.6334687070539735,1.6337660508404903,1.6340677336693903,1.634373803978507,1.6346843106931597,1.6349993032464991,1.6353188316011444,1.6356429462721394,1.6359716983512627,1.6363051395327084,1.6366433221401644,1.6369862991553037,1.6373341242477029,1.6376868518061933,1.6380445369716499,1.638407235671212,1.638775004653929,1.6391479015278088,1.6395259847982513,1.6399093139078276,1.6402979492773677,1.6406919523483066,1.6410913856262188,1.6414963127254907,1.641906798415024,1.6423229086648934,1.6427447106938546,1.6431722730175673,1.6436056654974236,1.644044959389824,1.644490227395744,1.6449415437104258,1.6453989840730008,1.6458626258158437,1.646332547913443,1.6468088310305553,1.6472915575693938,1.6477808117156,1.648276679482705,1.6487792487548059,1.6492886093271488,1.6498048529442932,1.6503280733355374,1.6508583662472547,1.651395829471794,1.6519405628725734,1.6524926684050014,1.6530522501328442,1.6536194142396554,1.6541942690348717,1.6547769249541986,1.6553674945538726,1.6559660924984347,1.656572835541604,1.6571878424998914,1.6578112342185665,1.6584431335296297,1.6590836652014271,1.6597329558795968,1.6603911340190105,1.6610583298064385,1.66173467507366,1.6624203032007805,1.663115349009544,1.6638199486464569,1.664534239455584,1.6652583598408952,1.665992449118098,1.6667366473559218,1.6674910952068602,1.6682559337274256,1.6690313041880178,1.66981734787254,1.6706142058679645,1.6714220188440712,1.672240926823659,1.6730710689435488,1.6739125832067643,1.6747656062263165,1.675630272961065,1.6765067164441672,1.6773950675046818,1.6782954544829283,1.6792080029402385,1.6801328353637874,1.681070070867206,1.6820198248877396,1.6829822088807078,1.6839573300120896,1.6849452908500384,1.685946189056194,1.6869601170776374,1.6879871618403761,1.6890274044452398,1.6900809198670745,1.6911477766581369,1.6922280366565765,1.6933217547008865,1.6944289783512108,1.695549747618369,1.6966840947014414,1.6978320437347563,1.6989936105450776,1.70016880241978,1.701357617886758,1.7025600465067983,1.7037760686790955,1.7050056554605633,1.70624876839955,1.70750535938453,1.7087753705082955,1.7100587339481228,1.711355371862354,1.7126651963037756,1.7139881091501257,1.7153240020520197,1.716672756398524,1.718034243300562,1.719408323592275,1.7207948478504265,1.7221936564318654,1.7236045795290191,1.7250274372433505,1.726462039676631,1.7279081870398663,1.7293656697796356,1.7308342687215683,1.7323137552306407,1.7338038913879177,1.7353044301833283,1.7368151157240233,1.738335683457823,1.739865860411216,1.7414053654413522,1.742953909501419,1.7445111959187813,1.7460769206852134,1.7476507727585446,1.7492324343750074,1.7508215813715524,1.7524178835173911,1.7540210048539873,1.7556306040427367,1.7572463347195273,1.7588678458554043,1.7604947821225168,1.7621267842645627,1.7637634894709158,1.7654045317536329,1.7670495423265564,1.7686981499857042,1.7703499814901886,1.7720046619428798,1.7736618151700676,1.7753210640993855,1.7769820311352658,1.778644338531243,1.7803076087584129,1.781971464869391,1.7836355308571485,1.7852994320081126,1.7869627952489513,1.788625249486492,1.7902864259402569,1.7919459584671118,1.7936034838775752,1.795258642243353,1.7969110771956942,1.7985604362142034,1.800206370905771,1.8018485372733188,1.8034865959740816,1.8051202125671866,1.8067490577503202,1.8083728075853032,1.8099911437124192,1.811603753553388,1.8132103305028824,1.8148105741085343,1.8164041902393997,1.8179908912428648,1.8195703960900278,1.821142430509588,1.8227067271103228,1.8242630254922392,1.8258110723465115,1.827350621544352,1.828881434214956,1.830403278812706,1.8319159311738222,1.8334191745626687,1.8349127997079342,1.836396604828936,1.8378703956522802,1.8393339854191555,1.8407871948835264,1.842229852301506,1.8436617934121982,1.845082861410315,1.8464929069108582,1.8478917879061878,1.8492793697157803,1.8506555249289978,1.8520201333411825,1.8533730818834002,1.8547142645461472,1.8560435822973471,1.8573609429949478,1.858666261294445,1.8599594585516337,1.8612404627209151,1.862509208249448,1.863765635967463,1.8650096929750264,1.8662413325255511,1.8674605139063454,1.8686672023164665,1.8698613687421695,1.8710429898302052,1.8722120477592332,1.8733685301096035,1.874512429731741,1.8756437446133847,1.8767624777458953,1.8778686369898618]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.8479019075685057,1.8479019064439077,1.8479019052983812,1.847901904131536,1.8479019029429755,1.847901901732296,1.8479019004990853,1.8479018992429244,1.8479018979633863,1.8479018966600358,1.8479018953324298,1.8479018939801168,1.847901892602637,1.8479018911995218,1.847901889770295,1.8479018883144693,1.847901886831551,1.8479018853210354,1.8479018837824088,1.847901882215148,1.8479018806187204,1.847901878992583,1.8479018773361828,1.847901875648957,1.8479018739303315,1.8479018721797222,1.8479018703965338,1.8479018685801598,1.847901866729983,1.8479018648453742,1.8479018629256927,1.8479018609702853,1.8479018589784877,1.8479018569496224,1.8479018548829995,1.8479018527779165,1.8479018506336577,1.8479018484494938,1.8479018462246826,1.8479018439584671,1.8479018416500772,1.8479018392987274,1.8479018369036193,1.8479018344639375,1.847901831978853,1.8479018294475205,1.8479018268690799,1.847901824242654,1.8479018215673497,1.8479018188422582,1.847901816066452,1.8479018132389875,1.8479018103589042,1.8479018074252216,1.8479018044369429,1.8479018013930522,1.8479017982925139,1.8479017951342742,1.8479017919172596,1.8479017886403755,1.8479017853025084,1.8479017819025232,1.847901778439264,1.847901774911553,1.8479017713181907,1.8479017676579557,1.8479017639296031,1.8479017601318652,1.8479017562634514,1.8479017523230459,1.8479017483093088,1.8479017442208756,1.8479017400563562,1.8479017358143348,1.8479017314933686,1.8479017270919886,1.847901722608699,1.8479017180419746,1.8479017133902629,1.8479017086519822,1.8479017038255219,1.8479016989092407,1.8479016939014667,1.8479016888004975,1.847901683604599,1.8479016783120044,1.8479016729209135,1.8479016674294941,1.847901661835879,1.8479016561381663,1.8479016503344183,1.847901644422662,1.8479016384008877,1.8479016322670472,1.847901626019056,1.8479016196547888,1.8479016131720818,1.8479016065687313,1.847901599842492,1.8479015929910767,1.8479015860121561,1.847901578903357,1.8479015716622629,1.8479015642864114,1.8479015567732948,1.8479015491203583,1.8479015413250004,1.8479015333845703,1.8479015252963684,1.8479015170576443,1.8479015086655972,1.8479015001173735,1.8479014914100669,1.8479014825407172,1.847901473506308,1.847901464303768,1.8479014549299682,1.8479014453817215,1.847901435655782,1.847901425748842,1.8479014156575335,1.8479014053784253,1.8479013949080227,1.8479013842427654,1.8479013733790277,1.847901362313115,1.8479013510412656,1.847901339559647,1.8479013278643552,1.8479013159514142,1.8479013038167726,1.8479012914563056,1.8479012788658098,1.847901266041005,1.8479012529775305,1.847901239670945,1.847901226116724,1.8479012123102587,1.8479011982468556,1.847901183921733,1.8479011693300196,1.8479011544667552,1.8479011393268852,1.847901123905263,1.847901108196645,1.8479010921956904,1.847901075896958,1.8479010592949072,1.8479010423838935,1.8479010251581662,1.8479010076118696,1.847900989739037,1.8479009715335923,1.847900952989346,1.8479009340999921,1.847900914859109,1.8479008952601543,1.8479008752964652,1.8479008549612532,1.8479008342476053,1.8479008131484789,1.8479007916566996,1.8479007697649608,1.8479007474658191,1.8479007247516934,1.8479007016148605,1.8479006780474547,1.8479006540414622,1.8479006295887221,1.8479006046809199,1.8479005793095877,1.847900553466099,1.8479005271416675,1.847900500327343,1.847900473014009,1.8479004451923786,1.8479004168529933,1.847900387986218,1.8479003585822376,1.8479003286310556,1.8479002981224892,1.8479002670461653,1.8479002353915184,1.8479002031477862,1.8479001703040066,1.8479001368490127,1.8479001027714306,1.847900068059674,1.8479000327019415,1.847899996686212,1.847899960000241,1.8478999226315558,1.8478998845674506,1.8478998457949856,1.8478998063009782,1.8478997660720007,1.8478997250943772,1.8478996833541748,1.8478996408372026,1.8478995975290067,1.8478995534148626,1.8478995084797722,1.8478994627084586,1.847899416085361,1.847899368594628,1.8478993202201142,1.847899270945373,1.8478992207536529,1.8478991696278892,1.847899117550701,1.8478990645043831,1.8478990104709012,1.8478989554318854,1.847898899368624,1.8478988422620575,1.8478987840927705,1.8478987248409882,1.8478986644865663,1.8478986030089861,1.8478985403873478,1.8478984766003612,1.8478984116263422,1.8478983454432008,1.8478982780284376,1.847898209359134,1.8478981394119451,1.8478980681630919,1.8478979955883519,1.8478979216630533,1.8478978463620641,1.8478977696597851,1.8478976915301415,1.8478976119465722,1.8478975308820225,1.847897448308934,1.8478973641992364,1.8478972785243364,1.8478971912551083,1.847897102361885,1.8478970118144484,1.8478969195820163,1.8478968256332349,1.847896729936167,1.8478966324582808,1.84789653316644,1.847896432026891,1.847896329005252,1.847896224066503,1.8478961171749708,1.8478960082943197,1.8478958973875372,1.8478957844169226,1.8478956693440736,1.8478955521298739,1.8478954327344788,1.847895311117303,1.8478951872370053,1.8478950610514766,1.8478949325178233,1.8478948015923542,1.8478946682305661,1.8478945323871263,1.84789439401586,1.8478942530697333,1.8478941095008365,1.84789396326037,1.847893814298625,1.8478936625649685,1.8478935080078265,1.8478933505746644,1.8478931902119706,1.8478930268652385,1.8478928604789473,1.8478926909965436,1.8478925183604216,1.847892342511905,1.8478921633912246,1.8478919809375003,1.8478917950887201,1.847891605781717,1.8478914129521504,1.8478912165344825,1.8478910164619564,1.8478908126665736,1.8478906050790715,1.8478903936288982,1.8478901782441899,1.847889958851747,1.847889735377007,1.8478895077440218,1.8478892758754308,1.8478890396924337,1.847888799114766,1.8478885540606689,1.8478883044468646,1.8478880501885255,1.8478877911992468,1.8478875273910174,1.8478872586741883,1.847886984957445,1.847886706147773,1.8478864221504296,1.8478861328689098,1.8478858382049137,1.8478855380583148,1.847885232327123,1.847884920907453,1.8478846036934873,1.8478842805774416,1.8478839514495264,1.8478836161979117,1.8478832747086882,1.8478829268658281,1.8478825725511472,1.8478822116442635,1.8478818440225573,1.847881469561129,1.847881088132757,1.847880699607855,1.847880303854427,1.847879900738023,1.847879490121696,1.8478790718659501,1.8478786458286984,1.8478782118652124,1.8478777698280728,1.8478773195671214,1.8478768609294085,1.8478763937591407,1.8478759178976303,1.8478754331832394,1.8478749394513274,1.8478744365341924,1.8478739242610158,1.8478734024578065,1.847872870947338,1.8478723295490918,1.8478717780791942,1.8478712163503552,1.847870644171805,1.847870061349228,1.8478694676847003,1.8478688629766187,1.8478682470196357,1.8478676196045876,1.8478669805184265,1.8478663295441455,1.847865666460707,1.847864991042967,1.8478643030615995,1.84786360228302,1.8478628884693042,1.8478621613781085,1.8478614207625892,1.8478606663713184,1.8478598979481986,1.8478591152323762,1.8478583179581543,1.8478575058549016,1.847856678646962,1.847855836053561,1.8478549777887119,1.8478541035611176,1.847853213074073,1.847852306025367,1.847851382107175,1.8478504410059615,1.8478494824023697,1.8478485059711163,1.8478475113808803,1.8478464982941925,1.8478454663673196,1.8478444152501523,1.8478433445860822,1.8478422540118855,1.8478411431575998,1.847840011646397,1.8478388590944606,1.8478376851108531,1.8478364892973844,1.8478352712484802,1.8478340305510441,1.8478327667843173,1.8478314795197393,1.847830168320802,1.8478288327429035,1.8478274723331995,1.8478260866304492,1.8478246751648615,1.847823237457937,1.8478217730223072,1.8478202813615696,1.8478187619701243,1.8478172143329994,1.847815637925682,1.8478140322139411,1.847812396653648,1.847810730690593,1.8478090337603021,1.847807305287844,1.8478055446876418,1.8478037513632728,1.8478019247072712,1.8478000641009231,1.8477981689140612,1.847796238504852,1.8477942722195817,1.8477922693924387,1.8477902293452884,1.847788151387449,1.8477860348154596,1.8477838789128447,1.8477816829498757,1.8477794461833275,1.8477771678562296,1.8477748471976136,1.8477724834222573,1.8477700757304214,1.847767623307583,1.8477651253241665,1.8477625809352645,1.8477599892803573,1.8477573494830284,1.847754660650671,1.8477519218741913,1.8477491322277075,1.8477462907682418,1.8477433965354062,1.8477404485510858,1.8477374458191127,1.8477343873249363,1.8477312720352865,1.8477280988978326,1.847724866840834,1.8477215747727846,1.8477182215820545,1.8477148061365196,1.8477113272831887,1.847707783847824,1.847704174634551,1.847700498425466,1.8476967539802334,1.8476929400356787,1.84768905530537,1.8476850984791966,1.847681068222938,1.847676963177825,1.847672781960092,1.8476685231605257,1.8476641853440008,1.8476597670490094,1.8476552667871842,1.8476506830428086,1.8476460142723234,1.847641258903821,1.8476364153365312,1.8476314819403015,1.8476264570550625,1.8476213389902882,1.8476161260244461,1.8476108164044374,1.8476054083450253,1.8475999000282568,1.8475942896028732,1.8475885751837087,1.84758275485108,1.8475768266501669,1.8475707885903778,1.847564638644709,1.8475583747490891,1.8475519948017156,1.8475454966623759,1.847538878151761,1.847532137050764,1.8475252710997692,1.8475182779979258,1.8475111554024128,1.8475039009276897,1.8474965121447338,1.8474889865802644,1.8474813217159567,1.8474735149876385,1.847465563784476,1.8474574654481446,1.8474492172719863,1.8474408165001535,1.8474322603267364,1.8474235458948784,1.8474146702958738,1.847405630568255,1.8473964236968592,1.8473870466118831,1.8473774961879215,1.8473677692429888,1.8473578625375255,1.8473477727733891,1.8473374965928246,1.847327030577426,1.8473163712470722,1.8473055150588498,1.8472944584059605,1.8472831976166062,1.8472717289528606,1.847260048609519,1.8472481527129334,1.8472360373198253,1.8472236984160824,1.847211131915536,1.8471983336587185,1.8471852994116007,1.8471720248643106,1.8471585056298339,1.8471447372426884,1.8471307151575855,1.8471164347480677,1.8471018913051231,1.8470870800357808,1.8470719960616886,1.8470566344176622,1.8470409900502178,1.847025057816081,1.8470088324806753,1.8469923087165838,1.8469754811019945,1.8469583441191175,1.8469408921525832,1.846923119487814,1.8469050203093755,1.8468865886993033,1.8468678186354044,1.8468487039895378,1.8468292385258678,1.8468094158990966,1.8467892296526691,1.8467686732169564,1.846747739907411,1.846726422922702,1.8467047153428204,1.8466826101271632,1.8466601001125906,1.8466371780114585,1.8466138364096254,1.8465900677644342,1.8465658644026695,1.8465412185184873,1.8465161221713207,1.8464905672837615,1.8464645456394129,1.8464380488807195,1.84641106850677,1.8463835958710768,1.846355622179328,1.846327138487115,1.8462981356976347,1.846268604559368,1.8462385356637319,1.846207919442706,1.846176746166439,1.8461450059408226,1.846112688705052,1.846079784229153,1.8460462821114918,1.8460121717762572,1.8459774424709245,1.845942083263692,1.8459060830409,1.8458694305044252,1.8458321141690548,1.8457941223598404,1.8457554432094296,1.8457160646553825,1.8456759744374667,1.8456351600949303,1.8455936089637652,1.8455513081739494,1.8455082446466702,1.8454644050915412,1.845419776003792,1.8453743436614607,1.84532809412256,1.8452810132222386,1.8452330865699338,1.8451842995465093,1.8451346373013924,1.8450840847496999,1.8450326265693582,1.8449802471982242,1.8449269308312006,1.8448726614173496,1.8448174226570109,1.8447611979989191,1.844703970637327,1.8446457235091334,1.844586439291022,1.844526100396607,1.8444646889735912,1.8444021869009402,1.8443385757860706,1.8442738369620586,1.8442079514848695,1.844140900130608,1.8440726633928002,1.8440032214796969,1.8439325543116172,1.843860641518317,1.8437874624364048,1.8437129961067875,1.8436372212721712,1.843560116374598,1.8434816595530412,1.8434018286410498,1.8433206011644543,1.8432379543391273,1.8431538650688177,1.8430683099430465,1.8429812652350799,1.8428927068999785,1.8428026105727282,1.8427109515664568,1.842617704870743,1.842522845150018,1.8424263467420712,1.842328183656658,1.8422283295742192,1.8421267578447176,1.8420234414865924,1.8419183531858443,1.841811465295249,1.8417027498337142,1.8415921784857727,1.8414797226012354,1.8413653531949925,1.841249040946984,1.8411307562023347,1.8410104689716686,1.840888148931607,1.8407637654254512,1.8406372874640713,1.8405086837269906,1.840377922563688,1.840244971995117,1.8401097997154523,1.8399723730940747,1.8398326591777914,1.8396906246933178,1.8395462360500119,1.8393994593428766,1.8392502603558452,1.8390986045653426,1.8389444571441507,1.8387877829655703,1.838628546607897,1.838466712359221,1.8383022442225547,1.8381351059213025,1.8379652609050832,1.8377926723559053,1.8376173031947205,1.8374391160883512,1.8372580734568098,1.8370741374810187,1.8368872701109378,1.8366974330741157,1.8365045878846709,1.8363086958527117,1.8361097180942136,1.8359076155413532,1.8357023489533204,1.8354938789276125,1.8352821659118215,1.8350671702159305,1.8348488520251243,1.8346271714131277,1.83440208835608,1.834173562746965,1.833941554410589,1.8337060231191429,1.8334669286083343,1.8332242305941175,1.8329778887900259,1.8327278629251136,1.8324741127625255,1.8322165981186975,1.8319552788832043,1.8316901150392573,1.831421066684872,1.8311480940547045,1.830871157542576,1.8305902177246887,1.8303052353835436,1.8300161715325713,1.8297229874414827,1.8294256446623498,1.8291241050564209,1.8288183308216832,1.8285082845211786,1.828193929112078,1.827875227975522,1.8275521449472398,1.827224644348941,1.8268926910204983,1.8265562503529165,1.826215288322101,1.825869771523424,1.825519667207091,1.825164943314318,1.824805568514317,1.824441512242088,1.8240727447370293,1.8236992370823548,1.8233209612453247,1.8229378901182915,1.822549997560552,1.822157258441003,1.8217596486816106,1.8213571453016681,1.8209497264628536,1.8205373715150759,1.8201200610430974,1.8196977769139322,1.8192705023250055,1.818838221853066,1.8184009215038393,1.817958588762409,1.8175112126443105,1.8170587837473233,1.8166012943039482,1.816138738234542,1.8156711112011004,1.8151984106616637,1.814720635925319,1.8142377882077858,1.8137498706875446,1.8132568885624962,1.8127588491071105,1.8122557617300439,1.811747638032185,1.8112344918651035,1.8107163393898569,1.8101931991361255,1.809665092061632,1.809132041611806,1.8085940737796502,1.8080512171657632,1.8075035030384732,1.80695096539403,1.8063936410168138,1.805831569539492,1.8052647935030914,1.8046933584169074,1.8041173128182082,1.8035367083316647,1.802951599728449,1.8023620449849318,1.801768105340921,1.8011698453573646,1.8005673329734577,1.7999606395630745,1.7993498399904544,1.7987350126650745,1.7981162395956214,1.7974936064429903,1.7968672025722352,1.7962371211033825,1.7956034589610275,1.7949663169226393,1.794325799665475,1.793682015812027,1.7930350779739168,1.79238510279414,1.7917322109875837,1.7910765273797165,1.7904181809433708,1.7897573048335151,1.789094036419939,1.7884285173177406,1.787760893415544,1.7870913149013379,1.7864199362858504,1.785746916423365,1.7850724185298859,1.784396610198556,1.7837196634122428,1.7830417545531856,1.7823630644096289,1.781683778179335,1.7810040854698939,1.7803241802957361,1.7796442610717556,1.778964530603457,1.7782851960735393,1.777606469024821,1.776928565339425,1.7762517052141362,1.7755761131318444,1.774902017828988,1.774229652258916,1.7735592535510867,1.772891062966019,1.7722253258459155,1.7715622915608864,1.7709022134506833,1.7702453487618806,1.7695919585804174,1.7689423077594362,1.7682966648423388,1.7676553019809964,1.7670184948490384,1.766386522550159,1.7657596675213745,1.7651382154311677,1.7645224550724639,1.7639126782503736,1.7633091796646545,1.7627122567868352,1.762122209731954,1.7615393411248663,1.7609639559610801,1.7603963614620768,1.7598368669250923,1.7592857835673177,1.758743424364506,1.7582101038839597,1.7576861381118882,1.7571718442751278,1.7566675406572259,1.756173546408896,1.7556901813528574,1.7552177657830859,1.7547566202585074,1.754307065391173,1.7538694216289745,1.7534440090329584,1.7530311470493132,1.7526311542761197,1.7522443482249597,1.7518710450774988,1.7515115594371655,1.7511662040760736,1.7508352896773367,1.7505191245729528,1.750218014477442,1.7499322622174454,1.7496621674575055,1.7494080264222676,1.7491701316153585,1.7489487715352228,1.7487442303882057,1.7485567877991983,1.74838671852018,1.7482342921370033,1.7480997727747971,1.7479834188023744,1.747885482536052,1.7478062099433105,1.7477458403467332,1.7477046061286938,1.7476827324372581,1.7476804368938088,1.7476979293028856,1.7477354113647792,1.747793076391405,1.7478711090260066,1.7479696849672548,1.7480889706982954,1.7482291232213385,1.7483902897983596,1.7485726076985018,1.748776203952769,1.7490011951166002,1.7492476870409115,1.7495157746521934,1.7498055417422431,1.7501170607681036,1.7504503926627768,1.750805586657259,1.75118268011444,1.751581698375385,1.752002654618508,1.7524455497321183,1.7529103722007993,1.7533970980060614,1.753905690541677,1.7544361005440772,1.7549882660381677,1.7555621122988763,1.756157551828723,1.7567744843516566,1.7574127968233793,1.7580723634583226,1.758753045773421,1.7594546926487697,1.7601771404052229,1.7609202128989454,1.7616837216328831,1.7624674658850803,1.7632712328537252,1.7640947978187647,1.7649379243198815,1.7658003643505948,1.766681858568187,1.7675821365191353,1.768500916879676,1.7694379077110893,1.7703928067292645,1.7713653015880557,1.7723550701759145,1.7733617809252424,1.7743850931338865,1.775424657298157,1.776480115456734,1.7775511015447922,1.7786372417576557,1.7797381549232751,1.7808534528827968,1.7819827408784796,1.7831256179482053,1.78428167732581,1.7854505068464641,1.7866316893563157,1.7878248031256123,1.7890294222645187,1.7902451171408393,1.7914714547988735,1.792707999378624,1.7939543125345945,1.7952099538534314,1.7964744812696556,1.797747451478773,1.7990284203470508,1.8003169433172685,1.8016125758097865,1.8029148736182758,1.8042233932995009,1.8055376925565487,1.806857330614946,1.8081818685911224,1.8095108698526992,1.8108439003701358,1.8121805290592712,1.813520328114346,1.8148628733311134,1.8162077444196816,1.8175545253067622,1.8189028044270321,1.820252175003345,1.8216022353155625,1.8229525889578104,1.824302845083985,1.82565261864138,1.827001530592323,1.8283492081237454,1.8296952848446366,1.8310394009713644,1.8323812035008602,1.8337203463717153,1.8350564906132296,1.8363893044825068,1.8377184635896953,1.8390436510115016,1.8403645573931278,1.8416808810387915,1.842992327991026,1.8442986120989526,1.84559945507575,1.846894586545553,1.8481837440800288,1.8494666732248917,1.850743127516622,1.852012868489682,1.8532756656745084,1.8545312965865868,1.8557795467069098,1.857020209454136,1.8582530861487563,1.859477985969599,1.8606947259029862,1.8619031306848737,1.8631030327362945,1.8642942720924356,1.8654766963256717,1.8666501604628811,1.8678145268973594,1.8689696652956642,1.8701154524996855,1.871251772424275,1.8723785159507254,1.8734955808164089,1.8746028715008711,1.8757002991086666,1.8767877812492282,1.8778652419140363,1.8789326113513694,1.8799898259388923,1.8810368280543417,1.8820735659445524,1.8830999935930672,1.884116070586566,1.8851217619803269,1.8861170381629486],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.7113429599838623,1.7113429600381758,1.7113429600935002,1.7113429601498542,1.7113429602072572,1.711342960265728,1.7113429603252874,1.7113429603859551,1.711342960447752,1.7113429605106985,1.7113429605748167,1.7113429606401283,1.7113429607066553,1.7113429607744202,1.711342960843446,1.7113429609137571,1.7113429609853763,1.7113429610583282,1.711342961132638,1.7113429612083306,1.711342961285432,1.7113429613639681,1.711342961443966,1.7113429615254525,1.7113429616084554,1.7113429616930032,1.7113429617791245,1.7113429618668485,1.7113429619562048,1.7113429620472242,1.7113429621399376,1.7113429622343763,1.7113429623305727,1.7113429624285592,1.7113429625283694,1.7113429626300365,1.7113429627335965,1.7113429628390833,1.7113429629465333,1.711342963055983,1.7113429631674695,1.7113429632810313,1.7113429633967057,1.7113429635145332,1.7113429636345536,1.7113429637568076,1.7113429638813367,1.7113429640081834,1.7113429641373905,1.7113429642690026,1.7113429644030638,1.71134296453962,1.7113429646787175,1.7113429648204035,1.7113429649647265,1.7113429651117353,1.71134296526148,1.7113429654140115,1.711342965569382,1.7113429657276433,1.7113429658888502,1.7113429660530575,1.7113429662203206,1.7113429663906963,1.711342966564243,1.7113429667410192,1.7113429669210856,1.711342967104503,1.7113429672913334,1.7113429674816414,1.7113429676754908,1.7113429678729475,1.7113429680740795,1.7113429682789543,1.711342968487642,1.7113429687002135,1.711342968916741,1.711342969137298,1.71134296936196,1.711342969590803,1.7113429698239044,1.7113429700613443,1.7113429703032028,1.7113429705495626,1.7113429708005072,1.711342971056122,1.7113429713164936,1.7113429715817112,1.7113429718518645,1.7113429721270452,1.7113429724073477,1.7113429726928662,1.7113429729836986,1.7113429732799432,1.7113429735817014,1.7113429738890753,1.7113429742021697,1.7113429745210909,1.7113429748459472,1.7113429751768492,1.7113429755139096,1.711342975857243,1.7113429762069656,1.7113429765631971,1.7113429769260577,1.7113429772956716,1.7113429776721643,1.711342978055664,1.7113429784463003,1.7113429788442067,1.7113429792495183,1.7113429796623731,1.7113429800829114,1.7113429805112759,1.7113429809476126,1.71134298139207,1.7113429818447987,1.711342982305953,1.7113429827756896,1.7113429832541684,1.711342983741552,1.711342984238006,1.7113429847436994,1.7113429852588038,1.711342985783495,1.7113429863179508,1.711342986862353,1.7113429874168873,1.7113429879817417,1.7113429885571083,1.711342989143183,1.7113429897401649,1.711342990348257,1.711342990967666,1.7113429915986031,1.711342992241282,1.7113429928959218,1.7113429935627447,1.711342994241978,1.7113429949338521,1.7113429956386026,1.7113429963564692,1.7113429970876957,1.711342997832531,1.7113429985912283,1.7113429993640452,1.7113430001512455,1.711343000953096,1.7113430017698694,1.7113430026018444,1.7113430034493027,1.7113430043125328,1.7113430051918286,1.711343006087489,1.7113430069998186,1.7113430079291274,1.7113430088757315,1.711343009839953,1.7113430108221195,1.7113430118225652,1.7113430128416303,1.711343013879661,1.7113430149370108,1.711343016014039,1.7113430171111121,1.711343018228603,1.7113430193668913,1.7113430205263647,1.7113430217074175,1.7113430229104507,1.711343024135874,1.711343025384104,1.7113430266555651,1.71134302795069,1.7113430292699185,1.7113430306136994,1.7113430319824905,1.7113430333767565,1.7113430347969714,1.711343036243619,1.7113430377171905,1.7113430392181874,1.7113430407471202,1.7113430423045084,1.7113430438908823,1.7113430455067808,1.7113430471527542,1.7113430488293617,1.7113430505371736,1.7113430522767705,1.7113430540487446,1.711343055853698,1.7113430576922446,1.7113430595650103,1.7113430614726313,1.7113430634157567,1.7113430653950472,1.7113430674111763,1.711343069464829,1.7113430715567048,1.7113430736875146,1.7113430758579828,1.7113430780688483,1.7113430803208625,1.7113430826147913,1.7113430849514153,1.7113430873315292,1.7113430897559423,1.7113430922254793,1.7113430947409802,1.7113430973033006,1.7113430999133121,1.7113431025719024,1.7113431052799752,1.7113431080384527,1.7113431108482726,1.7113431137103907,1.7113431166257804,1.7113431195954334,1.71134312262036,1.711343125701589,1.7113431288401686,1.7113431320371661,1.7113431352936697,1.7113431386107867,1.7113431419896454,1.7113431454313952,1.7113431489372068,1.7113431525082734,1.7113431561458092,1.7113431598510518,1.7113431636252616,1.7113431674697226,1.7113431713857428,1.7113431753746544,1.7113431794378142,1.7113431835766044,1.7113431877924337,1.7113431920867357,1.7113431964609718,1.7113432009166298,1.711343205455226,1.7113432100783044,1.711343214787438,1.7113432195842284,1.7113432244703084,1.7113432294473405,1.7113432345170174,1.7113432396810648,1.7113432449412393,1.7113432502993307,1.7113432557571626,1.711343261316592,1.7113432669795103,1.711343272747845,1.7113432786235583,1.7113432846086503,1.7113432907051573,1.711343296915154,1.711343303240754,1.7113433096841095,1.7113433162474134,1.7113433229328996,1.7113433297428429,1.7113433366795614,1.7113433437454157,1.7113433509428109,1.7113433582741961,1.7113433657420671,1.7113433733489656,1.7113433810974803,1.7113433889902498,1.7113433970299592,1.7113434052193464,1.711343413561198,1.711343422058354,1.7113434307137076,1.7113434395302043,1.711343448510846,1.7113434576586901,1.7113434669768506,1.7113434764685003,1.7113434861368702,1.7113434959852523,1.711343506017,1.7113435162355288,1.7113435266443178,1.7113435372469115,1.7113435480469201,1.7113435590480213,1.7113435702539614,1.7113435816685565,1.7113435932956935,1.7113436051393327,1.7113436172035075,1.711343629492326,1.711343642009975,1.711343654760717,1.7113436677488958,1.7113436809789349,1.7113436944553413,1.7113437081827054,1.711343722165704,1.7113437364091004,1.7113437509177472,1.7113437656965875,1.7113437807506569,1.711343796085084,1.711343811705094,1.7113438276160098,1.7113438438232524,1.7113438603323463,1.7113438771489164,1.7113438942786943,1.7113439117275182,1.711343929501335,1.7113439476062038,1.7113439660482952,1.7113439848338956,1.7113440039694097,1.7113440234613604,1.7113440433163936,1.7113440635412784,1.711344084142912,1.7113441051283176,1.711344126504652,1.7113441482792058,1.7113441704594035,1.7113441930528108,1.7113442160671337,1.7113442395102223,1.7113442633900735,1.7113442877148333,1.7113443124928005,1.7113443377324293,1.7113443634423313,1.711344389631279,1.7113444163082099,1.7113444434822274,1.7113444711626065,1.7113444993587947,1.7113445280804165,1.7113445573372765,1.7113445871393629,1.7113446174968507,1.7113446484201051,1.7113446799196854,1.711344712006349,1.7113447446910537,1.7113447779849638,1.7113448118994514,1.7113448464461023,1.711344881636719,1.7113449174833248,1.7113449539981689,1.7113449911937295,1.711345029082719,1.7113450676780873,1.7113451069930277,1.7113451470409804,1.711345187835638,1.71134522939095,1.711345271721126,1.711345314840644,1.7113453587642522,1.7113454035069764,1.7113454490841238,1.7113454955112888,1.7113455428043587,1.7113455909795192,1.7113456400532596,1.711345690042379,1.7113457409639914,1.7113457928355336,1.711345845674768,1.711345899499793,1.7113459543290452,1.7113460101813094,1.711346067075722,1.71134612503178,1.7113461840693474,1.7113462442086615,1.7113463054703402,1.711346367875389,1.7113464314452107,1.7113464962016085,1.7113465621667974,1.7113466293634112,1.7113466978145093,1.7113467675435856,1.7113468385745776,1.7113469109318726,1.7113469846403195,1.7113470597252345,1.7113471362124117,1.7113472141281325,1.7113472934991734,1.7113473743528165,1.71134745671686,1.7113475406196268,1.7113476260899738,1.7113477131573043,1.7113478018515775,1.7113478922033192,1.7113479842436312,1.7113480780042059,1.7113481735173335,1.7113482708159165,1.7113483699334804,1.7113484709041862,1.7113485737628409,1.7113486785449126,1.711348785286541,1.7113488940245523,1.7113490047964703,1.711349117640532,1.7113492325956996,1.7113493497016763,1.7113494689989188,1.7113495905286535,1.7113497143328902,1.7113498404544383,1.7113499689369216,1.7113500998247944,1.7113502331633585,1.711350368998777,1.7113505073780948,1.7113506483492529,1.7113507919611075,1.7113509382634466,1.7113510873070092,1.7113512391435044,1.711351393825628,1.7113515514070845,1.7113517119426056,1.7113518754879704,1.7113520421000263,1.7113522118367097,1.7113523847570677,1.7113525609212796,1.7113527403906794,1.7113529232277789,1.7113531094962906,1.711353299261152,1.7113534925885487,1.71135368954594,1.7113538902020842,1.711354094627064,1.7113543028923135,1.711354515070644,1.7113547312362718,1.7113549514648478,1.7113551758334835,1.7113554044207817,1.7113556373068668,1.7113558745734134,1.7113561163036797,1.711356362582538,1.7113566134965075,1.711356869133786,1.7113571295842869,1.7113573949396708,1.7113576652933824,1.7113579407406863,1.711358221378703,1.7113585073064483,1.71135879862487,1.711359095436888,1.7113593978474357,1.711359705963498,1.7113600198941568,1.711360339750631,1.7113606656463216,1.711360997696857,1.711361336020137,1.711361680736382,1.7113620319681782,1.7113623898405281,1.7113627544809007,1.711363126019281,1.7113635045882236,1.7113638903229056,1.7113642833611808,1.7113646838436365,1.7113650919136496,1.7113655077174452,1.7113659314041563,1.7113663631258849,1.7113668030377647,1.7113672512980243,1.7113677080680527,1.7113681735124666,1.7113686477991779,1.711369131099464,1.7113696235880405,1.7113701254431315,1.7113706368465469,1.7113711579837594,1.7113716890439796,1.7113722302202405,1.711372781709477,1.7113733437126095,1.7113739164346322,1.7113745000846994,1.7113750948762165,1.7113757010269308,1.711376318759029,1.71137694829923,1.711377589878888,1.71137824373409,1.711378910105764,1.71137958923978,1.7113802813870642,1.7113809868037075,1.711381705751081,1.711382438495952,1.711383185310605,1.7113839464729645,1.71138472226672,1.7113855129814557,1.7113863189127834,1.7113871403624754,1.711387977638606,1.7113888310556933,1.7113897009348427,1.7113905876039,1.7113914913976007,1.7113924126577307,1.7113933517332844,1.7113943089806305,1.711395284763683,1.7113962794540716,1.7113972934313222,1.7113983270830382,1.7113993808050878,1.7114004550017956,1.71140155008614,1.7114026664799544,1.711403804614135,1.7114049649288525,1.7114061478737705,1.7114073539082693,1.7114085835016741,1.7114098371334925,1.7114111152936535,1.7114124184827575,1.7114137472123299,1.7114151020050814,1.711416483395177,1.711417891928511,1.7114193281629886,1.7114207926688156,1.711422286028798,1.7114238088386433,1.7114253617072783,1.711426945257169,1.7114285601246515,1.7114302069602705,1.7114318864291302,1.7114335992112495,1.711435346001932,1.7114371275121416,1.711438944468892,1.711440797615644,1.711442687712715,1.7114446155376983,1.7114465818858962,1.7114485875707635,1.7114506334243622,1.711452720297828,1.7114548490618566,1.7114570206071922,1.711459235845138,1.7114614957080765,1.7114638011500078,1.7114661531470983,1.7114685526982478,1.7114710008256704,1.7114734985754938,1.7114760470183725,1.7114786472501193,1.7114813003923552,1.7114840075931759,1.7114867700278371,1.71148958889946,1.7114924654397539,1.711495400909763,1.7114983966006312,1.7115014538343871,1.7115045739647539,1.7115077583779807,1.711511008493697,1.7115143257657903,1.7115177116833094,1.7115211677713935,1.711524695592225,1.7115282967460113,1.7115319728719922,1.7115357256494783,1.7115395567989151,1.7115434680829784,1.7115474613077013,1.711551538323631,1.7115557010270204,1.7115599513610487,1.7115642913170812,1.711568722935962,1.7115732483093404,1.7115778695810397,1.7115825889484597,1.711587408664019,1.7115923310366412,1.7115973584332755,1.7116024932804674,1.7116077380659669,1.7116130953403847,1.711618567718893,1.7116241578829745,1.7116298685822176,1.7116357026361633,1.7116416629362028,1.7116477524475253,1.7116539742111232,1.7116603313458472,1.7116668270505226,1.711673464606121,1.7116802473779902,1.711687178818148,1.7116942624676363,1.7117015019589377,1.7117089010184616,1.7117164634690931,1.7117241932328129,1.7117320943333871,1.7117401708991296,1.7117484271657364,1.7117568674791979,1.7117654962987867,1.7117743182001253,1.711783337878335,1.7117925601512665,1.711801989962817,1.7118116323863324,1.7118214926280988,1.7118315760309244,1.711841888077815,1.7118524343957402,1.7118632207595033,1.7118742530957023,1.7118855374867976,1.7118970801752775,1.711908887567933,1.7119209662402382,1.711933322940838,1.711945964596151,1.7119588983150826,1.7119721313938556,1.7119856713209587,1.7119995257822143,1.7120137026659699,1.7120282100684137,1.712043056299016,1.7120582498861006,1.7120737995825468,1.7120897143716267,1.7121060034729734,1.712122676348692,1.7121397427096083,1.7121572125216575,1.71217509601242,1.7121934036778017,1.7122121462888615,1.7122313348987908,1.7122509808500435,1.7122710957816185,1.7122916916365025,1.712312780669264,1.7123343754538096,1.712356488891303,1.7123791342182408,1.7124023250146965,1.712426075212727,1.7124503991049462,1.7124753113532671,1.7125008269978084,1.712526961465976,1.7125537305817091,1.712581150574902,1.7126092380909925,1.7126380102007248,1.7126674844100853,1.7126976786704045,1.7127286113886382,1.7127603014378132,1.7127927681676482,1.7128260314153425,1.712860111516532,1.7128950293164185,1.7129308061810562,1.7129674640088133,1.7130050252419848,1.7130435128785748,1.71308295048423,1.7131233622043316,1.7131647727762354,1.7132072075416622,1.713250692459231,1.7132952541171318,1.713340919745935,1.7133877172315282,1.713435675128179,1.7134848226717199,1.7135351897928406,1.7135868071304905,1.713639706045378,1.7136939186335647,1.7137494777401392,1.7138064169729703,1.7138647707165233,1.7139245741457354,1.7139858632399403,1.714048674796822,1.714113046446406,1.7141790166650537,1.7142466247894674,1.7143159110306814,1.714386916488031,1.7144596831630856,1.71453425397353,1.7146106727669805,1.7146889843347215,1.7147692344253433,1.7148514697582697,1.7149357380371535,1.7150220879631255,1.7151105692478792,1.715201232626569,1.715294129870509,1.7153893137996452,1.7154868382947868,1.7155867583095743,1.7156891298821624,1.7157940101465954,1.715901457343855,1.7160115308325559,1.7161242910992696,1.716239799768447,1.7163581196119215,1.716479314557965,1.7166034496998752,1.7167305913040658,1.7168608068176343,1.7169941648753895,1.7171307353062997,1.7172705891393483,1.7174137986087614,1.7175604371585875,1.7177105794465994,1.7178643013474904,1.7180216799553416,1.7181827935853347,1.7183477217746734,1.7185165452827031,1.7186893460901878,1.7188662073977223,1.7190472136232606,1.7192324503987195,1.7194220045656456,1.719615964169911,1.719814418455415,1.7200174578567675,1.7202251739909267,1.7204376596477664,1.7206550087795511,1.7208773164892928,1.7211046790179645,1.7213371937305517,1.7215749591009135,1.7218180746954332,1.7220666411554402,1.7223207601783728,1.722580534497671,1.7228460678613697,1.7231174650093797,1.7233948316494287,1.7236782744316523,1.7239679009218087,1.7242638195731,1.7245661396965863,1.7248749714301725,1.7251904257061468,1.7255126142172608,1.7258416493813318,1.7261776443043506,1.7265207127420805,1.726870969060131,1.7272285281924937,1.7275935055985225,1.727966017218348,1.728346179426706,1.7287341089851742,1.729129922992796,1.7295337388350855,1.7299456741313943,1.730365846680632,1.7307943744053271,1.731231375294018,1.7316769673419565,1.732131268490123,1.732594396562533,1.7330664692018325,1.7335476038031672,1.7340379174463205,1.734537526826112,1.7350465481810453,1.7355650972202052,1.7360932890483944,1.7366312380895077,1.7371790580081385,1.7377368616294238,1.7383047608571156,1.7388828665898957,1.7394712886359238,1.74007013562564,1.7406795149228165,1.7412995325338796,1.741930293015516,1.7425718993805739,1.743224453002289,1.743888053516854,1.7445627987243626,1.7452487844881608,1.745946104632641,1.7466548508395232,1.7473751125426702,1.7481069768214859,1.7488505282929596,1.7496058490024162,1.7503730183130435,1.7511521127942742,1.7519432061091031,1.7527463689004315,1.7535616686765338,1.754389169695754,1.755228932850538,1.7560810155509279,1.7569454716076391,1.75782235111486,1.7587117003329154,1.7596135615709418,1.7605279730697403,1.7614549688849668,1.762394578770839,1.7633468280645408,1.764311737571515,1.7652893234518436,1.7662795971079157,1.767282565073604,1.7682982289051574,1.7693265850740445,1.7703676248619722,1.7714213342583194,1.7724876938602234,1.7735666787755684,1.7746582585291195,1.7757623969720593,1.7768790521951796,1.7780081764459823,1.7791497160499468,1.7803036113362232,1.7814697965680044,1.78264819987783,1.7838387432080784,1.7850413422568878,1.7862559064297572,1.787482338797058,1.7887205360576948,1.7899703885091318,1.7912317800240067,1.7925045880335366,1.7937886835179082,1.7950839310038444,1.7963901885695122,1.7977073078569359,1.7990351340920594,1.8003735061125898,1.8017222564037334,1.8030812111419285,1.8044501902466512,1.8058290074403618,1.8072174703166335,1.8086153804164957,1.8100225333129956,1.8114387187039627,1.812863720512954,1.8142973169983148,1.8157392808702948,1.8171893794161156,1.8186473746328835,1.820113023368209,1.8215860774683812,1.823066283933921,1.8245533850823208,1.8260471187177585,1.8275472183075487,1.829053413165089,1.8305654286390276,1.83208298630837,1.833605804183227,1.8351335969108837,1.8366660759868636,1.8382029499706452,1.8397439247056715,1.841288703543292,1.8428369875702568,1.8443884758393774,1.8459428656029655,1.8474998525486441,1.8490591310371318,1.850620394341587,1.852183334888104,1.8537476444969454,1.8553130146240984,1.8568791366027422,1.8584457018842173,1.8600124022780886,1.8615789301908996,1.8631449788632237,1.864710242604621,1.8662744170261172,1.867837199269837,1.8693982882354239,1.8709573848028975,1.8725141920516073,1.8740684154749547,1.8756197631905716,1.8771679461456512,1.8787126783171515,1.8802536769065945,1.8817906625292118,1.8833233593971987,1.8848514954968507,1.886374802759381,1.8878930172252337,1.8894058792017177,1.8909131334138114,1.8924145291480037,1.893909820389052,1.895398765949556,1.8968811295922667,1.8983566801450664,1.8998251916085624,1.9012864432562704,1.90274021972737,1.9041863111120239,1.9056245130292888,1.9070546266976365,1.9084764589981338,1.9098898225303427,1.9112945356610056,1.9126904225656074,1.9140773132629048,1.915455043642536,1.9168234554858268,1.918182396479923,1.919531720225391,1.9208712862374302,1.9222009599408592,1.923520612659033,1.9248301215968677,1.9261293698181492,1.9274182462173066,1.928696645485839,1.9299644680735895,1.931221620145061,1.9324680135309749,1.9337035656752686,1.9349281995777454,1.9361418437325713,1.937344432062832,1.938535903851352,1.9397162036679854,1.9408852812935777,1.942043091640813,1.9431895946721354,1.9443247553149605,1.9454485433743633,1.9465609334434435,1.947661904811562,1.9487514413706353,1.9498295315196745,1.9508961680677512,1.951951348135569,1.9529950730558112,1.9540273482724364,1.9550481832390858,1.9560575913167604,1.9570555896709232,1.9580421991681776,1.9590174442726618],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.7267954193751747,1.7267954194821693,1.7267954195911548,1.7267954197021682,1.7267954198152482,1.7267954199304323,1.72679542004776,1.7267954201672713,1.7267954202890068,1.7267954204130076,1.7267954205393163,1.7267954206679756,1.7267954207990295,1.7267954209325218,1.7267954210684988,1.7267954212070065,1.7267954213480914,1.7267954214918022,1.7267954216381876,1.7267954217872972,1.7267954219391817,1.726795422093893,1.7267954222514834,1.7267954224120066,1.726795422575517,1.7267954227420705,1.7267954229117237,1.7267954230845342,1.726795423260561,1.7267954234398633,1.7267954236225025,1.7267954238085408,1.7267954239980414,1.7267954241910686,1.726795424387688,1.7267954245879664,1.7267954247919723,1.726795424999775,1.7267954252114446,1.7267954254270539,1.7267954256466753,1.726795425870384,1.7267954260982563,1.726795426330369,1.7267954265668015,1.7267954268076344,1.7267954270529489,1.726795427302829,1.7267954275573592,1.7267954278166266,1.7267954280807192,1.7267954283497264,1.7267954286237401,1.7267954289028533,1.7267954291871606,1.7267954294767593,1.7267954297717472,1.7267954300722252,1.726795430378295,1.7267954306900613,1.7267954310076294,1.7267954313311076,1.726795431660606,1.7267954319962364,1.7267954323381132,1.7267954326863522,1.7267954330410722,1.7267954334023936,1.7267954337704396,1.726795434145335,1.7267954345272072,1.7267954349161865,1.7267954353124046,1.726795435715997,1.7267954361270998,1.7267954365458538,1.726795436972401,1.7267954374068863,1.7267954378494579,1.726795438300266,1.7267954387594637,1.7267954392272071,1.7267954397036556,1.7267954401889714,1.7267954406833188,1.7267954411868665,1.7267954416997855,1.7267954422222502,1.726795442754438,1.7267954432965305,1.7267954438487116,1.726795444411169,1.7267954449840943,1.726795445567682,1.7267954461621304,1.726795446767642,1.7267954473844227,1.726795448012682,1.7267954486526336,1.7267954493044952,1.7267954499684883,1.7267954506448389,1.7267954513337769,1.7267954520355366,1.7267954527503564,1.7267954534784795,1.7267954542201536,1.7267954549756308,1.726795455745168,1.7267954565290269,1.7267954573274742,1.726795458140781,1.726795458969224,1.7267954598130852,1.7267954606726517,1.726795461548215,1.7267954624400734,1.72679546334853,1.7267954642738939,1.7267954652164799,1.7267954661766078,1.726795467154605,1.7267954681508035,1.7267954691655423,1.7267954701991663,1.726795471252027,1.7267954723244827,1.726795473416898,1.7267954745296439,1.7267954756630994,1.7267954768176499,1.7267954779936874,1.7267954791916125,1.7267954804118322,1.726795481654762,1.7267954829208239,1.7267954842104487,1.726795485524075,1.7267954868621493,1.7267954882251273,1.7267954896134716,1.7267954910276548,1.726795492468158,1.7267954939354708,1.7267954954300921,1.7267954969525308,1.7267954985033034,1.7267955000829387,1.7267955016919732,1.726795503330954,1.7267955050004387,1.7267955067009952,1.7267955084332016,1.7267955101976469,1.7267955119949314,1.7267955138256663,1.7267955156904744,1.7267955175899892,1.7267955195248574,1.726795521495737,1.7267955235032977,1.726795525548223,1.726795527631208,1.726795529752961,1.7267955319142039,1.7267955341156715,1.726795536358113,1.7267955386422906,1.726795540968981,1.7267955433389763,1.726795545753082,1.7267955482121191,1.7267955507169244,1.7267955532683494,1.7267955558672619,1.726795558514546,1.7267955612111023,1.7267955639578478,1.7267955667557169,1.7267955696056607,1.7267955725086492,1.7267955754656696,1.7267955784777274,1.7267955815458473,1.7267955846710732,1.726795587854468,1.7267955910971142,1.7267955944001152,1.7267955977645941,1.7267956011916956,1.7267956046825856,1.7267956082384515,1.7267956118605026,1.726795615549971,1.726795619308112,1.7267956231362043,1.7267956270355493,1.7267956310074741,1.72679563505333,1.7267956391744927,1.7267956433723646,1.726795647648374,1.7267956520039747,1.7267956564406495,1.7267956609599073,1.7267956655632852,1.72679567025235,1.7267956750286968,1.72679567989395,1.7267956848497659,1.7267956898978294,1.7267956950398589,1.7267957002776035,1.7267957056128456,1.7267957110474,1.726795716583116,1.726795722221877,1.7267957279656019,1.7267957338162447,1.7267957397757967,1.726795745846285,1.7267957520297756,1.7267957583283726,1.7267957647442191,1.726795771279499,1.7267957779364353,1.726795784717294,1.726795791624382,1.7267957986600504,1.7267958058266935,1.7267958131267498,1.7267958205627039,1.7267958281370868,1.726795835852476,1.7267958437114972,1.7267958517168258,1.726795859871186,1.7267958681773534,1.726795876638155,1.7267958852564709,1.7267958940352341,1.7267959029774336,1.7267959120861123,1.7267959213643718,1.7267959308153695,1.7267959404423237,1.7267959502485108,1.7267959602372696,1.7267959704120006,1.7267959807761675,1.7267959913332995,1.7267960020869897,1.726796013040901,1.7267960241987619,1.726796035564372,1.7267960471416008,1.726796058934391,1.7267960709467578,1.726796083182792,1.7267960956466604,1.7267961083426075,1.726796121274957,1.7267961344481129,1.7267961478665619,1.7267961615348741,1.726796175457705,1.7267961896397968,1.72679620408598,1.7267962188011754,1.726796233790396,1.7267962490587474,1.7267962646114314,1.7267962804537464,1.7267962965910895,1.7267963130289588,1.726796329772955,1.726796346828783,1.726796364202255,1.7267963818992906,1.7267963999259206,1.726796418288288,1.7267964369926505,1.7267964560453835,1.7267964754529797,1.726796495222055,1.726796515359347,1.72679653587172,1.726796556766167,1.7267965780498107,1.726796599729907,1.7267966218138475,1.7267966443091622,1.726796667223521,1.7267966905647376,1.7267967143407714,1.7267967385597311,1.726796763229876,1.7267967883596205,1.7267968139575358,1.7267968400323537,1.7267968665929685,1.7267968936484412,1.726796921208003,1.7267969492810562,1.7267969778771795,1.7267970070061314,1.7267970366778516,1.7267970669024666,1.7267970976902922,1.7267971290518367,1.7267971609978054,1.726797193539104,1.7267972266868417,1.7267972604523363,1.7267972948471164,1.7267973298829276,1.7267973655717344,1.7267974019257253,1.7267974389573177,1.726797476679161,1.7267975151041417,1.726797554245387,1.7267975941162712,1.7267976347304177,1.7267976761017065,1.7267977182442766,1.7267977611725323,1.726797804901148,1.7267978494450733,1.7267978948195375,1.726797941040056,1.726797988122435,1.7267980360827766,1.7267980849374853,1.726798134703274,1.726798185397168,1.7267982370365127,1.726798289638979,1.72679834322257,1.726798397805625,1.7267984534068295,1.726798510045219,1.7267985677401874,1.7267986265114916,1.7267986863792604,1.7267987473640014,1.7267988094866062,1.7267988727683607,1.7267989372309496,1.7267990028964668,1.7267990697874207,1.7267991379267433,1.726799207337798,1.7267992780443886,1.7267993500707661,1.7267994234416386,1.726799498182179,1.7267995743180349,1.7267996518753368,1.7267997308807068,1.7267998113612701,1.7267998933446624,1.726799976859041,1.7268000619330932,1.7268001485960487,1.7268002368776876,1.7268003268083534,1.7268004184189605,1.7268005117410092,1.7268006068065929,1.7268007036484125,1.7268008022997863,1.7268009027946623,1.7268010051676306,1.7268011094539357,1.7268012156894883,1.7268013239108788,1.7268014341553908,1.7268015464610127,1.7268016608664531,1.7268017774111541,1.7268018961353044,1.726802017079856,1.7268021402865361,1.7268022657978637,1.726802393657165,1.7268025239085887,1.7268026565971213,1.726802791768604,1.7268029294697493,1.726803069748157,1.726803212652332,1.7268033582317033,1.7268035065366383,1.7268036576184642,1.7268038115294861,1.726803968323005,1.7268041280533373,1.7268042907758356,1.7268044565469078,1.7268046254240386,1.7268047974658092,1.7268049727319206,1.7268051512832134,1.7268053331816913,1.726805518490544,1.7268057072741696,1.7268058995981987,1.7268060955295188,1.7268062951362988,1.7268064984880132,1.72680670565547,1.7268069167108344,1.7268071317276572,1.726807350780902,1.7268075739469713,1.7268078013037373,1.72680803293057,1.726808268908366,1.7268085093195795,1.7268087542482529,1.726809003780049,1.7268092580022818,1.7268095170039501,1.726809780875771,1.7268100497102146,1.7268103236015366,1.7268106026458168,1.7268108869409944,1.7268111765869036,1.7268114716853142,1.726811772339968,1.7268120786566197,1.7268123907430757,1.7268127087092378,1.7268130326671425,1.7268133627310052,1.7268136990172647,1.7268140416446267,1.7268143907341098,1.7268147464090933,1.726815108795363,1.726815478021161,1.7268158542172358,1.7268162375168918,1.7268166280560422,1.7268170259732611,1.7268174314098392,1.7268178445098368,1.7268182654201416,1.7268186942905264,1.7268191312737073,1.7268195765254037,1.7268200302044014,1.726820492472613,1.726820963495143,1.726821443440354,1.7268219324799325,1.7268224307889581,1.7268229385459721,1.7268234559330502,1.7268239831358743,1.7268245203438075,1.72682506774997,1.7268256255513161,1.726826193948716,1.726826773147033,1.7268273633552098,1.7268279647863511,1.7268285776578123,1.7268292021912848,1.726829838612889,1.7268304871532651,1.7268311480476686,1.7268318215360652,1.7268325078632305,1.7268332072788501,1.7268339200376226,1.726834646399364,1.7268353866291164,1.7268361409972564,1.726836909779608,1.7268376932575566,1.726838491718167,1.7268393054543014,1.726840134764744,1.726840979954323,1.7268418413340407,1.7268427192212037,1.726843613939554,1.726844525819408,1.7268454551977943,1.7268464024185952,1.726847367832694,1.726848351798122,1.7268493546802104,1.7268503768517454,1.7268514186931274,1.7268524805925323,1.726853562946076,1.726854666157985,1.7268557906407682,1.7268569368153928,1.726858105111465,1.7268592959674156,1.7268605098306846,1.7268617471579166,1.726863008415155,1.7268642940780445,1.7268656046320336,1.7268669405725856,1.7268683024053908,1.726869690646586,1.7268711058229769,1.7268725484722656,1.7268740191432834,1.7268755183962277,1.7268770468029044,1.726878604946977,1.726880193424217,1.7268818128427632,1.726883463823385,1.7268851469997522,1.7268868630187075,1.7268886125405498,1.7268903962393172,1.7268922148030812,1.726894068934244,1.7268959593498425,1.7268978867818596,1.72689985197754,1.726901855699713,1.726903898727125,1.7269059818547723,1.7269081058942468,1.7269102716740858,1.7269124800401288,1.7269147318558813,1.7269170280028874,1.7269193693811085,1.7269217569093092,1.7269241915254514,1.726926674187097,1.7269292058718158,1.7269317875776047,1.726934420323312,1.7269371051490725,1.7269398431167484,1.7269426353103816,1.7269454828366508,1.726948386825342,1.726951348429823,1.7269543688275304,1.7269574492204656,1.7269605908356969,1.7269637949258747,1.7269670627697538,1.7269703956727267,1.7269737949673674,1.726977262013981,1.72698079820117,1.726984404946405,1.7269880836966096,1.726991835928753,1.7269956631504568,1.726999566900609,1.7270035487499913,1.7270076103019176,1.7270117531928835,1.7270159790932262,1.7270202897077984,1.7270246867766537,1.7270291720757414,1.7270337474176183,1.727038414652169,1.727043175667341,1.727048032389893,1.7270529867861546,1.727058040862802,1.727063196667645,1.7270684562904297,1.727073821863654,1.7270792955633998,1.727084879610176,1.7270905762697812,1.7270963878541787,1.7271023167223876,1.7271083652813908,1.727114535987058,1.7271208313450879,1.7271272539119626,1.7271338062959247,1.7271404911579697,1.7271473112128548,1.727154269230131,1.7271613680351887,1.7271686105103277,1.7271759995958431,1.7271835382911354,1.727191229655838,1.727199076810968,1.7272070829401005,1.7272152512905616,1.7272235851746478,1.7272320879708676,1.7272407631252094,1.7272496141524312,1.727258644637379,1.7272678582363297,1.727277258678362,1.7272868497667564,1.7272966353804198,1.727306619475344,1.7273168060860917,1.7273271993273147,1.7273378033953029,1.727348622569567,1.7273596612144544,1.7273709237808006,1.7273824148076133,1.7273941389237966,1.7274061008499104,1.727418305399969,1.727430757483278,1.7274434621063137,1.7274564243746426,1.7274696494948847,1.7274831427767185,1.7274969096349346,1.7275109555915298,1.7275252862778556,1.7275399074368076,1.7275548249250714,1.7275700447154119,1.7275855728990228,1.7276014156879227,1.7276175794174078,1.727634070548561,1.7276508956708194,1.7276680615045936,1.7276855749039546,1.7277034428593758,1.7277216725005389,1.7277402710992031,1.7277592460721383,1.7277786049841235,1.7277983555510132,1.7278185056428719,1.727839063287177,1.7278600366720922,1.7278814341498152,1.7279032642399952,1.7279255356332257,1.7279482571946145,1.7279714379674267,1.7279950871768084,1.7280192142335864,1.7280438287381499,1.728068940484412,1.728094559463852,1.7281206958696411,1.7281473601008528,1.7281745627667557,1.7282023146911938,1.72823062691705,1.7282595107108005,1.7282889775671533,1.7283190392137764,1.7283497076161158,1.728380994982301,1.728412913768143,1.7284454766822193,1.7284786966910541,1.7285125870243865,1.72854716118053,1.728582432931827,1.7286184163301908,1.7286551257127427,1.72869257570754,1.7287307812393948,1.728769757535785,1.7288095201328575,1.7288500848815211,1.7288914679536311,1.7289336858482611,1.7289767553980702,1.7290206937757502,1.7290655185005697,1.7291112474449966,1.7291578988414142,1.7292054912889154,1.7292540437601842,1.7293035756084583,1.72935410657457,1.7294056567940683,1.7294582468044186,1.7295118975522725,1.7295666304008162,1.7296224671371851,1.729679429979949,1.7297375415866614,1.729796825061473,1.7298573039628038,1.7299190023110715,1.7299819445964755,1.7300461557868292,1.7301116613354388,1.730178487189025,1.7302466597956832,1.7303162061128765,1.7303871536154616,1.7304595303037345,1.7305333647115009,1.7306086859141592,1.7306855235367924,1.7307639077622656,1.7308438693393178,1.73092543959065,1.7310086504209938,1.731093534325162,1.7311801243960685,1.7312684543327126,1.731358558448122,1.731450471677241,1.7315442295847632,1.731639868372895,1.7317374248890423,1.7318369366334116,1.7319384417665196,1.732041979116595,1.7321475881868673,1.7322553091627337,1.7323651829187883,1.7324772510257067,1.7325915557569762,1.7327081400954547,1.7328270477397525,1.7329483231104208,1.7330720113559384,1.7331981583584786,1.733326810739451,1.7334580158647965,1.7335918218500308,1.733728277565013,1.7338674326384365,1.734009337462017,1.7341540431943725,1.7343016017645738,1.7344520658753533,1.7346054890059586,1.7347619254146323,1.7349214301407057,1.735084059006287,1.735249868617532,1.7354189163654778,1.7355912604264285,1.7357669597618695,1.7359460741178987,1.7361286640241596,1.736314790792254,1.7365045165136248,1.7366979040568842,1.7368950170645796,1.7370959199493725,1.7373006778896203,1.7375093568243372,1.7377220234475266,1.7379387452018598,1.7381595902716902,1.7383846275753834,1.7386139267569491,1.7388475581769594,1.7390855929027307,1.7393281026977634,1.739575160010416,1.7398268379618007,1.7400832103328856,1.7403443515507853,1.7406103366742283,1.7408812413781851,1.7411571419376395,1.741438115210492,1.74172423861958,1.7420155901337981,1.7423122482483082,1.7426142919638223,1.7429218007649447,1.7432348545975633,1.7435535338452708,1.7438779193048073,1.7442080921605108,1.7445441339577574,1.744886126575387,1.7452341521970927,1.7455882932817688,1.7459486325328002,1.7463152528662833,1.7466882373781636,1.7470676693102818,1.7474536320153118,1.747846208920585,1.7482454834907797,1.7486515391894748,1.7490644594395495,1.7494843275824186,1.7499112268360952,1.7503452402520667,1.7507864506709783,1.7512349406771064,1.7516907925516219,1.7521540882246245,1.7526249092259476,1.753103336634718,1.75358945102767,1.7540833324262006,1.7545850602421655,1.7550947132224064,1.7556123693920076,1.7561381059962797,1.7566719994414672,1.7572141252341802,1.7577645579195535,1.7583233710181343,1.7588906369615018,1.7594664270266325,1.7600508112690125,1.7606438584545139,1.7612456359900488,1.761856209853014,1.7624756445195557,1.763104002891666,1.763741346223147,1.76438773404447,1.7650432240865614,1.765707872203559,1.7663817322945756,1.76706485622452,1.7677572937440245,1.7684590924085413,1.7691702974966586,1.7698909519277122,1.7706210961787594,1.7713607682009933,1.7721100033356814,1.7728688342297134,1.7736372907508577,1.7744153999028194,1.775203185740214,1.7760006692835602,1.7768078684344146,1.777624797890772,1.7784514690628537,1.7792878899894322,1.7801340652548168,1.7809899959066593,1.781855679374726,1.78273110939079,1.7836162759098126,1.7845111650325716,1.7854157589299158,1.7863300357688083,1.7872539696403513,1.78818753048996,1.7891306840498764,1.7900833917742098,1.791045610776686,1.7920172937713015,1.792998389016065,1.7939888402600244,1.7949885866937574,1.795997562903525,1.7970156988292667,1.7980429197266223,1.7990791461331628,1.8001242938390039,1.8011782738619766,1.8022409924275204,1.8033123509534568,1.8043922460398016,1.80548056946376,1.8065772081800409,1.8076820443266213,1.8087949552360836,1.8099158134526274,1.8110444867548645,1.812180838184474,1.813324726080807,1.814476004121486,1.8156345213690688,1.816800122323795,1.8179726469824558,1.81915193090338,1.8203378052775436,1.8215300970057757,1.8227286287820263,1.8239332191826476,1.8251436827616254,1.8263598301516757,1.8275814681711127,1.8288083999363804,1.830040424980121,1.8312773393746369,1.8325189358605976,1.8337650039808198,1.8350153302189387,1.8362696981427762,1.837527888552199,1.8387896796312446,1.8400548471042837,1.8413231643959798,1.8425944027947878,1.843868331619736,1.845144718390213,1.8464233289984895,1.8477039278846803,1.8489862782138609,1.8502701420550391,1.851555280561679,1.8528414541534735,1.8541284226990558,1.8554159456993433,1.8567037824711992,1.8579916923311064,1.859279434778541,1.8605667696787407,1.861853457444562,1.8631392592171303,1.8644239370449796,1.865707254061401,1.8669889746597075,1.8682688646661443,1.8695466915101708,1.8708222243918589,1.8720952344461503,1.873365494903732,1.8746327812483032,1.8758968713700053,1.8771575457148084,1.8784145874296585,1.8796677825031922,1.8809169199018545,1.8821617917012459,1.8834021932125646,1.8846379231039943,1.885868783516923,1.8870945801768844,1.8883151224991164,1.8895302236886657,1.8907397008349587,1.8919433750007844,1.8931410713056511,1.8943326190034728,1.8955178515545794,1.896696606692034,1.897868726482267,1.8990340573800417,1.9001924502777812,1.9013437605492909,1.902487848087929,1.9036245773392835,1.9047538173284229,1.9058754416817985,1.9069893286438855,1.9080953610886568,1.9091934265259893,1.9102834171031178,1.9113652296012453,1.912438765427436,1.91350393060192,1.914560635740936,1.9156087960352604,1.9166483312245504,1.917679165567662,1.918701227809078,1.9197144511416062,1.9207187731654962,1.9217141358441343,1.9227004854564693,1.923677772546329,1.9246459518687855,1.925604982333728,1.9265548269467994,1.9274954527478556,1.9284268307471062,1.9293489358590852,1.9302617468346153,1.9311652461909083,1.9320594201399572,1.9329442585153667,1.9338197546977631,1.9346859055389332,1.935542711284821,1.9363901754975288,1.9372283049764458,1.9380571096786405,1.9388766026386377,1.9396867998877012,1.940487720372745,1.9412793858749817,1.942061820928419],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.6424622012161014,1.6424622012587429,1.642462201302178,1.6424622013464212,1.642462201391488,1.6424622014373937,1.6424622014841535,1.6424622015317836,1.6424622015803,1.6424622016297197,1.6424622016800585,1.6424622017313348,1.642462201783565,1.6424622018367674,1.6424622018909598,1.6424622019461608,1.6424622020023893,1.642462202059664,1.6424622021180049,1.6424622021774316,1.642462202237964,1.642462202299623,1.6424622023624296,1.642462202426405,1.6424622024915712,1.64246220255795,1.6424622026255644,1.642462202694437,1.6424622027645916,1.6424622028360514,1.6424622029088414,1.6424622029829863,1.642462203058511,1.6424622031354412,1.6424622032138032,1.6424622032936234,1.6424622033749292,1.6424622034577483,1.6424622035421088,1.6424622036280394,1.6424622037155692,1.6424622038047278,1.6424622038955459,1.6424622039880545,1.6424622040822843,1.642462204178268,1.6424622042760382,1.6424622043756278,1.6424622044770711,1.6424622045804025,1.6424622046856567,1.64246220479287,1.642462204902079,1.64246220501332,1.6424622051266318,1.642462205242052,1.6424622053596207,1.6424622054793772,1.6424622056013631,1.6424622057256189,1.6424622058521876,1.6424622059811114,1.642462206112435,1.6424622062462029,1.64246220638246,1.6424622065212535,1.64246220666263,1.6424622068066381,1.642462206953326,1.6424622071027442,1.6424622072549433,1.642462207409975,1.6424622075678923,1.6424622077287487,1.642462207892599,1.6424622080594988,1.6424622082295048,1.6424622084026752,1.6424622085790685,1.642462208758745,1.6424622089417655,1.6424622091281929,1.6424622093180896,1.6424622095115207,1.642462209708552,1.6424622099092507,1.6424622101136848,1.642462210321924,1.642462210534039,1.6424622107501021,1.6424622109701863,1.6424622111943674,1.6424622114227208,1.6424622116553245,1.6424622118922572,1.6424622121336,1.6424622123794355,1.6424622126298463,1.6424622128849178,1.642462213144737,1.6424622134093925,1.642462213678974,1.642462213953573,1.6424622142332832,1.6424622145181997,1.6424622148084196,1.6424622151040416,1.6424622154051658,1.642462215711895,1.6424622160243338,1.642462216342588,1.6424622166667662,1.6424622169969783,1.6424622173333372,1.6424622176759567,1.642462218024954,1.6424622183804476,1.642462218742558,1.642462219111409,1.6424622194871261,1.6424622198698366,1.6424622202596713,1.642462220656762,1.6424622210612447,1.642462221473257,1.6424622218929383,1.6424622223204322,1.6424622227558834,1.6424622231994408,1.6424622236512552,1.64246222411148,1.6424622245802718,1.642462225057791,1.6424622255441983,1.642462226039661,1.6424622265443471,1.6424622270584281,1.6424622275820795,1.6424622281154786,1.6424622286588078,1.6424622292122515,1.6424622297759983,1.64246223035024,1.6424622309351722,1.6424622315309938,1.6424622321379074,1.64246223275612,1.642462233385842,1.6424622340272872,1.6424622346806748,1.6424622353462262,1.6424622360241687,1.6424622367147328,1.642462237418154,1.642462238134671,1.6424622388645282,1.6424622396079742,1.642462240365262,1.642462241136649,1.6424622419223984,1.642462242722777,1.6424622435380587,1.6424622443685195,1.6424622452144433,1.6424622460761173,1.6424622469538357,1.6424622478478967,1.6424622487586056,1.6424622496862715,1.642462250631211,1.642462251593746,1.642462252574204,1.6424622535729192,1.6424622545902312,1.6424622556264872,1.6424622566820397,1.6424622577572483,1.6424622588524798,1.6424622599681065,1.642462261104509,1.6424622622620744,1.642462263441197,1.6424622646422784,1.6424622658657277,1.6424622671119622,1.6424622683814063,1.6424622696744926,1.6424622709916619,1.6424622723333628,1.642462273700053,1.6424622750921978,1.6424622765102719,1.6424622779547584,1.64246227942615,1.6424622809249485,1.6424622824516641,1.642462284006818,1.6424622855909399,1.6424622872045698,1.6424622888482585,1.6424622905225663,1.6424622922280634,1.6424622939653322,1.6424622957349648,1.6424622975375651,1.6424622993737474,1.6424623012441382,1.6424623031493757,1.64246230509011,1.6424623070670024,1.642462309080728,1.642462311131974,1.64246231322144,1.642462315349839,1.6424623175178974,1.6424623197263555,1.6424623219759664,1.6424623242674983,1.6424623266017335,1.642462328979469,1.642462331401516,1.6424623338687023,1.6424623363818696,1.6424623389418764,1.6424623415495971,1.6424623442059225,1.6424623469117594,1.6424623496680326,1.6424623524756834,1.6424623553356712,1.6424623582489735,1.6424623612165854,1.6424623642395213,1.6424623673188146,1.6424623704555172,1.6424623736507025,1.6424623769054616,1.6424623802209084,1.6424623835981758,1.6424623870384196,1.6424623905428164,1.6424623941125642,1.642462397748885,1.6424624014530227,1.6424624052262446,1.6424624090698423,1.6424624129851313,1.6424624169734516,1.6424624210361687,1.642462425174674,1.6424624293903842,1.642462433684743,1.6424624380592217,1.6424624425153191,1.6424624470545617,1.642462451678505,1.6424624563887333,1.642462461186862,1.642462466074535,1.642462471053429,1.6424624761252509,1.6424624812917403,1.64246248655467,1.6424624919158453,1.6424624973771054,1.6424625029403253,1.6424625086074143,1.6424625143803186,1.64246252026102,1.6424625262515384,1.6424625323539312,1.642462538570296,1.6424625449027686,1.6424625513535251,1.6424625579247838,1.6424625646188036,1.642462571437887,1.6424625783843798,1.6424625854606723,1.6424625926691987,1.6424626000124418,1.6424626074929285,1.6424626151132358,1.6424626228759882,1.6424626307838601,1.642462638839577,1.6424626470459156,1.6424626554057054,1.6424626639218294,1.6424626725972256,1.6424626814348875,1.6424626904378654,1.6424626996092675,1.6424627089522612,1.642462718470074,1.6424627281659947,1.6424627380433743,1.6424627481056286,1.6424627583562372,1.6424627687987465,1.6424627794367705,1.6424627902739914,1.642462801314163,1.642462812561109,1.6424628240187267,1.6424628356909883,1.6424628475819416,1.6424628596957107,1.6424628720365002,1.6424628846085938,1.642462897416358,1.6424629104642414,1.6424629237567798,1.6424629372985942,1.6424629510943953,1.6424629651489837,1.6424629794672516,1.6424629940541862,1.6424630089148693,1.642463024054481,1.6424630394783013,1.6424630551917112,1.642463071200195,1.642463087509344,1.6424631041248559,1.6424631210525387,1.6424631382983124,1.6424631558682117,1.6424631737683875,1.6424631920051096,1.6424632105847694,1.6424632295138817,1.642463248799088,1.6424632684471576,1.6424632884649923,1.642463308859627,1.6424633296382334,1.6424633508081228,1.6424633723767488,1.6424633943517102,1.6424634167407532,1.642463439551776,1.6424634627928305,1.6424634864721255,1.6424635105980305,1.6424635351790793,1.6424635602239719,1.6424635857415792,1.6424636117409463,1.6424636382312954,1.6424636652220301,1.6424636927227394,1.6424637207432,1.6424637492933818,1.642463778383452,1.642463808023777,1.6424638382249293,1.64246386899769,1.6424639003530535,1.6424639323022323,1.6424639648566615,1.642463998028003,1.6424640318281505,1.6424640662692342,1.642464101363627,1.6424641371239475,1.6424641735630663,1.642464210694112,1.6424642485304757,1.6424642870858157,1.642464326374066,1.6424643664094396,1.642464407206435,1.6424644487798428,1.642464491144752,1.642464534316556,1.6424645783109593,1.6424646231439837,1.6424646688319762,1.6424647153916154,1.6424647628399183,1.6424648111942481,1.6424648604723222,1.6424649106922191,1.642464961872386,1.6424650140316484,1.6424650671892167,1.6424651213646957,1.642465176578093,1.6424652328498277,1.6424652902007402,1.6424653486520995,1.6424654082256163,1.6424654689434486,1.6424655308282157,1.6424655939030048,1.642465658191385,1.6424657237174154,1.642465790505657,1.642465858581185,1.642465927969599,1.6424659986970351,1.6424660707901795,1.6424661442762787,1.6424662191831545,1.6424662955392157,1.6424663733734723,1.6424664527155488,1.6424665335956987,1.642466616044819,1.6424667000944653,1.6424667857768653,1.642466873124938,1.6424669621723067,1.6424670529533159,1.6424671455030493,1.6424672398573468,1.6424673360528215,1.6424674341268788,1.6424675341177348,1.6424676360644352,1.642467740006876,1.6424678459858217,1.6424679540429292,1.6424680642207654,1.6424681765628324,1.6424682911135873,1.6424684079184673,1.6424685270239119,1.642468648477388,1.6424687723274136,1.6424688986235858,1.6424690274166038,1.642469158758299,1.6424692927016602,1.6424694293008635,1.6424695686113011,1.6424697106896111,1.6424698555937085,1.6424700033828161,1.6424701541174986,1.6424703078596936,1.6424704646727486,1.642470624621453,1.642470787772078,1.6424709541924094,1.6424711239517895,1.6424712971211544,1.6424714737730743,1.6424716539817956,1.6424718378232823,1.6424720253752603,1.642472216717263,1.642472411930675,1.6424726110987822,1.642472814306819,1.6424730216420176,1.6424732331936611,1.6424734490531345,1.642473669313981,1.6424738940719545,1.642474123425081,1.642474357473714,1.6424745963205962,1.6424748400709215,1.6424750888323996,1.642475342715321,1.6424756018326234,1.642475866299963,1.642476136235784,1.6424764117613933,1.6424766930010337,1.6424769800819636,1.6424772731345343,1.6424775722922726,1.6424778776919648,1.6424781894737424,1.6424785077811708,1.6424788327613404,1.64247916456496,1.6424795033464525,1.6424798492640555,1.6424802024799194,1.6424805631602144,1.6424809314752364,1.6424813075995177,1.6424816917119387,1.6424820839958452,1.6424824846391675,1.6424828938345426,1.64248331177944,1.6424837386762907,1.6424841747326213,1.642484620161189,1.6424850751801214,1.6424855400130611,1.6424860148893143,1.6424865000439999,1.6424869957182069,1.6424875021591543,1.6424880196203546,1.6424885483617822,1.6424890886500467,1.64248964075857,1.642490204967769,1.6424907815652414,1.6424913708459585,1.6424919731124623,1.6424925886750656,1.642493217852062,1.6424938609699344,1.642494518363577,1.6424951903765168,1.6424958773611424,1.6424965796789401,1.642497297700734,1.6424980318069338,1.6424987823877868,1.6424995498436397,1.6425003345852014,1.6425011370338183,1.642501957621752,1.6425027967924652,1.642503655000916,1.6425045327138565,1.642505430410141,1.642506348581039,1.6425072877305602,1.6425082483757816,1.6425092310471856,1.6425102362890052,1.6425112646595774,1.6425123167317048,1.6425133930930245,1.6425144943463863,1.6425156211102392,1.6425167740190267,1.6425179537235894,1.6425191608915797,1.6425203962078818,1.642521660375043,1.6425229541137139,1.6425242781630967,1.6425256332814062,1.6425270202463356,1.642528439855536,1.6425298929271035,1.6425313803000785,1.642532902834951,1.64253446141418,1.6425360569427208,1.6425376903485633,1.6425393625832816,1.6425410746225906,1.6425428274669185,1.6425446221419842,1.6425464596993906,1.6425483412172237,1.6425502678006674,1.642552240582624,1.642554260724351,1.642556329416105,1.6425584478777964,1.6425606173596592,1.6425628391429279,1.6425651145405264,1.6425674448977696,1.6425698315930746,1.6425722760386836,1.6425747796813988,1.642577344003326,1.6425799705226343,1.6425826607943206,1.642585416410992,1.6425882390036541,1.6425911302425136,1.6425940918377913,1.6425971255405465,1.6426002331435126,1.6426034164819439,1.6426066774344734,1.6426100179239829,1.6426134399184829,1.642616945432005,1.6426205365255049,1.642624215307777,1.64262798393638,1.6426318446185728,1.6426357996122656,1.6426398512269758,1.6426440018248025,1.6426482538214064,1.6426526096870058,1.6426570719473796,1.6426616431848862,1.6426663260394923,1.6426711232098117,1.6426760374541594,1.6426810715916165,1.6426862285031045,1.6426915111324765,1.6426969224876187,1.6427024656415636,1.642708143733619,1.642713959970508,1.6427199176275231,1.6427260200496954,1.6427322706529752,1.6427386729254314,1.6427452304284633,1.6427519467980252,1.6427588257458734,1.6427658710608253,1.642773086610035,1.642780476340289,1.6427880442793181,1.6427957945371296,1.6428037313073587,1.6428118588686387,1.6428201815859935,1.6428287039122536,1.6428374303894915,1.642846365650484,1.6428555144201964,1.6428648815172957,1.6428744718556871,1.6428842904460808,1.6428943423975864,1.64290463291934,1.6429151673221587,1.6429259510202334,1.6429369895328492,1.6429482884861473,1.642959853614922,1.6429716907644543,1.6429838058923885,1.642996205070649,1.6430088944874006,1.6430218804490535,1.6430351693823175,1.6430487678363002,1.6430626824846606,1.6430769201278113,1.6430914876951772,1.643106392247508,1.6431216409792513,1.6431372412209833,1.6431532004419038,1.6431695262523947,1.643186226406643,1.6432033088053355,1.6432207814984205,1.6432386526879468,1.6432569307309721,1.643275624142553,1.6432947415988126,1.6433142919400907,1.6433342841741758,1.6433547274796272,1.643375631209181,1.6433970048932514,1.6434188582435207,1.6434412011566293,1.6434640437179595,1.643487396205521,1.6435112690939395,1.6435356730585484,1.6435606189795873,1.6435861179465114,1.64361218126241,1.6436388204485406,1.6436660472489768,1.643693873635375,1.6437223118118627,1.6437513742200465,1.6437810735441454,1.6438114227162521,1.6438424349217198,1.6438741236046819,1.643906502473703,1.6439395855075647,1.643973386961187,1.6440079213716885,1.644043203564586,1.644079248660135,1.6441160720798158,1.6441536895529607,1.6441921171235314,1.6442313711570418,1.6442714683476316,1.6443124257252888,1.644354260663227,1.6443969908854141,1.644440634474254,1.6444852098784266,1.6445307359208843,1.6445772318070007,1.6446247171328856,1.6446732118938512,1.6447227364930417,1.6447733117502217,1.6448249589107262,1.6448776996545678,1.6449315561057078,1.644986550841488,1.645042706902219,1.6451000478009357,1.6451585975333045,1.6452183805876983,1.6452794219554245,1.6453417471411125,1.6454053821732602,1.6454703536149338,1.6455366885746234,1.6456044147172522,1.6456735602753358,1.6457441540602935,1.6458162254739042,1.6458898045199093,1.6459649218157617,1.646041608604508,1.6461198967668158,1.6461998188331302,1.6462814079959656,1.6463646981223217,1.6464497237662266,1.6465365201813988,1.6466251233340252,1.6467155699156508,1.6468078973561757,1.6469021438369547,1.6469983483039918,1.6470965504812292,1.6471967908839191,1.6472991108320791,1.647403552464017,1.647510158749927,1.6476189735055442,1.6477300414058538,1.6478434079988462,1.6479591197193109,1.6480772239026615,1.6481977687987854,1.6483208035859045,1.6484463783844443,1.6485745442709006,1.6487053532916913,1.6488388584769886,1.6489751138545174,1.649114174463314,1.6492560963674296,1.6494009366695737,1.649548753524678,1.6496996061533795,1.6498535548554039,1.650010661022834,1.6501709871532635,1.6503345968628078,1.6505015548989679,1.65067192715333,1.6508457806740904,1.6510231836783804,1.6512042055643916,1.6513889169232705,1.651577389550775,1.6517696964586739,1.6519659118858678,1.6521661113092205,1.6523703714540727,1.6525787703044252,1.6527913871127675,1.6530083024095323,1.6532295980121494,1.65345535703368,1.6536856638910062,1.6539206043125458,1.6541602653454714,1.654404735362404,1.654654104067547,1.6549084625022397,1.655167903049888,1.6554325194402466,1.6557024067530137,1.6559776614207007,1.6562583812307419,1.6565446653268032,1.6568366142092414,1.657134329734681,1.6574379151146539,1.6577474749132548,1.6580631150437672,1.6583849427642012,1.6587130666716916,1.6590475966956986,1.6593886440899515,1.659736321423074,1.6600907425678264,1.660452022688896,1.6608202782291699,1.66119562689442,1.6615781876363158,1.6619680806337036,1.6623654272720603,1.66277035012105,1.6631829729100918,1.6636034205018613,1.664031818863632,1.6644682950363725,1.6649129771015025,1.665365994145221,1.6658274762203087,1.6662975543053113,1.666776360261007,1.6672640267840626,1.6677606873577784,1.6682664761998316,1.668781528206912,1.669305978896164,1.6698399643433322,1.6703836211175243,1.6709370862124941,1.67150049697436,1.672073991025671,1.6726577061857408,1.6732517803871654,1.6738563515884617,1.6744715576827438,1.6750975364023881,1.675734425219622,1.6763823612429904,1.6770414811096557,1.6777119208735003,1.6783938158890055,1.679087300690892,1.6797925088695151,1.680509572942023,1.6812386242192943,1.681979792668684,1.682733206772618,1.6834989933830966,1.684277277572169,1.6850681824784686,1.6858718291499009,1.6866883363826044,1.6875178205563082,1.6883603954662325,1.6892161721516967,1.6900852587216104,1.690967760177042,1.6918637782310804,1.6927734111262158,1.693696753449487,1.694633895945661,1.6955849253287216,1.6965499240919644,1.697528970317016,1.698522137482102,1.6995294942699102,1.7005511043754105,1.7015870263140047,1.702637313230392,1.7037020127085567,1.704781166583283,1.7058748107536286,1.7069829749987828,1.7081056827967571,1.7092429511463543,1.7103947903928736,1.7115612040580122,1.7127421886744238,1.7139377336254058,1.7151478209901754,1.7163724253952028,1.7176115138720613,1.7188650457222527,1.7201329723894567,1.721415237339651,1.7227117759495298,1.724022515403646,1.7253473746006842,1.726686264069257,1.7280390858936028,1.729405733649541,1.7307860923510252,1.732180038407611,1.7335874395931303,1.7350081550258463,1.736442035160327,1.7378889217912603,1.739348648069395,1.7408210385297662,1.7423059091323447,1.743803067315192,1.745312312060201,1.7468334339714533,1.7483662153661887,1.7499104303783632,1.751465845074725,1.7530322175833097,1.7546092982342258,1.7561968297125616,1.7577945472232188,1.7594021786674356,1.7610194448307475,1.762646059582078,1.7642817300836515,1.7659261570113607,1.7675790347852174,1.7692400518094749,1.7709088907219885,1.772585228652359,1.7742687374883754,1.7759590841502608,1.7776559308721962,1.7793589354905885,1.781067751738527,1.7827820295458614,1.7845014153443268,1.786225552377115,1.787954081012311,1.789686639059576,1.791422862089482,1.7931623837548805,1.7949048361136963,1.796649849952547,1.7983970551105695,1.8001460808028682,1.8018965559429854,1.803648109463814,1.8054003706363773,1.8071529693859225,1.8089055366047753,1.8106577044614272,1.8124091067053438,1.8141593789669914,1.8159081590526127,1.8176550872332846,1.8193998065278323,1.8211419629791754,1.8228812059237203,1.82461718825343,1.8263495666702234,1.8280780019323888,1.829802159092711,1.831521707728046,1.833236322160098,1.8349456816671756,1.8366494706867342,1.8383473790085376,1.8400391019582902,1.841724340571623,1.8434028017583364,1.8450741984568286,1.8467382497786615,1.8483946811432364,1.8500432244025822,1.8516836179562675,1.853315606856478,1.8549389429033234,1.856553384730443,1.858158697881015,1.8597546548742796,1.8613410352627096,1.8629176256799702,1.8644842198798357,1.866040618766237,1.8675866304146225,1.8691220700848414,1.870646760225754,1.8721605304717908,1.873663217631697,1.8751546656696907,1.8766347256792895,1.8781032558500512,1.8795601214274906,1.8810051946664315,1.8824383547780599,1.88385948787095,1.8852684868863296,1.8866652515278601,1.8880496881862041,1.8894217098586514,1.890781236064079,1.8921281927535119,1.893462512216559,1.8947841329839856,1.8960929997266909,1.8973890631513464,1.898672279892956,1.8999426124045848,1.90120002884451,1.90244450296103,1.9036760139751738,1.9048945464615359,1.906100090227469,1.9072926401908477,1.9084721962566173,1.9096387631923364,1.9107923505029065,1.9119329723046892],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.5904340855898043,1.5904340852254688,1.590434084854353,1.5904340844763307,1.5904340840912736,1.5904340836990505,1.5904340832995283,1.590434082892571,1.59043408247804,1.5904340820557952,1.5904340816256923,1.5904340811875852,1.590434080741325,1.5904340802867603,1.590434079823736,1.590434079352095,1.5904340788716769,1.590434078382318,1.5904340778838528,1.5904340773761112,1.5904340768589202,1.5904340763321048,1.5904340757954853,1.5904340752488797,1.590434074692102,1.5904340741249627,1.590434073547269,1.5904340729588249,1.59043407235943,1.5904340717488803,1.5904340711269684,1.5904340704934834,1.5904340698482093,1.590434069190927,1.590434068521413,1.5904340678394393,1.5904340671447743,1.5904340664371823,1.5904340657164222,1.590434064982249,1.5904340642344132,1.5904340634726608,1.5904340626967324,1.590434061906364,1.5904340611012877,1.590434060281229,1.5904340594459099,1.5904340585950456,1.5904340577283473,1.5904340568455204,1.5904340559462646,1.590434055030274,1.5904340540972377,1.5904340531468382,1.590434052178752,1.5904340511926507,1.5904340501881986,1.5904340491650544,1.59043404812287,1.5904340470612917,1.5904340459799582,1.5904340448785015,1.590434043756548,1.5904340426137156,1.5904340414496163,1.5904340402638542,1.5904340390560257,1.5904340378257207,1.590434036572521,1.5904340352960002,1.5904340339957246,1.5904340326712523,1.5904340313221326,1.5904340299479074,1.5904340285481091,1.5904340271222621,1.5904340256698817,1.5904340241904742,1.5904340226835363,1.5904340211485561,1.590434019585012,1.590434017992372,1.590434016370095,1.5904340147176292,1.5904340130344137,1.590434011319875,1.5904340095734315,1.590434007794489,1.5904340059824429,1.5904340041366767,1.590434002256564,1.5904340003414648,1.590433998390729,1.5904339964036927,1.5904339943796806,1.5904339923180053,1.5904339902179654,1.590433988078847,1.5904339858999235,1.590433983680454,1.5904339814196842,1.5904339791168456,1.590433976771155,1.5904339743818159,1.5904339719480158,1.5904339694689273,1.590433966943708,1.5904339643714995,1.5904339617514274,1.5904339590826015,1.5904339563641148,1.5904339535950425,1.5904339507744445,1.5904339479013614,1.590433944974817,1.5904339419938165,1.5904339389573472,1.5904339358643764,1.5904339327138535,1.5904339295047074,1.5904339262358478,1.5904339229061633,1.5904339195145225,1.5904339160597727,1.59043391254074,1.5904339089562285,1.5904339053050198,1.5904339015858728,1.5904338977975245,1.5904338939386864,1.590433890008048,1.5904338860042737,1.5904338819260015,1.5904338777718476,1.5904338735403987,1.5904338692302176,1.5904338648398395,1.5904338603677728,1.5904338558124977,1.5904338511724663,1.5904338464461016,1.5904338416317985,1.5904338367279205,1.5904338317328015,1.5904338266447446,1.5904338214620206,1.5904338161828688,1.5904338108054952,1.5904338053280733,1.590433799748742,1.590433794065606,1.5904337882767334,1.5904337823801586,1.590433776373878,1.590433770255851,1.5904337640239996,1.5904337576762062,1.5904337512103146,1.5904337446241281,1.5904337379154099,1.5904337310818804,1.5904337241212185,1.5904337170310605,1.5904337098089971,1.5904337024525756,1.5904336949592972,1.590433687326617,1.5904336795519423,1.5904336716326324,1.590433663565998,1.5904336553492988,1.5904336469797447,1.5904336384544933,1.590433629770649,1.590433620925263,1.5904336119153313,1.5904336027377945,1.5904335933895357,1.590433583867381,1.5904335741680968,1.5904335642883898,1.590433554224905,1.590433543974226,1.590433533532872,1.5904335228972983,1.5904335120638937,1.5904335010289805,1.5904334897888122,1.5904334783395733,1.5904334666773772,1.5904334547982641,1.590433442698203,1.5904334303730858,1.5904334178187287,1.5904334050308715,1.5904333920051734,1.5904333787372131,1.5904333652224882,1.5904333514564122,1.590433337434313,1.5904333231514323,1.5904333086029236,1.5904332937838503,1.5904332786891837,1.5904332633138025,1.59043324765249,1.590433231699933,1.590433215450719,1.5904331988993354,1.590433182040168,1.5904331648674976,1.5904331473754996,1.5904331295582403,1.5904331114096772,1.5904330929236548,1.5904330740939039,1.5904330549140386,1.5904330353775555,1.5904330154778292,1.5904329952081127,1.5904329745615335,1.5904329535310915,1.590432932109657,1.5904329102899684,1.5904328880646283,1.5904328654261046,1.5904328423667227,1.5904328188786678,1.59043279495398,1.5904327705845513,1.590432745762124,1.5904327204782869,1.5904326947244736,1.5904326684919592,1.5904326417718562,1.5904326145551133,1.5904325868325118,1.5904325585946615,1.590432529831999,1.590432500534784,1.5904324706930946,1.5904324402968275,1.5904324093356905,1.590432377799202,1.590432345676686,1.590432312957269,1.5904322796298769,1.5904322456832305,1.590432211105841,1.5904321758860094,1.590432140011818,1.5904321034711302,1.590432066251585,1.5904320283405926,1.5904319897253307,1.5904319503927393,1.5904319103295188,1.5904318695221227,1.5904318279567544,1.5904317856193626,1.5904317424956371,1.5904316985710027,1.5904316538306158,1.5904316082593586,1.5904315618418339,1.5904315145623615,1.590431466404971,1.5904314173533978,1.590431367391077,1.590431316501139,1.590431264666403,1.590431211869371,1.5904311580922232,1.5904311033168113,1.590431047524652,1.590430990696923,1.5904309328144541,1.5904308738577229,1.5904308138068475,1.5904307526415802,1.5904306903413001,1.5904306268850081,1.5904305622513184,1.5904304964184517,1.5904304293642288,1.5904303610660626,1.5904302915009512,1.59043022064547,1.5904301484757644,1.5904300749675415,1.590430000096063,1.5904299238361361,1.5904298461621063,1.5904297670478482,1.5904296864667578,1.5904296043917427,1.5904295207952146,1.5904294356490798,1.5904293489247299,1.5904292605930328,1.590429170624323,1.5904290789883924,1.59042898565448,1.5904288905912625,1.5904287937668444,1.590428695148747,1.590428594703898,1.5904284923986216,1.590428388198627,1.5904282820689981,1.5904281739741815,1.5904280638779753,1.5904279517435183,1.5904278375332774,1.590427721209036,1.590427602731882,1.5904274820621946,1.5904273591596327,1.5904272339831216,1.5904271064908404,1.590426976640208,1.5904268443878706,1.5904267096896876,1.590426572500717,1.5904264327752025,1.5904262904665587,1.590426145527356,1.5904259979093067,1.5904258475632491,1.590425694439131,1.5904255384859984,1.5904253796519745,1.5904252178842468,1.5904250531290496,1.5904248853316478,1.5904247144363197,1.59042454038634,1.5904243631239627,1.590424182590402,1.5904239987258164,1.590423811469289,1.5904236207588085,1.5904234265312525,1.5904232287223659,1.5904230272667426,1.5904228220978058,1.5904226131477877,1.5904224003477085,1.590422183627358,1.5904219629152707,1.5904217381387091,1.5904215092236371,1.5904212760947025,1.5904210386752122,1.5904207968871094,1.590420550650952,1.5904202998858885,1.5904200445096344,1.5904197844384471,1.590419519587105,1.5904192498688778,1.5904189751955053,1.5904186954771706,1.5904184106224744,1.5904181205384085,1.5904178251303291,1.5904175243019318,1.5904172179552218,1.5904169059904876,1.5904165883062724,1.590416264799347,1.590415935364679,1.5904155998954053,1.590415258282802,1.590414910416254,1.5904145561832252,1.5904141954692284,1.5904138281577924,1.5904134541304327,1.590413073266618,1.5904126854437386,1.590412290537074,1.590411888419759,1.5904114789627506,1.590411062034796,1.590410637502395,1.5904102052297677,1.59040976507882,1.590409316909106,1.5904088605777946,1.5904083959396316,1.5904079228469044,1.5904074411494047,1.5904069506943912,1.5904064513265521,1.5904059428879678,1.5904054252180715,1.5904048981536107,1.5904043615286094,1.5904038151743274,1.5904032589192219,1.5904026925889065,1.590402116006111,1.5904015289906421,1.5904009313593415,1.5904003229260442,1.5903997035015385,1.5903990728935233,1.5903984309065669,1.5903977773420648,1.5903971119981963,1.5903964346698836,1.5903957451487476,1.5903950432230671,1.5903943286777322,1.5903936012942046,1.5903928608504734,1.59039210712101,1.5903913398767278,1.5903905588849345,1.5903897639092934,1.5903889547097767,1.5903881310426222,1.5903872926602916,1.5903864393114264,1.5903855707408039,1.590384686689295,1.5903837868938218,1.5903828710873138,1.5903819389986673,1.5903809903527015,1.5903800248701179,1.5903790422674593,1.5903780422570684,1.5903770245470472,1.5903759888412177,1.590374934839083,1.5903738622357873,1.5903727707220798,1.5903716599842757,1.5903705297042214,1.5903693795592577,1.5903682092221862,1.590367018361235,1.5903658066400268,1.5903645737175462,1.5903633192481115,1.5903620428813443,1.590360744262142,1.5903594230306524,1.590358078822249,1.5903567112675077,1.5903553199921852,1.590353904617201,1.590352464758618,1.5903510000276286,1.5903495100305407,1.590347994368766,1.5903464526388118,1.5903448844322743,1.5903432893358347,1.5903416669312584,1.5903400167953965,1.59033833850019,1.5903366316126775,1.590334895695007,1.5903331303044497,1.5903313349934167,1.5903295093094816,1.590327652795406,1.5903257649891671,1.590323845423993,1.5903218936283985,1.590319909126229,1.5903178914367058,1.5903158400744781,1.59031375454968,1.5903116343679922,1.5903094790307089,1.5903072880348117,1.5903050608730462,1.5903027970340096,1.5903004960022382,1.5902981572583084,1.5902957802789381,1.590293364537099,1.5902909095021338,1.5902884146398837,1.5902858794128185,1.5902833032801813,1.5902806856981346,1.590278026119919,1.5902753239960195,1.5902725787743415,1.5902697899003932,1.5902669568174812,1.5902640789669125,1.590261155788209,1.5902581867193315,1.590255171196913,1.590252108656505,1.5902489985328339,1.5902458402600679,1.5902426332720978,1.590239377002829,1.5902360708864842,1.590232714357922,1.5902293068529647,1.5902258478087437,1.5902223366640544,1.5902187728597277,1.5902151558390147,1.5902114850479858,1.5902077599359452,1.5902039799558607,1.5902001445648069,1.5901962532244283,1.590192305401413,1.5901883005679887,1.590184238202429,1.5901801177895833,1.590175938821417,1.5901717007975757,1.5901674032259623,1.5901630456233342,1.590158627515919,1.5901541484400477,1.5901496079428064,1.5901450055827095,1.590140340930388,1.5901356135693008,1.5901308230964637,1.5901259691231977,1.590121051275899,1.5901160691968286,1.5901110225449187,1.5901059109966051,1.5901007342466764,1.590095492009142,1.5900901840181252,1.5900848100287734,1.5900793698181899,1.5900738631863855,1.590068289957253,1.5900626499795578,1.590056943127953,1.590051169304012,1.5900453284372837,1.590039420486364,1.5900334454399905,1.5900274033181552,1.5900212941732357,1.5900151180911475,1.590008875192513,1.5900025656338488,1.5899961896087726,1.589989747349226,1.5899832391267152,1.5899766652535692,1.5899700260842098,1.589963322016444,1.5899565534927667,1.5899497210016775,1.589942825079014,1.5899358663092975,1.5899288453270881,1.589921762818357,1.5899146195218645,1.589907416230551,1.5899001537929374,1.5898928331145317,1.5898854551592467,1.5898780209508219,1.5898705315742518,1.5898629881772202,1.5898553919715375,1.5898477442345815,1.589840046310741,1.5898322996128573,1.5898245056236724,1.5898166658972688,1.5898087820605131,1.5898008558144945,1.58979288893596,1.5897848832787442,1.5897768407751953,1.5897687634375919,1.5897606533595543,1.5897525127174472,1.5897443437717726,1.5897361488685513,1.5897279304406968,1.5897196910093754,1.589711433185353,1.589703159670331,1.5896948732582692,1.5896865768366906,1.5896782733879766,1.589669965990644,1.589661657820607,1.5896533521524274,1.5896450523605414,1.58963676192048,1.589628484410068,1.5896202235106067,1.5896119830080464,1.589603766794139,1.5895955788675789,1.5895874233351293,1.5895793044127353,1.5895712264266235,1.5895631938143941,1.5895552111260964,1.5895472830253012,1.5895394142901615,1.5895316098144687,1.5895238746087035,1.5895162138010837,1.5895086326386112,1.5895011364881193,1.5894937308373245,1.589486421295881,1.5894792135964457,1.5894721135957521,1.5894651272756977,1.5894582607444447,1.589451520237545,1.5894449121190808,1.5894384428828334,1.58943211915348,1.5894259476878205,1.5894199353760423,1.5894140892430186,1.589408416449656,1.5894029242942802,1.5893976202140778,1.5893925117865884,1.5893876067312547,1.5893829129110362,1.5893784383340839,1.5893741911554893,1.5893701796791044,1.589366412359439,1.5893628978036414,1.5893596447735636,1.5893566621879158,1.5893539591245154,1.5893515448226336,1.5893494286854433,1.5893476202825711,1.5893461293527609,1.5893449658066472,1.5893441397296473,1.5893436613849712,1.5893435412167554,1.589343789853323,1.5893444181105716,1.5893454369954974,1.5893468577098477,1.5893486916539177,1.5893509504304837,1.5893536458488777,1.58935678992921,1.5893603949067352,1.589364473236367,1.5893690375973417,1.5893741008980349,1.5893796762809267,1.5893857771277222,1.589392417064625,1.5893996099677623,1.5894073699687679,1.5894157114605152,1.5894246491030077,1.58943419782942,1.5894443728522931,1.5894551896698808,1.5894666640726487,1.5894788121499193,1.5894916502966683,1.589505195220465,1.5895194639485588,1.589534473835106,1.5895502425685382,1.5895667881790665,1.589584129046322,1.5896022839071278,1.5896212718633977,1.5896411123901655,1.5896618253437311,1.5896834309699293,1.5897059499125126,1.589729403221648,1.5897538123625166,1.589779199224023,1.5898055861276017,1.5898329958361197,1.5898614515628715,1.5898909769806622,1.5899215962309732,1.5899533339332066,1.589986215194007,1.5900202656166504,1.5900555113105037,1.5900919789005399,1.5901296955369184,1.5901686889046145,1.5902089872330967,1.590250619306054,1.5902936144711597,1.5903380026498757,1.590383814347286,1.5904310806619641,1.5904798332958614,1.590530104564221,1.590581927405507,1.5906353353913516,1.5906903627365097,1.5907470443088279,1.5908054156392124,1.5908655129316052,1.590927373072957,1.5909910336431974,1.5910565329252038,1.591123909914757,1.5911932043304944,1.591264456623846,1.5913377079889617,1.5914130003726217,1.5914903764841317,1.5915698798052025,1.5916515545998073,1.5917354459240247,1.5918215996358562,1.5919100624050286,1.592000881722768,1.5920941059115592,1.5921897841348767,1.5922879664068958,1.592388703602179,1.5924920474653428,1.592598050620694,1.5927067665818513,1.5928182497613348,1.5929325554801377,1.593049739977269,1.5931698604192759,1.593292974909737,1.5934191424987343,1.5935484231922936,1.5936808779618037,1.5938165687534018,1.5939555584973328,1.5940979111172755,1.5942436915396347,1.5943929657028,1.5945458005663626,1.5947022641202928,1.5948624253940693,1.5950263544657606,1.5951941224710493,1.5953658016121977,1.5955414651669426,1.5957211874973205,1.595905044058407,1.5960931114069674,1.5962854672100033,1.5964821902531927,1.5966833604492,1.5968890588458549,1.5970993676341778,1.5973143701562378,1.597534150912829,1.5977587955709431,1.5979883909710224,1.598223025133966,1.5984627872678725,1.5987077677744879,1.5989580582553369,1.599213751517507,1.5994749415790517,1.5997417236739846,1.6000141942568273,1.6002924510066714,1.6005765928307203,1.600866719867262,1.6011629334880333,1.6014653362999283,1.601774032145995,1.602089126105678,1.6024107244942403,1.6027389348613164,1.603073865988529,1.6034156278861067,1.6037643317884405,1.6041200901485002,1.6044830166310489,1.604853226104569,1.60523083463183,1.6056159594590116,1.6060087190032957,1.6064092328388473,1.606817621681086,1.6072340073691582,1.6076585128465162,1.6080912621395023,1.6085323803338416,1.608981993548936,1.6094402289098586,1.6099072145169395,1.610383079412835,1.6108679535469728,1.611361967737259,1.6118652536289384,1.6123779436504928,1.6129001709664703,1.613432069427127,1.6139737735147786,1.614525418286743,1.6150871393147694,1.6156590726208513,1.6162413546093113,1.6168341219950626,1.6174375117279522,1.6180516609130828,1.6186767067270382,1.619312786329917,1.6199600367731113,1.6206185949027456,1.6212885972587274,1.6219701799693427,1.6226634786413634,1.6233686282456168,1.624085762998003,1.6248150162359365,1.6255565202902136,1.6263104063523093,1.6270768043371304,1.6278558427412544,1.6286476484967056,1.6294523468203326,1.6302700610588639,1.631100912529739,1.6319450203578227,1.6328025013081353,1.6336734696147375,1.6345580368059394,1.635456311526008,1.6363683993535778,1.6372944026169751,1.6382344202066963,1.639188547385291,1.640156875594923,1.6411394922629001,1.6421364806054783,1.643147919430269,1.6441738829375905,1.645214440521125,1.6462696565682584,1.6473395902604888,1.6484242953743196,1.6495238200830489,1.650638206759896,1.6517674917829046,1.652911705342083,1.6540708712492445,1.655245006751027,1.6564341223455672,1.6576382216033196,1.6588573009925118,1.6600913497097258,1.6613403495161045,1.6626042745796703,1.6638830913242524,1.665176758285502,1.6664852259744811,1.6678084367492927,1.6691463246952138,1.6704988155137845,1.671865826421283,1.6732472660570115,1.674643034401798,1.6760530227070918,1.6774771134350202,1.6789151802097488,1.6803670877804597,1.6818326919962365,1.6833118397931288,1.6848043691936236,1.6863101093187352,1.687828880412885,1.6893604938817208,1.6909047523429834,1.6924614496904953,1.6940303711713207,1.6956112934761034,1.6972039848425537,1.6988082051720284,1.700423706159101,1.7020502314339963,1.7036875167177137,1.7053352899896494,1.706993271667468,1.7086611747989637,1.7103387052656036,1.7120255619974185,1.7137214371988814,1.7154260165853679,1.7171389796297878,1.7188599998189242,1.720588744919019,1.7223248772500876,1.72406805396846,1.7258179273569885,1.7275741451223705,1.7293363506990103,1.731104183558822,1.732877279526371,1.7346552710987404,1.7364377877694985,1.7382244563561302,1.7400149013303106,1.7418087451503723,1.7436056085953313,1.7454051110998436,1.7472068710894475,1.74901050631548,1.7508156341890366,1.7526218721133726,1.7544288378141462,1.7562361496669132,1.7580434270213146,1.7598502905213878,1.7616563624214727,1.763461266897193,1.765264630351013,1.7670660817118897,1.7688652527285669,1.7706617782560798,1.772455296535057,1.7742454494634394,1.7760318828602526,1.7778142467211027,1.779592195465084,1.781365388172818,1.7831334888153705,1.7848961664738119,1.7866530955492246,1.7884039559629759,1.7901484333471074,1.79188621922472,1.7936170111802479,1.795340513019553,1.797056434919789,1.798764493569006,1.8004644122954963,1.802155921186898,1.8038387571991017,1.8055126642550139,1.8071773933332704,1.8088327025469924,1.8104783572127086,1.8121141299095782,1.8137398005290697,1.8153551563152623,1.816959991895955,1.8185541093047746,1.820137317994498,1.8217094348418057,1.8232702841436905,1.8248196976057747,1.8263575143227706,1.8278835807513472,1.8293977506756638,1.8308998851658365,1.8323898525296123,1.8338675282575256,1.8353327949618157,1.8367855423093893,1.8382256669491075,1.8396530724336846,1.841067669136477,1.8424693741634506,1.8438581112606034,1.8452338107171238,1.8465964092645646,1.8479458499722992,1.8492820821395364,1.8506050611841571,1.851914748528633,1.8532111114832843,1.85449412312713,1.8557637621865692,1.8570200129121421,1.8582628649535953,1.8594923132334817,1.8607083578195174,1.8619110037959015,1.863100261133814,1.8642761445612834],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","FigRMSEGRidRidge_log = visuRMSEGrid(Ridge(), 'Ridge', alphasridge_log, 'alpha',\n","                                    GridRidge_log)\n","FigRMSEGRidRidge_log.show()\n","if write_data is True:\n","    FigRMSEGRidRidge_log.write_image('./Figures/ConsoGraphRMSERidge_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.3 Modèle Lasso"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      0.1\n","               R²      RMSE       MAE\n","Lasso() -0.136205  2.358827  1.130725\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logLasso=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.077891212337764,20.640083694252265,20.617947525879288,27.749268515308884,20.987918427174932,20.54712135233482,20.5420334573681,21.031125590029212,20.554208063181328,24.174843208852796,20.551363353433924,20.569653418207665,20.606504918328458,20.58557506096878,20.575091959697037,22.926197815543443,22.762749231598455,20.8591929479615,23.679016063735162,20.595268143382672,20.842644095711407,20.59599362320336,20.537434301080445,20.54256376859774,21.876575830423533,20.54742291648143,20.70020476235031,20.62003873350726,21.024640205750046,20.63858979797387,20.840379377846627,20.526819859103583,21.12715395071053,20.751990702416972,20.957580507531663,20.567172169937667,21.279543415184097,20.614059915359245,20.83987594232245,20.612762837687207,20.83797572904456,21.879021281996728,20.52460800328677,21.041817453421768,31.24780062737394,20.970182837069977,25.099839203527157,20.651339673517175,33.10508209875741,21.049598810186,20.637137032018956,20.687189006685127,20.543932019159868,20.608654560168876,20.815715615219986,20.65345350224886,20.813847393905046,20.812318559471183,22.13745763251123,20.630656034118886,20.636124703389886,20.83827167487245,20.53880026302964,20.64124736881083,21.043479422910004,20.565578203567835,21.057883221296898,20.586543613268432,20.585362061440104,21.393989807324065,20.568237410031173,20.54675793126577,20.841674842715587,20.55114757142521,20.58827726672471,20.852845932970578,22.48278964802229,20.587415626386314,20.58268649005787,20.846907405537717,20.538994505325167,20.660853389714823,20.6531612033761,20.511876821694393,20.75977341111632,21.08319399189013,20.62431070347901,20.66550897933534,20.778551729451888,20.56239419863945,21.661957741683047,21.296044372506444,20.566260226564395,20.777771945760428,21.514820779657608,22.819495532235923,20.56329981537976,25.381058904139465,20.8591929479615,20.837556872090108,20.717146679271675,20.527187697507575,21.443782683938313,20.80445682906932,20.76356366247705,20.54809882969296,21.781388601086913,20.726954245219712,20.776348660090342,20.591245642413732,20.658954908739766,20.606584598696617,22.684791750722486,20.752749884120323,20.684680171813135,20.892960401109278,21.495192166884422,20.576834198078394,22.36502136127912,20.64268605643414,20.732572577111295,20.4995279294258,20.543127790226457,20.526420657444103,20.539382989916223,20.579002152323348,20.53275368834628,20.562904705828906,21.036494218701513,20.540411044643083,20.58143357350117,21.225949653317436,20.664530620704234,20.608100982186187,20.61153324791899,21.177494115471386,20.539369906497548,21.12191048358347,20.54849638529987,20.873137263929106,20.637857087670014,20.540654963657904,21.603747995929016,20.579816716788464,24.348369599867947,20.715695089576407,20.680559624298475,22.227367376100737,20.531193443693205,21.557911947505563,20.60655317061819,20.53018599394955,20.506795977478568,33.005056340840795,20.52183304096641,20.773926084824282,21.167663999763366,21.192921524207737,20.527077810830637,20.55105624447267,20.53904835812958,20.587157850032046,20.588375869144322,20.598238445869434,21.639667444322768,21.083319158791216,20.659623454406976,20.577291607354958,21.00325865515808,22.200304349897948,20.526374167966384,20.64324977785771,20.79975137385156,22.093967952079097,21.205489948324296,20.750961749746914,20.543932019159868,20.56788505613321,20.683916915498706,20.574641112804148,21.414714085354568,20.56261971251959,20.543100246596737,20.700184200393842,20.63747969565337,20.570371296647956,24.229599853536076,20.753174913423287,21.304426740198352,20.618001647238987,20.546106279693678,20.61354569012951,20.546826815542925,20.703442458254305,20.550683281347343,21.082051407338334,20.613126137531193,20.76356366247705,20.6118290194508,21.927714051681626,20.896043537582557,20.60124910523295,20.570108018825643,20.57943453099135,20.554157976545675,21.037418294190623,20.61304462827913,20.54521652466255,20.673591843956423,20.627639272930654,20.693889065728133,20.706602773916956,20.532997060090846,20.646358162503535,20.601375429042967,20.630144309379965,22.431449579121367,21.088662808426985,20.971548558462636,22.880051018044075,20.645682123623324,20.566608281100944,20.5793781051534,21.000569615474653,20.670147388299497,20.626773265670483,20.633656716923383,21.340572865039984,23.196711944862066,20.59251143109162,20.594908634521705,20.665521470687985,20.58280705359356,20.69520269947442,21.930406944747986,22.473203052897706,20.763594861082595,20.540038168441534,20.575499565531704,21.162742105601556,20.54192071192744,21.021154009868624,21.29481496585882,20.56582504603833,20.67044617604047,20.567485504371067,20.873663374472336,20.53189861554716,20.537053003906387,20.92400145188562,21.284219707763192,20.52993396130417,20.869369361023427,20.581179684472588,21.485327417246392,20.569327382351762,20.641975984349028,20.562031431643828,20.671404239836328,20.566520518365575,20.67061159553753,20.66665452089858,21.18544312190193,20.598539991721605,20.531683377779576,20.535033131949717,20.847660623527535,20.692239144735424,20.5160437474659,20.537847768376793,20.862828122078422,20.595368672600024,20.539903057997794,20.551018689598017,20.566140693872093,20.67931096668708,20.727297477268525,21.186814351766614,20.684193875397348,20.64300189616779,21.075168755924704,20.71194613939398,20.64074132010553,20.54321148331062,20.57349598662905,21.159980777429624,21.17563886666888,20.549696669629043,20.65100879088724,20.653608418175263,22.173311424652987,20.59014358897304,20.842820187413196,23.02323614729836,20.515074726402272,20.549696669629043,20.732443631152236,20.662889679652547,20.56456860681103,20.76874558605964,22.035948164414183,20.84272679672529,20.68754218001845,20.677414354437296,20.979773669671573,21.157933099439724,20.53097979701651,21.745749398677543,20.646398043807626,20.594948858606465,20.600741929246624,20.70038487020329,20.57509132083943,20.68744078569164,20.609203696337634,20.63980107234031,21.269376785094657,20.609178673223987,20.54052968645624,20.597614944495366,20.94473483254438,20.82722807683708,20.867219048902577,21.191562746626875,20.548800608309058,21.269107458187726,20.58516844572023,20.96484445330709,21.0255595623491,21.67575700402983,20.548136424975965,20.53596126898601,20.672225962823937,20.79433640907187,21.07333165373539,20.601897720081954,20.775400664551167,20.56225971962497,20.567745635058248,20.71235896784049,20.546637092998097,21.999671504170554,21.934138691073596,20.539903057997794,20.77299104736249,20.645346458103404,20.655740431226096,20.590288812470284,21.340482359834173,20.949339156752252,20.610187376087143,20.645220179498523,20.591038525974668,20.800700431978257,20.994430127377864,20.545079355154314,21.391436137251986,20.55682330281135,20.525007133393483,20.56199036758014,20.700397361555936,20.6084737696899,20.72051071673873,20.534604532218495,20.654156295327276,20.726250424910962,20.580205201379517,20.978598691992527,20.55433256497522,21.72358422728458,20.53022853850442,20.67441925837339,20.59356442877511,20.532308810830717,20.637930482986658,20.564760913106962,20.62159756756004,20.642231584660244,20.615026516210026,20.558061660506084,21.03357294461303,20.773624327225352,21.76227103067037,20.515450046703307,20.752015563897203,20.813467972718207,20.739291340446087,20.53371863394342,20.954206416868292,20.549941038968576,21.18869755021533,20.766436972151155,20.86775945997019,21.047439297753144,20.527187697507575,21.2859715738114,20.596028606470785,20.74584916145242,20.756201252888506,20.798120810627253,20.566268625344456,20.647513370536785,20.532835063976083,20.546808330847266,22.18926702720805,20.61180304371088,20.863225856174342,21.550583276502437,20.70100519237288,20.62697373343816,20.591020070715697,20.578146011801042,23.653266405130235,20.63941781252128,20.611704684788645,20.52645083121248,20.674536147504757,24.83327514435524,21.099003892873377,20.809594403930195,20.535798825453004,20.767295541029352,20.539681850602374,20.57089709228988,20.63181422666982,20.55115027308854,21.68912193478546,20.691125706187897,20.62697620548224,20.53381928062322,20.82467345779539,20.55867563597846,20.541074739685136,20.512928403486736,20.658204169942525,22.792340587888106,20.568760728300493,20.62409777510496,20.80982465821512,20.787521094616782,21.088662808426985,22.524795522683924,21.138695662184812,20.571141764543135,22.566676401126468,21.138695662184812,22.077450248661876,20.53096129188656,20.57338792339455,20.86986442508051,21.334181146260452,20.68924389182522,20.74140120273705,20.994493955535226,20.702916407150717,20.728377511146572,20.576142210803923,20.690359299768446,20.666539182018187,21.53079335015244,20.5741788400703,20.637024124944876,25.034633081539464,21.158651056338265,21.136403407555367,20.671950142856833,21.054883865051355,21.414373823063997,20.568644446598395,20.695842026023435,20.54475280909713,23.29522173417613,20.684770215178364,22.49322709306258,21.377709319404012,20.642318793968126,20.65349724600678,20.699168828159436,20.73798601614304,22.781865345771198,20.54942711554244,20.659266974760058,20.973960343979506,20.68246155675474,20.56381365798919,20.55567431962689,20.736300776009326,20.68505213939255,20.66514511366589,20.5335140649199,22.332333362201517,20.567679746666595,20.615440747264483,20.770995710953642,20.670147388299497,20.556369791954133,20.637024124944876,20.572750260704996,20.659396082518647,20.578597174712254,20.646398043807626,20.71831694422429,20.519516293679295,20.586847075118815,20.563829894333725,20.661500385013372,20.8758921022165,20.557303408148766,20.60907215325547,20.61243754565246,20.559678128873383,20.643235366325566,20.61615884530335,20.56486632591334,20.63150322706734,21.119787369320896,20.84272679672529,20.530457257432253,20.644995690450482,20.548086217115248,23.779006202585894,20.712553129319307,20.53186357004555,20.521582184416765,20.87368663350525,21.159980777429624,20.597594281152084,20.68966378703285,20.55162025453446,20.500661174717983,20.65176007652468,20.69856429535561,20.527882310703085,20.520729382848362,21.221605873789294,20.58209138956617,20.614788584640973,21.642215515446452,20.535656370104878,20.555769475776316,20.632312263624915,21.527929943356238,20.523587407944895,20.6811235131352,20.83600261784547,20.58501731678749,20.594947370792514,20.97664147203218,20.647544740347644,20.881031726538733,20.61417923167189,20.745302198903275,20.53274742246578,21.095353983754524,20.60533142259644,20.5511441284331,20.616183927623982,20.594898891624744,20.687189006685127,20.696175265685152,20.803778170609476,20.86764366503686,20.97313175114403,21.54191694241492,21.167600796011403,21.781388601086913,21.527929943356238,20.540529565231175,20.543967202605156,20.823323365988276,20.892421656611265,20.643496638022533,20.927848904554942,21.6191063020805,20.622589433062377,20.618851768278972,20.562031431643828,20.78721880229588,20.538342853753075,20.683222704328063,21.65195594499088,21.443782683938313,21.0913201506163,20.57943453099135,23.447198492376817,20.65202852236478,22.374965781890232,20.926489168077893,20.77144546498799,20.768391846167447,20.646897505163494,20.83604736811134,20.73870691453002,20.57820159207332,20.544778791337784,20.625340808445582,20.59251143109162,20.808184847042195,20.610281508600497,20.571079899584376,22.024229004093545,20.565267342265397,20.504928359905243,20.578923589898174,20.58609948534397,20.81339701220919,20.563788634875543,20.55514159855925,20.54525985980083,20.642997830876684,20.550229229063262,20.599034172284693,21.446406318850546,20.648271627404778,20.633400693474233,20.655195259214164,20.68429565482617,25.74897578387607,20.55821200122975,21.57667241960955,20.56942424767207,20.531493166219665,20.841674842715587,20.726250424910962,21.064846156234747,21.446406318850546,20.89446974495107,20.54108734982082,20.55819130314858,20.631848548059494,20.541611859030873,21.22375710830423,20.56308673503438,20.540717016120606,20.54867520988233,20.614295032987666,20.646330482008253,21.43940800122528,20.544223558923488,20.59161025536306,20.635357521956703,20.53700818079803,20.663213213110314,20.549991166012582,20.55457386615917,20.637137032018956,20.887102578879524,21.92333432243657,22.14879351747807,20.621296966929417,20.733060750055092,21.254226088085282,20.78398713191914,20.6191288914594,20.635843395809705,22.246583759065498,21.41224033789941,23.915463907115313,20.598570788988503,20.523373457469575,20.63719965041561,20.69794113639959,20.988953528749487,20.56189904772437,20.68050502848565,20.65393961610693,22.173311424652987,20.60761413443814,20.533787584959683,20.54124666227783,20.724871769567347,20.79975137385156,20.653591773762074,20.603852764816015,21.128695139604936,20.690991182723693,20.6154353779895,20.56151412025372,20.800037875579697,20.68606733325876,21.25024086686165,20.64201069311792,20.613589853096613,20.608990615992248,20.587974933608976,20.92943101326261,20.681950490790193,20.679997152373613,20.586387450722476,20.640181534917037,20.606543620749285,20.525804746054085,20.577804770670323,21.425319913897784,20.551144047616393,20.508558808047795,21.480661703763072,20.542131023355523,20.562366239593484,20.548091630248024,20.59124572323044,21.158820620705452,20.545297940700706,20.597430187693174,20.813317941185893,20.71371231721197,20.70824829847065,20.619035019984437,21.030322037859325,20.590681753577005,20.53592682999807,20.62785719791412,20.605079059503307,20.61604607825296,27.13439712196665,20.577344316805718,20.911607661479927,22.80528621694911,21.25819853509778,21.201591058929658,20.566006756572857,21.07210892257191,21.064618094080686,20.68968935382327,20.883561661174653,20.89651654859114,20.638427843810494,20.810012634630144,20.55555117174327,20.768391846167447,20.561539062550658,20.953238413977967,20.559432148644625,20.566890205315126,20.534201025925277,20.5606932899081,20.599034172284693,20.514428659538314,20.589771505071013,20.71762855557941,21.270712254508936,20.84864665525105,20.630144309379965,20.559678128873383,20.68092304536753,20.72363581102781,21.077548024966124,20.72871131420503,20.60124910523295,20.61607110136661,20.677574183342582,20.586897080937756,20.530589554981262,20.82370315355065,21.52654113342888,20.80958197776367,20.63858979797387,20.638948875091867,22.129372288136622,20.73950898764249,20.534119650295473,20.906587030184415,20.67441925837339,22.583159307788193,20.599394292185533,20.78505555177839,22.096605281644642,25.90441205190384,20.684444429800674,21.96349176685698,20.65293966345891,20.612144877280205,21.550583276502437,20.69853979609535,20.564976546936496,20.617660118289145,20.906438350475046,21.368116623885182,20.541124823878757,20.867232545400626,20.64027615733319,20.68466996109035,20.63747969565337,21.125746985079285,20.525494881828454,20.53630640218193,21.59672421426338,21.90815196875258,20.588847473750505,20.77144546498799,20.528029174040256,20.82103121352206,20.543111229222607,21.47795613880232,20.618640193188686,20.61741999899071,20.751928043611965,20.69108622272326,20.68637131584762,20.6047755333174,20.67509569568499,20.773241231980382,20.559446379688424,20.75977341111632,21.92525261983504,20.8825590394778,21.657237987889996,21.205672778672252,20.85794532918989,20.586991778986924,20.804423428814996,21.450373090060065,20.79838165935843,20.83306112275596,22.170249067185548,22.40434804486065,20.589911050275397,21.12191048358347,20.629338400740455,20.717967816693996,20.68942200509332,20.642738688778397,20.673659444141805,20.614441972436378,20.872386334486116,22.701995211465416,20.930590365417007,20.52313552136906,20.582821202498664,20.674130669271143,21.335654549787336,20.722703815322394,20.530316260831434,20.57187162072169,22.26929908378235,20.75316429560927,20.559966407361646,20.537147796190958,20.56634499289484,20.838359518424525,21.014311627953298,20.538573637228335,20.983653272201035,20.990260181728413,20.548091630248024,25.803508838958507,20.64154375050017,20.6084737696899,20.46211207076254,21.046553008946404,20.618088714887897,20.534004560500975,20.68320366423643,21.270218336994198,20.784610714671185,21.017006997290252,20.797372189603916,20.90734686273932,20.675704413295023,20.53725885642642,20.77359529675062,20.73450746827123,20.564850059416074,20.542097148333823,21.09672805896244,20.661624012199724,20.971169510120127,25.30871579196034,21.046553008946404,20.709099902940302,20.615645061900906,20.63182364617091,20.663633498444977,21.030923448693027,21.942067248135583,21.5677741486985,20.83604736811134,20.637688387785097,20.64779615211289,20.649649874154168,20.61769064133971,20.72051071673873,20.603125416884016,20.4744934506323,20.54522905642355,21.07210892257191,20.605774200429323,20.580226925263933,20.617071646632922,20.690256093792215,20.56863842638323,20.562835117551924,20.59926046741443,20.826215750141728,23.51911904767567,20.530328634381135,21.01971593309254,20.558669329689604,20.587974933608976,20.935370303359708,20.52405033992219,20.889801343521643,20.54523289468648,21.058960480426872,20.664285350147978,20.64324977785771,20.790225064825048,20.669630201348877,20.727297477268525,20.823077035882857,20.57905959607305,20.60722998438821,21.05660054035406,21.41202002309175,20.840096998973387,21.327819781015254,20.64349234666002,20.553500059093082,20.615657674478616,20.52487198109891,20.580815492185287,20.756201252888506,20.526808802944217,20.713742270219463,20.59296781259205,27.13439712196665,21.084998152313943,20.798482780327188,20.62431070347901,20.54922668818312,20.585318159868244,22.292180680338518,22.170896445451447,20.801015123716738,20.711413776641233,20.937849209426005,20.648315362832545,20.684680171813135,20.55832825421074,20.75578090974855,20.595400166227016,20.540203820261837,20.540705131110265,20.530579427812473,20.533010508630106,21.631319358116755,20.567679746666595,20.541145951426458,20.539817928253704,20.568488045251208,20.568257969875106,20.547594121726412,20.53255918941453,22.126272379665693,20.665997398345272,20.936758676750834,20.859080866057678,20.598564150430413,20.990260181728413,20.565643335503804,20.925837476097445,20.666539182018187,20.724433157932285,20.527739065211463,20.942429850930644,21.464416601214605,20.97100856881476,21.114742360085376,20.795993263177216,20.559119399776517,20.642842649762063,20.5511441284331,20.60949249868852,21.696923088292866,20.59020844349248,20.544777953435844,20.740097980409576,20.646944580132004,20.647068371796156,28.150263847825308,20.837327308560138,21.321830179494725,20.74522538041755,21.077548024966124,20.749897736696255,21.0255595623491,20.7552311057988,20.741161001419783,20.55231576727006,21.2553671077131,20.546808330847266,20.531827705649988,20.729861061717095,20.635459019276922,20.585938482037832,23.429654749593194,20.644737668512626,20.695179241507226,20.770535132613993,20.659893281825788,20.622979909853377,20.5900882300095,20.715281745766326,20.555173008778464,20.595724803492814,20.602200460550296,20.586508757980123,20.74950933292191,20.530749260488065,20.609203696337634,20.561840906756988,21.512929051710092,20.702239409198135,20.665521470687985,20.860096059923887,23.888502768547852,21.07041092975787,21.561697949702545,20.778514093760528,21.897283535752184,20.528599369165835,20.558669329689604,20.682387429816714,21.890890691998706,20.6275358946382,20.97907784057468,20.990294178955047,26.12602011241525,20.524837911277036,20.52307286256405,20.628220643657833,22.200998971956498,20.55204633440852,20.6568893832476,20.552249235121504,20.530378838819733,20.59161025536306,20.93645583302961,20.674130669271143,20.63823522360972,23.712826319781783,22.66197380597761,20.57509132083943,20.53275368834628,21.578329745990402,20.56081860751812,21.224534132101166,20.733437470662196,20.534610570385375,20.797398476836054,20.712615707307606,20.585877470341664,21.51103696602285,20.579779202322168,20.53189861554716,20.631911287681213,20.668536074555135,21.10078965143968,20.581884376537047,20.79338251449202,20.582996505369927,20.565655948081517,20.670673724623146,20.69009078192359,20.56487885767434,20.688928708811222,20.61402289079575,20.523083665577467,20.789856712497155,20.55902247382621,22.531219046702468,20.812302823600476,22.024229004093545,22.100631462497898,20.614441972436378,20.520556515390073,20.809699381013456,20.577980854210058,20.53608696904419,20.815565813134825,20.677414354437296,20.668536074555135,20.53673248205599,20.726416474576983,20.535611194869947,20.59926046741443,20.663259568643877,20.824637199719223,20.9579813622503,20.57261095421244,20.650226456385315,22.295711573307877,20.55931475578955,20.627862912105847,20.5746340505675,20.530579427812473,20.64372149075901,21.137934740539777,20.692060165870412,20.653591773762074,21.498262106854245,21.25587362041083,20.732572577111295,20.89363113318136,20.632654610692683,21.99579156427714,21.58325450610801,20.62128838191493,20.60165341420345,21.039068243037377,20.54396334856237,20.692877408060507,20.59253569307378,20.63628962018715,20.632157859379443,21.284219707763192,20.67914349489692,20.647795691078056,20.775017985429137,20.753199896128578,21.495192166884422,21.047324906602636,20.64476555472356,20.55117335604039,21.340215954378163,20.747668118161865,20.703828351490067,20.84454382125384,22.67311411890508,20.923549250945083,20.540223209057142,20.56311298268843,20.67876543794135,20.762040354446672,20.52645083121248,21.617484203835502,20.560191898242966,21.269376785094657,20.84454382125384,20.529985736197098,20.982092470417722,21.1428796092109,20.55106251035317,20.768998072506495,21.999671504170554,20.55757288141866,20.515450046703307,22.493471540221407,20.879322950990122,78.77989623059267,20.54675793126577,20.57871396222866,20.54638201884407,29.247234589034854,20.55819130314858,20.533737256731488,21.039636616438365,20.67303877665355,20.524388657060882,20.530924010831665,20.646897505163494,20.66665452089858,21.041817453421768,20.616942139572945,20.548866123754443,20.603536891174745,20.919468733600525,20.680521988607115,21.435272447603953,21.004154716478066,21.73368981463848,20.577834638761452,20.639066545272776,20.96233623001513,20.575911157409557,20.693680256842274,20.562522927014363,20.54261841376746,20.69862611118554,20.873137263929106,20.57197936459902,20.56193393424728,20.679698458394526,20.641376895762914,20.55548630280351,20.645152120911096,20.598921386435673,20.57586521007225,20.55948389415472,20.56081860751812,21.365773998450273,20.774341017346938,20.55722559055081,21.336048898804258,20.933926637524895,20.76463069831168,20.567485504371067,21.2522127035093,20.53296876208369,20.548549892272312,21.905075595704584,20.589997818666554,20.896540876064133,20.54806754069881,20.732298625787994,20.6005438161994,21.15582117458773,20.660122431277195,21.243059218563797,21.6191063020805,21.46702818412641,20.67050923279176,20.79563344510441,20.67252045920748,20.80981594893172,20.907491663411413,24.872037496780443,20.5684128546852,20.589702711723866,20.557575256916124,20.775679735448282,20.77315451715819,20.512487991976784,20.782958686446246,20.753199896128578,20.599541749013618,21.053081039962752,20.645220017865103,20.613849705091972,20.82209325223586,21.74536422109574,20.53629954965255,20.60334280027049,22.3402199997974,20.711393981835,21.35566572240626,20.531193443693205,21.04526160194333,20.763401509604613,20.80532915418372,20.524238275928862,20.57558098156986,20.589511256097893,20.57978314630547,20.515153227511128,20.733437470662196,21.269514771412876,20.779422565616446,20.530316260831434,20.68370505590157,20.927848904554942,21.49827840380422,21.279543415184097,20.77930359470364,20.605342521150206,20.56804939320779,20.580913523515353,21.626899098312943,20.516609713789435,20.67636140354617,20.793132312152395,20.54542948378287,20.596164358606927,20.685836795900045,20.64924259192161,21.245127948781278,20.596243682176684,22.093967952079097,20.71204185848847,20.559095490380376,21.030647020869246,20.994530364437054,20.691383643354683,20.58066182889162,21.579862563470787,20.650909356543202,20.589501423490322,20.59693506190854,20.549295572460274,21.187161859814847,20.882350182380723,21.563203371236334,20.757593295509984,21.590409095470115,23.524581473515678,20.506795977478568,20.53718366586041,20.54568024022797,20.58099971101005,22.75615116412441,20.686643834673195,21.12515018470129,20.59562578168353,20.60604195250118,20.567679896470757,20.628572328342074,20.90264078875147,20.967596169822986,20.545527425238415,20.548186592428326,20.552619516701615,20.712615707307606,20.583189259508778,21.057580161188238,20.585656557823647,20.515706232209773,21.12871876825442,20.781294750973267,20.519344623257666],"xaxis":"x","y":[22.06759946491166,20.51569310181126,19.61997633871636,25.808363100543477,21.795376715513594,20.34038041027431,18.86081419340691,24.078413808713293,19.082528830592434,27.09570141066156,18.66432922183982,20.146924115160264,20.694537918678005,20.34827538264879,22.1287945428413,23.512029578625103,23.710849830139047,21.16813940491307,24.138707165857692,20.413583384325577,20.971944131576173,19.330306789697467,19.240535149076692,19.593384739494034,23.207168669114285,20.243485920599927,24.816169078762734,20.239179241482244,21.93825114094642,22.544699771649725,20.581001696999003,19.006769325349232,22.852829884665343,20.97322158881004,21.37041284025584,19.8813455186455,22.291944364905724,19.352118206703615,20.690574372087426,19.616726513482636,21.73777568014623,25.92407619036082,19.62630225708032,24.328668011629535,28.119486884464127,23.268620237666717,24.937914667295647,20.538966591972148,26.49472805759287,21.551656870776874,19.967005268430295,20.53263058829885,19.077522473826907,20.411222614242394,20.118559661298907,21.75794824506378,21.969944344762656,20.941991359524017,24.30144839882211,20.08875201356742,20.413158139175476,17.21138734793915,20.10520261649224,21.318516744600927,21.383177447413818,18.901120542977083,21.140418528305776,22.317825778768146,19.694551733623157,24.452943059423166,19.34725801943171,19.360313431748278,21.62865690143549,18.725486086935668,19.22798647641259,22.00766259568964,23.581082748790354,19.483945640419044,19.555879359056263,20.81759187582515,20.004050690864457,19.7027197284411,19.957005833266606,18.935012496616167,21.132460035639845,21.104713276655342,21.0706503248077,19.995538068644624,20.45373482710394,20.377850211599817,24.218899085058613,23.648191447276112,19.27757204338643,20.547353262390168,22.570203152455143,28.033188230056272,20.740147099081785,24.844950181647548,21.11139041842472,21.618102712310883,20.71863237230781,19.638742582781422,19.346013720701006,21.12828261562749,21.344903224909334,19.828653485294133,22.842407702796528,22.86005581934068,20.23340552018178,22.244828354517683,19.8468416379525,20.153224049031966,23.16226531069458,22.47425899129698,20.440974576504654,21.34321011850724,21.96825774467718,19.581570228112746,22.97307940319244,21.943790024620068,24.922685338201692,18.397908557303648,21.170757004644493,19.439732238591404,19.06759979313172,19.78225599625029,20.88710307988011,20.06163488065717,21.227834779401228,20.925901548056537,19.64120627043042,21.51013491039007,19.943833745099443,20.041179196484592,19.31991683554051,21.305505708300736,19.083972676252298,22.36486950758078,21.00153722158492,20.91944021038121,19.71891096707218,20.43515061037962,23.756601825433552,19.29612083176196,27.288645329988054,20.004627845911102,20.581937941283943,23.06049210432163,19.0283827986455,22.26292897661663,18.98289333455679,19.08654493081137,20.26723932452398,26.772642502942784,23.150894170391226,22.789865927709045,21.316932785410458,21.47925775839002,19.52466856143405,17.31416721288844,19.67390026636621,21.280848347151327,20.575951881264743,19.36981390700933,22.906270958120995,23.259273464150183,21.034942664057727,19.478866371959384,21.521316718605924,23.251480243552532,18.947642644803835,18.234449818345436,23.74556559722114,23.70046067341344,22.17893043543761,23.314842649926593,19.204628901280135,21.028986544289005,19.811383905702034,20.105414612220844,23.470671408984025,19.185541351838207,21.863096949082852,21.439837984160558,20.28595076567891,20.09465504412721,24.347469240771108,21.026538976608304,22.25115601355335,19.17189511189754,19.330727902738385,20.455200776646638,18.7812284253763,19.9984664758684,19.71179802639097,23.222688495940556,20.75461515785591,21.453079563108336,19.968121134960665,23.31621219099147,22.229186273778243,19.29157093266521,19.725980900417238,19.393396349595932,19.510476738006748,23.86936283890287,20.803398322957783,19.50445726786541,20.266025864180154,20.318803159054653,19.88287753302219,21.755410336588376,20.021014007950576,19.81464925130666,19.600081112964823,20.195578868662224,22.127386699288984,22.615494713832,21.994462034082755,23.008367409641956,20.02036350615788,19.496913617064063,19.65284705132002,22.970383373621917,19.800442868861996,19.544786543412965,21.196126618068195,22.706377974056313,25.14188655199703,19.430343691015025,20.82549425312562,21.300868648362627,20.935011057360228,20.238709984208988,23.361271159574002,23.97311707542301,20.93275451807793,19.135842027978637,19.419925212472517,22.69657897049395,18.04707863542646,21.639091593296747,24.902956648700677,19.472341179018663,19.630944296038688,19.581570228112746,0,21.279464358226686,20.6373536565784,20.55168503037007,22.252924089212918,17.011336466938804,20.539904719671835,20.287228770242784,22.579718287600105,20.016747429777542,19.919862789535355,21.590649178257912,20.459085985918723,19.305685493406397,20.69934199839951,21.814001605688915,21.49724409211446,17.96562762596234,18.897059576214055,20.218149025767367,21.663202256858984,21.537426968242833,19.881144027962787,18.109927551497627,20.650313109616484,19.551649361459194,18.954616641321604,18.33673536238825,19.272777840955303,21.22721049508145,23.36907437848721,21.839279632705356,20.080313988880153,19.791725695449298,22.830034474576443,20.038176858751854,19.882805718288704,19.137324212876308,19.89611063211052,20.922839608419043,22.73108571789678,18.798041102461394,19.809989451392685,21.628482474975726,22.66651647870706,20.105565995835796,21.324104166055236,23.33275256138136,20.80501366619404,18.787924871827784,20.381292625866948,23.456138303517488,20.59828494694422,20.938551777966236,22.709886920964298,20.558379254361796,19.758613410846518,21.618914296187317,22.889842311610867,21.30122097904847,19.0336054859068,23.801438495307856,24.840309004310797,19.694954646749302,20.944503948247828,20.620570884389835,21.72592495902031,20.296054717472362,19.58398989769835,20.2956272827196,21.085706497425413,19.461766940454588,19.29900449887296,22.510563057499144,20.841106100432793,21.801619166438428,21.147132242697182,21.551582010518878,20.4427602890876,24.611415306054898,21.490696787354505,21.24369655260969,22.432940196049124,22.234424235191682,20.099462108962847,20.800416502964755,20.17949699394329,21.746512886013605,22.496434708234126,19.78606039431591,22.229470444639624,19.19842345841963,21.071706539354626,21.28737646300594,20.120777025175872,23.69806207100269,24.25287711008343,19.074352191519743,20.558351575498786,21.437400189861094,20.352963298343866,21.233800456337935,22.14457282995132,20.722700819009958,20.69715285443839,20.03253125195289,20.0566191944814,22.70946891023392,21.137414277628565,19.808943756196857,20.143175790739598,21.040198736231254,19.712186074557728,19.35146728073557,20.861088989018167,20.755317114550273,0,19.017540081468233,21.419330156983328,20.67873788317677,19.93668394257525,22.409766675366317,19.27540160418116,23.728782472563765,20.376074503254586,19.31077695697734,21.065895324749285,20.3484179021553,20.07860380095908,19.976548041795603,20.235086321270654,21.01213158467734,21.41999462819776,19.174863067555084,21.10241852261651,20.85643673732399,22.586773372950685,20.037769462388923,21.559823540829672,23.13793422908773,20.558605220150522,19.683189962828273,20.39789928981696,19.337397595787667,22.26889032385089,21.195293500127566,22.559724250840063,24.39611039895289,19.59271980995523,22.136011068345166,19.33796659591498,20.797749380419276,21.567058771175564,20.725266392758815,18.468910364525364,19.663251453070878,18.918473122196858,19.198971627350232,24.325766932844235,19.86193501563865,21.156628899315663,22.938277348217966,20.78722963747693,20.66031803587651,20.155379081002646,19.457981357627542,25.713720052658104,20.828559454854545,20.52544147367378,21.147722912842873,20.03302501817969,24.630356510701596,23.121909718158822,23.106213073613993,22.51080834568276,20.883881259335507,21.661257500272384,19.85555820528158,19.526632957320885,19.191856386440307,22.96557469826004,19.96989517504386,19.756880613939327,20.841336970061143,21.615161729704354,19.630957845101975,19.107266091841186,18.360084050573274,20.178689314774683,22.61224657942639,20.902818910258993,19.692513024244924,21.22795725431778,21.431836012474292,22.58649267850949,23.56945490190484,22.15739813287633,19.523673217064264,23.682062419502355,22.448170385491473,23.624330847094043,20.4998468600676,19.29906600505008,21.469193647687533,22.680649469911085,21.178626711012377,21.686546691263366,21.428479917582088,22.88912263482666,20.804770608680126,19.28855746111665,20.502951107445575,20.22322663992422,22.31457162800466,19.478115876564008,18.795995233915015,25.17177163891695,23.160406649511106,19.25160491688623,20.826937118763155,19.953135265171547,23.933068642871486,19.355480148534408,20.44485249695551,19.56616125697471,25.23181389258149,21.345469949872594,22.776042002687795,25.303110182218393,19.58463718752616,20.204391739449186,19.893828839264728,20.33821258053668,24.68330601022857,18.821811344100055,19.840511525069328,22.297566604177444,20.78875924862035,19.741131185755275,20.04722528013383,22.468811326543563,19.99457856414159,21.80053568878591,19.167558108393887,23.846962977749094,19.07645844042533,19.307592868706436,19.883803436319816,19.767052898077655,18.952911838336565,18.62774549084819,20.73926969664862,18.627786784209636,19.177777189052396,20.089958597754855,20.801380865239704,17.764101156137162,19.11561871251436,19.24667566091232,20.93405408413005,21.340147837597844,21.090725928345268,19.745386795650543,19.095931371701568,19.810878030088894,20.251155291034372,19.165221068220944,20.724802969675316,19.74733497869572,22.856134451929176,20.542598325656595,19.502159734210206,21.979041347822633,21.2197623270523,26.969103234931445,20.18342033833375,21.02499268359878,20.592440384261195,19.990444456103596,20.867172390851817,21.273367636241293,21.243839329097405,21.363500625759567,20.18486444945059,20.241113097705927,22.693372098426487,18.623284957897813,19.205602363695707,21.545170940853957,20.80034066129298,19.981973707277767,22.19000668879013,22.718490023684353,20.707922743251714,19.214188218113872,22.472827160217914,19.07108521476948,20.346151863763343,21.63013067480897,18.214892995560163,19.93614600441511,22.53034703709958,19.469859640957857,22.895450751816803,21.00807772677881,21.431258349952667,20.941808459871957,22.99407646829777,19.76111291920157,19.008644725628592,18.355858634806896,20.297451997659746,20.474380309584664,21.870871511939562,20.759740299921024,21.439000175436664,22.030880607811586,23.00587909853429,20.655014419256375,22.965146977089795,22.43523710461151,20.18471556395119,19.019883876304206,23.05070508803856,21.499214768467855,20.59203905710115,21.024704798577186,22.72181503272866,22.377050963043796,19.71871179715288,21.93139047566312,21.827309362120346,19.1796783064222,19.826388660339887,23.65560944108187,19.199993380834425,22.21417220655097,19.269925661522706,23.782692532330046,22.173362238186836,23.41808975500013,20.872940195293037,21.54216753533708,21.03393309328942,21.663729644062077,20.699641074284234,21.39606694375647,19.593416427729718,21.355622092522935,19.49242240707679,19.406777996109206,22.625652665035147,19.860111721120674,14.03702355520748,22.00972851632279,20.33705521659505,20.155471422765178,20.40843263005365,23.540959857497864,22.64902697475969,19.51901613478499,20.748003621067692,19.075332900397907,21.535462867247563,19.35190112925433,16.543892756530866,22.048803804340604,19.94957596782787,23.253137677025506,15.802066947826521,22.01816825105002,24.65495079669392,19.684337709357063,23.07232514287102,19.441821622178967,20.90283362751463,21.65340972782181,20.672219865399406,22.248853822319084,22.007999054329076,21.514848387611945,18.91123104739614,22.195045634281605,21.092338092530998,19.27798585366196,21.725547245838506,20.321666107127033,18.844349286283478,20.211183384442457,20.06069065496676,20.388592288941165,25.880225035189923,19.68504303470752,20.795739259102447,19.874915797147626,20.484467680558765,21.537004350283745,19.454353518988064,20.471143313052202,20.221414835639845,21.30681378731033,22.893554470389834,23.892417838743924,19.773417735905042,21.39811634221899,21.94580529932221,22.093666756982273,19.566140232030243,19.62938152434226,22.598971957007752,22.606867611519395,23.548837661891714,19.57985452716288,19.257215175267675,20.260215495083163,21.269107840654502,21.892874617975046,20.911342976063032,20.690267441816314,19.415723203109316,22.536424619535012,19.241608485058645,19.162579413291002,20.470409642279566,20.97348174432658,23.73507977629211,14.55836091863428,23.336410397420998,21.627932359551377,21.49391553553141,20.265469687416747,19.426854398695138,20.625264709408093,20.223930403747513,21.865935415382868,21.881578906408237,20.14846091060974,19.81407448278221,21.0024847282008,20.945391681085823,22.291777470342872,20.551017265346704,19.577722584113683,19.444205825873514,21.46402517777047,18.277738745265133,18.799932943985397,21.60163932304092,19.80978333592071,20.91463622707276,21.547692385839394,19.17999258814081,19.549428996570953,19.54492482512454,19.294068756456163,21.69613129076446,20.048671453648097,19.49098989799628,20.42092755989845,20.047998538616795,22.735567798975932,20.159778968313912,20.920832580620292,19.699545342858148,20.678206389277292,20.079358473730228,19.917597162486278,19.692880228017533,26.26193365893149,20.33327111425421,21.201441496966886,24.081396558763124,22.374129418340807,22.181711726885904,18.957794711482784,21.63591845810156,22.742154187536123,20.277557571355647,21.712167598440185,20.922319986303112,19.981118924422468,18.376324675581085,19.093096903348385,21.041684548715036,19.88267606965292,21.79470380164697,19.394929062748655,19.711931797206645,20.701777070204972,18.966254774482525,16.57594392051226,19.3661810523187,19.637857750008234,21.05876451742095,21.39188913879269,20.922214169866013,20.158015290040524,19.840625811644227,20.287989620862316,20.392027938746434,21.440477364164977,20.18838178436931,19.209224880205603,19.523793560708743,19.767710849855263,21.57006553499441,20.748925773432315,21.47821316674072,24.169496968680054,22.350651659007692,22.60440595404428,20.973185759004682,22.891726790683144,20.2096151875794,20.22349858267661,20.901209408549594,19.053706845403045,23.000908813982363,19.549547798448483,22.637005220660473,23.38302130821851,25.236220259743373,22.107609158967936,23.256395298587126,21.31115959043422,19.242152996635603,23.069120127461606,19.88568939220856,20.74441094675746,20.977820033541025,21.403421935393933,23.155520679582406,17.74375624808358,21.65558749657167,22.72177363339029,20.057436191229318,20.31682150206961,21.442049003709382,20.447439187365134,18.64679105166152,23.081069556252004,23.109422435163253,19.40981146751902,21.538742146089504,20.451115883369862,20.520204300445414,19.072268708954628,21.853488891111965,19.870759032200375,19.874160110175985,20.45611373193672,20.923455889638944,15.423378576518493,19.495183602853512,20.532198000108426,20.172754664219987,19.621134659665625,21.149770683357705,25.5492833010917,20.91962424027046,23.144648130459657,22.724819827816283,20.76732896720974,20.90466916348112,21.019251701774245,21.870062515886435,21.968529606994224,18.28303322918483,22.7967578369547,24.617902294510255,20.337523908425865,22.278345092825212,20.439903152029967,20.745981176528062,20.50317548758169,20.148792615003963,20.18507504172347,19.887802201303376,21.335667578998795,23.547889563240428,22.184719195487645,19.124759964560997,19.09456742423413,23.546328229722338,23.676810419954805,21.378549935187863,18.772971741326142,20.92343911431291,23.66501619396984,20.362922084924325,19.335122075152558,20.6322831501737,22.014406532680294,20.862569257472042,22.377941930707426,16.70615872384773,21.114598939015725,25.312874128977917,19.557332226454086,25.0888235331049,0,20.821375100396438,19.69889252664812,20.730065203035867,20.737794346856816,19.720177445507503,19.751886432327307,21.436234720492326,19.12391425698277,21.093445290401018,22.18427320470571,23.32578333060166,20.235023779555554,20.38834949087981,19.076215392899233,21.890199695637197,19.626240196846272,19.473911816076555,22.65051997686549,21.07153535807478,20.152641197123305,25.817249936117022,20.72049143867345,20.000328449472278,19.677575133698696,19.854767962698624,20.90714373186156,21.852326049162542,22.34522357506863,21.88381079939571,20.64279914121781,20.008240359915625,20.079932477068777,20.97646133780068,20.617037831360907,20.86340137149202,20.10297814136792,18.939248297157278,19.337083944174847,21.21789500066683,19.959413629413806,20.57158693763026,20.1483293247843,22.45828794986169,19.223167412657357,22.090969450288238,20.54580254002858,20.99006223015013,23.933428716720645,18.97273303540371,21.839004592901738,17.66325927218184,21.098649782686852,22.108159109401438,19.70668845124008,20.687407722542194,21.23800756191085,22.54894428694912,20.440949745295235,18.136670762341843,21.301837769324877,21.032952737462605,23.31051349089806,20.969732200565332,20.351613259246466,19.678510738089678,22.059041559037812,22.10097025846529,21.127912480118983,24.425354799557958,19.405573621751774,18.954513256380405,20.62820521809647,19.21222562648202,18.9881876444261,21.62524006847279,18.78833918294514,19.827821621310783,19.4644839297056,26.34702224299619,21.38205653661138,21.78972267747008,21.087755789009996,18.999355953219133,19.451729034530373,23.289040906826244,24.16646354231811,21.536078922759724,21.530906360636845,20.97243154975957,20.089266034468004,20.436904665975636,19.20088663487939,20.554011225634962,19.277982733151024,18.304477915148002,19.118363214083377,20.261720490850706,19.26508618967162,23.063753511610845,19.05336380976314,19.240852168960235,19.807795916443087,19.527755462443025,19.369940873414674,18.557194427644223,19.092216129486825,23.457850638636746,20.3986497634222,20.79850265219969,20.611470634120458,21.03390016791011,24.957468167964976,19.132327452939787,22.668945885967275,20.192548076466373,21.926818619899166,21.236410750491928,22.00439862331522,22.64890259783158,19.876543924901863,22.174820865681593,20.001613842126076,20.63081635106876,19.653189714082398,19.305175301710232,20.224827212369068,22.6279649673046,21.547003936026666,18.961602303420257,20.695469281090816,20.114481067581714,20.790519525351012,26.143565404950515,21.151917099556776,22.601975102609295,18.386700720994313,21.48658316808204,19.99564295418165,22.704086007044513,23.646718002570474,20.17407123394833,20.29544946282625,21.94083977054476,19.087820197169854,20.49274951238928,21.58556546053215,19.868098908707033,19.806349800221792,23.600603274171462,19.846459862589864,21.099407208256782,21.035986420389108,23.747615973251953,21.078530347858226,20.86782723578085,20.073998999202523,19.385623092000827,20.214357069981805,21.756421739524402,19.573874587511387,22.17732889464112,20.092049623606254,19.621522497700703,20.13212471121583,22.84612630027242,21.610638293924836,20.381527241115613,22.69863600121815,24.241029788998915,23.65992231174119,22.296121251970213,20.702441582535556,22.897176211133427,19.850331267520566,17.677965293255863,20.87727804782974,24.045999647847655,20.173501472804098,23.33182107390064,23.30215527163116,27.231751620760654,20.650611348398325,16.829816883719186,21.108904064764797,23.735782522109897,20.656510624448828,20.096689421256738,22.83710046840733,19.824675810238606,20.823945313943256,23.256137551486724,23.633181830866906,19.043844337217745,23.65829637497728,23.50747794569296,21.73683233585413,20.94197381362215,24.18225862695574,20.14794970691058,22.344681402352435,20.447944597167705,19.21271648764287,22.508853453023672,21.75045501313216,20.060863127783303,23.041170672161062,19.554654657325607,21.26551890511897,19.47575297115503,20.304093087106327,21.96980947524255,18.890513667640615,20.207528404900692,20.825342365179193,19.58596744655146,19.86547955920327,20.093122217042076,19.51138012046539,20.89052275919248,19.726434174485984,19.26180569731426,21.223090796665396,20.071086524480997,23.60048678836413,21.682830461889793,22.115896159241377,23.245004397316144,19.929597223002617,20.206849822052806,20.80607598372765,20.972697281096956,20.548932178877266,20.790175259909486,21.65577788956451,20.396533708238668,17.677595133772794,20.206889160267256,19.864164508631692,20.604884938737765,19.69979302428942,21.112079923463828,21.776216944868956,19.41019874513054,20.084542414131516,25.538294718669484,19.887154874828134,22.02600227823102,20.06242269100319,19.908561207609715,19.025787914622292,22.519912388376707,21.15310774495207,15.434839951572775,23.416469489833915,21.472713147192902,24.799241151178187,21.565777300167408,21.24511744232927,23.786142675104543,21.775846548503456,20.874095085342866,20.525903087310763,22.190296420947053,17.526091919071504,22.787613064091445,19.95220428650718,21.56191951932416,20.302727045932432,22.39076356657128,17.788361488629835,19.89495594402764,20.849863964556313,22.01047671458767,21.9419703223203,21.769327477492673,22.134078601621123,23.02213648046312,21.659326126254854,22.503975157123477,21.687400780796548,21.299913824117656,24.41218042654058,21.628416169590647,20.661666774415252,19.13829898996108,20.6534474504661,20.599627769624064,19.476347089125902,22.633338504709375,19.004867830543457,20.729309561171004,21.215021611787833,20.55962915472571,23.96741801447904,20.948249582299137,19.034944006994706,21.25562508835354,23.6936808146177,18.876298103633747,20.12057913956066,22.988276940898057,20.761017121909465,29.702932107840674,19.34717798381203,19.04780522125321,19.72022056098374,25.87586915107988,22.381251164844315,21.45471998887307,21.894206057556605,20.997149152437558,19.346653316091935,19.69023397394279,21.75892392666085,21.695257182632144,24.304938166770185,19.96312680068985,21.08058139373211,18.457821713581545,21.058645462338415,20.370456476417175,22.39756222611425,21.96355600303241,24.29338064398619,19.533504643608662,19.24597971762759,21.061479969798718,19.653131971320136,19.991828535916927,19.41354451956652,19.024392506006556,19.995956185326282,21.080207924079772,22.895336936087247,19.00379513256332,19.59081933226322,19.683155687698772,19.724580465382388,20.49815774428782,21.41646356230674,20.374729613814893,19.698149844649336,20.827570650601533,21.55980522344904,20.203106297950406,20.998113148879906,22.5464416348448,21.94562707940607,20.109117749689304,19.61600010557632,22.28815948936714,20.00070564420032,19.48773603183014,22.96164822427184,18.99960834234589,20.932965503614184,18.548961097016324,21.40256127224899,23.343368321662904,21.395751936133603,20.367816242708102,21.629837063073623,22.815206783859768,22.704911104222457,20.7857551664019,20.3067025549342,20.61336030755643,21.781868242254394,20.56504163001677,24.58750691743097,21.042555130756018,19.718574682047347,20.82900495573487,21.89094777167843,20.46100849412487,20.676958661968467,22.376303078727783,21.96712491711981,21.09072172933059,20.373076483151994,19.53720891947194,19.37233862557095,0,22.656078929958284,19.971799365414682,21.138173662648782,25.881734625061707,0,22.042981377352834,19.12600868541295,20.929534739770034,20.604603448464516,21.393924057871644,18.99682101244613,19.31053914628864,18.986258705756388,16.7233596191811,19.687016476370815,20.364732076794134,18.828527980887348,21.061860954738233,18.824250825082274,19.28149315085171,21.023652845270533,23.394529944648035,22.371116977331784,21.19573925103901,18.93658618322981,19.94401710262461,19.47904998638186,22.850840758637354,20.76699650557159,20.40036143331872,17.795936296212627,19.421904563712328,20.64673096490357,19.639706353282477,18.775317705171794,20.047574770414855,19.61260734468754,23.666636204423178,23.480370137373004,19.218266855752937,21.60748880425466,21.257489419039537,21.101232900314493,21.320633929890864,21.849984955018474,20.40434121243742,20.032547562218568,20.035932572983462,20.864093173595,21.51301334156099,22.29182284846925,23.44987910374218,20.855425894539348,22.64951341150128,23.894186440194584,20.26686372980413,20.408744363117133,19.046718202293015,20.904214174162476,24.308788888311135,20.77592509493985,0,19.644171392658865,18.610921424553307,22.26726247442381,20.903586582709064,21.2082248307893,20.867431932395597,19.247883485213144,19.09858987177298,20.769205079804532,21.73660672493293,20.24751150890443,21.281868210231526,19.724170230709305,20.553728301262954,21.112413416140527,20.46206885900041,20.668149201097204],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso_log = np.linspace(0.1, 1, 5)\n","param_gridLasso_log = {'lasso__alpha': alphaslasso_log}\n","\n","GridLasso_log, \\\n","BestParametresLasso_log, \\\n","ScoresLasso_log, \\\n","SiteEnergyUse_pred_logLasso_log, \\\n","figLasso_log = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=SiteEnergyUse_train_log,\n","                            y_test=SiteEnergyUse_test_log,\n","                            y_test_name='SiteEnergyUse_test_log',\n","                            y_pred_name='SiteEnergyUse_pred_logLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso_log)\n","\n","print(BestParametresLasso_log)\n","print(ScoresLasso_log)\n","figLasso_log.show()\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.7016867735572467,1.7336574959150695,1.8010527288975666,1.8992372838478535,1.9756753704187162]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.7705945527640947,1.7843195376892211,1.8433869803244576,1.9352839505300776,2.0033800692672337]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.6327789943503987,1.6829954541409178,1.7587184774706757,1.8631906171656294,1.9479706715701985]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.7973391650718995,1.7618359071943763,1.7858933387867588,1.8618931513216976,1.9505364220727244],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.719720969772216,1.7697271673387869,1.8465377894092783,1.945871583419393,2.018629053585503],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.74082939708864,1.785527185260207,1.8474696755439297,1.925089664978525,1.9880927750039346],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.6482533456190775,1.7011528185395983,1.7906367312243532,1.910315824833915,1.9799188333383584],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.6022909902344002,1.6500444012423796,1.7347261095235127,1.853016194685737,1.9411997680930608],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","FigRMSEGRidLasso_log = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso_log, 'alpha',\n","                                    GridLasso_log, None, None)\n","FigRMSEGRidLasso_log.show()\n","if write_data is True:\n","    FigRMSEGRidLasso_log.write_image('./Figures/ConsoGraphRMSELasso_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.822e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.956e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.988e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.193e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.099e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.959e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.827e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.196e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.102e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.831e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.991e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.199e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.994e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.963e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.836e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.966e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.105e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.840e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.108e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.998e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.973e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.001e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.203e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.845e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.115e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.213e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.970e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.005e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.210e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.851e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.112e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.856e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.008e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.206e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.862e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.012e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.977e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.016e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.981e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.990e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.119e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.986e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.123e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.131e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.126e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.225e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.867e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.217e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.221e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.230e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.874e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.021e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.025e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.880e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.995e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.886e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.030e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.010e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.135e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.035e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.999e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.149e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.249e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.893e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.004e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.139e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.244e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.900e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.040e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.144e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.234e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.908e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.045e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.239e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.915e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.050e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.015e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.056e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.033e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.021e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.026e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.154e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.170e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.159e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.164e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.266e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.923e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.254e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.272e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.932e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.260e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.062e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.068e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.940e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.949e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.039e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.074e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.081e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.176e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.059e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.052e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.958e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.298e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.195e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.045e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.188e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.088e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.182e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.284e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.291e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.968e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.278e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.978e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.095e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.988e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.067e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.102e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.110e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.202e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.998e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.090e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.082e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.074e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.118e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.009e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.216e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.328e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.224e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.209e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.126e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.320e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.313e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.021e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.099e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.305e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.232e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.134e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.126e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.032e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.108e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.258e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.143e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.364e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.044e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.057e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.354e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.240e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.162e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.117e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.337e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.136e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.249e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.070e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.083e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.267e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.345e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.172e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.182e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.167e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.146e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.156e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.276e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.404e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.296e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.286e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.097e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.373e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.383e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.111e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.393e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.192e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.125e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.203e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.140e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.214e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.178e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.226e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.214e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.190e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.156e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.307e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.340e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.317e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.202e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.450e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.237e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.171e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.438e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.415e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.329e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.250e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.426e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.188e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.227e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.204e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.262e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.352e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.275e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.267e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.240e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.222e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.253e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.365e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.390e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.502e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.239e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.289e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.377e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.462e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.488e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.475e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.303e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.257e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.282e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.317e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.276e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.296e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.404e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.327e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.331e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.295e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.418e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.447e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.560e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.311e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.314e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.516e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.346e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.545e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.432e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.362e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.343e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.530e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.354e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.334e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.462e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.394e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.378e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.393e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.359e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.624e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.376e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.375e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.510e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.397e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.477e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.427e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.493e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.608e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.410e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.575e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.411e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.591e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.418e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.440e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.526e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.463e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.445e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.467e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.448e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.579e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.429e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.486e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.463e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.696e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.561e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.677e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.544e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.500e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.481e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.659e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.642e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.486e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.509e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.533e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.597e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.519e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.538e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.547e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.506e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.526e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.773e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.557e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.655e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.616e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.635e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.581e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.558e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.714e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.578e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.606e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.753e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.734e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.568e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.598e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.631e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.675e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.589e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.619e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.633e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.695e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.611e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.856e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.657e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.793e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.683e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.716e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.835e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.662e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.641e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.814e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.709e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.655e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.684e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.735e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.758e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.725e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.678e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.762e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.706e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.824e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.780e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.945e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.729e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.701e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.789e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.878e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.922e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.802e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.816e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.751e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.900e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.748e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.774e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.843e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.846e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.773e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.870e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.821e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.797e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.869e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.821e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.037e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.915e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.797e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.898e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.967e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.926e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.014e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.845e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.953e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.990e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.868e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.846e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.939e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.921e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.871e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.896e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.981e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.010e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.963e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.986e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.132e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.917e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.084e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.108e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.061e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.009e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.037e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.065e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.941e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.965e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.972e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.023e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.947e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.990e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.092e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.058e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.034e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.120e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.229e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.107e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.014e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.998e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.038e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.156e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.205e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.049e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.083e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.148e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.180e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.131e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.063e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.175e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.075e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.126e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.087e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.203e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.325e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.156e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.230e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.204e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.112e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.136e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.100e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.253e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.301e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.151e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.257e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.180e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.228e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.284e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.160e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.277e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.184e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.177e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.227e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.202e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.252e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.300e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.310e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.276e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.336e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.420e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.349e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.397e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.208e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.373e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.232e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.362e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.388e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.252e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.256e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.323e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.279e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.326e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.277e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.393e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.413e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.301e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.347e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.438e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.512e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.370e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.302e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.444e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.325e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.467e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.463e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.350e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.487e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.348e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.420e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.415e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.370e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.374e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.482e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.511e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.438e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.535e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.397e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.600e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.578e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.534e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.460e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.392e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.414e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.443e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.558e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.556e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.580e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.435e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.456e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.465e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.509e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.602e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.682e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.624e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.565e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.524e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.545e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.477e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.497e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.621e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.641e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.531e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.666e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.517e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.592e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.586e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.537e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.552e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.572e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.686e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.758e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.605e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.624e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.556e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.739e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.701e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.706e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.720e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.725e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.575e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.744e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.593e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.668e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.612e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.611e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.631e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.715e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.762e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.680e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.650e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.810e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.780e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.827e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.628e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.775e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.697e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.793e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.686e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.815e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.731e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.662e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.678e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.737e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.703e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.831e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.720e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.889e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.748e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.847e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.779e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.694e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.764e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.843e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.874e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.709e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.862e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.877e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.724e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.753e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.738e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.769e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.794e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.784e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.809e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.892e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.837e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.945e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.823e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.904e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.906e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.752e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.932e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.918e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.920e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.766e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.933e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.779e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.813e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.853e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.792e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.827e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.946e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.850e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.840e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.888e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.863e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.994e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.958e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.876e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.983e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.804e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.958e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.970e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.816e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.970e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.981e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.866e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.828e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.839e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.901e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.899e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.878e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.890e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.932e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.037e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.993e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.911e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.003e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.922e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.006e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.027e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.850e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.860e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.017e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.014e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.024e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.912e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.870e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.943e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.880e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.943e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.923e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.033e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.933e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.971e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.953e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.042e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.075e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.890e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.962e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.066e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.899e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.047e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.057e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.952e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.051e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.060e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.980e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.907e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.979e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.916e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.068e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.962e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.107e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.971e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.005e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.076e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.924e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.989e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.997e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.099e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.932e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.083e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.092e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.084e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.987e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.091e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.098e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.013e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.939e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.010e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.947e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.954e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.034e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.003e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.995e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.135e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.105e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.027e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.128e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.111e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.020e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.121e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.960e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.967e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.114e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.017e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.024e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.118e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.047e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.041e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.973e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.141e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.124e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.030e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.158e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.037e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.130e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.059e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.979e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.053e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.153e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.135e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.985e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.147e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.140e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.990e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.995e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.043e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.049e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.059e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.054e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.064e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.070e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.080e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.075e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.179e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.145e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.174e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.169e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.150e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.000e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.005e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.155e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.160e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.064e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.010e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.014e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.069e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.085e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.079e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.074e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.089e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.196e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.098e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.168e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.019e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.192e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.094e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.183e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.187e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.172e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.176e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.083e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.027e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.030e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.102e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.095e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.087e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.179e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.091e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.034e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.113e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.106e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.183e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.109e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.186e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.203e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.038e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.199e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.041e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.098e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.189e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.116e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.102e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.108e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.044e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.192e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.119e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.105e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.222e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.126e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.047e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.213e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.123e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.219e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.195e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.198e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.216e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.050e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.201e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.053e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.112e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.055e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.114e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.120e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.117e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.128e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.134e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.203e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.136e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.131e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.227e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.206e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.230e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.058e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.225e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.061e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.208e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.123e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.063e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.125e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.130e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.139e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.065e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.145e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.213e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.215e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.241e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.141e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.127e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.239e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.067e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.235e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.143e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.069e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.237e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.132e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.217e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.218e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.147e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.071e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.138e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.073e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.220e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.153e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.134e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.248e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.222e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.075e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.136e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.077e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.149e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.224e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.151e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.140e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.078e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.154e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.225e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.141e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.144e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.227e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.080e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.159e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.156e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.254e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.081e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.228e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.143e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.253e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.250e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.158e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.083e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.229e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.231e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.146e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.084e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.086e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.150e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.232e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.160e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.147e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.149e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.087e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.233e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.259e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.163e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.162e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.258e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.088e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.234e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.257e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.255e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.089e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.151e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.236e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.153e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.165e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.155e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.090e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.167e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.169e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.154e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.237e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.263e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.260e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.168e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.238e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.262e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.091e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.092e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.261e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.239e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.156e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.239e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.093e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.170e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.159e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.094e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.157e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.172e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.158e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.267e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.240e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.171e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.241e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.266e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.096e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.171e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.264e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.095e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.242e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.160e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.265e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.097e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.173e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.097e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.161e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.162e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.243e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.161e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.174e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.175e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.098e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.269e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.175e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.269e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.267e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.244e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.268e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.099e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.245e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.100e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.163e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.100e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.165e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.246e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.176e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.177e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.178e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.101e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.247e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.272e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.177e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.271e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.247e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.101e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.270e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.271e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.102e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.165e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.248e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.166e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.178e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.167e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.102e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.179e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.248e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.272e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.274e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.180e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.249e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.103e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.167e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.273e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.103e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.249e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.180e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.104e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.168e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.273e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.250e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.168e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.180e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.104e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.169e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.181e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.274e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.250e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.169e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.276e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.182e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.250e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.251e+03, tolerance: 1.652e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.181e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.105e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.105e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.275e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.106e+03, tolerance: 1.623e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.275e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.169e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.170e+03, tolerance: 1.636e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.182e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.183e+03, tolerance: 1.638e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.276e+03, tolerance: 1.657e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha      0.252354\n","1  elasticnet__l1_ratio      0.000000\n","                    R²      RMSE       MAE\n","ElasticNet() -0.149984  2.373087  1.125546\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning:\n","\n","Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.649e+03, tolerance: 2.051e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logEN=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.225771210305382,20.584008702401118,20.565108601399913,27.20214710093736,21.04585809245424,20.534479547222773,20.529762912129883,20.946982573507785,20.53210480526682,24.176253550103674,20.53841201268815,20.53705845820675,20.5876690085667,20.570127317623097,20.53575916888274,23.280776090862673,22.623908107976472,20.77774002625056,23.719513091324792,20.60731867658202,20.916418594359456,20.58942404775721,20.52549935282178,20.52134578032249,21.914092946613547,20.526238506224452,20.69561951084316,20.64269194362823,20.969410642692477,20.57194573911315,20.85501843794962,20.488406090899748,21.131857173224212,20.779326755624552,20.969914827093522,20.55306727172677,21.333177183664418,20.660825340620818,20.86079907559114,20.641313902764576,20.842997502315644,21.991853295095286,20.486355632540402,20.95589921079688,30.962037660028738,20.975479621685395,25.40472191022477,20.704805871839458,33.949146239607096,21.04995966041987,20.6362358573453,20.728618257085248,20.531522937293854,20.602080854634462,20.87458761839122,20.687707699469247,20.800051796864604,20.80799918646323,22.016503213467978,20.639919540922968,20.6287368094839,20.758822494658535,20.526765641553613,20.594483911178507,21.031236756174042,20.567480613809472,21.04222115418094,20.55266671670638,20.58823886747763,21.414479340314195,20.572363789018436,20.525198303880803,20.79843951923021,20.556600222590266,20.582363547205812,20.88438452284102,22.301650233592643,20.590826820465598,20.558505182726407,20.872154245292222,20.518001369299412,20.649277309399853,20.661322926030355,20.48426453515592,20.740980270497225,20.995115880270248,20.64203262400173,20.68084653194585,20.827001201157863,20.52183344483235,21.747297993363592,21.192136203247973,20.580090783757583,20.75927779857006,21.412484764422064,23.203655761378418,20.52222413216921,25.80893674937743,20.786684367085893,20.933896326928014,20.684926207994206,20.5073856596307,21.329711339255084,20.84597971574261,20.73512822457953,20.526441357957008,22.39273451371809,20.70195417330064,20.789150313212186,20.54813077319666,20.693079713925997,20.598968471297,22.55275814316678,20.756431531537796,20.73934062219558,20.873391987360694,21.822071221769846,20.580333275899527,22.27111381297936,20.60262503583791,20.679145187630315,20.473203607447495,20.495011626687454,20.470011977271156,20.52730584729701,20.564034029184132,20.521160280882874,20.54932391904945,20.982347083084164,20.484407595413575,20.557808639922662,21.22417064358095,20.653205403775935,20.627627553225338,20.637991784853746,21.1648371634197,20.52689624988309,21.135267900279878,20.52786840824786,20.95505310921822,20.636934630440358,20.5195406652349,21.864369569259846,20.564789155492104,24.07361456872,20.6731455048216,20.694798942651666,22.110428287757443,20.492460538307185,21.620302066173114,20.636594273369663,20.509915483485194,20.48003209791689,32.10700172068562,20.48390667752088,20.74298088884903,21.184028144620267,21.269475978661145,20.507497992175157,20.492564958466566,20.51814271453952,20.576163545467097,20.554414745962358,20.59123133097221,21.580380958945252,21.09957274837744,20.81400687526052,20.580757308364774,20.948388512861687,22.37149046906245,20.50662873841513,20.59560332103859,20.733365948809958,22.508779780725767,21.135863140476236,20.743704183616273,20.522578596458523,20.519261032500783,20.670653351820537,20.570573284317838,21.384747370293727,20.54042873682595,20.494341053767876,20.630811159465058,20.674410972260915,20.521321779327362,24.553784182505787,20.762115548675677,21.19929937666109,20.62978816075351,20.524594202834425,20.57895560472617,20.51589749559173,20.633831664696956,20.492848782776274,21.055557361117604,20.595668012824373,20.7261838837442,20.55784753054074,21.777713420426917,21.013777864586977,20.60317406664236,20.553356937447234,20.5559714622508,20.55036738038295,21.054129036887183,20.56654456199872,20.52376937255956,20.633412575104355,20.572124450123113,20.68213688706712,20.6835227240938,20.476544321963534,20.635192842069227,20.594630208389543,20.629753388424533,22.5479493350063,21.128050164893192,20.97466084505455,22.859135576140854,20.582418003188028,20.561909195567154,20.555438208183247,21.020703408091016,20.657896309943702,20.617683986406963,20.669786414888012,21.24429195178664,22.977122097481953,20.585922212160757,20.551603812181494,20.662549104846274,20.548741463406305,20.68244976789514,21.843750210060936,22.38430819995112,20.726822462598687,20.53293468742668,20.57909603048723,21.36516899313781,20.539023059771605,20.97392239781684,21.329201906063524,20.561183112578714,20.705501047693772,20.571666749349536,20.828154784564788,20.474964150057932,20.48905754043046,20.838296634240944,21.18183880066893,20.48192829326972,20.85152236774896,20.529616862970794,21.57290351940108,20.602069962303794,20.575120220352197,20.531528807885955,20.642514038608326,20.55246317068039,20.658942233484428,20.658723046155494,21.19534991809663,20.59207063458221,20.524433277501046,20.478414767629168,20.80623222858785,20.669008203320463,20.47905191493303,20.48926463578489,20.86861997955107,20.588570962902576,20.527787966401334,20.510839150910513,20.573247686500864,20.656905044403572,20.74373558836735,21.178537831184986,20.716477012145404,20.64167276676272,21.0985651009227,20.718800810233535,20.639059014419413,20.54021964453655,20.54016617550373,21.057056725661287,21.24090514012063,20.54623161168081,20.65308742025587,20.652328031200877,22.18948286156113,20.547255961885487,20.876527779713633,23.01847432703989,20.459367887466296,20.555175952516144,20.741927167508873,20.59623799188762,20.572983826964048,20.74929669322309,21.978815295607326,20.799570543551546,20.71193118019008,20.64632135567482,20.963522731056916,21.17762274879434,20.510593734617085,22.18998922944072,20.654185766192644,20.597126123256107,20.59398308123777,20.70423321441966,20.54212371438246,20.692625732298335,20.59203178921828,20.613642789807702,21.15847018882232,20.601373258154705,20.53773353638414,20.581404764817087,20.952002592481573,20.833078943926328,20.90544827774037,21.237052581070714,20.53603626914998,21.252294115428796,20.556610646257138,20.988894936347286,21.042308443186766,21.842200512470004,20.535420550775786,20.505333578515017,20.659820034391966,20.77373979057662,21.0784405174387,20.59462359069419,20.70988147392696,20.548513279223293,20.54512512549955,20.724277911982224,20.535050037873713,21.881038205595424,22.009326222834304,20.518843625566,20.778425487411145,20.619321967882424,20.644537439651334,20.56617778676754,21.383867986298924,21.068914312267154,20.567168138133745,20.708020870660206,20.55551019118102,20.842693810419497,21.017400324158178,20.551365912075855,21.271622961740416,20.52525819336579,20.515169379505572,20.575937254694715,20.685935787320084,20.59317201852591,20.649654465180987,20.53277062382018,20.643071015662088,20.72821137902982,20.56514929265437,20.943593007001773,20.542150607469072,21.692038571780024,20.518819389328154,20.637619116160266,20.623624758987834,20.51180352491011,20.609946035431587,20.542436259801804,20.585212281999922,20.632294204137985,20.588111112578545,20.572295222424714,20.967126125254115,20.812195210067873,21.811115364397146,20.469299374456813,20.880080045276472,20.73582478041807,20.741258468444705,20.52205481512463,20.9394646297058,20.555402490408536,21.1935320989137,20.739653999070264,20.910582963690562,20.952727313218688,20.516330000466034,21.34514238471183,20.59000988354504,20.722319573481293,20.728303044488218,20.823084350261464,20.574859028921825,20.65521970836818,20.47567336382356,20.525395665625876,22.258272504597006,20.594350465872477,20.897215860801456,21.446410601589722,20.7048082721465,20.60850516005995,20.556866002131326,20.56218775788997,23.95283662123837,20.6294058701723,20.55004241169544,20.478433215467803,20.69035669221523,25.142371168241354,21.028010386942174,20.78555914915288,20.478420861851806,20.747888895799477,20.510412802172517,20.566019593065665,20.641862853277047,20.51096113285257,21.584204864176595,20.693948566251372,20.61851472047248,20.519539026152078,20.840550306926488,20.536246382632868,20.549376834115517,20.48489721328057,20.663433219519234,22.752124056133432,20.572847967730397,20.633512731345853,20.7780135902086,20.751774292236682,21.11910582405786,22.675371710814417,21.082887124057624,20.548214848958413,22.78847170504479,21.073942783222293,22.139083105250364,20.48281212026922,20.549885125487677,20.796871654252413,21.38623313542722,20.620692462727604,20.68732959507598,20.994289409239293,20.76150704792046,20.7488988394536,20.58018395712317,20.621703174458098,20.655392940595274,21.503095406544936,20.569283472159253,20.581204168112794,24.886799460355395,21.165147491172025,21.044242427410527,20.6135816607332,20.960800510661656,21.50263140717531,20.555667956924403,20.66340384423194,20.505030487003012,23.032081603717046,20.65313993510739,22.381777069063332,21.391261194052692,20.5771681482793,20.64245793797245,20.692823495258956,20.748514460952514,23.077722650505315,20.49999904668595,20.652908312106614,20.902913879651763,20.611847860968727,20.54995382787235,20.551773077048566,20.682601342655275,20.64403662139117,20.598328318899483,20.49475208368464,22.59450657409362,20.562902477095335,20.608630340521096,20.771585861577027,20.666840650779037,20.54305313515024,20.572259827277463,20.512561568668474,20.59305610115264,20.588527849250607,20.66313010702798,20.70324526852887,20.463232595647806,20.580671179988375,20.571789890156115,20.63359557980408,20.839260142449277,20.534974285237126,20.60127451086828,20.626280360150634,20.5575206989881,20.752292410401235,20.625299492125112,20.541985342573476,20.62913368938003,21.13115028499862,20.80851488438688,20.482794771412113,20.665376744058154,20.489811651774414,23.558209860755348,20.678895550873243,20.47629229286575,20.48386841085017,20.883198285200674,21.06600106649662,20.580744065994566,20.6300027535997,20.538650167908358,20.474114114413783,20.641921393057228,20.65758962387196,20.480492782536928,20.473395365989603,21.19636545373644,20.56971307371487,20.554763515692432,21.691703416454096,20.49747346828022,20.506162515859543,20.604509800873803,21.571934553022796,20.47660460447386,20.668068337140614,20.923029669356396,20.532992257680547,20.570401177888606,20.987948814572473,20.673557796115105,20.83550328452859,20.61302811096489,20.75410802012395,20.521154472218967,21.21049667517316,20.606751079350833,20.57482679830617,20.616811685238623,20.569932389674488,20.73756259792058,20.62710174543055,20.83622704543121,20.786037249324306,21.00276917078102,21.597782319354017,21.16922313252621,22.383790172882755,21.56299021218746,20.491750743937303,20.555967194297313,20.75882055296539,20.927819575546824,20.6439395088147,20.942352716852437,21.641831395367365,20.577390296798367,20.652849827890797,20.52258446705062,20.80556577825971,20.526341609088366,20.733885713667135,21.684580634727208,21.32076699841975,21.16282916804227,20.564915803086134,23.89244740788146,20.62545694480158,22.62391259569228,20.913838851521785,20.725942678735272,20.743009596597137,20.5881654028769,20.79337850782615,20.71208525485848,20.554284618899803,20.534050130617512,20.635066821686397,20.59486655299609,20.745696557377773,20.620704627429816,20.548060461641903,21.986720992546747,20.542357097063558,20.45034537895357,20.58432812798052,20.534095252121574,20.74460100532461,20.568239637644112,20.50529654816959,20.534561591698864,20.625855399536636,20.528416303685557,20.582604327696405,21.49826042512934,20.63229019489301,20.59758405533589,20.625723041463992,20.62727681629253,26.230910923604796,20.554125585931104,21.556455616541264,20.555218578130546,20.52185460772021,20.807383860065542,20.737155719865154,21.087662388953397,21.489316084294007,20.926263665128847,20.538250507471908,20.517520164783136,20.58577091531726,20.53909692314861,21.161738628230218,20.5126619340043,20.50128812345691,20.49035766618172,20.650284423252216,20.64550001424415,21.444257403607963,20.53215126442663,20.556678778989095,20.62564185596024,20.497850978413343,20.64204126241385,20.555448959719797,20.52328317490747,20.6362358573453,20.8794036604409,21.81963595680241,22.165750953349132,20.658169643841923,20.661276019775514,21.33657987061523,20.830546891456873,20.610597416439816,20.6399156822796,22.272306577849914,21.31778277171979,24.476345499818905,20.629138290751794,20.476209862036068,20.617984899556987,20.692603131903677,20.880200199878693,20.531406931402856,20.71291284207741,20.640759054123144,22.198427202396463,20.635576684305956,20.513559150466186,20.49310335187952,20.653697295260606,20.724421607974627,20.635907778670123,20.551422497901676,21.129021332397503,20.68724201494267,20.66308468025401,20.56613109264569,20.75391942029021,20.69096041739102,21.1773486181036,20.643172496462533,20.605462557545586,20.5645809093827,20.59066108032706,20.889273725913025,20.67073248126259,20.63922443014944,20.599577328619606,20.630858482487273,20.645208853928512,20.469156042919344,20.544628042626726,21.45079419121865,20.529264368616055,20.481156536122285,21.514372291418074,20.52115732995656,20.539667685674385,20.49900572094659,20.593693202886776,21.200277888297645,20.505535840762967,20.608791057755536,20.87212816019008,20.688914494531463,20.714700908730574,20.623493986635324,21.007340378273494,20.57486137870771,20.5254790377208,20.646471973366648,20.637854249112454,20.607739553797323,27.386995300318667,20.535035336500933,20.88173423031408,22.998493970105567,21.193669962101456,21.317530557451335,20.561351563832034,21.126796590446048,20.977620867861667,20.626792572479193,20.901297080354226,20.849594200354023,20.646797145702305,20.778187850125825,20.533680108068193,20.734065255761806,20.502282853183814,21.030181259879853,20.573656281626434,20.5438615410156,20.476939652555394,20.565370157673808,20.573659986861074,20.45889905198019,20.62881215231686,20.75574059530618,21.3029978715349,20.901468090566215,20.620809047589198,20.56646503982343,20.6951358451583,20.708735041403223,21.11467465573323,20.693935174116046,20.74514443928346,20.598398084860897,20.70645562879251,20.5347348568528,20.502095620181002,20.83173056469043,21.442431802604034,20.845238231821423,20.580890079948485,20.64824607441938,22.24336288522007,20.73113780826603,20.522426569614712,20.867428412194606,20.628674775324935,22.95525762486045,20.609309082929986,20.70948951209214,22.236589561819965,26.302562433766177,20.721451833503735,22.088266324270013,20.687503396574797,20.606135576507146,21.437466260754388,20.703732447460276,20.551669803159488,20.56547237393257,20.904195467853118,21.4108086226184,20.492302567008515,20.885575346708602,20.593583568272845,20.65304699648487,20.66546663142558,21.0711796028081,20.46927399120192,20.497200408055704,21.640613011469927,21.96673999971076,20.596541053071615,20.716998337899938,20.489527163033898,20.82969428434228,20.54907104674936,21.515659117409477,20.637397725917754,20.60086856042595,20.779268668985477,20.667939409161484,20.667034743604045,20.57927902351174,20.653115698869545,20.83239260034273,20.55526989272088,20.74992461133256,21.91756050074024,20.863749605274247,21.733139950225635,21.144197923573902,20.85273364319294,20.572061120964708,20.745753136657612,21.37182168414447,20.74411854852782,20.77226853155739,22.078074405275153,22.39468812137813,20.574146913047088,21.144212241115213,20.6024776408907,20.74904775821059,20.73961358778531,20.614175417615883,20.705061754206085,20.59688783224495,20.901560066767352,22.513804872567942,20.95705087259699,20.493934937357473,20.572712833979555,20.606658734937675,21.380608412491185,20.669996541976,20.51890071062286,20.566788473249453,22.17338062416243,20.738100223593865,20.546387308233147,20.498274588087238,20.506738098400866,20.81383094923541,20.940325951567125,20.481190766750846,20.993093634642076,21.05765971721327,20.49006138011126,25.835807128946296,20.576354741256683,20.584227677690578,20.41011097429961,21.065002942660662,20.59132413380378,20.47674163279304,20.615069680275734,21.358903498862578,20.736330482217433,21.066567473366568,20.757868551993237,20.886728679692315,20.63332044240125,20.525336710232366,20.726118851068048,20.688628218350043,20.55139788001954,20.53935971978091,21.040676688640133,20.613373686205694,20.945194359453108,25.62673643417601,21.073947283495997,20.678191195312625,20.61631214014258,20.667928199206525,20.662467798871393,21.087448663300048,22.019572534754086,21.601418342519796,20.802322848661486,20.628403080689733,20.582226547517397,20.59290862588641,20.691965683994308,20.65859880601632,20.588348749620476,20.42158889418083,20.53272533072271,21.135740931281383,20.59909763421346,20.530140444676768,20.626908613702255,20.667222821689922,20.572735543508518,20.513200944103406,20.564776297256692,20.78573425177475,23.665109596942663,20.50089852151429,21.00469970547145,20.526875870376905,20.581716739491725,20.976716903960483,20.485838661452636,20.863766729072633,20.52340233405425,21.18523459045663,20.69072742114348,20.604547661873923,20.796462178360724,20.60179172819563,20.75267992920268,20.80860795880035,20.564619393564588,20.608511104514807,21.199237157407527,21.483761354465496,20.84396272087573,21.326219097488423,20.64867386613359,20.549757470672663,20.652941846325174,20.514221078992726,20.540397032449327,20.719358703652887,20.52208916862033,20.643506499624106,20.601627302322907,27.395939641154,21.046961972996407,20.786034605078257,20.633088283166398,20.536431258295693,20.560944821567556,22.54246433580128,22.175141447217605,20.81774169034851,20.70550358226923,20.933314300021415,20.637654172920946,20.748284963030915,20.57400975387527,20.70037527198553,20.58999151072224,20.51912244143356,20.54684051980888,20.51020033367164,20.484780347248304,21.604294803433877,20.57184681793067,20.53821044474383,20.537763458404083,20.563651794739403,20.545151641269396,20.542864792176392,20.512419860345478,22.202138653425155,20.687533766602407,20.984247394082782,20.878602630467626,20.545550589048563,21.066604058048604,20.561014661325398,20.894925706048014,20.66433728143061,20.662235029622416,20.47111701753828,21.056779403494197,21.3660561128539,21.032511671514836,21.063601990697002,20.736481293692286,20.51718559040626,20.661845065290006,20.565882457470835,20.610634114663096,21.73110358729895,20.528708657734448,20.541671810513424,20.722739481838,20.674685903958366,20.599880159948576,27.6690887422765,20.895376258640646,21.446473608208624,20.764517650164986,21.123618996568563,20.704150484169883,21.0512527840221,20.682542518289644,20.72345109264108,20.53929492960209,21.30660331190383,20.53434000646121,20.530102985398273,20.731948391732104,20.644680564964332,20.570464220129733,23.585639991186735,20.635233857571514,20.624095821276537,20.72309214902111,20.603776757791643,20.652326481475377,20.56825455588991,20.76173866857036,20.532999339508574,20.579377432399344,20.59507214763058,20.607610897400097,20.731464095027068,20.51093840434115,20.583087448382944,20.50213776615884,21.479548522951582,20.632716401226716,20.653604764010943,20.934470767302813,24.13398989590348,21.10455230510773,21.548388496898603,20.767296741090217,21.84810741881866,20.49005575144948,20.51793152954157,20.743788379278847,21.980508839185852,20.591589034160545,20.94981330652089,21.030857958051257,26.5064408307428,20.505737329091325,20.484932509883066,20.60966108417754,22.213259172538166,20.530100816218734,20.637846176468646,20.502657400508898,20.47339636757182,20.598039342645905,20.978996980388096,20.61560307577301,20.65974549101889,23.916204633111477,22.89773052261601,20.533179373547128,20.512215940047543,21.668552850186888,20.556541990116624,21.31837090667259,20.771492005386502,20.51455365750129,20.739400964094287,20.742945357641826,20.6154947647213,21.602742504964716,20.592428051528106,20.483908490893267,20.622447090811114,20.668011542692163,21.100859029807832,20.521143584891465,20.82837300439615,20.549703341711524,20.606588708343327,20.68651952738426,20.71393892977643,20.550941300736625,20.6862625686074,20.60659717398166,20.49534739335591,20.818014133125605,20.518577711433736,22.925627632662774,20.764750088418793,21.977776651711416,22.156854476914543,20.587943491409614,20.49321906016279,20.79620646135784,20.581396261394598,20.515174316365453,20.826243623772477,20.655265696510156,20.676955883527494,20.497595397201415,20.720050463379824,20.51601486251898,20.55583195642136,20.688125925195116,20.79174527168593,20.888106063038705,20.55810919199848,20.64837015624807,22.25639674887052,20.536838866351435,20.609478506784114,20.533173278174715,20.519144674506972,20.579205799982425,21.1760557143147,20.622832459565007,20.62696343783479,21.677596639188856,21.301073837144784,20.688089528465646,20.993232484555197,20.587323267304946,22.021227953278096,21.535689352589674,20.60419231812191,20.577202542441622,21.02714745678316,20.52260763977806,20.667236114315717,20.586534338808725,20.589760909503024,20.6534027230451,21.1728944598336,20.67517714018118,20.69208588242764,20.745597153758304,20.716576353641194,21.47888123116017,20.954978114319797,20.61895700778321,20.492658163894426,21.403805499335807,20.702080804911223,20.65276324443557,20.76463696722994,23.12136528203797,20.931385866969883,20.52905935965758,20.541154876851586,20.77227593917125,20.750193989093194,20.469488874632468,21.554412498576934,20.509978331279036,21.167414529657655,20.773581308065275,20.50037979130893,20.910457801754763,21.20580224741965,20.50151510796581,20.76888936835653,21.87209386476009,20.56247744304787,20.46035503362148,22.494420382996825,20.882591384934837,79.98772071789668,20.534142644716137,20.573131534236442,20.543158828473743,29.91045777289384,20.52646450561847,20.486298368231456,21.047053677479543,20.615385371363878,20.46784328487625,20.49221076575916,20.57922106204157,20.66766738699083,21.047564678945832,20.626879237163486,20.519886056345428,20.550643960660377,20.94682421051915,20.67645504624083,21.548700709391856,20.96752819622785,21.573326553409608,20.568328496437672,20.6516154319694,20.937705514144533,20.5255675453513,20.643090810756465,20.558121946699462,20.539793583087558,20.711213131180607,20.946108768382885,20.539020079166725,20.56652027312749,20.704882484726184,20.614135908984412,20.542234113539283,20.65927083266619,20.582499771746072,20.580464260136882,20.518686655849546,20.565486330951956,21.363559185225427,20.754483830092486,20.58042424534441,21.499046895798784,20.987276342823595,20.8090854875193,20.5627224085142,21.181736917950545,20.504318909498394,20.499185833738903,21.98304421734309,20.60241046524973,20.91077254851706,20.544721359064862,20.720098350958782,20.538232025466744,21.18977540607495,20.666973633664846,21.283321053443892,21.6507757362027,21.48015256910923,20.71315559376348,20.85287511883987,20.66009304159562,21.00497687212902,20.938815879882245,25.06020421493322,20.563582090772513,20.601887096955476,20.54478506210897,20.77300605978259,20.75501001248243,20.47586494532568,20.726018224287245,20.725520694476526,20.592439533064972,20.966901616637365,20.643729033785977,20.66063576599634,20.78044261330415,21.882099859451213,20.51682725189178,20.586528378914213,22.43576963657536,20.696129992477747,21.526525772229704,20.501404879142516,21.033617331181606,20.70772370859625,20.7736882763561,20.476648217777804,20.5519181578553,20.611341789466444,20.55694719989079,20.45971662147373,20.780436346221837,21.243398223314593,20.731520908502024,20.509956369787524,20.67940584750584,20.933408376017102,21.597815685962555,21.342121524499753,20.76802863274256,20.616012098324656,20.553880484673822,20.57551117669527,21.569823102338617,20.498424164388844,20.65428904897886,20.7352859242963,20.496293119112966,20.552690574364046,20.626035511941463,20.59253106273242,21.19091779278247,20.627966294005326,22.517724121561102,20.641756855754558,20.605249512034206,21.074267745773653,20.97691513289332,20.64112449582018,20.538679114301246,21.514846544953024,20.612385211759204,20.537144925079616,20.55340504002467,20.518186109171285,21.19705480083315,20.873252125539008,21.812368203804148,20.770200496826224,21.617794317738102,23.95189319066379,20.471087757081555,20.525267006265477,20.542508258116104,20.567178342278083,23.21454848246845,20.709803858897903,21.02438802354325,20.58433923668499,20.599288749571958,20.554300682915514,20.618501980442428,21.024942183681585,21.00661200573219,20.551845907263946,20.553776064514437,20.512598598252634,20.65127988949287,20.531089429798694,21.01959343256122,20.579567533845953,20.49747177547887,21.13604256981652,20.838219488597687,20.472111651266047],"xaxis":"x","y":[22.06759946491166,20.51569310181126,19.61997633871636,25.808363100543477,21.795376715513594,20.34038041027431,18.86081419340691,24.078413808713293,19.082528830592434,27.09570141066156,18.66432922183982,20.146924115160264,20.694537918678005,20.34827538264879,22.1287945428413,23.512029578625103,23.710849830139047,21.16813940491307,24.138707165857692,20.413583384325577,20.971944131576173,19.330306789697467,19.240535149076692,19.593384739494034,23.207168669114285,20.243485920599927,24.816169078762734,20.239179241482244,21.93825114094642,22.544699771649725,20.581001696999003,19.006769325349232,22.852829884665343,20.97322158881004,21.37041284025584,19.8813455186455,22.291944364905724,19.352118206703615,20.690574372087426,19.616726513482636,21.73777568014623,25.92407619036082,19.62630225708032,24.328668011629535,28.119486884464127,23.268620237666717,24.937914667295647,20.538966591972148,26.49472805759287,21.551656870776874,19.967005268430295,20.53263058829885,19.077522473826907,20.411222614242394,20.118559661298907,21.75794824506378,21.969944344762656,20.941991359524017,24.30144839882211,20.08875201356742,20.413158139175476,17.21138734793915,20.10520261649224,21.318516744600927,21.383177447413818,18.901120542977083,21.140418528305776,22.317825778768146,19.694551733623157,24.452943059423166,19.34725801943171,19.360313431748278,21.62865690143549,18.725486086935668,19.22798647641259,22.00766259568964,23.581082748790354,19.483945640419044,19.555879359056263,20.81759187582515,20.004050690864457,19.7027197284411,19.957005833266606,18.935012496616167,21.132460035639845,21.104713276655342,21.0706503248077,19.995538068644624,20.45373482710394,20.377850211599817,24.218899085058613,23.648191447276112,19.27757204338643,20.547353262390168,22.570203152455143,28.033188230056272,20.740147099081785,24.844950181647548,21.11139041842472,21.618102712310883,20.71863237230781,19.638742582781422,19.346013720701006,21.12828261562749,21.344903224909334,19.828653485294133,22.842407702796528,22.86005581934068,20.23340552018178,22.244828354517683,19.8468416379525,20.153224049031966,23.16226531069458,22.47425899129698,20.440974576504654,21.34321011850724,21.96825774467718,19.581570228112746,22.97307940319244,21.943790024620068,24.922685338201692,18.397908557303648,21.170757004644493,19.439732238591404,19.06759979313172,19.78225599625029,20.88710307988011,20.06163488065717,21.227834779401228,20.925901548056537,19.64120627043042,21.51013491039007,19.943833745099443,20.041179196484592,19.31991683554051,21.305505708300736,19.083972676252298,22.36486950758078,21.00153722158492,20.91944021038121,19.71891096707218,20.43515061037962,23.756601825433552,19.29612083176196,27.288645329988054,20.004627845911102,20.581937941283943,23.06049210432163,19.0283827986455,22.26292897661663,18.98289333455679,19.08654493081137,20.26723932452398,26.772642502942784,23.150894170391226,22.789865927709045,21.316932785410458,21.47925775839002,19.52466856143405,17.31416721288844,19.67390026636621,21.280848347151327,20.575951881264743,19.36981390700933,22.906270958120995,23.259273464150183,21.034942664057727,19.478866371959384,21.521316718605924,23.251480243552532,18.947642644803835,18.234449818345436,23.74556559722114,23.70046067341344,22.17893043543761,23.314842649926593,19.204628901280135,21.028986544289005,19.811383905702034,20.105414612220844,23.470671408984025,19.185541351838207,21.863096949082852,21.439837984160558,20.28595076567891,20.09465504412721,24.347469240771108,21.026538976608304,22.25115601355335,19.17189511189754,19.330727902738385,20.455200776646638,18.7812284253763,19.9984664758684,19.71179802639097,23.222688495940556,20.75461515785591,21.453079563108336,19.968121134960665,23.31621219099147,22.229186273778243,19.29157093266521,19.725980900417238,19.393396349595932,19.510476738006748,23.86936283890287,20.803398322957783,19.50445726786541,20.266025864180154,20.318803159054653,19.88287753302219,21.755410336588376,20.021014007950576,19.81464925130666,19.600081112964823,20.195578868662224,22.127386699288984,22.615494713832,21.994462034082755,23.008367409641956,20.02036350615788,19.496913617064063,19.65284705132002,22.970383373621917,19.800442868861996,19.544786543412965,21.196126618068195,22.706377974056313,25.14188655199703,19.430343691015025,20.82549425312562,21.300868648362627,20.935011057360228,20.238709984208988,23.361271159574002,23.97311707542301,20.93275451807793,19.135842027978637,19.419925212472517,22.69657897049395,18.04707863542646,21.639091593296747,24.902956648700677,19.472341179018663,19.630944296038688,19.581570228112746,0,21.279464358226686,20.6373536565784,20.55168503037007,22.252924089212918,17.011336466938804,20.539904719671835,20.287228770242784,22.579718287600105,20.016747429777542,19.919862789535355,21.590649178257912,20.459085985918723,19.305685493406397,20.69934199839951,21.814001605688915,21.49724409211446,17.96562762596234,18.897059576214055,20.218149025767367,21.663202256858984,21.537426968242833,19.881144027962787,18.109927551497627,20.650313109616484,19.551649361459194,18.954616641321604,18.33673536238825,19.272777840955303,21.22721049508145,23.36907437848721,21.839279632705356,20.080313988880153,19.791725695449298,22.830034474576443,20.038176858751854,19.882805718288704,19.137324212876308,19.89611063211052,20.922839608419043,22.73108571789678,18.798041102461394,19.809989451392685,21.628482474975726,22.66651647870706,20.105565995835796,21.324104166055236,23.33275256138136,20.80501366619404,18.787924871827784,20.381292625866948,23.456138303517488,20.59828494694422,20.938551777966236,22.709886920964298,20.558379254361796,19.758613410846518,21.618914296187317,22.889842311610867,21.30122097904847,19.0336054859068,23.801438495307856,24.840309004310797,19.694954646749302,20.944503948247828,20.620570884389835,21.72592495902031,20.296054717472362,19.58398989769835,20.2956272827196,21.085706497425413,19.461766940454588,19.29900449887296,22.510563057499144,20.841106100432793,21.801619166438428,21.147132242697182,21.551582010518878,20.4427602890876,24.611415306054898,21.490696787354505,21.24369655260969,22.432940196049124,22.234424235191682,20.099462108962847,20.800416502964755,20.17949699394329,21.746512886013605,22.496434708234126,19.78606039431591,22.229470444639624,19.19842345841963,21.071706539354626,21.28737646300594,20.120777025175872,23.69806207100269,24.25287711008343,19.074352191519743,20.558351575498786,21.437400189861094,20.352963298343866,21.233800456337935,22.14457282995132,20.722700819009958,20.69715285443839,20.03253125195289,20.0566191944814,22.70946891023392,21.137414277628565,19.808943756196857,20.143175790739598,21.040198736231254,19.712186074557728,19.35146728073557,20.861088989018167,20.755317114550273,0,19.017540081468233,21.419330156983328,20.67873788317677,19.93668394257525,22.409766675366317,19.27540160418116,23.728782472563765,20.376074503254586,19.31077695697734,21.065895324749285,20.3484179021553,20.07860380095908,19.976548041795603,20.235086321270654,21.01213158467734,21.41999462819776,19.174863067555084,21.10241852261651,20.85643673732399,22.586773372950685,20.037769462388923,21.559823540829672,23.13793422908773,20.558605220150522,19.683189962828273,20.39789928981696,19.337397595787667,22.26889032385089,21.195293500127566,22.559724250840063,24.39611039895289,19.59271980995523,22.136011068345166,19.33796659591498,20.797749380419276,21.567058771175564,20.725266392758815,18.468910364525364,19.663251453070878,18.918473122196858,19.198971627350232,24.325766932844235,19.86193501563865,21.156628899315663,22.938277348217966,20.78722963747693,20.66031803587651,20.155379081002646,19.457981357627542,25.713720052658104,20.828559454854545,20.52544147367378,21.147722912842873,20.03302501817969,24.630356510701596,23.121909718158822,23.106213073613993,22.51080834568276,20.883881259335507,21.661257500272384,19.85555820528158,19.526632957320885,19.191856386440307,22.96557469826004,19.96989517504386,19.756880613939327,20.841336970061143,21.615161729704354,19.630957845101975,19.107266091841186,18.360084050573274,20.178689314774683,22.61224657942639,20.902818910258993,19.692513024244924,21.22795725431778,21.431836012474292,22.58649267850949,23.56945490190484,22.15739813287633,19.523673217064264,23.682062419502355,22.448170385491473,23.624330847094043,20.4998468600676,19.29906600505008,21.469193647687533,22.680649469911085,21.178626711012377,21.686546691263366,21.428479917582088,22.88912263482666,20.804770608680126,19.28855746111665,20.502951107445575,20.22322663992422,22.31457162800466,19.478115876564008,18.795995233915015,25.17177163891695,23.160406649511106,19.25160491688623,20.826937118763155,19.953135265171547,23.933068642871486,19.355480148534408,20.44485249695551,19.56616125697471,25.23181389258149,21.345469949872594,22.776042002687795,25.303110182218393,19.58463718752616,20.204391739449186,19.893828839264728,20.33821258053668,24.68330601022857,18.821811344100055,19.840511525069328,22.297566604177444,20.78875924862035,19.741131185755275,20.04722528013383,22.468811326543563,19.99457856414159,21.80053568878591,19.167558108393887,23.846962977749094,19.07645844042533,19.307592868706436,19.883803436319816,19.767052898077655,18.952911838336565,18.62774549084819,20.73926969664862,18.627786784209636,19.177777189052396,20.089958597754855,20.801380865239704,17.764101156137162,19.11561871251436,19.24667566091232,20.93405408413005,21.340147837597844,21.090725928345268,19.745386795650543,19.095931371701568,19.810878030088894,20.251155291034372,19.165221068220944,20.724802969675316,19.74733497869572,22.856134451929176,20.542598325656595,19.502159734210206,21.979041347822633,21.2197623270523,26.969103234931445,20.18342033833375,21.02499268359878,20.592440384261195,19.990444456103596,20.867172390851817,21.273367636241293,21.243839329097405,21.363500625759567,20.18486444945059,20.241113097705927,22.693372098426487,18.623284957897813,19.205602363695707,21.545170940853957,20.80034066129298,19.981973707277767,22.19000668879013,22.718490023684353,20.707922743251714,19.214188218113872,22.472827160217914,19.07108521476948,20.346151863763343,21.63013067480897,18.214892995560163,19.93614600441511,22.53034703709958,19.469859640957857,22.895450751816803,21.00807772677881,21.431258349952667,20.941808459871957,22.99407646829777,19.76111291920157,19.008644725628592,18.355858634806896,20.297451997659746,20.474380309584664,21.870871511939562,20.759740299921024,21.439000175436664,22.030880607811586,23.00587909853429,20.655014419256375,22.965146977089795,22.43523710461151,20.18471556395119,19.019883876304206,23.05070508803856,21.499214768467855,20.59203905710115,21.024704798577186,22.72181503272866,22.377050963043796,19.71871179715288,21.93139047566312,21.827309362120346,19.1796783064222,19.826388660339887,23.65560944108187,19.199993380834425,22.21417220655097,19.269925661522706,23.782692532330046,22.173362238186836,23.41808975500013,20.872940195293037,21.54216753533708,21.03393309328942,21.663729644062077,20.699641074284234,21.39606694375647,19.593416427729718,21.355622092522935,19.49242240707679,19.406777996109206,22.625652665035147,19.860111721120674,14.03702355520748,22.00972851632279,20.33705521659505,20.155471422765178,20.40843263005365,23.540959857497864,22.64902697475969,19.51901613478499,20.748003621067692,19.075332900397907,21.535462867247563,19.35190112925433,16.543892756530866,22.048803804340604,19.94957596782787,23.253137677025506,15.802066947826521,22.01816825105002,24.65495079669392,19.684337709357063,23.07232514287102,19.441821622178967,20.90283362751463,21.65340972782181,20.672219865399406,22.248853822319084,22.007999054329076,21.514848387611945,18.91123104739614,22.195045634281605,21.092338092530998,19.27798585366196,21.725547245838506,20.321666107127033,18.844349286283478,20.211183384442457,20.06069065496676,20.388592288941165,25.880225035189923,19.68504303470752,20.795739259102447,19.874915797147626,20.484467680558765,21.537004350283745,19.454353518988064,20.471143313052202,20.221414835639845,21.30681378731033,22.893554470389834,23.892417838743924,19.773417735905042,21.39811634221899,21.94580529932221,22.093666756982273,19.566140232030243,19.62938152434226,22.598971957007752,22.606867611519395,23.548837661891714,19.57985452716288,19.257215175267675,20.260215495083163,21.269107840654502,21.892874617975046,20.911342976063032,20.690267441816314,19.415723203109316,22.536424619535012,19.241608485058645,19.162579413291002,20.470409642279566,20.97348174432658,23.73507977629211,14.55836091863428,23.336410397420998,21.627932359551377,21.49391553553141,20.265469687416747,19.426854398695138,20.625264709408093,20.223930403747513,21.865935415382868,21.881578906408237,20.14846091060974,19.81407448278221,21.0024847282008,20.945391681085823,22.291777470342872,20.551017265346704,19.577722584113683,19.444205825873514,21.46402517777047,18.277738745265133,18.799932943985397,21.60163932304092,19.80978333592071,20.91463622707276,21.547692385839394,19.17999258814081,19.549428996570953,19.54492482512454,19.294068756456163,21.69613129076446,20.048671453648097,19.49098989799628,20.42092755989845,20.047998538616795,22.735567798975932,20.159778968313912,20.920832580620292,19.699545342858148,20.678206389277292,20.079358473730228,19.917597162486278,19.692880228017533,26.26193365893149,20.33327111425421,21.201441496966886,24.081396558763124,22.374129418340807,22.181711726885904,18.957794711482784,21.63591845810156,22.742154187536123,20.277557571355647,21.712167598440185,20.922319986303112,19.981118924422468,18.376324675581085,19.093096903348385,21.041684548715036,19.88267606965292,21.79470380164697,19.394929062748655,19.711931797206645,20.701777070204972,18.966254774482525,16.57594392051226,19.3661810523187,19.637857750008234,21.05876451742095,21.39188913879269,20.922214169866013,20.158015290040524,19.840625811644227,20.287989620862316,20.392027938746434,21.440477364164977,20.18838178436931,19.209224880205603,19.523793560708743,19.767710849855263,21.57006553499441,20.748925773432315,21.47821316674072,24.169496968680054,22.350651659007692,22.60440595404428,20.973185759004682,22.891726790683144,20.2096151875794,20.22349858267661,20.901209408549594,19.053706845403045,23.000908813982363,19.549547798448483,22.637005220660473,23.38302130821851,25.236220259743373,22.107609158967936,23.256395298587126,21.31115959043422,19.242152996635603,23.069120127461606,19.88568939220856,20.74441094675746,20.977820033541025,21.403421935393933,23.155520679582406,17.74375624808358,21.65558749657167,22.72177363339029,20.057436191229318,20.31682150206961,21.442049003709382,20.447439187365134,18.64679105166152,23.081069556252004,23.109422435163253,19.40981146751902,21.538742146089504,20.451115883369862,20.520204300445414,19.072268708954628,21.853488891111965,19.870759032200375,19.874160110175985,20.45611373193672,20.923455889638944,15.423378576518493,19.495183602853512,20.532198000108426,20.172754664219987,19.621134659665625,21.149770683357705,25.5492833010917,20.91962424027046,23.144648130459657,22.724819827816283,20.76732896720974,20.90466916348112,21.019251701774245,21.870062515886435,21.968529606994224,18.28303322918483,22.7967578369547,24.617902294510255,20.337523908425865,22.278345092825212,20.439903152029967,20.745981176528062,20.50317548758169,20.148792615003963,20.18507504172347,19.887802201303376,21.335667578998795,23.547889563240428,22.184719195487645,19.124759964560997,19.09456742423413,23.546328229722338,23.676810419954805,21.378549935187863,18.772971741326142,20.92343911431291,23.66501619396984,20.362922084924325,19.335122075152558,20.6322831501737,22.014406532680294,20.862569257472042,22.377941930707426,16.70615872384773,21.114598939015725,25.312874128977917,19.557332226454086,25.0888235331049,0,20.821375100396438,19.69889252664812,20.730065203035867,20.737794346856816,19.720177445507503,19.751886432327307,21.436234720492326,19.12391425698277,21.093445290401018,22.18427320470571,23.32578333060166,20.235023779555554,20.38834949087981,19.076215392899233,21.890199695637197,19.626240196846272,19.473911816076555,22.65051997686549,21.07153535807478,20.152641197123305,25.817249936117022,20.72049143867345,20.000328449472278,19.677575133698696,19.854767962698624,20.90714373186156,21.852326049162542,22.34522357506863,21.88381079939571,20.64279914121781,20.008240359915625,20.079932477068777,20.97646133780068,20.617037831360907,20.86340137149202,20.10297814136792,18.939248297157278,19.337083944174847,21.21789500066683,19.959413629413806,20.57158693763026,20.1483293247843,22.45828794986169,19.223167412657357,22.090969450288238,20.54580254002858,20.99006223015013,23.933428716720645,18.97273303540371,21.839004592901738,17.66325927218184,21.098649782686852,22.108159109401438,19.70668845124008,20.687407722542194,21.23800756191085,22.54894428694912,20.440949745295235,18.136670762341843,21.301837769324877,21.032952737462605,23.31051349089806,20.969732200565332,20.351613259246466,19.678510738089678,22.059041559037812,22.10097025846529,21.127912480118983,24.425354799557958,19.405573621751774,18.954513256380405,20.62820521809647,19.21222562648202,18.9881876444261,21.62524006847279,18.78833918294514,19.827821621310783,19.4644839297056,26.34702224299619,21.38205653661138,21.78972267747008,21.087755789009996,18.999355953219133,19.451729034530373,23.289040906826244,24.16646354231811,21.536078922759724,21.530906360636845,20.97243154975957,20.089266034468004,20.436904665975636,19.20088663487939,20.554011225634962,19.277982733151024,18.304477915148002,19.118363214083377,20.261720490850706,19.26508618967162,23.063753511610845,19.05336380976314,19.240852168960235,19.807795916443087,19.527755462443025,19.369940873414674,18.557194427644223,19.092216129486825,23.457850638636746,20.3986497634222,20.79850265219969,20.611470634120458,21.03390016791011,24.957468167964976,19.132327452939787,22.668945885967275,20.192548076466373,21.926818619899166,21.236410750491928,22.00439862331522,22.64890259783158,19.876543924901863,22.174820865681593,20.001613842126076,20.63081635106876,19.653189714082398,19.305175301710232,20.224827212369068,22.6279649673046,21.547003936026666,18.961602303420257,20.695469281090816,20.114481067581714,20.790519525351012,26.143565404950515,21.151917099556776,22.601975102609295,18.386700720994313,21.48658316808204,19.99564295418165,22.704086007044513,23.646718002570474,20.17407123394833,20.29544946282625,21.94083977054476,19.087820197169854,20.49274951238928,21.58556546053215,19.868098908707033,19.806349800221792,23.600603274171462,19.846459862589864,21.099407208256782,21.035986420389108,23.747615973251953,21.078530347858226,20.86782723578085,20.073998999202523,19.385623092000827,20.214357069981805,21.756421739524402,19.573874587511387,22.17732889464112,20.092049623606254,19.621522497700703,20.13212471121583,22.84612630027242,21.610638293924836,20.381527241115613,22.69863600121815,24.241029788998915,23.65992231174119,22.296121251970213,20.702441582535556,22.897176211133427,19.850331267520566,17.677965293255863,20.87727804782974,24.045999647847655,20.173501472804098,23.33182107390064,23.30215527163116,27.231751620760654,20.650611348398325,16.829816883719186,21.108904064764797,23.735782522109897,20.656510624448828,20.096689421256738,22.83710046840733,19.824675810238606,20.823945313943256,23.256137551486724,23.633181830866906,19.043844337217745,23.65829637497728,23.50747794569296,21.73683233585413,20.94197381362215,24.18225862695574,20.14794970691058,22.344681402352435,20.447944597167705,19.21271648764287,22.508853453023672,21.75045501313216,20.060863127783303,23.041170672161062,19.554654657325607,21.26551890511897,19.47575297115503,20.304093087106327,21.96980947524255,18.890513667640615,20.207528404900692,20.825342365179193,19.58596744655146,19.86547955920327,20.093122217042076,19.51138012046539,20.89052275919248,19.726434174485984,19.26180569731426,21.223090796665396,20.071086524480997,23.60048678836413,21.682830461889793,22.115896159241377,23.245004397316144,19.929597223002617,20.206849822052806,20.80607598372765,20.972697281096956,20.548932178877266,20.790175259909486,21.65577788956451,20.396533708238668,17.677595133772794,20.206889160267256,19.864164508631692,20.604884938737765,19.69979302428942,21.112079923463828,21.776216944868956,19.41019874513054,20.084542414131516,25.538294718669484,19.887154874828134,22.02600227823102,20.06242269100319,19.908561207609715,19.025787914622292,22.519912388376707,21.15310774495207,15.434839951572775,23.416469489833915,21.472713147192902,24.799241151178187,21.565777300167408,21.24511744232927,23.786142675104543,21.775846548503456,20.874095085342866,20.525903087310763,22.190296420947053,17.526091919071504,22.787613064091445,19.95220428650718,21.56191951932416,20.302727045932432,22.39076356657128,17.788361488629835,19.89495594402764,20.849863964556313,22.01047671458767,21.9419703223203,21.769327477492673,22.134078601621123,23.02213648046312,21.659326126254854,22.503975157123477,21.687400780796548,21.299913824117656,24.41218042654058,21.628416169590647,20.661666774415252,19.13829898996108,20.6534474504661,20.599627769624064,19.476347089125902,22.633338504709375,19.004867830543457,20.729309561171004,21.215021611787833,20.55962915472571,23.96741801447904,20.948249582299137,19.034944006994706,21.25562508835354,23.6936808146177,18.876298103633747,20.12057913956066,22.988276940898057,20.761017121909465,29.702932107840674,19.34717798381203,19.04780522125321,19.72022056098374,25.87586915107988,22.381251164844315,21.45471998887307,21.894206057556605,20.997149152437558,19.346653316091935,19.69023397394279,21.75892392666085,21.695257182632144,24.304938166770185,19.96312680068985,21.08058139373211,18.457821713581545,21.058645462338415,20.370456476417175,22.39756222611425,21.96355600303241,24.29338064398619,19.533504643608662,19.24597971762759,21.061479969798718,19.653131971320136,19.991828535916927,19.41354451956652,19.024392506006556,19.995956185326282,21.080207924079772,22.895336936087247,19.00379513256332,19.59081933226322,19.683155687698772,19.724580465382388,20.49815774428782,21.41646356230674,20.374729613814893,19.698149844649336,20.827570650601533,21.55980522344904,20.203106297950406,20.998113148879906,22.5464416348448,21.94562707940607,20.109117749689304,19.61600010557632,22.28815948936714,20.00070564420032,19.48773603183014,22.96164822427184,18.99960834234589,20.932965503614184,18.548961097016324,21.40256127224899,23.343368321662904,21.395751936133603,20.367816242708102,21.629837063073623,22.815206783859768,22.704911104222457,20.7857551664019,20.3067025549342,20.61336030755643,21.781868242254394,20.56504163001677,24.58750691743097,21.042555130756018,19.718574682047347,20.82900495573487,21.89094777167843,20.46100849412487,20.676958661968467,22.376303078727783,21.96712491711981,21.09072172933059,20.373076483151994,19.53720891947194,19.37233862557095,0,22.656078929958284,19.971799365414682,21.138173662648782,25.881734625061707,0,22.042981377352834,19.12600868541295,20.929534739770034,20.604603448464516,21.393924057871644,18.99682101244613,19.31053914628864,18.986258705756388,16.7233596191811,19.687016476370815,20.364732076794134,18.828527980887348,21.061860954738233,18.824250825082274,19.28149315085171,21.023652845270533,23.394529944648035,22.371116977331784,21.19573925103901,18.93658618322981,19.94401710262461,19.47904998638186,22.850840758637354,20.76699650557159,20.40036143331872,17.795936296212627,19.421904563712328,20.64673096490357,19.639706353282477,18.775317705171794,20.047574770414855,19.61260734468754,23.666636204423178,23.480370137373004,19.218266855752937,21.60748880425466,21.257489419039537,21.101232900314493,21.320633929890864,21.849984955018474,20.40434121243742,20.032547562218568,20.035932572983462,20.864093173595,21.51301334156099,22.29182284846925,23.44987910374218,20.855425894539348,22.64951341150128,23.894186440194584,20.26686372980413,20.408744363117133,19.046718202293015,20.904214174162476,24.308788888311135,20.77592509493985,0,19.644171392658865,18.610921424553307,22.26726247442381,20.903586582709064,21.2082248307893,20.867431932395597,19.247883485213144,19.09858987177298,20.769205079804532,21.73660672493293,20.24751150890443,21.281868210231526,19.724170230709305,20.553728301262954,21.112413416140527,20.46206885900041,20.668149201097204],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN_log = np.logspace(-1, 3, 200)\n","l1ratioEN_log = np.linspace(0, 1, 6)\n","param_gridEN_log = {\n","    'elasticnet__alpha': alphasEN_log,\n","    'elasticnet__l1_ratio': l1ratioEN_log\n","}\n","\n","GridEN_log, \\\n","BestParametresEN_log, \\\n","ScoresEN_log, \\\n","SiteEnergyUse_pred_logEN, \\\n","figEN_log = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log,\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_logEN',\n","                         score=score,\n","                         param_grid=param_gridEN_log)\n","\n","print(BestParametresEN_log)\n","print(ScoresEN_log)\n","figEN_log.show()\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"y":[1.7008108033993288,1.700733374923295,1.7006561011997987,1.7005792196990182,1.7005029923364354,1.7004277071146714,1.7003536798210868,1.700281255778449,1.7002108116454218,1.7001427572630483,1.700077537542788,1.7000156343909965,1.6999575686640278,1.699903902147403,1.6998552395516893,1.6998122305169172,1.699775571616513,1.6997460083508273,1.6997243371194473,1.6997114071605517,1.6997081224446458,1.6997154435090578,1.6997343892186811,1.6997660384374853,1.6998115315944518,1.6998720721266694,1.6999489277814774,1.7000434317586781,1.7001569836730255,1.7002910503163526,1.7004471661979021,1.7006269338405815,1.7008320238100219,1.70106417445244,1.7013251913163774,1.7016169462323973,1.7019413760237847,1.702300480820173,1.7026963219448497,1.7031310193452893,1.7036067485352249,1.7041257370154206,1.7046902601391962,1.7053026363879205,1.7059652220210775,1.706680405065395,1.7074505986079476,1.7082782333593491,1.7091657494552428,1.7101155874675715,1.7111301786016209,1.7122119340608848,1.7133632335694813,1.7145864130513295,1.7158837514766474,1.7172574568995984,1.71870965172609,1.7202423572676673,1.7218574776559976,1.723556783212333,1.7253418933871465,1.7272142594064945,1.7291751467829084,1.7312256178692345,1.7333665146530997,1.7355984420068484,1.7379217516222034,1.740336526869766,1.74284256883016,1.7454393837455418,1.748126172136828,1.750901819823032,1.753764891064294,1.7567136240296009,1.7597459287639143,1.762859387797929,1.7660512595074445,1.7693184842891818,1.7726576935766638,1.7760652216745576,1.7795371203438048,1.7830691760240687,1.7866569295357646,1.7902956980623106,1.7939805991753595,1.7977065766326075,1.8014684276501,1.8052608313294833,1.8090783779057005,1.8129155984725913,1.8167669948425782,1.8206270692020947,1.824490353236125,1.828351436412791,1.8322049931415347,1.8360458085454596,1.8398688026188765,1.8436690525741983,1.8474418132171349,1.851182535224774,1.85488688123678,1.8585507397047714,1.8621702364783648,1.8657417441377233,1.869261889111285,1.8727275566432962,1.8761358936985577,1.879484309911274,1.8827704767009692,1.885992324691171,1.8891480395760287,1.8922360565863883,1.8952550537103288,1.898203943823987,1.9010818658869924,1.9038881753532244,1.9066224339422697,1.9092843989101234,1.9118740119497173,1.914391387842945,1.9168368029763667,1.9192106838228202,1.9215135954810836,1.9237462303555772,1.9259093970481487,1.928004009524279,1.9300310766067619,1.9319916918410924,1.9338870237685335,1.9357183066351447,1.9374868315580205,1.939193938163529,1.940841006706589,1.9424294506748445,1.9439607098770408,1.945436244010954,1.9468575267027994,1.948226040007139,1.9495432693539279,1.9508106989273233,1.9520298074593572,1.953202064420347,1.9543289265870563,1.9554118349690444,1.9564522120733034,1.957451459487197,1.9584109557597997,1.9593320545619846,1.9602160831060118,1.9610643408058668,1.9618780981602009,1.9626585958403908,1.9634070439669684,1.964124621558429,1.9648124761372219,1.9654717234785495,1.9661034474883767,1.9667087001979198,1.967288501862625,1.9678438411544827,1.9683756754372506,1.9688849311149212,1.9693725040444747,1.9698392600046326,1.9702860352130027,1.9707136368845926,1.9711228438252857,1.9715144070544113,1.9718890504510633,1.9722474714193177,1.972590341567945,1.972918307400657,1.9732319910133036,1.973531990794828,1.9738188821291054,1.97409321809513,1.9743555301632867,1.974606328885723,1.974846104579079,1.9750753279980622,1.9752944509985462,1.975503907189085,1.9757041125698762,1.9758954661583872,1.9760783506009791,1.976253132770004,1.9764201643459565,1.976579782384368,1.9767323098672223,1.9768780562387511,1.977017317925549,1.9771503788409972,1.9772775108740646,1.977398974362579,1.9775150185511308,1.9776258820337858,1.9777317931818412,1.9778329705568616,1.9779296233092776,1.9780219515628368]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"y":[1.7809639522446488,1.7805830908152342,1.780191711645284,1.7797898732433237,1.7793776711913647,1.7789552413172698,1.7785227630020441,1.778080462617835,1.777628617090965,1.7771675575826773,1.7766976732784765,1.7762194152749617,1.7757333005508813,1.7752399160067949,1.7747399225551694,1.7742340592400188,1.7737231473622648,1.7732080945839117,1.7726898989808528,1.7721696530107238,1.7716485473586838,1.7711278746203443,1.7706090327773996,1.7700935284177477,1.7695829796482356,1.7690791186445163,1.768583793779077,1.7680989712652584,1.7676267362522076,1.7671692933032102,1.7667289661879533,1.7663081969179972,1.7659095439543127,1.7655356795162844,1.765189385923301,1.764873550903069,1.7645911618053867,1.764345298666394,1.7641391260765242,1.7639758838156647,1.7638588762314975,1.7637914603517881,1.7637770327383977,1.7638190151101103,1.7639208387825844,1.7640859279967294,1.7643176822309241,1.7646194576172551,1.7649945476064404,1.7654461630495877,1.7659774118862461,1.7665912786464633,1.767290603988654,1.7680780645041576,1.7689561530226494,1.7699271596496193,1.7709931537577839,1.7721559671387492,1.773417178500098,1.7747780994672988,1.776239762220639,1.7778029088663394,1.7794679826095827,1.7812351207670603,1.7831041496291657,1.7850745811583124,1.78714561149085,1.7893161211960944,1.7915846772370372,1.793949536572996,1.7964086513440127,1.7989596755792536,1.801599973375834,1.8043266284992527,1.8071364553607678,1.8100260113297262,1.8129916103391643,1.816029337740561,1.8191350663580805,1.8223044736841287,1.8255330601467774,1.8288161683660793,1.8321490033012067,1.8355266531743626,1.8389441110414322,1.8423962968641359,1.84587807992477,1.8493843014132216,1.8529097970072963,1.8564494192620697,1.8599980596220889,1.863550669872128,1.8671022828476382,1.8706480322350694,1.8741831713044073,1.8777030904313767,1.8812033332842064,1.884679611569208,1.8881278182501167,1.8915440391776226,1.8949245630872646,1.8982658899453215,1.9015647376430864,1.9048180470595397,1.908022985530552,1.9111769487791515,1.9142775613758196,1.9173226758101203,1.9203103702651718,1.9232389451944925,1.9261069188066822,1.9289130215672972,1.9316561898292872,1.9343355587036322,1.9369504542805547,1.9395003853090236,1.941985034438463,1.9444042491217899,1.9467580322733546,1.9490465327691673,1.9512700358702313,1.9534289536428944,1.955523815443152,1.9575552585247717,1.9595240188241907,1.961430921968351,1.9632768745451208,1.9650628556697276,1.9667899088747622,1.9684591343458147,1.9700716815197346,1.9716287420578058,1.973131543201908,1.9745813415178746,1.9759794170268312,1.9773270677222827,1.9786256044680415,1.9798763462697946,1.9810806159111773,1.9822397359435338,1.983355025017237,1.984427794541335,1.9854593456574587,1.9864509665133088,1.9874039298206256,1.9883194906822987,1.9891988846731903,1.9900433261592894,1.990854006839994,1.9916320944985688,1.9923787319461959,1.9930950361454394,1.99378209749944,1.9944409792936664,1.9950727172776128,1.9956783193744245,1.996258765506996,1.99681500752975,1.9973479692558516,1.9978585465702714,1.9983476076196693,1.9988159930706855,1.9992645164287766,1.999693964410294,2.000105097361047,2.0004986497150754,2.000875330487884,2.0012358237988206,2.0015807894177473,2.0019108633315623,2.0022266583265154,2.0025287645826557,2.002817750277064,2.0030941621928773,2.0033585263313904,2.0036113485248257,2.0038531150475976,2.0040842932241567,2.004305332031723,2.0045166626964086,2.0047186992814297,2.0049118392662884,2.0050964641159372,2.0052729398391187,2.005441617535172,2.0056028339287417,2.005756911891915,2.00590416095343,2.0060448777946625,2.0061793467322016,2.0063078401868815,2.006430619139196,2.0065479335711007,2.0066600228942315,2.0067671163646246,2.0068694334840655,2.0069671843882255,2.007060570221755,2.00714978350057,2.007235008461536]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"y":[1.6206576545540088,1.6208836590313556,1.6211204907543135,1.6213685661547126,1.6216283134815062,1.621900172912073,1.6221845966401294,1.622482048939063,1.6227930061998785,1.6231179569434193,1.6234574018070993,1.6238118535070312,1.6241818367771743,1.624567888288011,1.624970556548209,1.6253904017938157,1.6258279958707613,1.626283922117743,1.6267587752580417,1.6272531613103796,1.6277676975306077,1.6283030123977713,1.6288597456599627,1.629438548457223,1.6300400835406679,1.6306650256088224,1.631314061783878,1.6319878922520978,1.6326872310938434,1.6334128073294951,1.634165366207851,1.6349456707631658,1.635754503665731,1.6365926693885955,1.6374609967094538,1.6383603415617256,1.6392915902421827,1.640255662973952,1.641253517813175,1.642286154874914,1.6433546208389522,1.644460013679053,1.6456034875399947,1.6467862576657306,1.6480096052595705,1.6492748821340606,1.650583514984971,1.6519370091014431,1.6533369513040452,1.6547850118855554,1.6562829453169956,1.6578325894753063,1.6594358631503086,1.6610947615985014,1.6628113499306454,1.6645877541495775,1.6664261496943962,1.6683287473965853,1.6702977768118972,1.6723354669573673,1.674444024553654,1.6766256099466497,1.678882310956234,1.6812161149714087,1.6836288796770336,1.6861223028553844,1.688697891753557,1.6913569325434374,1.694100460423283,1.6969292309180875,1.6998436929296432,1.7028439640668103,1.705929808752754,1.709100619559949,1.7123554021670608,1.715692764266132,1.7191109086757246,1.7226076308378027,1.726180320795247,1.7298259696649865,1.7335411805408323,1.737322183682058,1.7411648557703225,1.7450647429502586,1.7490170873092867,1.753016856401079,1.75705877537543,1.761137361245745,1.7652469588041046,1.7693817776831129,1.7735359300630675,1.7777034685320614,1.7818784236246117,1.7860548405905128,1.7902268149786622,1.7943885266595425,1.7985342719535466,1.8026584935791887,1.806755808184153,1.8108210312719253,1.8148491993862954,1.8188355894642212,1.8227757353136431,1.826665441215907,1.8305007926920183,1.8342781645074409,1.8379942260212958,1.8416459440124275,1.8452305831367666,1.8487457041878494,1.8521891603453753,1.8555590916054794,1.8588539175913703,1.862072328944342,1.86521327749343,1.8682759653974252,1.8712598334460764,1.8741645486984568,1.87698999162608,1.8797362429167228,1.8824035700825021,1.884992414002746,1.8875033755190151,1.8899372021863827,1.8922947752721067,1.894577097080207,1.896785278668403,1.8989205280124573,1.9009841386623048,1.9029774789244747,1.9049019815963064,1.9067591342692523,1.9085504702112701,1.9102775598318145,1.9119420027272505,1.9135454202996254,1.9150894489375574,1.9165757337444833,1.9180059227966784,1.9193816619111128,1.9207045899014774,1.9219763342993588,1.9231985075166538,1.92437270342478,1.9255004943259812,1.9265834282920953,1.9276230268464092,1.9286207829646798,1.9295781593720296,1.9304965871131647,1.9313774643742059,1.9322221555353423,1.9330319904344968,1.9338082638231915,1.934552234996831,1.9352651275826744,1.9359481294697574,1.9366023928660896,1.9372290344693983,1.937829135738694,1.9384037432548318,1.938953869159157,1.939480491660173,1.9399845555989712,1.9404669730649586,1.9409286240541097,1.9413703571626872,1.941792990310002,1.9421973114843791,1.9425840795070732,1.9429540248093748,1.9433078502186585,1.943646231749543,1.943969819396779,1.9442792379268206,1.9445750876654342,1.944857945278976,1.9451283645472892,1.9453868771264349,1.9456339932997158,1.9458702027156625,1.9460959751118816,1.9463117610238152,1.9465179924776557,1.946715083666786,1.946903431611266,1.9470834167999977,1.947255403815306,1.9474197419397823,1.9475767657453007,1.9477267956642166,1.9478701385427988,1.9480070881770284,1.9481379258309266,1.9482629207376372,1.948382330583506,1.948496401975457,1.948605370891968,1.9487094631179855,1.9488088946641375]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.8262121232172763,1.825336047924797,1.8244307666649076,1.8234957999114008,1.8225307017077172,1.821535063738673,1.8205085196040536,1.819450749291731,1.8183614838464723,1.8172405102289828,1.8160876763579639,1.8149028963259928,1.8136861557779602,1.8124375174385305,1.8111571267726694,1.8098452177607462,1.8085021187669965,1.8071282584773518,1.805724171879712,1.804290506256774,1.802828027158514,1.8013376243183914,1.7998203174743617,1.7982772620528702,1.7967097546712028,1.7951192384109271,1.793507307812735,1.791875713540801,1.7902263666628695,1.7885613424906734,1.7868828839240378,1.7851934042410909,1.7834954892764598,1.781791898929102,1.7800855679415686,1.7783796058929389,1.776677296348404,1.7749820951094961,1.7732976275101926,1.7716276847055914,1.7699762189015005,1.7683473374751526,1.7667452959393353,1.7651744897045767,1.7636394445967387,1.7621448060905311,1.7606953272232337,1.7592958551574711,1.7579513163674325,1.7566667004297205,1.755447042408268,1.7542974038327608,1.7532228522819695,1.7522284395975645,1.7513191787704856,1.7505000195609184,1.74977582293434,1.749151334419895,1.7486311565232802,1.7482197203540315,1.7479212566560929,1.74773976646019,1.7476789916060174,1.7477423854107188,1.7479330837865674,1.748253877134111,1.748707183356216,1.7492950223523995,1.7500189923605505,1.7508802485137533,1.7518794839727316,1.753016913978899,1.754292263148947,1.7557047562993,1.757253113047981,1.7589355463931153,1.7607497654123296,1.762692982166932,1.7647619228303792,1.7669528429937669,1.7692615470336357,1.7716834113610889,1.774213411307772,1.776846151345426,1.7795758982829866,1.7823966170399135,1.785302008557676,1.7882855493839098,1.7913405324461245,1.7944601085242609,1.7976373279336058,1.8008651819412687,1.804136643459836,1.8074447065901018,1.8107824246197946,1.8141429461258523,1.8175195488726457,1.8209056712464524,1.8242949410160074,1.827681201258965,1.8310585333434426,1.8344212769014188,1.83776404677582,1.8410817469648408,1.8443695816249261,1.8476230632273867,1.8508380179926511,1.854010588750486,1.857137235394191,1.860214733111888,1.8632401685887867,1.8662109343810045,1.8691247216644986,1.87197951156228,1.874773565249772,1.8775054130323183,1.880173842580893,1.882777886502382,1.8853168094098058,1.8877900946458708,1.8901974308006075,1.8925386981508716,1.8948139551363827,1.8970234249740276,1.8991674824994769,1.9012466413129723,1.9032615412945237,1.9052129365428034,1.9071016837818264,1.9089287312700942,1.9106951082382546,1.912401914873557,1.914050312862372,1.915641516495869,1.917176784338472,1.9186574114540156,1.9200847221804531,1.9214600634405588,1.9227847985732303,1.9240603016676838,1.925287952381016,1.9264691312182187,1.927605215252721,1.9286975742648795,1.9297475672754738,1.9307565394511488,1.9317258193588849,1.9326567165468613,1.9335505194295588,1.9344084934555283,1.9352318795369612,1.9360218927209618,1.9367797210832742,1.937506524826096,1.938203435562519,1.9388715557710787,1.9395119584048133,1.9401256866401775,1.940713753752059,1.9412771431020517,1.9418168082280098,1.9423336730237497,1.9428286319985775,1.943302550607106,1.9437562656405518,1.9441905856714243,1.9446062915441678,1.9450041369049693,1.9453848487645107,1.9457491280880248,1.9460976504075114,1.946431066451478,1.9467500027879934,1.9470550624772924,1.9473468257305304,1.9476258505716622,1.9478926734997484,1.948147810149292,1.9483917559464918,1.94862498675956,1.9488479595414743,1.9490611129637667,1.9492648680401303,1.9494596287388148,1.9496457825829367,1.9498237012379769,1.9499937410858736,1.9501562437852358,1.9503115368173052,1.9504599340173965,1.9506017360916286,1.950737231118834,1.950866695037605,1.9509903921184917,1.951108575421419,1.9512214872384421,1.95132935952199,1.9514324142987922,1.9515308640697047,1.9516249121956812],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.7130833186277468,1.7131866784637222,1.7132965772505455,1.7134134383600153,1.713537711195903,1.7136698725041837,1.7138104277104615,1.7139599122800968,1.7141188930957902,1.7142879698466371,1.7144677764218537,1.7146589823015455,1.7148622939360365,1.7150784561044068,1.715308253242001,1.7155525107258074,1.715812096105747,1.716087920269087,1.7163809385244035,1.7166921515908113,1.7170226064775025,1.7173733972380736,1.7177456655836605,1.7181406013385034,1.7185594427213533,1.7190034764359712,1.71947403755399,1.7199725091735327,1.7205003218372255,1.7210589526936106,1.72164992438644,1.7222748036568787,1.7229351996442825,1.7236327618719005,1.72436917790455,1.7251461706660407,1.7259654954048174,1.7268289362969669,1.7277383026763493,1.7286954248821977,1.7297021497150398,1.730760335492291,1.7318718466953422,1.7330385482004775,1.7342622990865455,1.735544946013074,1.7368883161635051,1.7382942097495675,1.739764392074603,1.7413005851560128,1.7429044589100466,1.7445776219060098,1.7463216117017262,1.7481378847778655,1.7500278060955834,1.7519926383098405,1.754033530679804,1.7561515077277814,1.75834745770912,1.7606221209672401,1.7629760782602286,1.7654097391579429,1.7679233306209805,1.7705168858847953,1.773190233783269,1.775942988655687,1.7787745409888736,1.7816840489517884,1.784670430982653,1.7877323595883876,1.7908682565123835,1.7940762894192332,1.7973543702338228,1.800700155257181,1.8041110471627202,1.8075841989543109,1.8111165199422676,1.8147046837653777,1.818345138457069,1.8220341185224371,1.825767658960863,1.8295416111371274,1.8333516603731297,1.837193345103268,1.8410620774100774,1.8449531647334623,1.8488618325274324,1.8527832476231374,1.8567125420464976,1.8606448370331314,1.8645752669825393,1.868499003097675,1.8724112764647525,1.8763074003412143,1.8801827914366471,1.8840329899916555,1.8878536784826125,1.8916406988052308,1.8953900678163247,1.8990979911403643,1.9027608751747862,1.9063753372549654,1.9099382139656964,1.9134465676105785,1.9168976908733726,1.9202891097259474,1.923618584655612,1.926884110300253,1.9300839135927268,1.9332164505263527,1.9362804016611683,1.939274666495975,1.942198356834216,1.9450507892726179,1.9478314769404936,1.9505401206148278,1.953176599332055,1.9557409606119622,1.958233410402687,1.960654302848515,1.963004129974377,1.9652835113727283,1.9674931839701117,1.9696339919422476,1.9717068768381572,1.9737128679656752,1.9756530730828776,1.977528669432482,1.9793408951492735,1.9810910410640632,1.982780442921677,1.9844104740249606,1.9859825383118468,1.9874980638680761,1.988958496874258,1.9903652959825333,1.9917199271151476,1.9930238586747406,1.9942785571540789,1.9954854831312347,1.9966460876348773,1.9977618088632958,1.9988340692400228,1.9998642727884377,2.000853802807444,2.0018040198302516,2.0027162598483823,2.0035918327832416,2.0044320211879856,2.005238079162836,2.0060112314675687,2.006752672815474,2.0074635673337737,2.008145048176136,2.008798217273667,2.0094241452114816,2.0100238712186616,2.010598403260192,2.0111487182201238,2.0116757621659747,2.012180450685021,2.0126636692838407,2.013126273843074,2.013569091120011,2.0139929192921824,2.0143985285356996,2.014786661632618,2.0151580346020905,2.0155133373505523,2.0158532343366207,2.0161783652467964,2.0164893456784543,2.016786767826952,2.0170712011740255,2.0173431931749484,2.0176032699422186,2.0178519369237877,2.0180896795741012,2.018316964016432,2.018534237695197,2.0187419300171223,2.0189404529803046,2.0191302017903574,2.0193115554629752,2.019484877412375,2.019650516025188,2.019808805219468,2.0199600649885956,2.020104601929908,2.02024270975799,2.0203746698025995,2.020500751491278,2.0206212128167347,2.0207363007891432,2.020846251873522,2.020951292412415,2.021051639034104,2.0211474990466125,2.0212390708177983,2.0213265441418082],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.7298586590025582,1.7300159600513036,1.7301813419777627,1.7303552174567611,1.7305380193966937,1.7307302017404378,1.7309322402704683,1.7311446334147123,1.7313679030492959,1.7316025952939167,1.731849281295134,1.7321085579924433,1.7323810488615088,1.7326674046284802,1.7329683039488333,1.7332844540436996,1.7336165912861783,1.7339654817296697,1.7343319215698276,1.7347167375313126,1.7351207871701546,1.735544959082186,1.735990173007716,1.7364573798223777,1.7369475614038887,1.7374617303643547,1.7380009296376713,1.7385662319115955,1.739158738894109,1.739779580403833,1.740429913274405,1.7411109200629589,1.7418238075530867,1.742569805042929,1.7433501624093186,1.744166147939172,1.745019045919601,1.7459101539784425,1.7468407801671446,1.7478122397781521,1.7488258518891207,1.7498829356265144,1.7509848061413649,1.7521327702902998,1.7533281220153722,1.7545721374168428,1.755866069513914,1.7572111426895984,1.758608546817465,1.7600594310700777,1.7615648974115472,1.763125993779894,1.7647437069688852,1.7664189552237521,1.768152580570723,1.7699453409066348,1.771797901881994,1.7737108286186671,1.775684577311803,1.7777194867744912,1.7798157699928205,1.7819735057682546,1.7841926305332656,1.7864729304347204,1.7888140337872402,1.7912154040053585,1.793676333128425,1.7961959360555317,1.7987731456089802,1.8014067085436827,1.8040951826162095,1.8068369348207993,1.8096301408904634,1.81247278614936,1.8153626677880053,1.8182973986157718,1.8212744123258493,1.824290970286729,1.8273441698517647,1.8304309541550259,1.8335481233379476,1.8366923471278809,1.8398601786670696,1.8430480694694455,1.8462523853634651,1.849469423262522,1.8526954285906418,1.8559266131805967,1.8591591734544335,1.8623893086929102,1.865613239200448,1.8688272241759016,1.8720275791065517,1.8752106925129777,1.8783730418855258,1.88151120866858,1.8846218921663165,1.8877019222625882,1.89074827086759,1.8937580620244878,1.8967285806298027,1.8996572797415814,1.90254178646888,1.9053799064544525,1.9081696269795307,1.910909118734933,1.913596736316296,1.916231017512856,1.9188106814688766,1.9213346258044943,1.9238019227885044,1.9262118146594667,1.9285637081936522,1.9308571686188414,1.933091912972048,1.9352678029969963,1.9373848376738343,1.9394431454692846,1.9414429763904044,1.9433846939194979,1.945268766901688,1.9470957614503237,1.9488663329289277,1.9505812180619027,1.952241227219779,1.953847236918551,1.9554001825666152,1.9569010514871075,1.9583508762380626,1.9597507282477955,1.961101711778307,1.9624049582253047,1.9636616207596291,1.9648728693114974,1.9660398858959778,1.9671638602755017,1.9682459859529744,1.969287456487141,1.9702894621202836,1.9712531867070349,1.9721798049320745,1.9730704798036978,1.9739263604096988,1.9747485799216353,1.9755382538333721,1.976296478419748,1.9770243294013081,1.977722860801251,1.9783931039810205,1.9790360668413625,1.979652733176079,1.980244062166215,1.9808109880029152,1.9813544196277455,1.981875240579832,1.9823743089397463,1.9828524573606399,1.9833104931777115,1.9837491985876448,1.984169330890224,1.9845716227848622,1.9849567827153078,1.9853254952562924,1.9856784215363654,1.9860161996916283,1.9863394453455032,1.9866487521100988,1.9869446921051166,1.9872278164906145,1.987498656010282,1.9877577215422098,1.988005504654432,1.9882424781628016,1.9884690966890142,1.9886857972168435,1.9888929996448645,1.9890911073341466,1.989280507649593,1.9894615724937663,1.9896346588322011,1.9898001092093494,1.9899582522544357,1.9901094031766104,1.990253864248913,1.990391925280635,1.9905238640777785,1.9906499468913705,1.9907704288534749,1.9908855544008017,1.9909955576858678,1.991100662975721,1.9912010850382738,1.9912970295163401,1.9913886932894975,1.9914762648239275,1.9915599245104136,1.9916398449906982,1.991716191472414,1.99178912203283,1.9918587879116527],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.6448788819078635,1.6450162926426923,1.6451610999338049,1.6453136855812918,1.645474451371802,1.6456438201259085,1.6458222367842346,1.646010169531387,1.6462081109564395,1.64641657924841,1.6466361194248476,1.6468673045912916,1.647110737229004,1.6473670505080016,1.6476369096220118,1.6479210131415711,1.6482200943810712,1.6485349227751187,1.6488663052591381,1.649215087648684,1.6495821560114599,1.6499684380255444,1.6503749043167875,1.6508025697677864,1.651252494790215,1.6517257865515964,1.6522236001468218,1.6527471397038207,1.653297659411769,1.6538764644590038,1.6544849118664526,1.655124411200758,1.6557964251494366,1.6565024699382942,1.6572441155689088,1.658022985851308,1.6588407582039673,1.6596991631900146,1.6605999837550074,1.661545054127976,1.6625362583436087,1.6635755283396432,1.6646648415798102,1.6658062181492252,1.6670017172660871,1.6682534331511705,1.6695634901950431,1.6709340373624992,1.672367241774596,1.6738652814111465,1.6754303368808494,1.6770645822125863,1.6787701746300576,1.6805492432829325,1.6824038769212266,1.6843361105156738,1.6863479108453778,1.6884411610948984,1.6906176445258432,1.692879027312735,1.695226840658829,1.6976624623342356,1.7001870978054026,1.7028017611510238,1.7055072559840034,1.7083041566212893,1.711192789762436,1.7141732169527513,1.7172452181170828,1.7204082764550117,1.7236615649868317,1.727003935031825,1.7304339068857342,1.733949662942936,1.7375490434808265,1.7412295452897444,1.74498832329197,1.7488221952488165,1.7527276496065283,1.7567008564808342,1.7607376817277745,1.7648337039962312,1.768984234606737,1.7731843400529805,1.7774288668782015,1.781712468639475,1.7860296346397095,1.7903747200807774,1.7947419772720885,1.799125587517446,1.8035196932992545,1.8079184303829445,1.8123159594754379,1.8167064970891345,1.821084345286417,1.8254439200083545,1.829779777724049,1.8340866401730627,1.8383594170114885,1.8425932262115294,1.8467834121039781,1.85092556099188,1.8550155143010856,1.8590493792687337,1.863023537203362,1.8669346493798544,1.870779660658534,1.8745558009401495,1.8782605845872073,1.8818918079570621,1.885447545203506,1.88892614251143,1.892326210933719,1.8956466180011387,1.898886478274883,1.9020451430080243,1.9051221890766221,1.9081174073341158,1.9110307905340946,1.913862520956968,1.9166129578657345,1.9192826249052002,1.9218721975479196,1.9243824906789726,1.9268144464007044,1.9291691221278056,1.9314476790328206,1.933651370892358,1.9357815333750725,1.9378395738039154,1.9398269614172585,1.9417452181463182,1.94359590991979,1.945380638500829,1.9471010338563737,1.9487587470543402,1.9503554436803578,1.9518927977624252,1.9533724861891393,1.954796183604892,1.9561655577636448,1.9574822653215027,1.9587479480473047,1.959964229429753,1.961132711659211,1.9622549729621541,1.9633325652663292,1.964367012174927,1.9653598072285017,1.9663124124338915,1.9672262570400605,1.9681027365414963,1.9689432118905976,1.9697490089013294,1.970521417827283,1.9712616930981797,1.97197105319975,1.9726506806828141,1.9733017222882754,1.973925289175612,1.97452245724329,1.9750942675303513,1.9756417266892108,1.976165807520461,1.9766674495612055,1.9771475597191275,1.9776070129451544,1.978046652938196,1.9784672928760092,1.9788697161667932,1.9792546772166186,1.9796229022082774,1.9799750898875808,1.9803119123535367,1.9806340158492288,1.9809420215505646,1.981236526350383,1.981518103635714,1.9817873040562604,1.9820446562824072,1.9822906677513157,1.9825258253998486,1.9827505963832743,1.9829654287788687,1.9831707522736866,1.983366978835919,1.9835545033693796,1.983733704350775,1.9839049444495245,1.9840685711299684,1.9842249172359119,1.9843743015574962,1.9845170293804733,1.9846533930180046,1.984783672325157,1.9849081351963125,1.9850270380457415,1.9851406262716282,1.9852491347038501,1.9853527880358512],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.10473708979594494,0.10969857978923835,0.11489510001873092,0.12033778407775896,0.12603829296797275,0.1320088400831418,0.13826221737646557,0.14481182276745339,0.15167168884709228,0.15885651294280528,0.1663816886076129,0.174263338600965,0.18251834943190434,0.19116440753857022,0.20022003718155845,0.2097046401323233,0.21963853724165458,0.2300430119772918,0.2409403560239525,0.2523539170434766,0.26430814869741054,0.2768286630392066,0.28994228538828765,0.30367711180354584,0.3180625692794119,0.3331294787934673,0.3489101213406773,0.3654383070957256,0.38274944785163123,0.40088063288984654,0.419870708444391,0.439760360930272,0.4605922041145106,0.482410870416537,0.5052631065335681,0.5291978735958442,0.5542664520663105,0.5805225516094898,0.6080224261649424,0.6368249944718588,0.6669919663030122,0.6985879746785247,0.7316807143427196,0.7663410868007459,0.8026433522257176,0.8406652885618325,0.8804883581643462,0.9221978823334327,0.9658832241158704,1.011637979766207,1.059560179277616,1.1097524964120722,1.1623224686798523,1.2173827277396614,1.2750512407130135,1.3354515629298989,1.3987131026472386,1.4649713983072856,1.5343684089300125,1.6070528182616393,1.6831803533309566,1.762914118095948,1.8464249428955435,1.933891750455231,2.0255019392306677,2.12145178491063,2.2219468609395236,2.3272024789604084,2.4374441501222206,2.552908068239518,2.6738416158399465,2.800503894183631,2.933166278390044,3.0721129988617575,3.217641750250737,3.370064329271928,3.52970730273065,3.6969127071950285,3.872038781812555,4.05546073584083,4.247571552536899,4.448782831127585,4.659525668664682,4.880251583654431,5.111433483440168,5.353566677410725,5.607169938205458,5.872786613189483,6.150985788580501,6.442363508721374,6.747544053110693,7.067181273927491,7.401959996915645,7.75259748862946,8.119844993184014,8.504489341802678,8.907354638610439,9.329304026284687,9.771241535346496,10.234114021054527,10.718913192051275,11.226677735108137,11.758495540521569,12.315506032928262,12.89890261253308,13.509935211980265,14.149912974345758,14.820207057988586,15.52225357427048,16.25755666443795,17.027691722258997,17.834308769319094,18.67913599020783,19.56398343517065,20.49074689815848,21.461411978584035,22.47805833548725,23.54286414322418,24.658110758226037,25.826187606826775,27.049597304631344,28.330961018393243,29.673024081888695,31.07866187782014,32.5508859983506,34.09285069746811,35.707859649004625,37.39937302478798,39.17101490809261,41.026581058271944,42.970047043208396,45.005576757004974,47.13753134116724,49.37047852839004,51.70920242896761,54.158713780794706,56.72426068491978,59.41133984965034,62.22570836730231,65.17339604882427,68.26071834272386,71.49428986597577,74.88103857590023,78.42822061337682,82.1434358491943,86.034644166845,90.11018251665018,94.37878277775381,98.84959046625586,103.53218432956626,108.43659686896109,113.57333583431051,118.95340673703195,124.58833642950081,130.4901978014403,136.67163564620074,143.14589375234786,149.92684327860457,157.02901247293775,164.46761779946644,172.25859653987874,180.41864093920717,188.96523396912096,197.91668678535575,207.29217795953718,217.11179456945052,227.39657523579274,238.16855519761583,249.45081352303166,261.2675225563329,273.6439997074672,286.606761694825,300.1835813575589,314.40354715915,329.29712550971516,344.896226040576,361.23426997094305,378.3462617131929,396.2688638701478,415.04047578504765,434.7013158125026,455.29350748669475,476.86116977144695,499.450511585514,523.1099308056264,547.8901179593945,573.8441648302393,601.0276782070382,629.4988990221888,659.3188271333548,690.5513520162331,723.2633896483534,757.5250258771913,793.4096665797492,830.9941949353396,870.3591361485165,911.5888299750828,954.7716114208056,1000],"xaxis":"x","y":[1.5900210342411987,1.590111895533959,1.5902107201719744,1.590317957185622,1.5904340780100603,1.590559577464154,1.590694974736216,1.5908408143743185,1.5909976672791113,1.5911661316972938,1.59134683421414,1.5915404307437084,1.5917476075156292,1.5919690820575967,1.59220560417293,1.5924579569127613,1.5927269575425724,1.59301345850291,1.5933183483641542,1.5936425527751767,1.5939870354055967,1.5943527988810944,1.5947408857108807,1.5951523792058895,1.595588404385599,1.5960501288704982,1.5965387637561685,1.5970555644636413,1.5976018315591547,1.5981789115346414,1.598788197538175,1.5994311300412212,1.600109197426842,1.6008239364799735,1.6015769327575404,1.6023698208125265,1.6032042842421335,1.604082055525944,1.6050049156155548,1.6059746932325274,1.6069932638268554,1.6080625481435014,1.6091845103401292,1.6103611555950244,1.6115945271406449,1.612886702655355,1.6142397899440428,1.6156559218376085,1.6171372502421169,1.6186859392708999,1.6203041573973938,1.6219940685731729,1.6237578222647668,1.6255975423745332,1.6275153150252186,1.6295131752049241,1.631593092288934,1.633756954477094,1.6360065522099427,1.6383435606531676,1.6407695213677627,1.6432858233118504,1.645893683348875,1.6485941264649142,1.6513879659244186,1.6542757836177968,1.657257910875067,1.6603344100363582,1.663505057081533,1.6667693256268732,1.6701263725959845,1.673575025864403,1.6771137741625026,1.6807407594992272,1.684453772340039,1.688250249736703,1.6921272765648043,1.696081589978053,1.7001095871375762,1.7042073362207237,1.7083705906588031,1.712594806498014,1.7168751627241143,1.721206584340432,1.725583767942067,1.7300012094876636,1.734453233935041,1.7389340263789947,1.743437664309358,1.747958150595207,1.752489446797044,1.7570255064126838,1.761560307674047,1.7660878855305278,1.7706023624792886,1.7750979779328546,1.779569115848759,1.7840103303836592,1.7884163693742625,1.792782195488523,1.7971030049318917,1.8013742436340123,1.8055916208803426,1.809751120390011,1.8138490088752341,1.8178818421483582,1.8218464688696943,1.8257400320526254,1.8295599684618438,1.8333040060560566,1.8369701596381767,1.8405567248840655,1.8440622709255583,1.8474856316650567,1.8508258959977637,1.854082397113955,1.8572547010479428,1.860342594632872,1.8633460730115938,1.8662653268438745,1.869100729339426,1.8718528232349776,1.874522307822076,1.8771100261207354,1.879616952282626,1.882044179296391,1.8843929070569727,1.8866644308507126,1.888860130298431,1.890981458789856,1.8930299334346061,1.8950071255475045,1.8969146516793085,1.898754165197952,1.9005273484201226,1.902235905288381,1.9038815545850638,1.9054660236708303,1.9069910427329075,1.908458339525771,1.9098696345851738,1.91122663689502,1.9125310399855353,1.9137845184405176,1.9149887247910162,1.916145286772683,1.9172558049240949,1.9183218505036428,1.9193449637029927,1.9203266521357159,1.921268389580335,1.9221716149578079,1.9230377315242824,1.9238681062608365,1.9246640694428092,1.925426914372261,1.9261578972580196,1.926858237228704,1.9275291164650221,1.9281716804385518,1.92878703824507,1.929376263021358,1.9299403924352179,1.9304804292392195,1.9309973418794455,1.9314920651512082,1.9319655008943892,1.9324185187216836,1.9328519567736298,1.9332666224948674,1.933663293426589,1.9340427180106419,1.9344056164011896,1.93475268128027,1.935084578673975,1.93540194876634,1.9357054067083683,1.9359955434199143,1.9362729263824452,1.9365381004209457,1.9367915884734697,1.9370338923470694,1.937265493459009,1.9374868535623646,1.9376984154552621,1.9379006036731576,1.9380938251636906,1.9382784699437583,1.9384549117385712,1.9386235086025334,1.9387846035218845,1.9389385249991058,1.9390855876191704,1.9392260925977591,1.9393603283116276,1.9394885708113458,1.9396110843166732,1.939728121694861,1.9398399249222047,1.9399467255291898],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=0.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN_log = visuRMSEGrid(ElasticNet(), 'EN', alphasEN_log, 'alpha',\n","                                 GridEN_log, BestParametresEN_log,\n","                                 'elasticnet__l1_ratio')\n","FigRMSEGRidEN_log.show()\n","if write_data is True:\n","    FigRMSEGRidEN_log.write_image('./Figures/ConsoGraphRMSEEN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                     37\n","                             R²      RMSE       MAE\n","KNeighborsRegressor()  0.349275  1.785116  0.845344\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_logkNN=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[22.092180463124407,20.476655072606295,20.340651091750974,25.134402013367925,20.955797324353707,19.785899577709383,19.452930590218575,21.982071473766194,19.690826711275353,25.00594962815102,19.90772192701854,20.151997675127348,19.96749826500713,19.87524695144101,20.739212015676195,24.248116137308728,23.76833817803965,21.577601506144045,23.451411829297843,19.746420697519792,20.728380481728436,19.60048240772142,19.52601189204592,19.59277605696514,23.713450791236053,19.855753953308238,20.77524619725501,20.054847323218258,21.721505813193946,20.10087828722303,21.08422533571769,19.284922862106843,21.44068068513483,20.75902933485369,21.72252621262995,19.789787878984985,22.331630303944337,20.175777626885314,21.27357979720535,20.21387475813626,21.26926585143911,23.50325767333493,19.284922862106843,21.982071473766194,25.068432206423434,21.75243072809084,24.641277261690483,20.28732499597751,25.35635937621815,21.838651533285677,20.179129193609707,20.58702369605029,19.54503377055488,19.78653002909203,20.692543424269093,20.232293571217227,21.020943428811858,21.126674595716448,23.524789670086417,19.794809322356503,20.09938551035008,21.54331608334182,19.49751589060578,20.676745066578988,21.676318362458414,19.47547889156245,21.633958128794724,20.345373573191804,19.581822944558017,22.53300985027436,19.568033512315854,19.81346210617757,21.028113125437766,19.142925624635893,19.597861065186482,21.250441042722017,23.699889819721115,19.67448657055061,19.824921988201428,21.353489760575904,19.67652862968584,20.220745185885054,20.359217129815182,19.77601852019259,20.89234251007435,22.083821064740253,20.048596344194547,20.292303699620273,20.721397689471733,20.141561157272506,22.99764936392105,22.486691926926753,19.47803666132166,20.875644583418023,22.540524957071447,23.416890591860955,20.170654129882138,24.617362098917088,21.529551081306874,21.628200450081625,20.729496638087916,19.620795491991732,22.455698673047287,20.82666329455564,20.824432283986905,19.869020420770354,22.73136693563282,20.77895367532111,20.461468781794153,20.6209203838603,20.253753766874702,19.73166322945902,23.86151242609872,20.841502766340312,20.38476698209859,21.452734975020807,22.416846306980993,19.615457954597716,23.503699604008435,21.082470005702433,18.647120329680295,19.984638847852,20.002536040276674,19.379293579088635,19.52272579764402,19.701118405232062,19.462741089256628,19.746733800500788,21.63096706120621,20.160003193769317,19.85257627790621,21.940745586756336,20.277478351557,19.82180365582882,19.694079192128598,22.037468531329193,19.525977580774402,21.841151628979297,19.606749577399633,21.008714396787152,19.916608927041565,19.613600610550495,23.123887062766304,19.660486337591475,24.996972051917673,21.155402672942287,20.361376409706537,23.47846609969145,19.259490662376514,23.182883127671307,19.555491740062887,19.64790530234936,19.93503531900843,25.071016025853886,19.338027069751575,21.378255665752384,22.232833428161655,21.95693396043051,19.65205020620671,20.312998292327652,19.663448681814263,20.995956225663665,20.360655022673825,19.62050308354277,22.537871716446272,21.84645642722495,21.756972402368262,19.577056335127864,21.76857713902834,23.513792450950916,19.620795491991732,20.728156939130592,21.58230215681571,22.838684601382788,21.62028339629768,20.916209042311348,19.71815101884322,20.298269720872796,20.648961867593947,19.67725446210525,22.421501093523688,19.930301943866777,20.12414306776263,21.15521106291222,20.23112282353954,20.354929512646134,24.31584338712397,20.71873689017801,20.782057893663456,19.665300461676754,19.787971245272264,20.55642405242197,19.368217635037713,21.15521106291222,20.278634928592687,21.954419990120073,20.15572396374041,20.283704399738948,20.531180969071276,23.08822084239271,21.84804451539434,19.750563004582602,19.994530215771924,19.93633613426349,19.233156464426767,22.237999411772794,20.63922728025974,19.71968615067705,21.137740184097627,20.341730636463247,20.400870145462942,20.484360735908577,19.64413814553528,20.086239280702983,19.695750543876763,20.106742539972256,23.736643945892272,22.111212647952012,21.77742117904014,23.792900140079162,20.78158834035479,19.60982329043465,19.85587428819224,22.11275462362534,20.388919095617563,20.01665653247869,20.275963869915575,20.77281719185561,24.26985372287237,19.683281256613913,20.620508999378917,20.414019214651347,20.23391603968175,20.67931968577733,23.224189454583254,23.61614486914496,20.256726441487476,19.583640974838644,19.614472333073294,22.386785042729358,19.130755309791212,21.684307955103236,22.645191815261065,19.594364554203715,20.1177109572519,19.583360642233824,21.29666928831725,19.42632815168579,19.918917822415384,21.89048893511814,22.50355917676954,19.575131894302544,21.246489132623278,20.670464692331695,22.98980925066116,20.132413245531257,20.132164997144695,20.457330018358146,20.49844770412326,19.81072807924065,20.32540344454036,20.136683240055756,22.02681241036587,19.61319225511766,19.326433910988385,19.66486322703235,20.994327770220472,20.55604610039459,19.555947040000195,19.72852381268384,21.352298167577825,19.66119689380422,19.44142672487493,19.77968104098314,19.3981746801491,20.494724993852838,20.532980157120313,21.94055681839362,20.416213611008715,20.165353294126078,21.69187564651007,20.148385626845904,20.16947338263098,19.130755309791212,20.357439258611038,21.527097415489685,22.302607602801142,19.15153977792805,19.893584212965518,20.246338278503153,23.718960529322043,20.6209203838603,20.859279130892897,23.627615252091438,19.469285923045227,19.109351723098563,20.12756543039048,20.34289999085592,20.32697705107564,20.931127534944494,23.331380227834735,20.940312342131772,20.415082613434258,20.855174557362915,21.610843091994628,22.188808077415892,19.681928168106662,22.493992586050684,20.350758523256843,19.761872579461205,19.674350339073364,20.6323718118306,20.376414718103753,20.10499315241608,20.150516782850755,20.338010833582363,20.642130107284547,19.75731314153246,19.105959053323726,19.914106462182676,21.527805504039996,21.111185186708813,21.530902715189754,22.258230809542027,19.78379010220327,22.163866319511797,20.346861100539655,21.66622858293598,21.801133382053646,22.607087919931395,19.78589957770938,19.349166423123776,20.43512776618195,21.009536808603183,22.07937987482423,19.618538384830824,21.327471165400144,19.704323670516512,19.767628267788506,20.579228715173258,19.909443495557905,23.379920097353263,23.701427408821733,19.568734996967027,20.801589111480986,20.805636678956162,20.23679022269858,19.980027077143568,22.402062120560075,21.726100248938884,20.73282680140547,20.79422167626378,20.36268209891905,20.981195224536794,21.84415028818768,19.059011964664826,20.842787180713955,19.812866785825253,19.940479383727958,19.646626077631375,20.68530137352942,20.322132168779603,21.064829827080647,19.1202633747914,20.264743850069728,20.74988172843938,19.660486337591472,21.440046990016604,19.72053455377984,23.109005384530136,19.462741089256628,20.62492422708798,20.1589171486978,19.681928168106662,20.65190930724086,20.020147332328822,20.773115996859165,19.95981819094754,20.38311325918973,19.646626077631375,21.893853793609583,20.682125633145414,22.784907353890386,19.91568511533269,21.949642081459093,21.373006833459993,20.8537592056155,19.462741089256628,21.54629842066077,19.109351723098563,22.023549646820754,20.947357554154596,21.38115933862267,21.34595286218003,19.580714284191767,22.27055027713205,19.696892767487327,20.694364971299095,20.803387639258666,21.045860334359908,19.370634000262616,20.350758523256843,19.44325582469827,19.70245649568138,23.67132581697367,20.18067953781606,21.195946577186636,22.60947505697324,20.6323718118306,20.303967303673208,20.36065502267382,19.959539503642013,23.909818418023907,20.10512749384882,20.132599611645237,19.68914276871927,20.00947774849063,24.548098026194875,22.070605954431066,21.081675580476702,19.490895873164447,20.931127534944498,20.01766528347124,19.604730416858025,19.80172553967447,19.779681040983135,22.177023241115233,20.430023691538352,19.931559167391384,19.81516025824058,21.019310440998755,19.787789911143786,19.901771025121054,19.667947408775376,20.389347308317895,23.569527990065954,19.568033512315854,20.09504274644393,21.0676842441838,21.084354200349242,22.030044839410664,23.77190995730746,21.684996254422188,19.929846222098842,23.678124204495564,21.510773245101618,23.11918293107774,19.692104522250855,19.79385954124495,21.394982748677204,22.082693799257537,21.04360792909606,18.71656794420192,21.795103754901128,20.86987371710007,20.623450300570386,19.501571236226678,21.09874556849535,20.227670426763673,22.877236163470894,19.730507037129797,20.314194518563053,25.23950733949597,21.785661130418337,22.232182835396635,20.807703903774446,21.438481843190786,22.5949605555579,19.964660828479843,20.796174802617205,19.637580273044776,24.321211049820867,20.80388958040022,23.291229486561168,22.41250840409565,19.96467971050023,20.264743850069728,20.099788083274017,20.899619361808703,23.994800366423316,20.2001052547582,20.041283607601677,21.912341856317234,20.44077296344048,19.784526937326035,19.28839056785636,18.716567944201916,21.13136198747562,20.39775398144771,19.380545588220908,23.10453396790939,19.62033366616821,19.9324959093744,20.984032870880053,20.445142015402283,19.755251360122575,19.72396049395236,20.106804178591855,20.27905286475323,19.47465522212037,20.158950060051698,20.961331592734123,19.41041046586897,19.61346709593544,19.266809345670776,20.437053252985557,21.34354967623402,19.730801523175852,19.75731314153246,20.48690377157989,19.706570440339256,21.00870057603892,19.64963787236327,19.824517759491517,19.84741547007486,22.50848554062269,21.21178516300074,19.4638063425513,20.06262160443711,20.157836706755383,24.835054550414675,20.810082670110155,20.047247591881124,19.345601899735527,21.32163904282843,22.22766634710603,19.964580455502876,21.017873109673904,19.90772192701854,19.927151600703077,20.132095057809412,20.837166088291312,19.533929999937897,19.572705932523526,22.218283686112922,20.168120740019084,20.572876544812317,22.55774290296256,20.157946969028675,20.166738015397733,20.670881170438587,22.475793415396307,19.60756453378259,20.58477834494246,21.45537151696851,20.63856145503615,19.98262014364318,21.767288944707627,20.404260975266432,21.2298896874832,19.918513150916894,20.167135471183485,19.462741089256628,21.92624642174452,19.83109748211482,19.50282235713418,20.04698714984439,19.9110615333519,20.560740248864942,21.14594490863828,20.840113189931074,21.69086814766314,21.49961980016151,23.34776375254703,21.912274772334708,22.701180453955562,22.533283612163633,20.094977345812996,19.401200791309662,21.689245177413074,21.544030922701907,20.207200864680466,21.55756866960986,22.698152078787967,20.69666645922282,19.824417281294462,20.62105854260072,20.576480434780443,19.545349932919546,20.58592217987126,22.985320812275422,20.91362144472658,22.327192890251276,19.794218656065834,24.03348254081028,20.319865525548952,23.674342312642796,21.44929570595223,20.65264572812252,21.156820704660475,20.960502963150283,20.940312342131772,20.74894563132266,19.843709502243097,20.057192071803232,19.72310473995287,19.739681652177925,21.74685825388036,19.828112563959547,19.87770616441834,23.36586136906544,19.824517759491517,19.815441875750665,19.901771025121057,20.71739477952836,21.31796619463197,19.508100024911084,20.243551133694233,19.397798057123612,20.69738069020881,19.89042908239232,19.87849971929058,22.232424060472752,20.355010893747806,21.025033867749205,20.723939670268617,20.979921986410172,24.8643450565404,19.367758873529773,22.69884081798975,19.690334817120828,20.09024506697989,21.211785163000744,20.640229301283725,22.040605309649585,22.57054367086121,21.220971211043423,19.130048095296974,20.220299062788285,20.59178385751041,19.084778564852733,22.054246861218683,20.4606041445132,19.37199399850082,20.108012444732395,20.12966671333723,20.16412904462082,22.480726613080897,19.771948451840416,20.39194523802381,20.123988048768396,19.406062701297138,20.169963650066414,19.109351723098563,19.708699905638444,20.179129193609707,21.30501332792991,23.216069491621862,23.6312638627526,20.18404037235011,21.09641583405983,21.400156492534443,21.178301803798,19.825113274498744,19.790274726553402,23.35891609264678,22.451190125247383,24.27502499630753,20.202165607518452,19.476865940151807,20.334848107169215,20.86089047826089,21.82843351288029,20.45848458832439,20.391148598392416,20.27298621539213,23.57975122547192,19.65149442456125,19.624356010237268,19.985406182601935,21.211250650601162,21.41725330072048,20.709369517331027,20.47450280534407,22.041669191827165,20.728891214349765,20.184391921796976,19.49640264773968,20.83300069867825,20.504249180159377,21.810738830697257,20.09676632959996,19.825049887094,20.49070780680831,19.62309012540676,21.48996013668916,20.405798475288087,21.127146064277273,19.669242367721637,20.048892625191563,20.166354742397463,19.467231555776973,20.376414718103756,22.63414530113868,19.778805561380356,19.826936724159488,22.242935631758904,19.582732338143707,19.706282863412035,20.097143281258663,19.653772482030053,22.080123051878143,19.708794942401738,19.79156019611738,20.92901977564521,20.79774689727496,20.706332452884848,19.54760851430416,21.74917761313047,19.84885818737452,19.988172586904223,19.9926541097757,20.285374195357086,19.823812371553217,24.956598458136448,20.479600310636272,21.47226800520617,23.767782039880977,22.231575281286716,22.300906143821077,19.594364554203715,21.76948857955305,22.069565232773755,21.118593940591904,21.51581460628398,21.226691194515073,20.268429027907935,21.0676842441838,19.680467854878437,20.89724759162303,20.11108551401245,21.84118739809028,19.47803666132166,19.824517759491517,19.368649539300314,19.453835351878094,20.093104494727818,19.582614752333754,20.081779975251667,20.37845294828817,22.22554528623816,21.063696803008927,20.04353436262362,19.69103441536864,20.361376409706537,20.689434187158454,22.252712982808113,20.259524752317823,21.133816756549045,20.137977713283792,20.06861591388174,20.672488567260018,20.334951564776045,21.058989049522012,22.511052924368055,21.10693020058578,20.882321318880095,20.164798218523263,23.463258504460033,21.072673675317407,19.462741089256628,21.29623641231697,20.923286218644378,23.95574167150964,19.5506919304679,21.395271293436647,23.8025477590792,24.716901529149265,20.392661519885817,23.58486093983155,20.242199080786502,19.962726119646508,22.045273471911987,20.610569594141747,19.725791320628357,20.539486905825,21.246589633364945,22.259454206117294,20.143118700939183,21.08822926663169,20.68798563297698,20.80388958040022,20.341298190392518,21.670404449718962,19.481026541546342,19.421480431845275,23.186295738719704,23.784932240834166,19.49459263509716,19.916657643725948,19.284922862106843,21.077682501811555,18.994020592566944,22.481618857757024,19.92302536913263,19.980095062730577,20.75902933485369,20.55604610039459,20.466276803274706,20.09215933503191,20.532937320672115,20.85968640573914,19.440147275075297,21.09375698842839,23.386266639601693,21.375738051842944,23.225310714169996,21.969000996925992,21.200857453598353,19.74004737301056,18.90327540175305,22.416544374516388,21.301882722568724,20.170917105923323,23.40265116664827,23.55014468344238,19.794077513506316,22.19223383264942,20.414032418262316,20.630261644821605,20.560740248864942,20.678803661584794,20.29230039204565,20.14338153097177,21.343590336509177,23.825649247997035,21.438897047420657,19.43692760058644,19.68020822798003,20.45195390769808,22.592553347676855,19.82740741833283,19.462741089256625,19.59939351915745,23.57068276545495,20.70243967106046,19.773508463389827,19.440521876190143,20.059876056448395,21.000279740951907,21.808634254063406,19.501460504856304,21.765256876286287,21.760065928077836,20.15399142578808,24.55444424625447,19.9449123434038,20.244290500577282,19.979419569174286,21.776850907004288,20.390100500037043,19.35643658744219,20.51318292293631,22.056680883418945,21.083569739999835,21.2760105376715,20.358504582197458,21.59108125314211,20.76727545131763,19.52601189204592,21.19274823632005,21.12752226697777,19.693439445341628,19.08146541869163,21.79432728689326,20.772814720766345,21.477559100386586,24.592612951131603,21.970076569058875,20.640202629377928,20.046987149844387,20.249721790582708,20.396455116581603,21.443135700579763,23.448077941481827,22.37897996990599,21.28516230997162,19.989677583012536,20.13009293284579,20.82050422439869,20.537444175418972,20.94638223046592,19.99274873406191,19.960894266581654,19.62681684368812,21.39124200676924,19.792063687034602,20.369903106482965,19.956969865646933,20.55604610039459,19.568033512315854,20.354266384882344,20.51464153226072,20.960524784318515,24.604868435630426,19.616056865591954,21.679773023465742,19.865874093802244,19.633962950667993,21.477445390183437,19.284922862106843,21.108255538480336,19.743685515188357,22.14870373227824,20.38063036315382,21.111621965814937,20.84821197708416,20.353673554509633,20.198999412154244,21.036014333357254,19.707475191057316,19.92113978945576,22.18352039235852,22.280089390457817,20.990308770373247,22.721102930342397,19.72941627598159,19.199425921306577,20.16635474239747,19.610399541232344,20.88146543702277,20.263611905341254,19.603511556391762,21.06482982708065,19.47379169808206,24.956598458136444,21.885974289440554,21.120229358080245,20.119511038323125,19.79446095506931,19.915464062852852,23.159934655964015,23.101566089990605,20.94847381097533,20.180512498160894,21.41898585229864,20.168642439513174,20.30941378426263,19.49419989095612,18.798574822234567,19.805866893924723,19.53956150880314,18.994020592566944,19.681928168106662,19.614882407955847,22.758072314232116,19.583360642233824,19.130048095296974,19.1771584909362,19.62033366616821,19.85939299017831,19.611305650651936,19.64902838186315,23.60604408417627,20.182178071702285,21.190671920978424,21.388523382861173,20.492223520825615,21.27503212391085,19.594364554203715,21.50523295206168,20.335237051140318,20.95004394179249,19.298071508528434,21.94781351786135,22.55278049348285,21.365339770895527,21.81217453291852,18.877697806724207,20.115072711202934,20.03300835545129,19.564326597513865,19.72811688264004,22.292853471408,20.242769493261143,19.130755309791212,20.912428119449793,20.359549607753493,20.720683230376178,25.131599189005023,21.28900003101755,22.57084190875984,20.640143064228884,22.01885927167849,21.054788853363164,21.962086557695738,21.2443422437492,20.997383642417937,19.858946003715193,22.12141001364848,19.718482930539484,19.16769110581906,20.750602184297858,20.183261573548997,19.865581905640685,23.833984560303314,20.056962340471053,20.468642884927373,21.123190669071246,20.694761157662384,20.233970205413556,19.84639886558722,20.26740745727167,19.641039532086097,19.884435224124868,19.66900161258229,19.537010374581225,20.87230874119969,19.591878660323452,20.340206030263495,19.880406255160068,22.680846595644194,21.15521106291222,20.36440763149773,21.54202705379614,24.571428814275183,21.705774474621965,22.67758180782115,21.059836498800315,23.201641250721856,19.259490662376514,20.220299062788285,20.788235807308574,23.8549004435663,20.922858158327365,21.484628792351486,21.769400740981517,24.394310316522056,19.678589163710697,19.3014150910819,20.42143211048946,23.70367679546345,19.746769785743275,20.4254372625974,20.192709248508812,19.434376043350078,20.498978715877975,21.241308911532045,20.803404899990767,19.97231444803639,23.928082286689516,23.722179861946955,20.560851793898305,19.681928168106662,22.96561051729359,19.495883224010466,22.06911954802191,20.814407922583545,19.75816516672006,18.86707422526588,20.86840340717135,20.182097807126635,23.32514366656498,19.58786786040071,19.611950872944828,20.029397821945746,20.15058217132905,21.855352744157955,20.412845645740106,21.1692970905961,20.347716464137964,19.733135365373847,20.086489397083835,20.44232388112861,19.810728079240654,20.39375557322686,19.81763594239225,20.39163893834596,21.103364386155675,19.861150565598894,23.921930011063843,21.382625228976885,23.357364003694812,23.844708107991302,20.40719214755093,20.391638938345963,21.073839275215022,19.606057776560817,19.664147367281586,21.141835723555708,20.591094397580065,19.911852309056645,19.406062701297138,20.97606124661353,19.92312052335696,20.650254723946617,20.40545533569451,21.17635648686131,21.768093168264443,19.64866253839307,20.27934149528222,23.738741439515707,19.730224890261155,20.368466542920903,20.513216008040473,19.462741089256628,19.90031110271128,21.493796989263195,20.53096535044579,20.36428574509442,22.259793773203445,21.91309748183712,21.087460855316397,21.815056231746283,20.668693890126196,23.693404300048226,22.609672835724723,20.39073718232146,20.057419266826223,21.66500615642432,19.718151018843223,20.739806263430673,19.625988102983648,20.684728387589423,19.95267614788131,20.663389616556778,20.567035492910705,20.464498418144807,20.883374764834517,20.316971091716688,22.41720571343516,21.35041837200914,20.333357477700133,20.312998292327652,22.154452024189766,21.096822437481457,19.86506992460738,21.543316083341814,23.19160536086581,21.402793230692737,19.934538586394222,19.47759297334046,20.444480290829603,21.24019865131144,19.38826814311641,22.672551112629073,20.396250853746153,22.44307991330954,21.47780018355521,19.372444919925208,21.918882234713454,22.077938373677494,20.209916786771306,20.98688664929606,23.162628142736644,19.34275081517922,19.810427008205245,23.35540496281319,20.933409668464833,25.0316680251341,19.78467459407331,19.628971246878553,19.123586369348438,25.20964968744201,19.865874093802244,19.888298990466858,22.06853700377211,20.755445219826033,19.510253266467643,19.259490662376518,20.26308655472834,20.55134185867959,21.05450276052657,19.953218176720533,20.222648849843623,20.509789594911123,21.47817673855912,20.65813251447764,22.661244277045416,21.463847710075754,22.26126745338968,19.676659874903162,19.719869398885475,21.410444341130145,20.331102068397055,19.862672878101964,19.556863658046595,19.100803028592225,20.13070474103041,21.06542589715735,20.26982584788844,19.474550607642442,20.454109097799567,20.403983778997148,19.72098713714006,19.974751767325454,19.87849971929058,19.64430135373859,20.176625008683523,19.453835351878094,22.30826384315339,21.00630956647892,19.532628419244567,22.25756727800984,21.76787980659358,20.642234865682912,19.62033366616821,21.800183514768623,20.228535491161875,20.2001052547582,23.098190185909953,19.668890318547934,21.307559938554324,19.111695729694098,20.5407036880314,20.317177599967344,21.999408729535727,20.3865664627893,22.557885841523145,22.515153865037895,22.604067119558245,20.610010518524675,20.931850663834044,20.43512776618195,22.139479042268814,21.172269181417807,24.436550304370375,19.62033366616821,19.71735629554763,19.66112365719303,20.662398074014995,20.79411578604134,19.639872258015558,18.866089773107813,20.79792490492888,19.57674179163013,22.069565232773755,20.134063781444134,20.175777626885314,20.38298097626083,23.848664083633878,19.950946333910636,20.314251052728686,23.437982826719622,20.947892727198553,22.365920109505968,19.471804981885175,22.284833353288093,18.750244554166294,21.064426214383598,19.635324162638074,19.828912268510503,19.585813188267515,19.55951032123604,19.52479279224915,20.750172589446574,22.17016456427829,21.0780072942256,19.681928168106662,20.65813251447764,21.667018928712796,22.287160567245635,22.266362710314052,21.05983649880032,19.600832763617493,19.832346806932875,19.596177727480693,22.587842808552708,19.94535687750491,20.591094397580065,18.82821933838806,20.199831116168163,20.620508999378917,20.92990038765364,20.82050422439869,21.50363697385861,20.122052264946923,22.834349763720258,21.122561588156852,19.944099437308275,22.054391518825003,21.636511791391673,19.862672878101968,20.516086282999083,22.710875590991407,21.25148609973516,20.676821516114746,20.620508999378913,19.529166399528613,22.07153113138802,21.324430464894007,22.28569033379753,20.34886947962182,22.473572400696035,24.179432617993996,20.10891315178573,19.52601189204592,19.13075530979121,19.944347287767926,23.32079414845575,20.464516361170855,21.455857641824768,19.823064966543132,19.80416028114442,19.731448988878103,20.508305628807822,21.705302769793303,21.633766105093667,19.083226195156197,19.017642059752998,19.77869359052392,20.71892464074925,20.69337079911719,21.713555107369054,19.61241377180355,19.757800077966614,21.879363792784204,20.737121198237414,19.59630648455019],"xaxis":"x","y":[22.06759946491166,20.51569310181126,19.61997633871636,25.808363100543477,21.795376715513594,20.34038041027431,18.86081419340691,24.078413808713293,19.082528830592434,27.09570141066156,18.66432922183982,20.146924115160264,20.694537918678005,20.34827538264879,22.1287945428413,23.512029578625103,23.710849830139047,21.16813940491307,24.138707165857692,20.413583384325577,20.971944131576173,19.330306789697467,19.240535149076692,19.593384739494034,23.207168669114285,20.243485920599927,24.816169078762734,20.239179241482244,21.93825114094642,22.544699771649725,20.581001696999003,19.006769325349232,22.852829884665343,20.97322158881004,21.37041284025584,19.8813455186455,22.291944364905724,19.352118206703615,20.690574372087426,19.616726513482636,21.73777568014623,25.92407619036082,19.62630225708032,24.328668011629535,28.119486884464127,23.268620237666717,24.937914667295647,20.538966591972148,26.49472805759287,21.551656870776874,19.967005268430295,20.53263058829885,19.077522473826907,20.411222614242394,20.118559661298907,21.75794824506378,21.969944344762656,20.941991359524017,24.30144839882211,20.08875201356742,20.413158139175476,17.21138734793915,20.10520261649224,21.318516744600927,21.383177447413818,18.901120542977083,21.140418528305776,22.317825778768146,19.694551733623157,24.452943059423166,19.34725801943171,19.360313431748278,21.62865690143549,18.725486086935668,19.22798647641259,22.00766259568964,23.581082748790354,19.483945640419044,19.555879359056263,20.81759187582515,20.004050690864457,19.7027197284411,19.957005833266606,18.935012496616167,21.132460035639845,21.104713276655342,21.0706503248077,19.995538068644624,20.45373482710394,20.377850211599817,24.218899085058613,23.648191447276112,19.27757204338643,20.547353262390168,22.570203152455143,28.033188230056272,20.740147099081785,24.844950181647548,21.11139041842472,21.618102712310883,20.71863237230781,19.638742582781422,19.346013720701006,21.12828261562749,21.344903224909334,19.828653485294133,22.842407702796528,22.86005581934068,20.23340552018178,22.244828354517683,19.8468416379525,20.153224049031966,23.16226531069458,22.47425899129698,20.440974576504654,21.34321011850724,21.96825774467718,19.581570228112746,22.97307940319244,21.943790024620068,24.922685338201692,18.397908557303648,21.170757004644493,19.439732238591404,19.06759979313172,19.78225599625029,20.88710307988011,20.06163488065717,21.227834779401228,20.925901548056537,19.64120627043042,21.51013491039007,19.943833745099443,20.041179196484592,19.31991683554051,21.305505708300736,19.083972676252298,22.36486950758078,21.00153722158492,20.91944021038121,19.71891096707218,20.43515061037962,23.756601825433552,19.29612083176196,27.288645329988054,20.004627845911102,20.581937941283943,23.06049210432163,19.0283827986455,22.26292897661663,18.98289333455679,19.08654493081137,20.26723932452398,26.772642502942784,23.150894170391226,22.789865927709045,21.316932785410458,21.47925775839002,19.52466856143405,17.31416721288844,19.67390026636621,21.280848347151327,20.575951881264743,19.36981390700933,22.906270958120995,23.259273464150183,21.034942664057727,19.478866371959384,21.521316718605924,23.251480243552532,18.947642644803835,18.234449818345436,23.74556559722114,23.70046067341344,22.17893043543761,23.314842649926593,19.204628901280135,21.028986544289005,19.811383905702034,20.105414612220844,23.470671408984025,19.185541351838207,21.863096949082852,21.439837984160558,20.28595076567891,20.09465504412721,24.347469240771108,21.026538976608304,22.25115601355335,19.17189511189754,19.330727902738385,20.455200776646638,18.7812284253763,19.9984664758684,19.71179802639097,23.222688495940556,20.75461515785591,21.453079563108336,19.968121134960665,23.31621219099147,22.229186273778243,19.29157093266521,19.725980900417238,19.393396349595932,19.510476738006748,23.86936283890287,20.803398322957783,19.50445726786541,20.266025864180154,20.318803159054653,19.88287753302219,21.755410336588376,20.021014007950576,19.81464925130666,19.600081112964823,20.195578868662224,22.127386699288984,22.615494713832,21.994462034082755,23.008367409641956,20.02036350615788,19.496913617064063,19.65284705132002,22.970383373621917,19.800442868861996,19.544786543412965,21.196126618068195,22.706377974056313,25.14188655199703,19.430343691015025,20.82549425312562,21.300868648362627,20.935011057360228,20.238709984208988,23.361271159574002,23.97311707542301,20.93275451807793,19.135842027978637,19.419925212472517,22.69657897049395,18.04707863542646,21.639091593296747,24.902956648700677,19.472341179018663,19.630944296038688,19.581570228112746,0,21.279464358226686,20.6373536565784,20.55168503037007,22.252924089212918,17.011336466938804,20.539904719671835,20.287228770242784,22.579718287600105,20.016747429777542,19.919862789535355,21.590649178257912,20.459085985918723,19.305685493406397,20.69934199839951,21.814001605688915,21.49724409211446,17.96562762596234,18.897059576214055,20.218149025767367,21.663202256858984,21.537426968242833,19.881144027962787,18.109927551497627,20.650313109616484,19.551649361459194,18.954616641321604,18.33673536238825,19.272777840955303,21.22721049508145,23.36907437848721,21.839279632705356,20.080313988880153,19.791725695449298,22.830034474576443,20.038176858751854,19.882805718288704,19.137324212876308,19.89611063211052,20.922839608419043,22.73108571789678,18.798041102461394,19.809989451392685,21.628482474975726,22.66651647870706,20.105565995835796,21.324104166055236,23.33275256138136,20.80501366619404,18.787924871827784,20.381292625866948,23.456138303517488,20.59828494694422,20.938551777966236,22.709886920964298,20.558379254361796,19.758613410846518,21.618914296187317,22.889842311610867,21.30122097904847,19.0336054859068,23.801438495307856,24.840309004310797,19.694954646749302,20.944503948247828,20.620570884389835,21.72592495902031,20.296054717472362,19.58398989769835,20.2956272827196,21.085706497425413,19.461766940454588,19.29900449887296,22.510563057499144,20.841106100432793,21.801619166438428,21.147132242697182,21.551582010518878,20.4427602890876,24.611415306054898,21.490696787354505,21.24369655260969,22.432940196049124,22.234424235191682,20.099462108962847,20.800416502964755,20.17949699394329,21.746512886013605,22.496434708234126,19.78606039431591,22.229470444639624,19.19842345841963,21.071706539354626,21.28737646300594,20.120777025175872,23.69806207100269,24.25287711008343,19.074352191519743,20.558351575498786,21.437400189861094,20.352963298343866,21.233800456337935,22.14457282995132,20.722700819009958,20.69715285443839,20.03253125195289,20.0566191944814,22.70946891023392,21.137414277628565,19.808943756196857,20.143175790739598,21.040198736231254,19.712186074557728,19.35146728073557,20.861088989018167,20.755317114550273,0,19.017540081468233,21.419330156983328,20.67873788317677,19.93668394257525,22.409766675366317,19.27540160418116,23.728782472563765,20.376074503254586,19.31077695697734,21.065895324749285,20.3484179021553,20.07860380095908,19.976548041795603,20.235086321270654,21.01213158467734,21.41999462819776,19.174863067555084,21.10241852261651,20.85643673732399,22.586773372950685,20.037769462388923,21.559823540829672,23.13793422908773,20.558605220150522,19.683189962828273,20.39789928981696,19.337397595787667,22.26889032385089,21.195293500127566,22.559724250840063,24.39611039895289,19.59271980995523,22.136011068345166,19.33796659591498,20.797749380419276,21.567058771175564,20.725266392758815,18.468910364525364,19.663251453070878,18.918473122196858,19.198971627350232,24.325766932844235,19.86193501563865,21.156628899315663,22.938277348217966,20.78722963747693,20.66031803587651,20.155379081002646,19.457981357627542,25.713720052658104,20.828559454854545,20.52544147367378,21.147722912842873,20.03302501817969,24.630356510701596,23.121909718158822,23.106213073613993,22.51080834568276,20.883881259335507,21.661257500272384,19.85555820528158,19.526632957320885,19.191856386440307,22.96557469826004,19.96989517504386,19.756880613939327,20.841336970061143,21.615161729704354,19.630957845101975,19.107266091841186,18.360084050573274,20.178689314774683,22.61224657942639,20.902818910258993,19.692513024244924,21.22795725431778,21.431836012474292,22.58649267850949,23.56945490190484,22.15739813287633,19.523673217064264,23.682062419502355,22.448170385491473,23.624330847094043,20.4998468600676,19.29906600505008,21.469193647687533,22.680649469911085,21.178626711012377,21.686546691263366,21.428479917582088,22.88912263482666,20.804770608680126,19.28855746111665,20.502951107445575,20.22322663992422,22.31457162800466,19.478115876564008,18.795995233915015,25.17177163891695,23.160406649511106,19.25160491688623,20.826937118763155,19.953135265171547,23.933068642871486,19.355480148534408,20.44485249695551,19.56616125697471,25.23181389258149,21.345469949872594,22.776042002687795,25.303110182218393,19.58463718752616,20.204391739449186,19.893828839264728,20.33821258053668,24.68330601022857,18.821811344100055,19.840511525069328,22.297566604177444,20.78875924862035,19.741131185755275,20.04722528013383,22.468811326543563,19.99457856414159,21.80053568878591,19.167558108393887,23.846962977749094,19.07645844042533,19.307592868706436,19.883803436319816,19.767052898077655,18.952911838336565,18.62774549084819,20.73926969664862,18.627786784209636,19.177777189052396,20.089958597754855,20.801380865239704,17.764101156137162,19.11561871251436,19.24667566091232,20.93405408413005,21.340147837597844,21.090725928345268,19.745386795650543,19.095931371701568,19.810878030088894,20.251155291034372,19.165221068220944,20.724802969675316,19.74733497869572,22.856134451929176,20.542598325656595,19.502159734210206,21.979041347822633,21.2197623270523,26.969103234931445,20.18342033833375,21.02499268359878,20.592440384261195,19.990444456103596,20.867172390851817,21.273367636241293,21.243839329097405,21.363500625759567,20.18486444945059,20.241113097705927,22.693372098426487,18.623284957897813,19.205602363695707,21.545170940853957,20.80034066129298,19.981973707277767,22.19000668879013,22.718490023684353,20.707922743251714,19.214188218113872,22.472827160217914,19.07108521476948,20.346151863763343,21.63013067480897,18.214892995560163,19.93614600441511,22.53034703709958,19.469859640957857,22.895450751816803,21.00807772677881,21.431258349952667,20.941808459871957,22.99407646829777,19.76111291920157,19.008644725628592,18.355858634806896,20.297451997659746,20.474380309584664,21.870871511939562,20.759740299921024,21.439000175436664,22.030880607811586,23.00587909853429,20.655014419256375,22.965146977089795,22.43523710461151,20.18471556395119,19.019883876304206,23.05070508803856,21.499214768467855,20.59203905710115,21.024704798577186,22.72181503272866,22.377050963043796,19.71871179715288,21.93139047566312,21.827309362120346,19.1796783064222,19.826388660339887,23.65560944108187,19.199993380834425,22.21417220655097,19.269925661522706,23.782692532330046,22.173362238186836,23.41808975500013,20.872940195293037,21.54216753533708,21.03393309328942,21.663729644062077,20.699641074284234,21.39606694375647,19.593416427729718,21.355622092522935,19.49242240707679,19.406777996109206,22.625652665035147,19.860111721120674,14.03702355520748,22.00972851632279,20.33705521659505,20.155471422765178,20.40843263005365,23.540959857497864,22.64902697475969,19.51901613478499,20.748003621067692,19.075332900397907,21.535462867247563,19.35190112925433,16.543892756530866,22.048803804340604,19.94957596782787,23.253137677025506,15.802066947826521,22.01816825105002,24.65495079669392,19.684337709357063,23.07232514287102,19.441821622178967,20.90283362751463,21.65340972782181,20.672219865399406,22.248853822319084,22.007999054329076,21.514848387611945,18.91123104739614,22.195045634281605,21.092338092530998,19.27798585366196,21.725547245838506,20.321666107127033,18.844349286283478,20.211183384442457,20.06069065496676,20.388592288941165,25.880225035189923,19.68504303470752,20.795739259102447,19.874915797147626,20.484467680558765,21.537004350283745,19.454353518988064,20.471143313052202,20.221414835639845,21.30681378731033,22.893554470389834,23.892417838743924,19.773417735905042,21.39811634221899,21.94580529932221,22.093666756982273,19.566140232030243,19.62938152434226,22.598971957007752,22.606867611519395,23.548837661891714,19.57985452716288,19.257215175267675,20.260215495083163,21.269107840654502,21.892874617975046,20.911342976063032,20.690267441816314,19.415723203109316,22.536424619535012,19.241608485058645,19.162579413291002,20.470409642279566,20.97348174432658,23.73507977629211,14.55836091863428,23.336410397420998,21.627932359551377,21.49391553553141,20.265469687416747,19.426854398695138,20.625264709408093,20.223930403747513,21.865935415382868,21.881578906408237,20.14846091060974,19.81407448278221,21.0024847282008,20.945391681085823,22.291777470342872,20.551017265346704,19.577722584113683,19.444205825873514,21.46402517777047,18.277738745265133,18.799932943985397,21.60163932304092,19.80978333592071,20.91463622707276,21.547692385839394,19.17999258814081,19.549428996570953,19.54492482512454,19.294068756456163,21.69613129076446,20.048671453648097,19.49098989799628,20.42092755989845,20.047998538616795,22.735567798975932,20.159778968313912,20.920832580620292,19.699545342858148,20.678206389277292,20.079358473730228,19.917597162486278,19.692880228017533,26.26193365893149,20.33327111425421,21.201441496966886,24.081396558763124,22.374129418340807,22.181711726885904,18.957794711482784,21.63591845810156,22.742154187536123,20.277557571355647,21.712167598440185,20.922319986303112,19.981118924422468,18.376324675581085,19.093096903348385,21.041684548715036,19.88267606965292,21.79470380164697,19.394929062748655,19.711931797206645,20.701777070204972,18.966254774482525,16.57594392051226,19.3661810523187,19.637857750008234,21.05876451742095,21.39188913879269,20.922214169866013,20.158015290040524,19.840625811644227,20.287989620862316,20.392027938746434,21.440477364164977,20.18838178436931,19.209224880205603,19.523793560708743,19.767710849855263,21.57006553499441,20.748925773432315,21.47821316674072,24.169496968680054,22.350651659007692,22.60440595404428,20.973185759004682,22.891726790683144,20.2096151875794,20.22349858267661,20.901209408549594,19.053706845403045,23.000908813982363,19.549547798448483,22.637005220660473,23.38302130821851,25.236220259743373,22.107609158967936,23.256395298587126,21.31115959043422,19.242152996635603,23.069120127461606,19.88568939220856,20.74441094675746,20.977820033541025,21.403421935393933,23.155520679582406,17.74375624808358,21.65558749657167,22.72177363339029,20.057436191229318,20.31682150206961,21.442049003709382,20.447439187365134,18.64679105166152,23.081069556252004,23.109422435163253,19.40981146751902,21.538742146089504,20.451115883369862,20.520204300445414,19.072268708954628,21.853488891111965,19.870759032200375,19.874160110175985,20.45611373193672,20.923455889638944,15.423378576518493,19.495183602853512,20.532198000108426,20.172754664219987,19.621134659665625,21.149770683357705,25.5492833010917,20.91962424027046,23.144648130459657,22.724819827816283,20.76732896720974,20.90466916348112,21.019251701774245,21.870062515886435,21.968529606994224,18.28303322918483,22.7967578369547,24.617902294510255,20.337523908425865,22.278345092825212,20.439903152029967,20.745981176528062,20.50317548758169,20.148792615003963,20.18507504172347,19.887802201303376,21.335667578998795,23.547889563240428,22.184719195487645,19.124759964560997,19.09456742423413,23.546328229722338,23.676810419954805,21.378549935187863,18.772971741326142,20.92343911431291,23.66501619396984,20.362922084924325,19.335122075152558,20.6322831501737,22.014406532680294,20.862569257472042,22.377941930707426,16.70615872384773,21.114598939015725,25.312874128977917,19.557332226454086,25.0888235331049,0,20.821375100396438,19.69889252664812,20.730065203035867,20.737794346856816,19.720177445507503,19.751886432327307,21.436234720492326,19.12391425698277,21.093445290401018,22.18427320470571,23.32578333060166,20.235023779555554,20.38834949087981,19.076215392899233,21.890199695637197,19.626240196846272,19.473911816076555,22.65051997686549,21.07153535807478,20.152641197123305,25.817249936117022,20.72049143867345,20.000328449472278,19.677575133698696,19.854767962698624,20.90714373186156,21.852326049162542,22.34522357506863,21.88381079939571,20.64279914121781,20.008240359915625,20.079932477068777,20.97646133780068,20.617037831360907,20.86340137149202,20.10297814136792,18.939248297157278,19.337083944174847,21.21789500066683,19.959413629413806,20.57158693763026,20.1483293247843,22.45828794986169,19.223167412657357,22.090969450288238,20.54580254002858,20.99006223015013,23.933428716720645,18.97273303540371,21.839004592901738,17.66325927218184,21.098649782686852,22.108159109401438,19.70668845124008,20.687407722542194,21.23800756191085,22.54894428694912,20.440949745295235,18.136670762341843,21.301837769324877,21.032952737462605,23.31051349089806,20.969732200565332,20.351613259246466,19.678510738089678,22.059041559037812,22.10097025846529,21.127912480118983,24.425354799557958,19.405573621751774,18.954513256380405,20.62820521809647,19.21222562648202,18.9881876444261,21.62524006847279,18.78833918294514,19.827821621310783,19.4644839297056,26.34702224299619,21.38205653661138,21.78972267747008,21.087755789009996,18.999355953219133,19.451729034530373,23.289040906826244,24.16646354231811,21.536078922759724,21.530906360636845,20.97243154975957,20.089266034468004,20.436904665975636,19.20088663487939,20.554011225634962,19.277982733151024,18.304477915148002,19.118363214083377,20.261720490850706,19.26508618967162,23.063753511610845,19.05336380976314,19.240852168960235,19.807795916443087,19.527755462443025,19.369940873414674,18.557194427644223,19.092216129486825,23.457850638636746,20.3986497634222,20.79850265219969,20.611470634120458,21.03390016791011,24.957468167964976,19.132327452939787,22.668945885967275,20.192548076466373,21.926818619899166,21.236410750491928,22.00439862331522,22.64890259783158,19.876543924901863,22.174820865681593,20.001613842126076,20.63081635106876,19.653189714082398,19.305175301710232,20.224827212369068,22.6279649673046,21.547003936026666,18.961602303420257,20.695469281090816,20.114481067581714,20.790519525351012,26.143565404950515,21.151917099556776,22.601975102609295,18.386700720994313,21.48658316808204,19.99564295418165,22.704086007044513,23.646718002570474,20.17407123394833,20.29544946282625,21.94083977054476,19.087820197169854,20.49274951238928,21.58556546053215,19.868098908707033,19.806349800221792,23.600603274171462,19.846459862589864,21.099407208256782,21.035986420389108,23.747615973251953,21.078530347858226,20.86782723578085,20.073998999202523,19.385623092000827,20.214357069981805,21.756421739524402,19.573874587511387,22.17732889464112,20.092049623606254,19.621522497700703,20.13212471121583,22.84612630027242,21.610638293924836,20.381527241115613,22.69863600121815,24.241029788998915,23.65992231174119,22.296121251970213,20.702441582535556,22.897176211133427,19.850331267520566,17.677965293255863,20.87727804782974,24.045999647847655,20.173501472804098,23.33182107390064,23.30215527163116,27.231751620760654,20.650611348398325,16.829816883719186,21.108904064764797,23.735782522109897,20.656510624448828,20.096689421256738,22.83710046840733,19.824675810238606,20.823945313943256,23.256137551486724,23.633181830866906,19.043844337217745,23.65829637497728,23.50747794569296,21.73683233585413,20.94197381362215,24.18225862695574,20.14794970691058,22.344681402352435,20.447944597167705,19.21271648764287,22.508853453023672,21.75045501313216,20.060863127783303,23.041170672161062,19.554654657325607,21.26551890511897,19.47575297115503,20.304093087106327,21.96980947524255,18.890513667640615,20.207528404900692,20.825342365179193,19.58596744655146,19.86547955920327,20.093122217042076,19.51138012046539,20.89052275919248,19.726434174485984,19.26180569731426,21.223090796665396,20.071086524480997,23.60048678836413,21.682830461889793,22.115896159241377,23.245004397316144,19.929597223002617,20.206849822052806,20.80607598372765,20.972697281096956,20.548932178877266,20.790175259909486,21.65577788956451,20.396533708238668,17.677595133772794,20.206889160267256,19.864164508631692,20.604884938737765,19.69979302428942,21.112079923463828,21.776216944868956,19.41019874513054,20.084542414131516,25.538294718669484,19.887154874828134,22.02600227823102,20.06242269100319,19.908561207609715,19.025787914622292,22.519912388376707,21.15310774495207,15.434839951572775,23.416469489833915,21.472713147192902,24.799241151178187,21.565777300167408,21.24511744232927,23.786142675104543,21.775846548503456,20.874095085342866,20.525903087310763,22.190296420947053,17.526091919071504,22.787613064091445,19.95220428650718,21.56191951932416,20.302727045932432,22.39076356657128,17.788361488629835,19.89495594402764,20.849863964556313,22.01047671458767,21.9419703223203,21.769327477492673,22.134078601621123,23.02213648046312,21.659326126254854,22.503975157123477,21.687400780796548,21.299913824117656,24.41218042654058,21.628416169590647,20.661666774415252,19.13829898996108,20.6534474504661,20.599627769624064,19.476347089125902,22.633338504709375,19.004867830543457,20.729309561171004,21.215021611787833,20.55962915472571,23.96741801447904,20.948249582299137,19.034944006994706,21.25562508835354,23.6936808146177,18.876298103633747,20.12057913956066,22.988276940898057,20.761017121909465,29.702932107840674,19.34717798381203,19.04780522125321,19.72022056098374,25.87586915107988,22.381251164844315,21.45471998887307,21.894206057556605,20.997149152437558,19.346653316091935,19.69023397394279,21.75892392666085,21.695257182632144,24.304938166770185,19.96312680068985,21.08058139373211,18.457821713581545,21.058645462338415,20.370456476417175,22.39756222611425,21.96355600303241,24.29338064398619,19.533504643608662,19.24597971762759,21.061479969798718,19.653131971320136,19.991828535916927,19.41354451956652,19.024392506006556,19.995956185326282,21.080207924079772,22.895336936087247,19.00379513256332,19.59081933226322,19.683155687698772,19.724580465382388,20.49815774428782,21.41646356230674,20.374729613814893,19.698149844649336,20.827570650601533,21.55980522344904,20.203106297950406,20.998113148879906,22.5464416348448,21.94562707940607,20.109117749689304,19.61600010557632,22.28815948936714,20.00070564420032,19.48773603183014,22.96164822427184,18.99960834234589,20.932965503614184,18.548961097016324,21.40256127224899,23.343368321662904,21.395751936133603,20.367816242708102,21.629837063073623,22.815206783859768,22.704911104222457,20.7857551664019,20.3067025549342,20.61336030755643,21.781868242254394,20.56504163001677,24.58750691743097,21.042555130756018,19.718574682047347,20.82900495573487,21.89094777167843,20.46100849412487,20.676958661968467,22.376303078727783,21.96712491711981,21.09072172933059,20.373076483151994,19.53720891947194,19.37233862557095,0,22.656078929958284,19.971799365414682,21.138173662648782,25.881734625061707,0,22.042981377352834,19.12600868541295,20.929534739770034,20.604603448464516,21.393924057871644,18.99682101244613,19.31053914628864,18.986258705756388,16.7233596191811,19.687016476370815,20.364732076794134,18.828527980887348,21.061860954738233,18.824250825082274,19.28149315085171,21.023652845270533,23.394529944648035,22.371116977331784,21.19573925103901,18.93658618322981,19.94401710262461,19.47904998638186,22.850840758637354,20.76699650557159,20.40036143331872,17.795936296212627,19.421904563712328,20.64673096490357,19.639706353282477,18.775317705171794,20.047574770414855,19.61260734468754,23.666636204423178,23.480370137373004,19.218266855752937,21.60748880425466,21.257489419039537,21.101232900314493,21.320633929890864,21.849984955018474,20.40434121243742,20.032547562218568,20.035932572983462,20.864093173595,21.51301334156099,22.29182284846925,23.44987910374218,20.855425894539348,22.64951341150128,23.894186440194584,20.26686372980413,20.408744363117133,19.046718202293015,20.904214174162476,24.308788888311135,20.77592509493985,0,19.644171392658865,18.610921424553307,22.26726247442381,20.903586582709064,21.2082248307893,20.867431932395597,19.247883485213144,19.09858987177298,20.769205079804532,21.73660672493293,20.24751150890443,21.281868210231526,19.724170230709305,20.553728301262954,21.112413416140527,20.46206885900041,20.668149201097204],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_logkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors_log = np.linspace(1, 100, dtype=int)\n","param_gridkNN_log = {'kneighborsregressor__n_neighbors': n_neighbors_log}\n","\n","\n","GridkNN_log, \\\n","BestParametreskNN_log, \\\n","ScoreskNN_log, \\\n","SiteEnergyUse_pred_logkNN_log, \\\n","figkNN_log = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log,\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_logkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN_log)\n","\n","print(BestParametreskNN_log)\n","print(ScoreskNN_log)\n","figkNN_log.show()\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[1.9956779188754943,1.678755891384221,1.5922264852325787,1.5586907234808294,1.544780394468114,1.5379596356147587,1.5171105621914702,1.5143512459188837,1.5087046317512984,1.5101919985791386,1.5073420333266891,1.5027525164682045,1.4976182257935284,1.497613773764377,1.4985195977514987,1.4977862059975435,1.4974634941305998,1.4965473472417192,1.4954964709020087,1.496569635786377,1.4967901191560578,1.4965973703155435,1.4957243499833084,1.4958782739907033,1.4966490507240462,1.4977825553331392,1.4973968337789079,1.4974503423451158,1.497707311164933,1.4981701903730444,1.4981104099202116,1.496889211042767,1.4977612723889742,1.4974045268459353,1.4981046349332776,1.4996306614963508,1.4999704536214975,1.499811316792636,1.5006528266028938,1.5005716607135722,1.501087466844299,1.5013132300475205,1.5014076080948007,1.5016613573824185,1.5024841272217317,1.5025673909444932,1.5039841006676886,1.5041628398298488,1.5047123216813563,1.505431453821938]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.2918685055592687,1.8060443425981563,1.6886042387300455,1.656264458006242,1.6396801799770344,1.639546365120307,1.621500531648694,1.611012010307183,1.602452359527454,1.6007992634339419,1.5946616009917995,1.5936217853855583,1.5921483060929997,1.588150572316314,1.5868079142988096,1.5867211465593742,1.5850536804774717,1.5839337049016304,1.5807269417412522,1.5805647926731485,1.5803582500640703,1.578783747027726,1.5770501608669871,1.5765689505144784,1.576921528861177,1.5774719860933426,1.5758100379997724,1.575352019367359,1.5759818537783095,1.5757031503162382,1.5754849322025206,1.5743441454048708,1.5753169210365727,1.5740391363969481,1.5744688282260797,1.5757263153125696,1.5761518079803887,1.576165717193507,1.576635358014313,1.577869624443525,1.5783839932710655,1.5784157233109877,1.5781311898548573,1.578174532295703,1.5798764418148152,1.579666837577001,1.5810952783955359,1.580477927459859,1.582461898434801,1.5831754748947422]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[1.6994873321917199,1.551467440170286,1.495848731735112,1.4611169889554168,1.4498806089591936,1.4363729061092103,1.4127205927342465,1.4176904815305844,1.4149569039751428,1.4195847337243352,1.4200224656615787,1.4118832475508507,1.4030881454940571,1.4070769752124401,1.4102312812041877,1.4088512654357128,1.4098733077837278,1.409160989581808,1.4102660000627651,1.4125744788996053,1.4132219882480452,1.414410993603361,1.4143985390996296,1.4151875974669281,1.4163765725869153,1.4180931245729358,1.4189836295580434,1.4195486653228726,1.4194327685515566,1.4206372304298507,1.4207358876379026,1.4194342766806631,1.4202056237413756,1.4207699172949224,1.4217404416404755,1.423535007680132,1.4237890992626063,1.4234569163917647,1.4246702951914745,1.4232736969836193,1.4237909404175326,1.4242107367840533,1.424684026334744,1.4251481824691339,1.4250918126286483,1.4254679443119853,1.4268729229398414,1.4278477521998385,1.4269627449279116,1.4276874327491338]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.4365552834619826,1.7356680996815528,1.5828416163673893,1.5588447676872081,1.5440847403762372,1.5309629809273715,1.5035286906231904,1.508440608287731,1.4877815877788585,1.4890668446405348,1.4910142579822543,1.4920617148421782,1.4954423246267639,1.5010617321645128,1.5028555594438744,1.5033048552048702,1.5009644132063882,1.5000175317070934,1.5040512043299357,1.5049500120391026,1.5039855916504212,1.5046268617038787,1.5050757419484604,1.5039702302574602,1.504133474644565,1.5040053733861118,1.5037984905785549,1.5028761371800943,1.5025017247521293,1.5029383959046292,1.4982219335888634,1.4982507221677572,1.5001224648709437,1.4987500630425568,1.49959892532954,1.5000570523107684,1.500589240297299,1.5007006824285727,1.5012962166082786,1.5021585821739598,1.5027019549266443,1.5031142285419934,1.502824900219278,1.5032524097342315,1.5040305526047943,1.5038781067098868,1.5052576389486456,1.5061871417887127,1.5074977772728402,1.508916060010017],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[1.9361773511662628,1.7511245718619142,1.68444420797453,1.661152894941475,1.6509659726732537,1.6597113966824482,1.6544778650752572,1.6313673588033921,1.6168227877825092,1.6150120004624173,1.605300888272889,1.6024522938555148,1.5989071040407117,1.596769256708593,1.6028450032807608,1.60375387976829,1.6013565412093744,1.5989604330805594,1.5892397855918712,1.5874214498897863,1.5855736684328985,1.583505295916189,1.5824033271830957,1.5831331482190076,1.5835217483016424,1.5843317573376243,1.5794849857628654,1.5783369511357916,1.5785762538437889,1.5793310713569122,1.5790669195237175,1.577494345372715,1.5776061541407804,1.5762636521222706,1.575594702982219,1.5773421894152861,1.5786345544730556,1.5776914927052739,1.577942965768741,1.5777991930997612,1.5776200017360806,1.5776503265665658,1.5771646291543737,1.577126817178794,1.5785058402720265,1.5775186219417088,1.5789519014962627,1.5791865958994487,1.580676949476115,1.580813396962214],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.1126982985310785,1.8147605398094315,1.6839294559562978,1.6534845569815808,1.6344796433076838,1.629343468678818,1.6121075240786606,1.6106902412783508,1.6134565610807694,1.6103681585736571,1.6043302072095356,1.6002530710823615,1.5979808672758904,1.5875724327339995,1.5756981363843374,1.5724342969661114,1.5711921870184689,1.5715120972213443,1.5713634200977311,1.5732977759220872,1.5749907238875474,1.5734745715089287,1.5706655907560643,1.5688515996551748,1.5695852587197845,1.5707298810973336,1.571678959869254,1.5724113905440114,1.5739054757093585,1.5739721846161958,1.5755828944855037,1.576120149157135,1.5771898617985525,1.5759689161648274,1.576726886323463,1.578352042123806,1.578333605583323,1.5784796358147115,1.5792710215703698,1.5808898269740081,1.5823523000887652,1.5824331234667002,1.582129252073431,1.5821773661222305,1.584106127550555,1.5847603502319425,1.586382654570738,1.5843244478696812,1.585886080518193,1.5866979081110777],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[1.9732363679808553,1.6431956937682317,1.5892508051388823,1.5256873480777275,1.5048859970443846,1.494594593848828,1.4428960263468713,1.4437273816302132,1.4475238749327497,1.4548306691529764,1.4561235882752344,1.4560408533842528,1.445037536859139,1.4488101902777106,1.4548275904529746,1.4566428248527552,1.4603603402771759,1.4595493524587642,1.460418583947411,1.4614261179930719,1.4630822473549978,1.4630604094536634,1.4614093799131036,1.4636079338809074,1.4644914725012714,1.4657646758425262,1.4669938116628258,1.4675460114303298,1.467599830738192,1.4653804309483132,1.4678839112802606,1.4622655259111816,1.4634295821822445,1.4647510178334213,1.4669937109098126,1.4681797495708748,1.4673137074504543,1.4685361170898343,1.4695866250391847,1.4699730067368264,1.4698783759808969,1.4698058831872534,1.4709872226069864,1.4712299585026953,1.4720482809150934,1.472291736503808,1.4733264911337287,1.4740553173937392,1.4750972138177936,1.4758652402026864],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[1.519722293237294,1.4490305517999753,1.4206663407257942,1.3942840497161555,1.3894856189390108,1.3751857379363281,1.3725427048333716,1.3775306395947309,1.377938347181605,1.3816823200661073,1.3799412248935328,1.3629546491767155,1.350723296165137,1.353855256937069,1.3563716991955463,1.3527951731956904,1.3534439889415923,1.3526973217408351,1.3524093605430938,1.355752823087837,1.3563183644544246,1.358319712995058,1.3590677101158182,1.3598284579409659,1.361513299452968,1.3640810890021007,1.3650279210210396,1.3660812214353517,1.3659532707811965,1.369228869039172,1.369796390722713,1.3703153126050458,1.3704582989523504,1.3712889850666008,1.371608949121354,1.374222274061019,1.3749811603033562,1.3736486559247874,1.375167304027895,1.3720376945833053,1.3728847014891081,1.3735625884750897,1.3739320364199348,1.3745202353741415,1.3737298347661886,1.3743881393351205,1.376001817189068,1.3770606961976632,1.3744035873218394,1.374864663823695],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour les paramètres de GridSearchCV\n","FigRMSEGRidkNN_log = visuRMSEGrid(KNeighborsRegressor(), 'kNN',\n","                                  n_neighbors_log, 'n neighbors', GridkNN_log)\n","FigRMSEGRidkNN_log.show()\n","if write_data is True:\n","    FigRMSEGRidkNN_log.write_image('./Figures/ConsoGraphRMSEkNN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                     464\n","1  randomforestregressor__max_features                    sqrt\n","                               R²     RMSE       MAE\n","RandomForestRegressor()  0.349607  1.78466  0.782007\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_pred_log_logRF=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[22.2365592943246,20.480031458961005,20.458855013878996,24.364439257643888,22.066893793547187,20.24731104090283,19.459775133530062,22.710621741302663,19.620828174024407,26.060207097554972,19.703879453500345,19.62877826958129,20.641491321395723,19.77669894455259,20.829677646174847,23.93142942376183,23.619445050279786,21.833494269951235,25.961787383485383,19.814790732054337,21.380807402153692,19.51531864130463,19.36439857844659,19.49713325732011,23.209374148037114,19.645581214141636,22.09925730580023,19.730868933100986,22.03054238102219,21.842005895268088,21.134296599229234,19.477889084112736,21.723566465677845,21.064910412769603,21.872582898822678,19.535537008597082,21.871944697679837,19.542366764046683,21.04950110673067,19.918610269781425,20.947007669422145,24.479695861929198,19.563268012001448,23.051107120124662,25.04854881950346,22.477325465996806,24.864173738846613,20.598521742518233,26.412493464365877,21.817396510508605,20.120098659038696,20.45808821961716,17.88117965189683,20.14548917762981,21.29004776859384,20.741304458877515,21.454371202274434,21.14145019621419,23.70947815254689,19.596326914787987,20.521196956510522,21.31058523344604,19.69225470706312,19.84685213327146,21.716683723675146,19.117373473841745,21.710840967824467,20.993711355141162,19.603188683238294,23.62032768503498,20.46368178873967,21.169145035381508,21.371818323119925,19.004699479760056,19.63727636261473,21.404498967940057,23.518920435992406,19.622923992270188,19.39691347904255,21.104203732254117,19.368526061513837,20.81300329294566,20.40380173945369,19.63335570172831,11.623033139539979,21.72458354279518,20.30614696814769,20.078140715954653,20.90597149018253,20.490153324316665,23.014225624538735,22.859610983699657,19.351013567716407,21.114860010777587,22.523552329557226,25.647218780930054,20.901965847335443,25.060225785286587,21.599839459573552,21.612870601741232,20.791197040116057,19.40535623858245,22.77303234535171,21.403576482149532,20.458247640527325,19.850905312828765,22.87972799530416,21.524394190766124,20.659175468732,21.136958818195925,20.572092377733163,19.68014499795873,24.400817614121515,21.5287100706176,19.62919243184986,21.528659439028377,22.899416430337347,19.638389400271535,24.09515834864982,21.47009273833463,21.00027870536398,19.712476973719234,20.80669083180646,19.397476570166752,18.978834108932094,19.4882600661419,19.651602958747958,19.98098033780415,22.005038794603124,20.855243229665408,19.468131774593715,22.35197109111888,20.222018516993998,19.695500485423143,19.601653551170998,21.459344012562994,19.287437980163297,22.299694576726363,20.654770131811816,21.073908854997413,20.190331784977758,19.91516073215726,22.842755632628336,19.56624468253808,24.90196732600775,20.708389598785814,20.3541859336633,23.71661409711274,19.085044190737428,22.76438984585223,19.514154957147156,19.573356734077073,20.74529013827652,20.44157279751536,20.45635203040184,22.042815811444296,21.990960601650336,22.03520921513142,19.39738544861002,19.706864884751795,19.712908864104186,21.18853228833887,21.09327916748316,19.851432805091914,22.434846301263466,22.170739928473452,20.43986140188131,20.189245254489553,21.49246610710222,23.53120915832101,19.291712151926923,20.63146984021604,21.619730922737297,24.007430058752615,21.312131169069172,21.72144729472613,18.774758711912646,20.181393569308558,20.095581841912466,17.98143383811516,23.122657764699937,20.106733467238932,21.354018544372163,20.036352388534258,20.54976100928857,20.37559409221155,24.485113031334922,20.77359749547807,22.45800333121813,19.6068472441545,19.49168774267765,20.935059032027755,19.61070396452972,21.86420384564998,20.105368856362716,21.953140278248462,20.384899225518577,20.368081954374226,19.134096868319467,23.361916452622538,21.69783841450416,20.776071889190373,19.50507484131605,19.793847339461976,19.633075245493906,22.812015017231058,21.115019822235787,19.43527545960534,21.037727448774177,20.432375925609442,20.997711708187072,20.9797110509852,20.146259868727903,19.93023212883989,19.816982501560748,20.19192067829799,23.791240432886795,22.231439238468617,22.104938808133696,23.798713751171594,20.981787895437193,19.455525961642163,19.787798187071022,22.39080098033494,20.947004497551347,20.465280451827685,20.298751823072596,22.787446952188844,25.248451562211116,19.579384625340904,20.9552102120015,20.390793604015435,20.717366094564817,20.61343044193443,23.373438162343056,23.437575344856334,20.8052644483252,19.19265388930953,19.80194538771755,22.740309598061163,18.927905831606843,21.22704524459275,23.468108633609265,19.447544430312618,19.94824401358469,19.680532446834604,21.54213218296562,19.285333880803908,20.114964150264125,21.516937441170256,22.789759689444008,19.490657693944204,21.0602276634631,20.252761980127932,22.689750426109327,19.71284982644051,22.763423771868737,20.502263436243087,20.933303059198504,19.61545859107182,20.511069298837704,21.108817810967807,21.89173536425926,19.207144100833958,19.10266543990124,20.336262850103967,21.185799293914787,20.68874980974958,19.956685598537895,19.38429741407963,21.12865541698311,19.740711831299226,18.672261613253042,19.68604588325621,19.485107883145805,20.91105580310707,20.45708904459211,21.975464437346155,20.457062005089195,20.057154198556553,22.16094624845558,20.44706998585741,20.083467915308603,19.031687531620953,20.4096898910628,21.378732497143822,23.01803212645101,19.0417588477906,20.407033934802786,20.61582559596798,23.319705920784298,20.24380441389864,20.93037241149282,23.555205179783012,19.775055840481127,18.954164030028846,20.23284948389994,21.49648382082606,20.20716296773149,20.808720176392903,23.145074823095836,20.81854732278104,20.13203896496716,21.618383583844448,21.61143768379296,22.396632473124285,18.64642359762636,23.173736813717856,20.259455127094146,19.82439823844121,19.72841091005552,20.34692133680915,20.067035993394022,20.35870790944598,20.14373307850868,20.54205402313969,22.992014378087212,19.780567431525576,19.17836336536449,20.43274260543632,21.316989637711437,21.122543488848386,21.33067515124036,22.267863586739242,20.280661074794953,22.943933469138106,20.619541076472927,21.93027413847825,21.49890291013605,22.88030901922125,19.775069368699057,20.121695364570776,20.25635012669861,21.224092582645124,22.38975460834681,19.497679478225624,21.84106804114607,19.43952691040375,20.611446845157527,21.633490845792597,20.273275761472703,23.908016837862927,24.181498380973153,18.904003172702033,21.024496071525387,20.966706423217467,19.909232472425877,19.96643841832348,21.744789917608497,21.831311303666258,20.612836980505893,19.986721303211226,20.60667574660124,21.51182291539642,21.36967707481728,19.66734767723628,18.238617370103658,20.3224200273347,20.02843563804375,19.486937458477968,20.484178161490924,20.25642289810522,20.81371006018869,19.604928065781994,20.284665632177934,20.67528945047146,19.729333547458484,21.854843114227506,20.14543017735941,23.439907914920262,19.583465220730826,21.36091341807566,20.081881786273776,19.36780647621798,20.520845095941226,20.012064884602854,20.432382984653476,20.465026591648098,20.572396257844023,19.387776184851532,22.28612089950343,20.676586795106548,22.8261707326932,20.009040499894986,21.052818428844258,22.239647648604596,20.902860082976325,19.39309956925155,21.490148168025893,19.098343802207385,21.824189891954322,21.110286561770955,21.128785096623297,21.57553426179939,19.424100739743384,23.13637269534899,19.90330744372827,21.364005239859548,20.69329628038559,21.178999409298065,18.894440262499153,20.15777619297087,19.086574821295134,19.65648215939647,23.976171168942646,20.36981067010708,21.220138775458704,22.854835685795504,20.542953984858855,20.124274584181663,21.420188718124688,19.631534635309443,23.84670929973615,20.15433993352537,20.562223222644217,19.606177881465815,20.071155366651084,24.768751049267003,22.06616818625884,21.765387370537383,19.800756396123642,20.824935021335374,20.571302773806067,19.503379575550348,19.75128528507316,19.878040955836813,23.29483897916551,20.43343585865395,19.970378428254097,19.969069601724637,21.2560795677993,19.44687078442524,19.461335889967522,18.202051855116984,20.120099182749787,23.313766955994527,20.432315177293184,19.70183276280529,20.76615330641956,21.173894444950655,22.006642454500316,23.867650221093587,21.900418310576104,19.665914475126833,23.88962586249875,21.71126992874862,23.24139626925324,19.718454393719647,19.59368826949605,21.350694476349993,22.403519412702686,21.17641076860783,20.92301788375019,21.57749601774572,21.33260470434538,21.139647162042703,19.365731450227454,20.931930626768114,20.62478488196739,22.61122030473027,20.14645913584661,19.08276046679455,25.15721321332523,22.08461325175234,21.523129074609802,21.627606063498778,21.433909769696,22.59112133274698,12.026571369861305,21.10129694074736,19.737730339582498,24.94716576772993,20.500050648022764,23.63011037864007,23.523390596073362,21.012171281739214,20.503201876749877,20.804980223148352,20.661725390041806,24.582639985640043,19.971339359354523,20.29537099007131,22.063549820726205,20.91733469058292,19.561032211529277,19.649487263445703,21.913540767248065,21.421100362716643,21.308748820036065,19.507719500871158,24.04444264170123,19.564923032280625,20.455725841971343,20.50934968430861,20.922938487411923,19.037563680504512,19.1412842567068,19.699070481801485,19.47416285378677,19.272633252456547,20.48480270118782,21.05555718916192,19.30209732830581,19.98735977715266,19.18877222673314,20.389838797919563,21.217648978305885,19.88061444575814,19.759862657369037,19.65254686503757,19.5643309800528,20.081554634361204,20.025095876400364,20.473503171884918,20.25633091511956,22.44210411333774,20.45189823241548,19.553706375591727,20.989419749387583,20.743831719963122,25.81435223273563,20.47439739675606,20.73011552776116,19.595083065692116,21.047666424098605,22.092267007923297,20.512173891537476,21.047195466418884,20.406229592224584,20.182746725137246,20.2342806019146,21.338272830957695,19.605942193970698,19.940727320612993,21.92967901464881,20.5473977334672,20.70375321011199,22.61614899112648,21.12352596694062,19.935677095599118,20.414655255708958,22.142709337421607,19.655957332773063,20.170505940942604,21.419766614906,20.512515686593748,19.464228242250957,21.966767448777734,20.548548906772634,21.863715735732022,20.57654763666959,20.634050325023498,19.651602958747958,22.55048640468412,20.08177661530851,19.04756230545327,19.592502440118036,19.327350826892935,20.368543924210034,20.567448820223483,20.914802672522352,21.568996956714344,21.603866794399664,22.87710225492418,21.834567175567145,23.220250286831398,22.51135385045085,19.676957209365675,18.986574000802424,21.69760221460726,21.43294654756083,20.511927884965328,21.74299527621655,21.945979353703052,20.510904398973395,19.51709388915447,20.503920688934166,21.05702359059354,19.362393321268197,20.479715534128353,23.342511798832703,22.73655748291558,22.36920507283907,19.819715750159805,24.133047472344362,21.45251652536331,23.779505187407768,21.015384964510123,21.895039396032637,21.362867532603968,22.151094708836965,21.34972464934717,20.732973684874175,20.109606195135477,20.381815158890014,19.71371234021262,19.56735279944077,21.480070370856033,19.791495001567803,18.5831567508193,23.26070140454708,19.88285042362248,19.486220549331914,20.03677763133293,20.98735757906579,22.438015604356295,19.794105910984943,20.102094471955862,19.407189583969654,20.58202421845618,19.603147420679342,20.32056835283193,22.40342864269293,20.504580549593282,21.511892285902327,19.672173652913568,21.09462462847361,24.741186839809505,19.2277213919075,22.63548695613437,19.546829884076175,20.33912043340399,21.19403081569259,20.940058666490287,22.347836404745305,22.580078157818892,21.121116001907218,18.964574105258233,20.455911801154794,20.878280438219576,19.718316358105184,22.100075269125135,19.573335939686082,18.499178803890178,19.71372503738948,20.1065407041059,20.17874907630613,24.154507065163493,20.03107455557932,20.785304381615074,19.82172392396909,20.011139277466253,21.100775576482206,19.08211809097811,20.242276143239522,20.120098659038696,21.13869175101342,23.12988921538639,23.90824577519125,19.76310249282308,21.099339933996387,21.986821017987257,21.783603037252636,19.910472039190413,20.322541441043352,23.832733603099033,22.398803669062683,24.36371338540339,18.95668753207947,20.080259207688833,20.01497818240321,20.54747383033489,21.758256045336214,20.571481269950027,20.295870321538136,21.317849700650815,23.247983723403284,19.34923541051239,19.67320919752108,19.99389759273224,21.611117349745157,21.488239600389683,21.64928203030754,21.54183135409095,21.660610121661012,20.621155370080327,19.78617803540702,19.259190944408434,21.406726662373597,18.93158732601015,21.025663270547447,20.736758222193604,19.567068866707142,20.624683021804646,19.639324066684193,22.69525178028068,20.85554091476957,21.39700476602799,19.536135739929584,20.03020990805242,20.181611884034723,19.37838062011518,20.029142234988566,22.903932018158716,19.54000848304532,19.98523305574904,22.55627600506388,19.935986873148337,19.56198334061722,20.029692577059528,19.42269011187141,22.240658037855447,19.46713633912393,19.610213909744086,20.657625657351037,20.61699159115971,20.209149357651174,19.809745933081064,22.073552782068933,19.719000094158016,19.91654932088995,20.155810986437295,19.74076965504873,19.253288622533418,25.340073022691204,20.191731912430146,21.61095739409516,23.990957718006822,13.71453519205685,22.509777978353235,19.35247841362939,21.807775699556828,21.746196159116614,20.926287057714653,21.409194052885997,21.376216017850776,19.903903631008944,20.28957737969161,20.3263697070072,21.125740135409707,20.145262828082664,21.965173701595596,19.162664094947964,19.754852038771954,18.775698896718687,20.033599179492775,20.8112220083677,19.496032827431737,19.65541687047436,20.630847149569064,21.968611127251304,20.900693974067448,20.15530560518836,20.00943442764282,20.2546389789509,20.520508840714566,22.309083317385454,20.793500751724757,20.276878215598433,20.110809477158803,20.112771351546133,20.899360245857892,20.312554485368924,21.55262273099966,22.93928989166726,21.53858199626856,21.936614766183,20.29793019857315,23.177840443222678,20.599326014484667,20.136162827156593,21.30204846125918,21.38688667163263,23.57109433956676,19.387865211003565,21.67401608053743,23.92580487320184,25.192192101668304,20.734226538564243,23.845859537589003,21.274407503295922,20.0186346965007,22.816220508504294,20.604422941849528,19.67646280454498,20.906119166056236,21.660109943606635,21.74393117554687,19.818395327420237,21.210218319061546,21.231407728705044,20.508768763389416,20.64838299892041,21.751430456852642,19.357753003915573,19.49286321784203,23.421594585257925,24.261802741186678,19.511709274034036,21.836466989331196,19.64989112550547,21.462657750435888,19.060913541106125,22.43938130472578,19.98490268058451,20.28961571815882,21.014190857086987,20.569220593006474,18.113999113345706,20.00114325303892,20.331873495942407,20.706194971169733,19.57752422905526,19.0751334531936,24.664262290674785,21.42075371905639,23.03255088748829,22.322581127634194,20.998782788751445,20.355016584687437,20.973005775971203,22.776882000475748,21.406510216416095,21.15425448910366,23.54059546601712,24.190644175058853,20.104506344958203,22.569262694669565,20.481409022814443,20.871575706521178,20.857278637733764,20.683513888841418,20.56507559696005,21.235097435291635,21.444680147268464,23.811518040034127,21.68817706783811,18.915836300666435,19.454565974397227,20.978927905900374,22.811737055640446,20.41965269304179,19.162064815053274,19.364509538144397,23.730938600816394,21.335971230632996,20.02799647158755,19.85333916748212,20.83974435568885,20.994122938068514,21.615083659806007,18.891598103946148,21.361277327527425,21.904842297259606,20.16243899984633,25.20286797760542,20.73145090819052,20.244583424487654,19.819465690332386,21.809747472398342,20.89593160053501,18.844852413566773,19.858467337839716,21.632314809445507,21.146566979958468,21.39888467809422,20.939990980075745,21.330521790860118,20.36034697176532,19.552285505538688,20.587172225763933,21.18525722083546,20.03970628781695,19.341043390738502,22.0747559282545,18.99581362041383,20.96838986099437,25.30379876376132,22.152019064760193,18.811550238659414,19.657005493103117,19.812372919564517,20.561287911124854,22.074934425168173,23.418153834367754,22.828841466548177,21.315472775157012,20.383519091441652,20.52448372597414,20.617725002540958,20.079382360269094,20.735008718406156,20.043795779014506,19.88720915171444,19.473846983085423,21.721206935040495,20.02398219352984,19.907798988715314,20.040774465113493,21.03856728232721,19.57645665383442,20.972972105373266,19.655227938329563,21.08162649539281,24.54984655272116,19.569455819361654,21.839486327223035,19.36249287349133,19.590828358856772,21.704558674588608,19.65143033134676,21.11731430247875,20.20760157923236,22.108107547835292,20.272741983658843,20.775800211953385,20.7258695708491,18.548464944393984,20.451438092199336,21.161649696630672,19.48597356223369,19.92592065533371,22.141780762524082,22.418260673288355,21.075190995158636,23.44357225991868,20.46373680360947,19.253401915785314,19.90170611046321,19.413226759213043,19.839559575178512,20.717687544417544,19.043415964989848,20.45245695607434,19.50813137960143,25.389029477734024,21.932181685779163,21.345648788737495,20.426519635009573,19.364777096129618,20.20931908938645,23.761612536582263,23.878018457848185,21.326673890709344,20.743514793438703,21.99413360971448,20.32654458279221,19.736184891276114,19.100817359935856,20.77364820767763,19.959404644788318,19.000925304104914,19.104908580372932,18.459865963970643,19.40933119746291,22.598134964464847,19.663516011043015,19.18863535413577,19.65755350701112,19.46941113532517,20.082063147065472,18.97231699954742,19.690204081558015,23.61916265921261,20.358978232829532,21.386461236797647,20.82993905711083,21.140484325653574,21.755423961448148,19.519800874993166,21.30218369991284,20.543199308495478,21.10958064897039,19.423690370709615,21.844848260267568,22.76968732948857,21.486564059483335,22.241760013631293,18.35648103209936,20.517493784374558,19.914986105075634,19.033191764004705,20.12719742061363,23.484676878943983,20.86512322797042,19.237648256809397,20.512035545106993,20.105868759556458,20.388572472847347,19.923469872804088,21.64074427649693,22.5043435667653,21.331489430429652,22.280753439891466,20.851073992312433,21.615547352172566,22.077442975979416,20.926609509488888,19.774270551353638,21.95427574561551,19.813982598120923,19.522183025622176,21.01442706231656,20.046407782189338,19.72349533239933,23.85591724717417,20.098888954779717,20.61711273840367,13.738668732090462,20.222499622691043,20.385548745441834,20.058524611301255,20.35461911466094,19.985480152818823,19.976118632884212,20.17436143099321,19.555759300763018,20.89818753963973,19.734209708263286,19.957982559217577,19.888249238636593,22.58185183594864,21.263500063002883,20.978536795649383,21.94510017242028,24.70938349999173,21.96393276806796,22.43130862250358,20.951566028067365,23.09986377384106,19.511887397101106,19.274533089368536,20.619815878795563,23.981361245330866,20.886151693604436,21.79775700012595,21.683267685860784,25.979961655619324,20.333180371039187,18.678450744583344,20.295314647705208,24.007420284362944,20.419079789482822,20.596900718690588,20.56783686750474,20.154405033614992,20.716899766053665,21.113078020485244,21.131330482156915,19.826472079728326,24.246798601673262,23.539555922800282,20.306645704701623,20.510508237939185,23.23761430019023,20.366688199527633,22.327730758260827,20.88298244830599,19.689625360784067,21.209727661783937,20.371496179731146,20.162947784694705,22.79751845440104,19.35883800053813,19.62316102791195,19.685351336750006,20.56577340333896,22.266009803128135,20.705353884170453,21.241320773025883,20.723609948680984,19.384763432198522,19.936973721935118,20.0282869147644,20.442270828346484,20.66620154350817,19.671874951852345,19.940466572235362,21.214170605405563,20.067994030712843,23.93238123075019,21.481012856965894,23.19180171244575,23.623931223303565,21.23704582983196,20.085936546651507,20.61619666238944,19.93470834904774,19.78545547179317,20.940488313457085,21.614034578668058,20.473613427332896,19.521735762848717,20.632462817585,20.03111076208181,20.12453634092169,20.380862152144275,20.769879630552214,22.119989542760944,19.44305977389001,20.346940699203135,24.598764396382816,19.532347905464537,20.495139230729073,20.254350230836682,18.35159198188545,19.879759350827886,21.9210540566641,20.68457307521431,21.00874007196982,21.984235409325265,21.97360080849044,21.164637100929163,21.518771457124696,20.623867736142998,23.491201060036918,22.649357191579007,20.232279738024136,20.301948485646893,21.642115818666237,18.779405996093644,21.22210178566901,19.744684950202828,21.105459180296776,20.00378543912883,23.06019123040009,19.772643366321002,20.171874510283775,20.847857586182023,18.427874898533585,23.147078397118314,22.131295363278067,21.222727571536208,20.859596489039337,22.381352387258314,21.232084959757348,20.81003685073171,21.758801587307794,24.22415242320519,21.73283417786633,20.34630676491097,20.342895521849115,20.558765969547775,20.668714744939113,19.370791092328137,22.584835814417946,20.047362405569586,22.79676409703167,21.501235265928155,20.05500479413263,22.16613577976448,21.97130631093039,19.408063948751046,20.998721836554537,23.724716920945617,19.22355181759946,19.493548633066254,23.879679935351795,21.55635960574648,22.752547483300592,21.21113425451199,19.379487899327017,18.832152589585576,25.196009812339977,20.22212176768592,20.636408250672655,22.158314544751786,20.464301272589946,20.484319660862532,19.458251913236236,21.926171196904306,21.27066689015961,22.837535274417032,19.744871031352478,20.31603740953387,20.350582213719317,21.471324899168927,20.26454933574609,22.763462145047697,21.172705845568252,23.510508063382314,19.48837051234833,20.286223530205994,21.28954715937177,20.92561990089597,21.33881962928461,19.82408368057546,19.211139150793063,20.115001619807305,21.07444006124077,20.537897000216496,19.39136715134036,20.13149120584638,19.832549117896743,20.072252384882862,20.23045780307787,20.16211792378075,20.098770783457173,19.73791118344172,20.07712214734757,22.456408735079904,20.618800370557334,19.47818740648093,22.357408697701683,21.580138649866964,20.57520796678564,19.680431300640837,22.25224445032471,20.183314479570917,20.036238854728342,23.139044868034958,19.386144900374184,21.52656597353561,18.892030262599476,20.615157777177636,22.21729208359878,21.670850584562523,20.36927554212764,21.8139836653052,21.838429768331277,22.672805380792198,20.64367770650817,20.807171455140132,20.455365392136528,21.617761273183177,20.8314157056926,24.90462786224165,19.835002523451184,19.587673102477513,20.19008340624039,21.028265375242757,20.736185243979627,19.201628860213827,20.441178758142804,20.59086517344795,20.102712197480752,21.726419499846813,19.908379031741323,19.544895673257383,20.47576769392615,22.574907917480417,19.967498504775143,20.913140481913377,24.922290809589576,20.30884242555516,22.019698016200017,19.351078699779364,22.268828527785036,19.76737136699568,20.648592550209923,20.31132305683289,19.991284577176916,19.432331785655457,18.732768831789407,20.067314802780487,20.791555474292576,20.893457756190095,22.036717794777484,19.285795172303665,19.935870323375035,21.96958725350251,22.919920123548284,21.8070175969817,21.044622004022177,19.56911790584676,19.74506254473687,19.455987218736446,22.862129727774526,20.33263688142612,20.956109055946733,20.156173624098418,19.152768041272658,21.06981764911759,20.745641538647316,20.49878155584812,21.941260872115855,20.446511874963274,22.73555741456136,21.18715972332738,19.709529757168216,21.85057905219436,21.594918009666166,21.31317449766844,20.82865615967791,22.186699782562023,21.08162798730648,20.19108419353115,20.596451883081667,20.270243300175455,21.648521558679747,21.43712282169316,22.98044903278527,20.513674311139372,22.816566097714702,24.31359142811577,20.4525179136944,19.60121853498919,18.971146704581383,20.64855408776365,24.32739191779286,20.431838072576987,20.999919124709784,19.735703104677558,20.282823960445697,20.29030763732059,21.017176796340713,21.484531269634363,21.473238782250206,19.441028017209412,18.856098121077476,20.716383193172767,20.813893779729952,20.308201154246678,21.819758779461203,19.601895065160633,20.359367743239385,22.193756981055504,20.868137086212258,20.178033851161977],"xaxis":"x","y":[22.06759946491166,20.51569310181126,19.61997633871636,25.808363100543477,21.795376715513594,20.34038041027431,18.86081419340691,24.078413808713293,19.082528830592434,27.09570141066156,18.66432922183982,20.146924115160264,20.694537918678005,20.34827538264879,22.1287945428413,23.512029578625103,23.710849830139047,21.16813940491307,24.138707165857692,20.413583384325577,20.971944131576173,19.330306789697467,19.240535149076692,19.593384739494034,23.207168669114285,20.243485920599927,24.816169078762734,20.239179241482244,21.93825114094642,22.544699771649725,20.581001696999003,19.006769325349232,22.852829884665343,20.97322158881004,21.37041284025584,19.8813455186455,22.291944364905724,19.352118206703615,20.690574372087426,19.616726513482636,21.73777568014623,25.92407619036082,19.62630225708032,24.328668011629535,28.119486884464127,23.268620237666717,24.937914667295647,20.538966591972148,26.49472805759287,21.551656870776874,19.967005268430295,20.53263058829885,19.077522473826907,20.411222614242394,20.118559661298907,21.75794824506378,21.969944344762656,20.941991359524017,24.30144839882211,20.08875201356742,20.413158139175476,17.21138734793915,20.10520261649224,21.318516744600927,21.383177447413818,18.901120542977083,21.140418528305776,22.317825778768146,19.694551733623157,24.452943059423166,19.34725801943171,19.360313431748278,21.62865690143549,18.725486086935668,19.22798647641259,22.00766259568964,23.581082748790354,19.483945640419044,19.555879359056263,20.81759187582515,20.004050690864457,19.7027197284411,19.957005833266606,18.935012496616167,21.132460035639845,21.104713276655342,21.0706503248077,19.995538068644624,20.45373482710394,20.377850211599817,24.218899085058613,23.648191447276112,19.27757204338643,20.547353262390168,22.570203152455143,28.033188230056272,20.740147099081785,24.844950181647548,21.11139041842472,21.618102712310883,20.71863237230781,19.638742582781422,19.346013720701006,21.12828261562749,21.344903224909334,19.828653485294133,22.842407702796528,22.86005581934068,20.23340552018178,22.244828354517683,19.8468416379525,20.153224049031966,23.16226531069458,22.47425899129698,20.440974576504654,21.34321011850724,21.96825774467718,19.581570228112746,22.97307940319244,21.943790024620068,24.922685338201692,18.397908557303648,21.170757004644493,19.439732238591404,19.06759979313172,19.78225599625029,20.88710307988011,20.06163488065717,21.227834779401228,20.925901548056537,19.64120627043042,21.51013491039007,19.943833745099443,20.041179196484592,19.31991683554051,21.305505708300736,19.083972676252298,22.36486950758078,21.00153722158492,20.91944021038121,19.71891096707218,20.43515061037962,23.756601825433552,19.29612083176196,27.288645329988054,20.004627845911102,20.581937941283943,23.06049210432163,19.0283827986455,22.26292897661663,18.98289333455679,19.08654493081137,20.26723932452398,26.772642502942784,23.150894170391226,22.789865927709045,21.316932785410458,21.47925775839002,19.52466856143405,17.31416721288844,19.67390026636621,21.280848347151327,20.575951881264743,19.36981390700933,22.906270958120995,23.259273464150183,21.034942664057727,19.478866371959384,21.521316718605924,23.251480243552532,18.947642644803835,18.234449818345436,23.74556559722114,23.70046067341344,22.17893043543761,23.314842649926593,19.204628901280135,21.028986544289005,19.811383905702034,20.105414612220844,23.470671408984025,19.185541351838207,21.863096949082852,21.439837984160558,20.28595076567891,20.09465504412721,24.347469240771108,21.026538976608304,22.25115601355335,19.17189511189754,19.330727902738385,20.455200776646638,18.7812284253763,19.9984664758684,19.71179802639097,23.222688495940556,20.75461515785591,21.453079563108336,19.968121134960665,23.31621219099147,22.229186273778243,19.29157093266521,19.725980900417238,19.393396349595932,19.510476738006748,23.86936283890287,20.803398322957783,19.50445726786541,20.266025864180154,20.318803159054653,19.88287753302219,21.755410336588376,20.021014007950576,19.81464925130666,19.600081112964823,20.195578868662224,22.127386699288984,22.615494713832,21.994462034082755,23.008367409641956,20.02036350615788,19.496913617064063,19.65284705132002,22.970383373621917,19.800442868861996,19.544786543412965,21.196126618068195,22.706377974056313,25.14188655199703,19.430343691015025,20.82549425312562,21.300868648362627,20.935011057360228,20.238709984208988,23.361271159574002,23.97311707542301,20.93275451807793,19.135842027978637,19.419925212472517,22.69657897049395,18.04707863542646,21.639091593296747,24.902956648700677,19.472341179018663,19.630944296038688,19.581570228112746,0,21.279464358226686,20.6373536565784,20.55168503037007,22.252924089212918,17.011336466938804,20.539904719671835,20.287228770242784,22.579718287600105,20.016747429777542,19.919862789535355,21.590649178257912,20.459085985918723,19.305685493406397,20.69934199839951,21.814001605688915,21.49724409211446,17.96562762596234,18.897059576214055,20.218149025767367,21.663202256858984,21.537426968242833,19.881144027962787,18.109927551497627,20.650313109616484,19.551649361459194,18.954616641321604,18.33673536238825,19.272777840955303,21.22721049508145,23.36907437848721,21.839279632705356,20.080313988880153,19.791725695449298,22.830034474576443,20.038176858751854,19.882805718288704,19.137324212876308,19.89611063211052,20.922839608419043,22.73108571789678,18.798041102461394,19.809989451392685,21.628482474975726,22.66651647870706,20.105565995835796,21.324104166055236,23.33275256138136,20.80501366619404,18.787924871827784,20.381292625866948,23.456138303517488,20.59828494694422,20.938551777966236,22.709886920964298,20.558379254361796,19.758613410846518,21.618914296187317,22.889842311610867,21.30122097904847,19.0336054859068,23.801438495307856,24.840309004310797,19.694954646749302,20.944503948247828,20.620570884389835,21.72592495902031,20.296054717472362,19.58398989769835,20.2956272827196,21.085706497425413,19.461766940454588,19.29900449887296,22.510563057499144,20.841106100432793,21.801619166438428,21.147132242697182,21.551582010518878,20.4427602890876,24.611415306054898,21.490696787354505,21.24369655260969,22.432940196049124,22.234424235191682,20.099462108962847,20.800416502964755,20.17949699394329,21.746512886013605,22.496434708234126,19.78606039431591,22.229470444639624,19.19842345841963,21.071706539354626,21.28737646300594,20.120777025175872,23.69806207100269,24.25287711008343,19.074352191519743,20.558351575498786,21.437400189861094,20.352963298343866,21.233800456337935,22.14457282995132,20.722700819009958,20.69715285443839,20.03253125195289,20.0566191944814,22.70946891023392,21.137414277628565,19.808943756196857,20.143175790739598,21.040198736231254,19.712186074557728,19.35146728073557,20.861088989018167,20.755317114550273,0,19.017540081468233,21.419330156983328,20.67873788317677,19.93668394257525,22.409766675366317,19.27540160418116,23.728782472563765,20.376074503254586,19.31077695697734,21.065895324749285,20.3484179021553,20.07860380095908,19.976548041795603,20.235086321270654,21.01213158467734,21.41999462819776,19.174863067555084,21.10241852261651,20.85643673732399,22.586773372950685,20.037769462388923,21.559823540829672,23.13793422908773,20.558605220150522,19.683189962828273,20.39789928981696,19.337397595787667,22.26889032385089,21.195293500127566,22.559724250840063,24.39611039895289,19.59271980995523,22.136011068345166,19.33796659591498,20.797749380419276,21.567058771175564,20.725266392758815,18.468910364525364,19.663251453070878,18.918473122196858,19.198971627350232,24.325766932844235,19.86193501563865,21.156628899315663,22.938277348217966,20.78722963747693,20.66031803587651,20.155379081002646,19.457981357627542,25.713720052658104,20.828559454854545,20.52544147367378,21.147722912842873,20.03302501817969,24.630356510701596,23.121909718158822,23.106213073613993,22.51080834568276,20.883881259335507,21.661257500272384,19.85555820528158,19.526632957320885,19.191856386440307,22.96557469826004,19.96989517504386,19.756880613939327,20.841336970061143,21.615161729704354,19.630957845101975,19.107266091841186,18.360084050573274,20.178689314774683,22.61224657942639,20.902818910258993,19.692513024244924,21.22795725431778,21.431836012474292,22.58649267850949,23.56945490190484,22.15739813287633,19.523673217064264,23.682062419502355,22.448170385491473,23.624330847094043,20.4998468600676,19.29906600505008,21.469193647687533,22.680649469911085,21.178626711012377,21.686546691263366,21.428479917582088,22.88912263482666,20.804770608680126,19.28855746111665,20.502951107445575,20.22322663992422,22.31457162800466,19.478115876564008,18.795995233915015,25.17177163891695,23.160406649511106,19.25160491688623,20.826937118763155,19.953135265171547,23.933068642871486,19.355480148534408,20.44485249695551,19.56616125697471,25.23181389258149,21.345469949872594,22.776042002687795,25.303110182218393,19.58463718752616,20.204391739449186,19.893828839264728,20.33821258053668,24.68330601022857,18.821811344100055,19.840511525069328,22.297566604177444,20.78875924862035,19.741131185755275,20.04722528013383,22.468811326543563,19.99457856414159,21.80053568878591,19.167558108393887,23.846962977749094,19.07645844042533,19.307592868706436,19.883803436319816,19.767052898077655,18.952911838336565,18.62774549084819,20.73926969664862,18.627786784209636,19.177777189052396,20.089958597754855,20.801380865239704,17.764101156137162,19.11561871251436,19.24667566091232,20.93405408413005,21.340147837597844,21.090725928345268,19.745386795650543,19.095931371701568,19.810878030088894,20.251155291034372,19.165221068220944,20.724802969675316,19.74733497869572,22.856134451929176,20.542598325656595,19.502159734210206,21.979041347822633,21.2197623270523,26.969103234931445,20.18342033833375,21.02499268359878,20.592440384261195,19.990444456103596,20.867172390851817,21.273367636241293,21.243839329097405,21.363500625759567,20.18486444945059,20.241113097705927,22.693372098426487,18.623284957897813,19.205602363695707,21.545170940853957,20.80034066129298,19.981973707277767,22.19000668879013,22.718490023684353,20.707922743251714,19.214188218113872,22.472827160217914,19.07108521476948,20.346151863763343,21.63013067480897,18.214892995560163,19.93614600441511,22.53034703709958,19.469859640957857,22.895450751816803,21.00807772677881,21.431258349952667,20.941808459871957,22.99407646829777,19.76111291920157,19.008644725628592,18.355858634806896,20.297451997659746,20.474380309584664,21.870871511939562,20.759740299921024,21.439000175436664,22.030880607811586,23.00587909853429,20.655014419256375,22.965146977089795,22.43523710461151,20.18471556395119,19.019883876304206,23.05070508803856,21.499214768467855,20.59203905710115,21.024704798577186,22.72181503272866,22.377050963043796,19.71871179715288,21.93139047566312,21.827309362120346,19.1796783064222,19.826388660339887,23.65560944108187,19.199993380834425,22.21417220655097,19.269925661522706,23.782692532330046,22.173362238186836,23.41808975500013,20.872940195293037,21.54216753533708,21.03393309328942,21.663729644062077,20.699641074284234,21.39606694375647,19.593416427729718,21.355622092522935,19.49242240707679,19.406777996109206,22.625652665035147,19.860111721120674,14.03702355520748,22.00972851632279,20.33705521659505,20.155471422765178,20.40843263005365,23.540959857497864,22.64902697475969,19.51901613478499,20.748003621067692,19.075332900397907,21.535462867247563,19.35190112925433,16.543892756530866,22.048803804340604,19.94957596782787,23.253137677025506,15.802066947826521,22.01816825105002,24.65495079669392,19.684337709357063,23.07232514287102,19.441821622178967,20.90283362751463,21.65340972782181,20.672219865399406,22.248853822319084,22.007999054329076,21.514848387611945,18.91123104739614,22.195045634281605,21.092338092530998,19.27798585366196,21.725547245838506,20.321666107127033,18.844349286283478,20.211183384442457,20.06069065496676,20.388592288941165,25.880225035189923,19.68504303470752,20.795739259102447,19.874915797147626,20.484467680558765,21.537004350283745,19.454353518988064,20.471143313052202,20.221414835639845,21.30681378731033,22.893554470389834,23.892417838743924,19.773417735905042,21.39811634221899,21.94580529932221,22.093666756982273,19.566140232030243,19.62938152434226,22.598971957007752,22.606867611519395,23.548837661891714,19.57985452716288,19.257215175267675,20.260215495083163,21.269107840654502,21.892874617975046,20.911342976063032,20.690267441816314,19.415723203109316,22.536424619535012,19.241608485058645,19.162579413291002,20.470409642279566,20.97348174432658,23.73507977629211,14.55836091863428,23.336410397420998,21.627932359551377,21.49391553553141,20.265469687416747,19.426854398695138,20.625264709408093,20.223930403747513,21.865935415382868,21.881578906408237,20.14846091060974,19.81407448278221,21.0024847282008,20.945391681085823,22.291777470342872,20.551017265346704,19.577722584113683,19.444205825873514,21.46402517777047,18.277738745265133,18.799932943985397,21.60163932304092,19.80978333592071,20.91463622707276,21.547692385839394,19.17999258814081,19.549428996570953,19.54492482512454,19.294068756456163,21.69613129076446,20.048671453648097,19.49098989799628,20.42092755989845,20.047998538616795,22.735567798975932,20.159778968313912,20.920832580620292,19.699545342858148,20.678206389277292,20.079358473730228,19.917597162486278,19.692880228017533,26.26193365893149,20.33327111425421,21.201441496966886,24.081396558763124,22.374129418340807,22.181711726885904,18.957794711482784,21.63591845810156,22.742154187536123,20.277557571355647,21.712167598440185,20.922319986303112,19.981118924422468,18.376324675581085,19.093096903348385,21.041684548715036,19.88267606965292,21.79470380164697,19.394929062748655,19.711931797206645,20.701777070204972,18.966254774482525,16.57594392051226,19.3661810523187,19.637857750008234,21.05876451742095,21.39188913879269,20.922214169866013,20.158015290040524,19.840625811644227,20.287989620862316,20.392027938746434,21.440477364164977,20.18838178436931,19.209224880205603,19.523793560708743,19.767710849855263,21.57006553499441,20.748925773432315,21.47821316674072,24.169496968680054,22.350651659007692,22.60440595404428,20.973185759004682,22.891726790683144,20.2096151875794,20.22349858267661,20.901209408549594,19.053706845403045,23.000908813982363,19.549547798448483,22.637005220660473,23.38302130821851,25.236220259743373,22.107609158967936,23.256395298587126,21.31115959043422,19.242152996635603,23.069120127461606,19.88568939220856,20.74441094675746,20.977820033541025,21.403421935393933,23.155520679582406,17.74375624808358,21.65558749657167,22.72177363339029,20.057436191229318,20.31682150206961,21.442049003709382,20.447439187365134,18.64679105166152,23.081069556252004,23.109422435163253,19.40981146751902,21.538742146089504,20.451115883369862,20.520204300445414,19.072268708954628,21.853488891111965,19.870759032200375,19.874160110175985,20.45611373193672,20.923455889638944,15.423378576518493,19.495183602853512,20.532198000108426,20.172754664219987,19.621134659665625,21.149770683357705,25.5492833010917,20.91962424027046,23.144648130459657,22.724819827816283,20.76732896720974,20.90466916348112,21.019251701774245,21.870062515886435,21.968529606994224,18.28303322918483,22.7967578369547,24.617902294510255,20.337523908425865,22.278345092825212,20.439903152029967,20.745981176528062,20.50317548758169,20.148792615003963,20.18507504172347,19.887802201303376,21.335667578998795,23.547889563240428,22.184719195487645,19.124759964560997,19.09456742423413,23.546328229722338,23.676810419954805,21.378549935187863,18.772971741326142,20.92343911431291,23.66501619396984,20.362922084924325,19.335122075152558,20.6322831501737,22.014406532680294,20.862569257472042,22.377941930707426,16.70615872384773,21.114598939015725,25.312874128977917,19.557332226454086,25.0888235331049,0,20.821375100396438,19.69889252664812,20.730065203035867,20.737794346856816,19.720177445507503,19.751886432327307,21.436234720492326,19.12391425698277,21.093445290401018,22.18427320470571,23.32578333060166,20.235023779555554,20.38834949087981,19.076215392899233,21.890199695637197,19.626240196846272,19.473911816076555,22.65051997686549,21.07153535807478,20.152641197123305,25.817249936117022,20.72049143867345,20.000328449472278,19.677575133698696,19.854767962698624,20.90714373186156,21.852326049162542,22.34522357506863,21.88381079939571,20.64279914121781,20.008240359915625,20.079932477068777,20.97646133780068,20.617037831360907,20.86340137149202,20.10297814136792,18.939248297157278,19.337083944174847,21.21789500066683,19.959413629413806,20.57158693763026,20.1483293247843,22.45828794986169,19.223167412657357,22.090969450288238,20.54580254002858,20.99006223015013,23.933428716720645,18.97273303540371,21.839004592901738,17.66325927218184,21.098649782686852,22.108159109401438,19.70668845124008,20.687407722542194,21.23800756191085,22.54894428694912,20.440949745295235,18.136670762341843,21.301837769324877,21.032952737462605,23.31051349089806,20.969732200565332,20.351613259246466,19.678510738089678,22.059041559037812,22.10097025846529,21.127912480118983,24.425354799557958,19.405573621751774,18.954513256380405,20.62820521809647,19.21222562648202,18.9881876444261,21.62524006847279,18.78833918294514,19.827821621310783,19.4644839297056,26.34702224299619,21.38205653661138,21.78972267747008,21.087755789009996,18.999355953219133,19.451729034530373,23.289040906826244,24.16646354231811,21.536078922759724,21.530906360636845,20.97243154975957,20.089266034468004,20.436904665975636,19.20088663487939,20.554011225634962,19.277982733151024,18.304477915148002,19.118363214083377,20.261720490850706,19.26508618967162,23.063753511610845,19.05336380976314,19.240852168960235,19.807795916443087,19.527755462443025,19.369940873414674,18.557194427644223,19.092216129486825,23.457850638636746,20.3986497634222,20.79850265219969,20.611470634120458,21.03390016791011,24.957468167964976,19.132327452939787,22.668945885967275,20.192548076466373,21.926818619899166,21.236410750491928,22.00439862331522,22.64890259783158,19.876543924901863,22.174820865681593,20.001613842126076,20.63081635106876,19.653189714082398,19.305175301710232,20.224827212369068,22.6279649673046,21.547003936026666,18.961602303420257,20.695469281090816,20.114481067581714,20.790519525351012,26.143565404950515,21.151917099556776,22.601975102609295,18.386700720994313,21.48658316808204,19.99564295418165,22.704086007044513,23.646718002570474,20.17407123394833,20.29544946282625,21.94083977054476,19.087820197169854,20.49274951238928,21.58556546053215,19.868098908707033,19.806349800221792,23.600603274171462,19.846459862589864,21.099407208256782,21.035986420389108,23.747615973251953,21.078530347858226,20.86782723578085,20.073998999202523,19.385623092000827,20.214357069981805,21.756421739524402,19.573874587511387,22.17732889464112,20.092049623606254,19.621522497700703,20.13212471121583,22.84612630027242,21.610638293924836,20.381527241115613,22.69863600121815,24.241029788998915,23.65992231174119,22.296121251970213,20.702441582535556,22.897176211133427,19.850331267520566,17.677965293255863,20.87727804782974,24.045999647847655,20.173501472804098,23.33182107390064,23.30215527163116,27.231751620760654,20.650611348398325,16.829816883719186,21.108904064764797,23.735782522109897,20.656510624448828,20.096689421256738,22.83710046840733,19.824675810238606,20.823945313943256,23.256137551486724,23.633181830866906,19.043844337217745,23.65829637497728,23.50747794569296,21.73683233585413,20.94197381362215,24.18225862695574,20.14794970691058,22.344681402352435,20.447944597167705,19.21271648764287,22.508853453023672,21.75045501313216,20.060863127783303,23.041170672161062,19.554654657325607,21.26551890511897,19.47575297115503,20.304093087106327,21.96980947524255,18.890513667640615,20.207528404900692,20.825342365179193,19.58596744655146,19.86547955920327,20.093122217042076,19.51138012046539,20.89052275919248,19.726434174485984,19.26180569731426,21.223090796665396,20.071086524480997,23.60048678836413,21.682830461889793,22.115896159241377,23.245004397316144,19.929597223002617,20.206849822052806,20.80607598372765,20.972697281096956,20.548932178877266,20.790175259909486,21.65577788956451,20.396533708238668,17.677595133772794,20.206889160267256,19.864164508631692,20.604884938737765,19.69979302428942,21.112079923463828,21.776216944868956,19.41019874513054,20.084542414131516,25.538294718669484,19.887154874828134,22.02600227823102,20.06242269100319,19.908561207609715,19.025787914622292,22.519912388376707,21.15310774495207,15.434839951572775,23.416469489833915,21.472713147192902,24.799241151178187,21.565777300167408,21.24511744232927,23.786142675104543,21.775846548503456,20.874095085342866,20.525903087310763,22.190296420947053,17.526091919071504,22.787613064091445,19.95220428650718,21.56191951932416,20.302727045932432,22.39076356657128,17.788361488629835,19.89495594402764,20.849863964556313,22.01047671458767,21.9419703223203,21.769327477492673,22.134078601621123,23.02213648046312,21.659326126254854,22.503975157123477,21.687400780796548,21.299913824117656,24.41218042654058,21.628416169590647,20.661666774415252,19.13829898996108,20.6534474504661,20.599627769624064,19.476347089125902,22.633338504709375,19.004867830543457,20.729309561171004,21.215021611787833,20.55962915472571,23.96741801447904,20.948249582299137,19.034944006994706,21.25562508835354,23.6936808146177,18.876298103633747,20.12057913956066,22.988276940898057,20.761017121909465,29.702932107840674,19.34717798381203,19.04780522125321,19.72022056098374,25.87586915107988,22.381251164844315,21.45471998887307,21.894206057556605,20.997149152437558,19.346653316091935,19.69023397394279,21.75892392666085,21.695257182632144,24.304938166770185,19.96312680068985,21.08058139373211,18.457821713581545,21.058645462338415,20.370456476417175,22.39756222611425,21.96355600303241,24.29338064398619,19.533504643608662,19.24597971762759,21.061479969798718,19.653131971320136,19.991828535916927,19.41354451956652,19.024392506006556,19.995956185326282,21.080207924079772,22.895336936087247,19.00379513256332,19.59081933226322,19.683155687698772,19.724580465382388,20.49815774428782,21.41646356230674,20.374729613814893,19.698149844649336,20.827570650601533,21.55980522344904,20.203106297950406,20.998113148879906,22.5464416348448,21.94562707940607,20.109117749689304,19.61600010557632,22.28815948936714,20.00070564420032,19.48773603183014,22.96164822427184,18.99960834234589,20.932965503614184,18.548961097016324,21.40256127224899,23.343368321662904,21.395751936133603,20.367816242708102,21.629837063073623,22.815206783859768,22.704911104222457,20.7857551664019,20.3067025549342,20.61336030755643,21.781868242254394,20.56504163001677,24.58750691743097,21.042555130756018,19.718574682047347,20.82900495573487,21.89094777167843,20.46100849412487,20.676958661968467,22.376303078727783,21.96712491711981,21.09072172933059,20.373076483151994,19.53720891947194,19.37233862557095,0,22.656078929958284,19.971799365414682,21.138173662648782,25.881734625061707,0,22.042981377352834,19.12600868541295,20.929534739770034,20.604603448464516,21.393924057871644,18.99682101244613,19.31053914628864,18.986258705756388,16.7233596191811,19.687016476370815,20.364732076794134,18.828527980887348,21.061860954738233,18.824250825082274,19.28149315085171,21.023652845270533,23.394529944648035,22.371116977331784,21.19573925103901,18.93658618322981,19.94401710262461,19.47904998638186,22.850840758637354,20.76699650557159,20.40036143331872,17.795936296212627,19.421904563712328,20.64673096490357,19.639706353282477,18.775317705171794,20.047574770414855,19.61260734468754,23.666636204423178,23.480370137373004,19.218266855752937,21.60748880425466,21.257489419039537,21.101232900314493,21.320633929890864,21.849984955018474,20.40434121243742,20.032547562218568,20.035932572983462,20.864093173595,21.51301334156099,22.29182284846925,23.44987910374218,20.855425894539348,22.64951341150128,23.894186440194584,20.26686372980413,20.408744363117133,19.046718202293015,20.904214174162476,24.308788888311135,20.77592509493985,0,19.644171392658865,18.610921424553307,22.26726247442381,20.903586582709064,21.2082248307893,20.867431932395597,19.247883485213144,19.09858987177298,20.769205079804532,21.73660672493293,20.24751150890443,21.281868210231526,19.724170230709305,20.553728301262954,21.112413416140527,20.46206885900041,20.668149201097204],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_pred_log_logRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF_log = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF_log = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF_log,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF_log, \\\n","BestParametresRF_log, \\\n","ScoresRF_log, \\\n","SiteEnergyUse_pred_logRF_log, \\\n","figRF_log = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log.ravel(),\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_pred_log_logRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF_log)\n","print(ScoresRF_log)\n","figRF_log.show()\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[2.0777740987378457,1.9124792334402663,1.6715391615750768,1.5625475745762667,1.5025172338977746,1.506553380734741,1.4918543027622821,1.4874268798629768,1.480738673204946,1.4864914466818275]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[2.220317438990204,2.006946520411355,1.7921935646779115,1.6568796275248834,1.5923954541750418,1.5708237361699071,1.5760721671220188,1.5619761368453102,1.5640026004167806,1.5615850243208922]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.9352307584854875,1.8180119464691775,1.550884758472242,1.46821552162765,1.4126390136205074,1.442283025299575,1.4076364384025455,1.4128776228806434,1.3974747459931112,1.4113978690427629]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.9557694427667744,1.8243508531447183,1.581143053622944,1.4870083523068442,1.5012420957104327,1.4891457612028078,1.4643142244070169,1.47905944203323,1.4687622373772322,1.4739446292826603],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.9911774112273732,2.0885408427314274,1.8290434834324119,1.691827488018081,1.5966772330346972,1.5916328196284337,1.6085596136363516,1.5823115532385346,1.5886424707409232,1.5797926569880378],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[2.226093397549524,1.9249671155833854,1.728330843135649,1.6027397348311996,1.576143340836843,1.5543565514479123,1.5473178229894087,1.5367277706705653,1.5348768512696391,1.5348944815101935],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.9414919856163249,1.8804055584887303,1.7298129013192873,1.6051526591555496,1.497488938799695,1.4944200357383814,1.4809798065492785,1.478935759128662,1.4716110165420597,1.4876737174155934],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[2.274338256529232,1.8441317972530706,1.4893655263650922,1.4260096385696577,1.3410345611072048,1.4032117356561697,1.3581000462293544,1.360099874243892,1.3398007900948747,1.3561517482126524],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=sqrt<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF_log = visuRMSEGrid(RandomForestRegressor(), 'RF',\n","                                 n_estimatorsRF_log, 'n estimators',\n","                                 GridRF_log, BestParametresRF_log,\n","                                 'randomforestregressor__max_features')\n","FigRMSEGRidRF_log.show()\n","if write_data is True:\n","    FigRMSEGRidRF_log.write_image('./Figures/ConsoGraphRMSERF_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 1.2.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                   1\n","1          adaboostregressor__loss              square\n","                           R²      RMSE       MAE\n","AdaBoostRegressor()  0.339668  1.798246  0.879046\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predAB_log=%{x}<br>SiteEnergyUse_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[21.81809985608627,20.56948622029852,20.56948622029852,25.701118112134086,21.81809985608627,19.654943852599523,19.654943852599523,21.81809985608627,19.654943852599523,25.701118112134086,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,23.821861517029674,23.821861517029674,21.33750669389373,25.701118112134086,19.654943852599523,21.33750669389373,19.654943852599523,19.654943852599523,19.654943852599523,23.821861517029674,19.654943852599523,20.56948622029852,20.56948622029852,21.81809985608627,20.56948622029852,21.33750669389373,19.654943852599523,21.81809985608627,20.56948622029852,21.33750669389373,19.654943852599523,22.811747322179404,19.654943852599523,21.33750669389373,19.654943852599523,21.33750669389373,22.811747322179404,19.654943852599523,21.81809985608627,25.701118112134086,21.81809985608627,25.701118112134086,20.56948622029852,25.701118112134086,21.81809985608627,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,21.33750669389373,20.56948622029852,21.33750669389373,21.33750669389373,23.821861517029674,20.56948622029852,20.56948622029852,21.33750669389373,19.654943852599523,20.56948622029852,21.81809985608627,19.654943852599523,21.81809985608627,19.654943852599523,19.654943852599523,22.811747322179404,19.654943852599523,19.654943852599523,21.33750669389373,19.654943852599523,19.654943852599523,21.33750669389373,23.821861517029674,19.654943852599523,19.654943852599523,21.33750669389373,19.654943852599523,20.56948622029852,20.56948622029852,20.189775880774164,20.56948622029852,21.81809985608627,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,22.811747322179404,22.811747322179404,19.654943852599523,21.33750669389373,22.811747322179404,23.821861517029674,19.654943852599523,25.701118112134086,21.33750669389373,21.33750669389373,20.56948622029852,20.189775880774164,22.811747322179404,21.33750669389373,20.56948622029852,19.654943852599523,22.811747322179404,20.56948622029852,21.33750669389373,19.654943852599523,20.56948622029852,19.654943852599523,23.821861517029674,21.33750669389373,20.56948622029852,21.33750669389373,22.811747322179404,19.654943852599523,23.821861517029674,19.654943852599523,20.56948622029852,20.189775880774164,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,21.81809985608627,20.189775880774164,19.654943852599523,21.81809985608627,20.56948622029852,19.654943852599523,19.654943852599523,21.81809985608627,19.654943852599523,21.81809985608627,20.189775880774164,21.33750669389373,20.56948622029852,19.654943852599523,22.811747322179404,19.654943852599523,25.701118112134086,20.56948622029852,20.56948622029852,23.821861517029674,19.654943852599523,22.811747322179404,19.654943852599523,19.654943852599523,20.189775880774164,25.701118112134086,19.654943852599523,20.56948622029852,21.81809985608627,21.81809985608627,20.189775880774164,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,22.811747322179404,21.81809985608627,20.56948622029852,19.654943852599523,21.81809985608627,23.821861517029674,20.189775880774164,20.56948622029852,21.33750669389373,22.811747322179404,21.81809985608627,21.33750669389373,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,22.811747322179404,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,23.821861517029674,20.56948622029852,21.81809985608627,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,21.81809985608627,20.56948622029852,20.56948622029852,20.56948622029852,23.821861517029674,21.33750669389373,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,21.81809985608627,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,23.821861517029674,21.81809985608627,21.81809985608627,23.821861517029674,20.56948622029852,19.654943852599523,19.654943852599523,21.81809985608627,20.56948622029852,20.56948622029852,20.56948622029852,22.811747322179404,25.701118112134086,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,23.821861517029674,23.821861517029674,21.33750669389373,19.654943852599523,19.654943852599523,21.81809985608627,19.654943852599523,21.81809985608627,22.811747322179404,19.654943852599523,20.56948622029852,19.654943852599523,21.33750669389373,19.654943852599523,19.654943852599523,21.33750669389373,22.811747322179404,19.654943852599523,21.33750669389373,19.654943852599523,22.811747322179404,20.189775880774164,20.56948622029852,20.189775880774164,20.56948622029852,19.654943852599523,20.56948622029852,20.56948622029852,21.81809985608627,19.654943852599523,20.189775880774164,19.654943852599523,21.33750669389373,20.56948622029852,20.189775880774164,19.654943852599523,21.33750669389373,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,21.81809985608627,20.56948622029852,20.56948622029852,21.81809985608627,20.56948622029852,20.56948622029852,19.654943852599523,19.654943852599523,21.81809985608627,21.81809985608627,19.654943852599523,20.56948622029852,20.56948622029852,23.821861517029674,19.654943852599523,20.56948622029852,23.821861517029674,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,23.821861517029674,21.33750669389373,20.56948622029852,20.56948622029852,21.81809985608627,21.81809985608627,19.654943852599523,22.811747322179404,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,21.81809985608627,19.654943852599523,19.654943852599523,19.654943852599523,21.81809985608627,21.33750669389373,21.33750669389373,21.81809985608627,19.654943852599523,22.811747322179404,19.654943852599523,21.81809985608627,21.81809985608627,22.811747322179404,19.654943852599523,19.654943852599523,20.56948622029852,21.33750669389373,21.81809985608627,19.654943852599523,21.33750669389373,19.654943852599523,19.654943852599523,20.56948622029852,20.189775880774164,23.821861517029674,23.821861517029674,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,19.654943852599523,22.811747322179404,21.81809985608627,20.56948622029852,19.654943852599523,19.654943852599523,21.33750669389373,21.33750669389373,19.654943852599523,22.811747322179404,19.654943852599523,20.189775880774164,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,20.189775880774164,20.56948622029852,20.56948622029852,19.654943852599523,21.81809985608627,20.189775880774164,23.821861517029674,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,19.654943852599523,21.81809985608627,20.56948622029852,22.811747322179404,20.189775880774164,20.56948622029852,21.33750669389373,20.56948622029852,19.654943852599523,21.33750669389373,19.654943852599523,21.81809985608627,21.33750669389373,20.56948622029852,21.81809985608627,20.189775880774164,22.811747322179404,19.654943852599523,21.33750669389373,20.56948622029852,21.33750669389373,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,23.821861517029674,20.56948622029852,21.33750669389373,22.811747322179404,20.56948622029852,20.56948622029852,19.654943852599523,19.654943852599523,23.821861517029674,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,25.701118112134086,21.81809985608627,21.33750669389373,19.654943852599523,20.56948622029852,20.189775880774164,19.654943852599523,20.56948622029852,19.654943852599523,22.811747322179404,20.56948622029852,20.56948622029852,20.189775880774164,21.33750669389373,19.654943852599523,20.189775880774164,20.189775880774164,20.56948622029852,23.821861517029674,19.654943852599523,19.654943852599523,21.33750669389373,21.33750669389373,21.81809985608627,23.821861517029674,21.81809985608627,19.654943852599523,23.821861517029674,21.81809985608627,23.821861517029674,19.654943852599523,19.654943852599523,21.33750669389373,22.811747322179404,20.56948622029852,20.56948622029852,21.33750669389373,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,20.56948622029852,22.811747322179404,19.654943852599523,20.56948622029852,25.701118112134086,21.81809985608627,21.81809985608627,20.56948622029852,21.81809985608627,22.811747322179404,19.654943852599523,20.56948622029852,19.654943852599523,23.821861517029674,20.56948622029852,23.821861517029674,22.811747322179404,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,23.821861517029674,19.654943852599523,20.56948622029852,21.81809985608627,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,19.654943852599523,23.821861517029674,19.654943852599523,20.56948622029852,21.33750669389373,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,21.33750669389373,19.654943852599523,19.654943852599523,20.56948622029852,20.189775880774164,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,22.811747322179404,21.33750669389373,19.654943852599523,20.56948622029852,19.654943852599523,25.701118112134086,20.56948622029852,20.189775880774164,20.189775880774164,21.33750669389373,21.81809985608627,19.654943852599523,20.56948622029852,19.654943852599523,20.189775880774164,20.56948622029852,20.56948622029852,19.654943852599523,19.654943852599523,21.81809985608627,20.56948622029852,20.56948622029852,22.811747322179404,20.189775880774164,19.654943852599523,20.56948622029852,22.811747322179404,20.189775880774164,20.56948622029852,21.33750669389373,19.654943852599523,19.654943852599523,21.81809985608627,20.56948622029852,21.33750669389373,19.654943852599523,20.56948622029852,19.654943852599523,21.81809985608627,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,21.33750669389373,21.81809985608627,22.811747322179404,22.811747322179404,22.811747322179404,22.811747322179404,19.654943852599523,20.189775880774164,21.33750669389373,21.33750669389373,20.56948622029852,21.33750669389373,22.811747322179404,20.56948622029852,20.56948622029852,20.189775880774164,20.56948622029852,19.654943852599523,20.56948622029852,22.811747322179404,22.811747322179404,21.81809985608627,19.654943852599523,23.821861517029674,20.56948622029852,23.821861517029674,21.33750669389373,21.33750669389373,21.33750669389373,20.56948622029852,21.33750669389373,20.56948622029852,19.654943852599523,20.189775880774164,20.56948622029852,19.654943852599523,21.81809985608627,19.654943852599523,19.654943852599523,23.821861517029674,19.654943852599523,20.189775880774164,20.189775880774164,19.654943852599523,21.33750669389373,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,22.811747322179404,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,23.821861517029674,19.654943852599523,22.811747322179404,19.654943852599523,20.189775880774164,21.33750669389373,20.56948622029852,21.81809985608627,22.811747322179404,21.33750669389373,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,21.81809985608627,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,22.811747322179404,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,21.33750669389373,23.821861517029674,23.821861517029674,19.654943852599523,20.56948622029852,21.81809985608627,21.33750669389373,20.56948622029852,20.56948622029852,23.821861517029674,22.811747322179404,23.821861517029674,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,21.81809985608627,20.189775880774164,20.56948622029852,20.56948622029852,23.821861517029674,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,21.33750669389373,20.56948622029852,20.56948622029852,21.81809985608627,20.56948622029852,19.654943852599523,19.654943852599523,21.33750669389373,20.56948622029852,21.81809985608627,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,21.33750669389373,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,19.654943852599523,22.811747322179404,19.654943852599523,20.189775880774164,22.811747322179404,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,21.81809985608627,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,19.654943852599523,21.81809985608627,19.654943852599523,20.189775880774164,20.56948622029852,19.654943852599523,19.654943852599523,25.701118112134086,19.654943852599523,21.33750669389373,23.821861517029674,21.81809985608627,21.81809985608627,19.654943852599523,21.81809985608627,21.81809985608627,20.56948622029852,21.33750669389373,21.33750669389373,20.56948622029852,21.33750669389373,19.654943852599523,21.33750669389373,19.654943852599523,21.33750669389373,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,21.81809985608627,21.33750669389373,20.56948622029852,20.189775880774164,20.56948622029852,20.56948622029852,21.81809985608627,20.56948622029852,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,20.189775880774164,21.33750669389373,22.811747322179404,21.33750669389373,20.56948622029852,20.56948622029852,23.821861517029674,20.56948622029852,19.654943852599523,21.33750669389373,20.56948622029852,23.821861517029674,19.654943852599523,21.33750669389373,23.821861517029674,25.701118112134086,20.56948622029852,23.821861517029674,20.56948622029852,20.56948622029852,22.811747322179404,20.56948622029852,19.654943852599523,20.56948622029852,21.33750669389373,22.811747322179404,19.654943852599523,21.33750669389373,20.56948622029852,20.56948622029852,20.56948622029852,21.81809985608627,19.654943852599523,19.654943852599523,22.811747322179404,23.821861517029674,19.654943852599523,21.33750669389373,19.654943852599523,21.33750669389373,19.654943852599523,22.811747322179404,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,23.821861517029674,21.33750669389373,22.811747322179404,21.81809985608627,21.33750669389373,19.654943852599523,21.33750669389373,22.811747322179404,21.33750669389373,21.33750669389373,23.821861517029674,23.821861517029674,19.654943852599523,21.81809985608627,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,21.33750669389373,23.821861517029674,21.81809985608627,19.654943852599523,19.654943852599523,20.56948622029852,22.811747322179404,20.56948622029852,19.654943852599523,19.654943852599523,23.821861517029674,21.33750669389373,19.654943852599523,19.654943852599523,19.654943852599523,21.33750669389373,21.81809985608627,19.654943852599523,21.33750669389373,21.81809985608627,19.654943852599523,25.701118112134086,20.56948622029852,20.56948622029852,20.189775880774164,21.81809985608627,20.56948622029852,19.654943852599523,20.56948622029852,21.81809985608627,21.33750669389373,21.81809985608627,21.33750669389373,21.33750669389373,20.56948622029852,19.654943852599523,21.33750669389373,20.56948622029852,19.654943852599523,19.654943852599523,21.81809985608627,20.56948622029852,21.33750669389373,25.701118112134086,21.81809985608627,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,21.81809985608627,23.821861517029674,22.811747322179404,21.33750669389373,20.56948622029852,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,20.189775880774164,19.654943852599523,21.81809985608627,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,21.33750669389373,23.821861517029674,19.654943852599523,21.81809985608627,19.654943852599523,19.654943852599523,21.33750669389373,19.654943852599523,21.33750669389373,19.654943852599523,21.81809985608627,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,21.33750669389373,19.654943852599523,19.654943852599523,21.81809985608627,22.811747322179404,21.33750669389373,22.811747322179404,20.56948622029852,19.654943852599523,19.654943852599523,20.189775880774164,20.56948622029852,20.56948622029852,20.189775880774164,20.56948622029852,19.654943852599523,25.701118112134086,21.81809985608627,21.33750669389373,19.654943852599523,19.654943852599523,19.654943852599523,23.821861517029674,23.821861517029674,21.33750669389373,20.56948622029852,21.33750669389373,20.56948622029852,20.56948622029852,20.189775880774164,20.56948622029852,20.56948622029852,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,22.811747322179404,19.654943852599523,19.654943852599523,20.189775880774164,19.654943852599523,19.654943852599523,20.189775880774164,20.189775880774164,23.821861517029674,20.56948622029852,21.81809985608627,21.33750669389373,20.56948622029852,21.81809985608627,19.654943852599523,21.33750669389373,20.56948622029852,20.56948622029852,19.654943852599523,21.33750669389373,22.811747322179404,21.81809985608627,21.81809985608627,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,22.811747322179404,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,25.701118112134086,21.33750669389373,21.81809985608627,20.56948622029852,21.81809985608627,20.56948622029852,21.81809985608627,21.33750669389373,20.56948622029852,19.654943852599523,21.81809985608627,19.654943852599523,20.189775880774164,20.56948622029852,20.56948622029852,19.654943852599523,23.821861517029674,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,20.189775880774164,20.56948622029852,19.654943852599523,22.811747322179404,20.56948622029852,20.56948622029852,21.33750669389373,25.701118112134086,21.81809985608627,22.811747322179404,20.56948622029852,22.811747322179404,19.654943852599523,19.654943852599523,20.56948622029852,23.821861517029674,20.56948622029852,21.81809985608627,21.81809985608627,25.701118112134086,20.189775880774164,19.654943852599523,20.56948622029852,23.821861517029674,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,19.654943852599523,21.81809985608627,20.56948622029852,20.56948622029852,23.821861517029674,23.821861517029674,19.654943852599523,19.654943852599523,22.811747322179404,19.654943852599523,21.81809985608627,20.56948622029852,20.189775880774164,21.33750669389373,20.56948622029852,19.654943852599523,22.811747322179404,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,21.81809985608627,19.654943852599523,21.33750669389373,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,20.56948622029852,20.189775880774164,21.33750669389373,19.654943852599523,23.821861517029674,21.33750669389373,23.821861517029674,23.821861517029674,20.56948622029852,20.189775880774164,21.33750669389373,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,20.189775880774164,20.56948622029852,20.56948622029852,21.33750669389373,21.81809985608627,19.654943852599523,20.56948622029852,23.821861517029674,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,21.81809985608627,20.56948622029852,20.56948622029852,22.811747322179404,21.81809985608627,20.56948622029852,21.33750669389373,20.56948622029852,23.821861517029674,22.811747322179404,20.56948622029852,20.56948622029852,21.81809985608627,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,20.56948622029852,22.811747322179404,20.56948622029852,20.56948622029852,20.56948622029852,20.56948622029852,22.811747322179404,21.81809985608627,20.56948622029852,19.654943852599523,22.811747322179404,20.56948622029852,20.56948622029852,21.33750669389373,23.821861517029674,21.33750669389373,20.189775880774164,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,22.811747322179404,19.654943852599523,21.81809985608627,21.33750669389373,19.654943852599523,21.81809985608627,21.81809985608627,19.654943852599523,21.33750669389373,23.821861517029674,19.654943852599523,20.189775880774164,23.821861517029674,21.33750669389373,25.701118112134086,19.654943852599523,19.654943852599523,19.654943852599523,25.701118112134086,19.654943852599523,20.189775880774164,21.81809985608627,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,21.81809985608627,19.654943852599523,20.189775880774164,20.56948622029852,21.33750669389373,20.56948622029852,22.811747322179404,21.81809985608627,22.811747322179404,19.654943852599523,20.56948622029852,21.33750669389373,20.56948622029852,20.56948622029852,19.654943852599523,19.654943852599523,20.56948622029852,21.33750669389373,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,19.654943852599523,20.56948622029852,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,22.811747322179404,20.56948622029852,19.654943852599523,22.811747322179404,21.33750669389373,20.56948622029852,19.654943852599523,22.811747322179404,20.189775880774164,19.654943852599523,23.821861517029674,19.654943852599523,21.81809985608627,19.654943852599523,21.33750669389373,20.56948622029852,21.81809985608627,20.56948622029852,21.81809985608627,22.811747322179404,22.811747322179404,20.56948622029852,20.56948622029852,20.56948622029852,21.33750669389373,21.33750669389373,25.701118112134086,19.654943852599523,19.654943852599523,19.654943852599523,20.56948622029852,21.33750669389373,20.189775880774164,21.33750669389373,20.56948622029852,19.654943852599523,21.81809985608627,20.56948622029852,19.654943852599523,21.33750669389373,22.811747322179404,20.189775880774164,20.56948622029852,23.821861517029674,20.56948622029852,22.811747322179404,19.654943852599523,21.81809985608627,20.56948622029852,21.33750669389373,19.654943852599523,19.654943852599523,19.654943852599523,19.654943852599523,20.189775880774164,20.56948622029852,21.81809985608627,21.33750669389373,19.654943852599523,20.56948622029852,21.33750669389373,22.811747322179404,22.811747322179404,20.56948622029852,19.654943852599523,19.654943852599523,19.654943852599523,22.811747322179404,20.189775880774164,20.56948622029852,21.33750669389373,19.654943852599523,19.654943852599523,20.56948622029852,20.56948622029852,21.81809985608627,20.56948622029852,22.811747322179404,20.56948622029852,19.654943852599523,21.81809985608627,21.81809985608627,20.56948622029852,19.654943852599523,22.811747322179404,20.56948622029852,19.654943852599523,19.654943852599523,19.654943852599523,21.81809985608627,21.33750669389373,22.811747322179404,20.56948622029852,22.811747322179404,23.821861517029674,20.189775880774164,19.654943852599523,19.654943852599523,20.56948622029852,23.821861517029674,20.56948622029852,21.81809985608627,19.654943852599523,20.56948622029852,19.654943852599523,20.56948622029852,21.33750669389373,21.81809985608627,20.189775880774164,19.654943852599523,19.654943852599523,20.56948622029852,19.654943852599523,21.81809985608627,19.654943852599523,20.189775880774164,22.811747322179404,21.33750669389373,19.654943852599523],"xaxis":"x","y":[22.06759946491166,20.51569310181126,19.61997633871636,25.808363100543477,21.795376715513594,20.34038041027431,18.86081419340691,24.078413808713293,19.082528830592434,27.09570141066156,18.66432922183982,20.146924115160264,20.694537918678005,20.34827538264879,22.1287945428413,23.512029578625103,23.710849830139047,21.16813940491307,24.138707165857692,20.413583384325577,20.971944131576173,19.330306789697467,19.240535149076692,19.593384739494034,23.207168669114285,20.243485920599927,24.816169078762734,20.239179241482244,21.93825114094642,22.544699771649725,20.581001696999003,19.006769325349232,22.852829884665343,20.97322158881004,21.37041284025584,19.8813455186455,22.291944364905724,19.352118206703615,20.690574372087426,19.616726513482636,21.73777568014623,25.92407619036082,19.62630225708032,24.328668011629535,28.119486884464127,23.268620237666717,24.937914667295647,20.538966591972148,26.49472805759287,21.551656870776874,19.967005268430295,20.53263058829885,19.077522473826907,20.411222614242394,20.118559661298907,21.75794824506378,21.969944344762656,20.941991359524017,24.30144839882211,20.08875201356742,20.413158139175476,17.21138734793915,20.10520261649224,21.318516744600927,21.383177447413818,18.901120542977083,21.140418528305776,22.317825778768146,19.694551733623157,24.452943059423166,19.34725801943171,19.360313431748278,21.62865690143549,18.725486086935668,19.22798647641259,22.00766259568964,23.581082748790354,19.483945640419044,19.555879359056263,20.81759187582515,20.004050690864457,19.7027197284411,19.957005833266606,18.935012496616167,21.132460035639845,21.104713276655342,21.0706503248077,19.995538068644624,20.45373482710394,20.377850211599817,24.218899085058613,23.648191447276112,19.27757204338643,20.547353262390168,22.570203152455143,28.033188230056272,20.740147099081785,24.844950181647548,21.11139041842472,21.618102712310883,20.71863237230781,19.638742582781422,19.346013720701006,21.12828261562749,21.344903224909334,19.828653485294133,22.842407702796528,22.86005581934068,20.23340552018178,22.244828354517683,19.8468416379525,20.153224049031966,23.16226531069458,22.47425899129698,20.440974576504654,21.34321011850724,21.96825774467718,19.581570228112746,22.97307940319244,21.943790024620068,24.922685338201692,18.397908557303648,21.170757004644493,19.439732238591404,19.06759979313172,19.78225599625029,20.88710307988011,20.06163488065717,21.227834779401228,20.925901548056537,19.64120627043042,21.51013491039007,19.943833745099443,20.041179196484592,19.31991683554051,21.305505708300736,19.083972676252298,22.36486950758078,21.00153722158492,20.91944021038121,19.71891096707218,20.43515061037962,23.756601825433552,19.29612083176196,27.288645329988054,20.004627845911102,20.581937941283943,23.06049210432163,19.0283827986455,22.26292897661663,18.98289333455679,19.08654493081137,20.26723932452398,26.772642502942784,23.150894170391226,22.789865927709045,21.316932785410458,21.47925775839002,19.52466856143405,17.31416721288844,19.67390026636621,21.280848347151327,20.575951881264743,19.36981390700933,22.906270958120995,23.259273464150183,21.034942664057727,19.478866371959384,21.521316718605924,23.251480243552532,18.947642644803835,18.234449818345436,23.74556559722114,23.70046067341344,22.17893043543761,23.314842649926593,19.204628901280135,21.028986544289005,19.811383905702034,20.105414612220844,23.470671408984025,19.185541351838207,21.863096949082852,21.439837984160558,20.28595076567891,20.09465504412721,24.347469240771108,21.026538976608304,22.25115601355335,19.17189511189754,19.330727902738385,20.455200776646638,18.7812284253763,19.9984664758684,19.71179802639097,23.222688495940556,20.75461515785591,21.453079563108336,19.968121134960665,23.31621219099147,22.229186273778243,19.29157093266521,19.725980900417238,19.393396349595932,19.510476738006748,23.86936283890287,20.803398322957783,19.50445726786541,20.266025864180154,20.318803159054653,19.88287753302219,21.755410336588376,20.021014007950576,19.81464925130666,19.600081112964823,20.195578868662224,22.127386699288984,22.615494713832,21.994462034082755,23.008367409641956,20.02036350615788,19.496913617064063,19.65284705132002,22.970383373621917,19.800442868861996,19.544786543412965,21.196126618068195,22.706377974056313,25.14188655199703,19.430343691015025,20.82549425312562,21.300868648362627,20.935011057360228,20.238709984208988,23.361271159574002,23.97311707542301,20.93275451807793,19.135842027978637,19.419925212472517,22.69657897049395,18.04707863542646,21.639091593296747,24.902956648700677,19.472341179018663,19.630944296038688,19.581570228112746,0,21.279464358226686,20.6373536565784,20.55168503037007,22.252924089212918,17.011336466938804,20.539904719671835,20.287228770242784,22.579718287600105,20.016747429777542,19.919862789535355,21.590649178257912,20.459085985918723,19.305685493406397,20.69934199839951,21.814001605688915,21.49724409211446,17.96562762596234,18.897059576214055,20.218149025767367,21.663202256858984,21.537426968242833,19.881144027962787,18.109927551497627,20.650313109616484,19.551649361459194,18.954616641321604,18.33673536238825,19.272777840955303,21.22721049508145,23.36907437848721,21.839279632705356,20.080313988880153,19.791725695449298,22.830034474576443,20.038176858751854,19.882805718288704,19.137324212876308,19.89611063211052,20.922839608419043,22.73108571789678,18.798041102461394,19.809989451392685,21.628482474975726,22.66651647870706,20.105565995835796,21.324104166055236,23.33275256138136,20.80501366619404,18.787924871827784,20.381292625866948,23.456138303517488,20.59828494694422,20.938551777966236,22.709886920964298,20.558379254361796,19.758613410846518,21.618914296187317,22.889842311610867,21.30122097904847,19.0336054859068,23.801438495307856,24.840309004310797,19.694954646749302,20.944503948247828,20.620570884389835,21.72592495902031,20.296054717472362,19.58398989769835,20.2956272827196,21.085706497425413,19.461766940454588,19.29900449887296,22.510563057499144,20.841106100432793,21.801619166438428,21.147132242697182,21.551582010518878,20.4427602890876,24.611415306054898,21.490696787354505,21.24369655260969,22.432940196049124,22.234424235191682,20.099462108962847,20.800416502964755,20.17949699394329,21.746512886013605,22.496434708234126,19.78606039431591,22.229470444639624,19.19842345841963,21.071706539354626,21.28737646300594,20.120777025175872,23.69806207100269,24.25287711008343,19.074352191519743,20.558351575498786,21.437400189861094,20.352963298343866,21.233800456337935,22.14457282995132,20.722700819009958,20.69715285443839,20.03253125195289,20.0566191944814,22.70946891023392,21.137414277628565,19.808943756196857,20.143175790739598,21.040198736231254,19.712186074557728,19.35146728073557,20.861088989018167,20.755317114550273,0,19.017540081468233,21.419330156983328,20.67873788317677,19.93668394257525,22.409766675366317,19.27540160418116,23.728782472563765,20.376074503254586,19.31077695697734,21.065895324749285,20.3484179021553,20.07860380095908,19.976548041795603,20.235086321270654,21.01213158467734,21.41999462819776,19.174863067555084,21.10241852261651,20.85643673732399,22.586773372950685,20.037769462388923,21.559823540829672,23.13793422908773,20.558605220150522,19.683189962828273,20.39789928981696,19.337397595787667,22.26889032385089,21.195293500127566,22.559724250840063,24.39611039895289,19.59271980995523,22.136011068345166,19.33796659591498,20.797749380419276,21.567058771175564,20.725266392758815,18.468910364525364,19.663251453070878,18.918473122196858,19.198971627350232,24.325766932844235,19.86193501563865,21.156628899315663,22.938277348217966,20.78722963747693,20.66031803587651,20.155379081002646,19.457981357627542,25.713720052658104,20.828559454854545,20.52544147367378,21.147722912842873,20.03302501817969,24.630356510701596,23.121909718158822,23.106213073613993,22.51080834568276,20.883881259335507,21.661257500272384,19.85555820528158,19.526632957320885,19.191856386440307,22.96557469826004,19.96989517504386,19.756880613939327,20.841336970061143,21.615161729704354,19.630957845101975,19.107266091841186,18.360084050573274,20.178689314774683,22.61224657942639,20.902818910258993,19.692513024244924,21.22795725431778,21.431836012474292,22.58649267850949,23.56945490190484,22.15739813287633,19.523673217064264,23.682062419502355,22.448170385491473,23.624330847094043,20.4998468600676,19.29906600505008,21.469193647687533,22.680649469911085,21.178626711012377,21.686546691263366,21.428479917582088,22.88912263482666,20.804770608680126,19.28855746111665,20.502951107445575,20.22322663992422,22.31457162800466,19.478115876564008,18.795995233915015,25.17177163891695,23.160406649511106,19.25160491688623,20.826937118763155,19.953135265171547,23.933068642871486,19.355480148534408,20.44485249695551,19.56616125697471,25.23181389258149,21.345469949872594,22.776042002687795,25.303110182218393,19.58463718752616,20.204391739449186,19.893828839264728,20.33821258053668,24.68330601022857,18.821811344100055,19.840511525069328,22.297566604177444,20.78875924862035,19.741131185755275,20.04722528013383,22.468811326543563,19.99457856414159,21.80053568878591,19.167558108393887,23.846962977749094,19.07645844042533,19.307592868706436,19.883803436319816,19.767052898077655,18.952911838336565,18.62774549084819,20.73926969664862,18.627786784209636,19.177777189052396,20.089958597754855,20.801380865239704,17.764101156137162,19.11561871251436,19.24667566091232,20.93405408413005,21.340147837597844,21.090725928345268,19.745386795650543,19.095931371701568,19.810878030088894,20.251155291034372,19.165221068220944,20.724802969675316,19.74733497869572,22.856134451929176,20.542598325656595,19.502159734210206,21.979041347822633,21.2197623270523,26.969103234931445,20.18342033833375,21.02499268359878,20.592440384261195,19.990444456103596,20.867172390851817,21.273367636241293,21.243839329097405,21.363500625759567,20.18486444945059,20.241113097705927,22.693372098426487,18.623284957897813,19.205602363695707,21.545170940853957,20.80034066129298,19.981973707277767,22.19000668879013,22.718490023684353,20.707922743251714,19.214188218113872,22.472827160217914,19.07108521476948,20.346151863763343,21.63013067480897,18.214892995560163,19.93614600441511,22.53034703709958,19.469859640957857,22.895450751816803,21.00807772677881,21.431258349952667,20.941808459871957,22.99407646829777,19.76111291920157,19.008644725628592,18.355858634806896,20.297451997659746,20.474380309584664,21.870871511939562,20.759740299921024,21.439000175436664,22.030880607811586,23.00587909853429,20.655014419256375,22.965146977089795,22.43523710461151,20.18471556395119,19.019883876304206,23.05070508803856,21.499214768467855,20.59203905710115,21.024704798577186,22.72181503272866,22.377050963043796,19.71871179715288,21.93139047566312,21.827309362120346,19.1796783064222,19.826388660339887,23.65560944108187,19.199993380834425,22.21417220655097,19.269925661522706,23.782692532330046,22.173362238186836,23.41808975500013,20.872940195293037,21.54216753533708,21.03393309328942,21.663729644062077,20.699641074284234,21.39606694375647,19.593416427729718,21.355622092522935,19.49242240707679,19.406777996109206,22.625652665035147,19.860111721120674,14.03702355520748,22.00972851632279,20.33705521659505,20.155471422765178,20.40843263005365,23.540959857497864,22.64902697475969,19.51901613478499,20.748003621067692,19.075332900397907,21.535462867247563,19.35190112925433,16.543892756530866,22.048803804340604,19.94957596782787,23.253137677025506,15.802066947826521,22.01816825105002,24.65495079669392,19.684337709357063,23.07232514287102,19.441821622178967,20.90283362751463,21.65340972782181,20.672219865399406,22.248853822319084,22.007999054329076,21.514848387611945,18.91123104739614,22.195045634281605,21.092338092530998,19.27798585366196,21.725547245838506,20.321666107127033,18.844349286283478,20.211183384442457,20.06069065496676,20.388592288941165,25.880225035189923,19.68504303470752,20.795739259102447,19.874915797147626,20.484467680558765,21.537004350283745,19.454353518988064,20.471143313052202,20.221414835639845,21.30681378731033,22.893554470389834,23.892417838743924,19.773417735905042,21.39811634221899,21.94580529932221,22.093666756982273,19.566140232030243,19.62938152434226,22.598971957007752,22.606867611519395,23.548837661891714,19.57985452716288,19.257215175267675,20.260215495083163,21.269107840654502,21.892874617975046,20.911342976063032,20.690267441816314,19.415723203109316,22.536424619535012,19.241608485058645,19.162579413291002,20.470409642279566,20.97348174432658,23.73507977629211,14.55836091863428,23.336410397420998,21.627932359551377,21.49391553553141,20.265469687416747,19.426854398695138,20.625264709408093,20.223930403747513,21.865935415382868,21.881578906408237,20.14846091060974,19.81407448278221,21.0024847282008,20.945391681085823,22.291777470342872,20.551017265346704,19.577722584113683,19.444205825873514,21.46402517777047,18.277738745265133,18.799932943985397,21.60163932304092,19.80978333592071,20.91463622707276,21.547692385839394,19.17999258814081,19.549428996570953,19.54492482512454,19.294068756456163,21.69613129076446,20.048671453648097,19.49098989799628,20.42092755989845,20.047998538616795,22.735567798975932,20.159778968313912,20.920832580620292,19.699545342858148,20.678206389277292,20.079358473730228,19.917597162486278,19.692880228017533,26.26193365893149,20.33327111425421,21.201441496966886,24.081396558763124,22.374129418340807,22.181711726885904,18.957794711482784,21.63591845810156,22.742154187536123,20.277557571355647,21.712167598440185,20.922319986303112,19.981118924422468,18.376324675581085,19.093096903348385,21.041684548715036,19.88267606965292,21.79470380164697,19.394929062748655,19.711931797206645,20.701777070204972,18.966254774482525,16.57594392051226,19.3661810523187,19.637857750008234,21.05876451742095,21.39188913879269,20.922214169866013,20.158015290040524,19.840625811644227,20.287989620862316,20.392027938746434,21.440477364164977,20.18838178436931,19.209224880205603,19.523793560708743,19.767710849855263,21.57006553499441,20.748925773432315,21.47821316674072,24.169496968680054,22.350651659007692,22.60440595404428,20.973185759004682,22.891726790683144,20.2096151875794,20.22349858267661,20.901209408549594,19.053706845403045,23.000908813982363,19.549547798448483,22.637005220660473,23.38302130821851,25.236220259743373,22.107609158967936,23.256395298587126,21.31115959043422,19.242152996635603,23.069120127461606,19.88568939220856,20.74441094675746,20.977820033541025,21.403421935393933,23.155520679582406,17.74375624808358,21.65558749657167,22.72177363339029,20.057436191229318,20.31682150206961,21.442049003709382,20.447439187365134,18.64679105166152,23.081069556252004,23.109422435163253,19.40981146751902,21.538742146089504,20.451115883369862,20.520204300445414,19.072268708954628,21.853488891111965,19.870759032200375,19.874160110175985,20.45611373193672,20.923455889638944,15.423378576518493,19.495183602853512,20.532198000108426,20.172754664219987,19.621134659665625,21.149770683357705,25.5492833010917,20.91962424027046,23.144648130459657,22.724819827816283,20.76732896720974,20.90466916348112,21.019251701774245,21.870062515886435,21.968529606994224,18.28303322918483,22.7967578369547,24.617902294510255,20.337523908425865,22.278345092825212,20.439903152029967,20.745981176528062,20.50317548758169,20.148792615003963,20.18507504172347,19.887802201303376,21.335667578998795,23.547889563240428,22.184719195487645,19.124759964560997,19.09456742423413,23.546328229722338,23.676810419954805,21.378549935187863,18.772971741326142,20.92343911431291,23.66501619396984,20.362922084924325,19.335122075152558,20.6322831501737,22.014406532680294,20.862569257472042,22.377941930707426,16.70615872384773,21.114598939015725,25.312874128977917,19.557332226454086,25.0888235331049,0,20.821375100396438,19.69889252664812,20.730065203035867,20.737794346856816,19.720177445507503,19.751886432327307,21.436234720492326,19.12391425698277,21.093445290401018,22.18427320470571,23.32578333060166,20.235023779555554,20.38834949087981,19.076215392899233,21.890199695637197,19.626240196846272,19.473911816076555,22.65051997686549,21.07153535807478,20.152641197123305,25.817249936117022,20.72049143867345,20.000328449472278,19.677575133698696,19.854767962698624,20.90714373186156,21.852326049162542,22.34522357506863,21.88381079939571,20.64279914121781,20.008240359915625,20.079932477068777,20.97646133780068,20.617037831360907,20.86340137149202,20.10297814136792,18.939248297157278,19.337083944174847,21.21789500066683,19.959413629413806,20.57158693763026,20.1483293247843,22.45828794986169,19.223167412657357,22.090969450288238,20.54580254002858,20.99006223015013,23.933428716720645,18.97273303540371,21.839004592901738,17.66325927218184,21.098649782686852,22.108159109401438,19.70668845124008,20.687407722542194,21.23800756191085,22.54894428694912,20.440949745295235,18.136670762341843,21.301837769324877,21.032952737462605,23.31051349089806,20.969732200565332,20.351613259246466,19.678510738089678,22.059041559037812,22.10097025846529,21.127912480118983,24.425354799557958,19.405573621751774,18.954513256380405,20.62820521809647,19.21222562648202,18.9881876444261,21.62524006847279,18.78833918294514,19.827821621310783,19.4644839297056,26.34702224299619,21.38205653661138,21.78972267747008,21.087755789009996,18.999355953219133,19.451729034530373,23.289040906826244,24.16646354231811,21.536078922759724,21.530906360636845,20.97243154975957,20.089266034468004,20.436904665975636,19.20088663487939,20.554011225634962,19.277982733151024,18.304477915148002,19.118363214083377,20.261720490850706,19.26508618967162,23.063753511610845,19.05336380976314,19.240852168960235,19.807795916443087,19.527755462443025,19.369940873414674,18.557194427644223,19.092216129486825,23.457850638636746,20.3986497634222,20.79850265219969,20.611470634120458,21.03390016791011,24.957468167964976,19.132327452939787,22.668945885967275,20.192548076466373,21.926818619899166,21.236410750491928,22.00439862331522,22.64890259783158,19.876543924901863,22.174820865681593,20.001613842126076,20.63081635106876,19.653189714082398,19.305175301710232,20.224827212369068,22.6279649673046,21.547003936026666,18.961602303420257,20.695469281090816,20.114481067581714,20.790519525351012,26.143565404950515,21.151917099556776,22.601975102609295,18.386700720994313,21.48658316808204,19.99564295418165,22.704086007044513,23.646718002570474,20.17407123394833,20.29544946282625,21.94083977054476,19.087820197169854,20.49274951238928,21.58556546053215,19.868098908707033,19.806349800221792,23.600603274171462,19.846459862589864,21.099407208256782,21.035986420389108,23.747615973251953,21.078530347858226,20.86782723578085,20.073998999202523,19.385623092000827,20.214357069981805,21.756421739524402,19.573874587511387,22.17732889464112,20.092049623606254,19.621522497700703,20.13212471121583,22.84612630027242,21.610638293924836,20.381527241115613,22.69863600121815,24.241029788998915,23.65992231174119,22.296121251970213,20.702441582535556,22.897176211133427,19.850331267520566,17.677965293255863,20.87727804782974,24.045999647847655,20.173501472804098,23.33182107390064,23.30215527163116,27.231751620760654,20.650611348398325,16.829816883719186,21.108904064764797,23.735782522109897,20.656510624448828,20.096689421256738,22.83710046840733,19.824675810238606,20.823945313943256,23.256137551486724,23.633181830866906,19.043844337217745,23.65829637497728,23.50747794569296,21.73683233585413,20.94197381362215,24.18225862695574,20.14794970691058,22.344681402352435,20.447944597167705,19.21271648764287,22.508853453023672,21.75045501313216,20.060863127783303,23.041170672161062,19.554654657325607,21.26551890511897,19.47575297115503,20.304093087106327,21.96980947524255,18.890513667640615,20.207528404900692,20.825342365179193,19.58596744655146,19.86547955920327,20.093122217042076,19.51138012046539,20.89052275919248,19.726434174485984,19.26180569731426,21.223090796665396,20.071086524480997,23.60048678836413,21.682830461889793,22.115896159241377,23.245004397316144,19.929597223002617,20.206849822052806,20.80607598372765,20.972697281096956,20.548932178877266,20.790175259909486,21.65577788956451,20.396533708238668,17.677595133772794,20.206889160267256,19.864164508631692,20.604884938737765,19.69979302428942,21.112079923463828,21.776216944868956,19.41019874513054,20.084542414131516,25.538294718669484,19.887154874828134,22.02600227823102,20.06242269100319,19.908561207609715,19.025787914622292,22.519912388376707,21.15310774495207,15.434839951572775,23.416469489833915,21.472713147192902,24.799241151178187,21.565777300167408,21.24511744232927,23.786142675104543,21.775846548503456,20.874095085342866,20.525903087310763,22.190296420947053,17.526091919071504,22.787613064091445,19.95220428650718,21.56191951932416,20.302727045932432,22.39076356657128,17.788361488629835,19.89495594402764,20.849863964556313,22.01047671458767,21.9419703223203,21.769327477492673,22.134078601621123,23.02213648046312,21.659326126254854,22.503975157123477,21.687400780796548,21.299913824117656,24.41218042654058,21.628416169590647,20.661666774415252,19.13829898996108,20.6534474504661,20.599627769624064,19.476347089125902,22.633338504709375,19.004867830543457,20.729309561171004,21.215021611787833,20.55962915472571,23.96741801447904,20.948249582299137,19.034944006994706,21.25562508835354,23.6936808146177,18.876298103633747,20.12057913956066,22.988276940898057,20.761017121909465,29.702932107840674,19.34717798381203,19.04780522125321,19.72022056098374,25.87586915107988,22.381251164844315,21.45471998887307,21.894206057556605,20.997149152437558,19.346653316091935,19.69023397394279,21.75892392666085,21.695257182632144,24.304938166770185,19.96312680068985,21.08058139373211,18.457821713581545,21.058645462338415,20.370456476417175,22.39756222611425,21.96355600303241,24.29338064398619,19.533504643608662,19.24597971762759,21.061479969798718,19.653131971320136,19.991828535916927,19.41354451956652,19.024392506006556,19.995956185326282,21.080207924079772,22.895336936087247,19.00379513256332,19.59081933226322,19.683155687698772,19.724580465382388,20.49815774428782,21.41646356230674,20.374729613814893,19.698149844649336,20.827570650601533,21.55980522344904,20.203106297950406,20.998113148879906,22.5464416348448,21.94562707940607,20.109117749689304,19.61600010557632,22.28815948936714,20.00070564420032,19.48773603183014,22.96164822427184,18.99960834234589,20.932965503614184,18.548961097016324,21.40256127224899,23.343368321662904,21.395751936133603,20.367816242708102,21.629837063073623,22.815206783859768,22.704911104222457,20.7857551664019,20.3067025549342,20.61336030755643,21.781868242254394,20.56504163001677,24.58750691743097,21.042555130756018,19.718574682047347,20.82900495573487,21.89094777167843,20.46100849412487,20.676958661968467,22.376303078727783,21.96712491711981,21.09072172933059,20.373076483151994,19.53720891947194,19.37233862557095,0,22.656078929958284,19.971799365414682,21.138173662648782,25.881734625061707,0,22.042981377352834,19.12600868541295,20.929534739770034,20.604603448464516,21.393924057871644,18.99682101244613,19.31053914628864,18.986258705756388,16.7233596191811,19.687016476370815,20.364732076794134,18.828527980887348,21.061860954738233,18.824250825082274,19.28149315085171,21.023652845270533,23.394529944648035,22.371116977331784,21.19573925103901,18.93658618322981,19.94401710262461,19.47904998638186,22.850840758637354,20.76699650557159,20.40036143331872,17.795936296212627,19.421904563712328,20.64673096490357,19.639706353282477,18.775317705171794,20.047574770414855,19.61260734468754,23.666636204423178,23.480370137373004,19.218266855752937,21.60748880425466,21.257489419039537,21.101232900314493,21.320633929890864,21.849984955018474,20.40434121243742,20.032547562218568,20.035932572983462,20.864093173595,21.51301334156099,22.29182284846925,23.44987910374218,20.855425894539348,22.64951341150128,23.894186440194584,20.26686372980413,20.408744363117133,19.046718202293015,20.904214174162476,24.308788888311135,20.77592509493985,0,19.644171392658865,18.610921424553307,22.26726247442381,20.903586582709064,21.2082248307893,20.867431932395597,19.247883485213144,19.09858987177298,20.769205079804532,21.73660672493293,20.24751150890443,21.281868210231526,19.724170230709305,20.553728301262954,21.112413416140527,20.46206885900041,20.668149201097204],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predAB_log"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB_log = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB_log = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB_log,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB_log, \\\n","BestParametresAB_log, \\\n","ScoresAB_log, \\\n","SiteEnergyUse_pred_logAB, \\\n","figAB_log = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=SiteEnergyUse_train_log.ravel(),\n","                         y_test=SiteEnergyUse_test_log,\n","                         y_test_name='SiteEnergyUse_test_log',\n","                         y_pred_name='SiteEnergyUse_predAB_log',\n","                         score=score,\n","                         param_grid=param_gridAB_log)\n","\n","print(BestParametresAB_log)\n","print(ScoresAB_log)\n","figAB_log.show()\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.5469398799999265,1.5612864522300371,1.5286327090432743,1.5385200021673335,1.5095124652936975,1.5607153188447598,1.5472282424982962,1.5981579138859705,1.7297418785419314,2.3394038039512566,2.0999152435974424,2.5963053983815234,2.286189402318796,2.7979771485056175,3.0163334490875453,2.9297025442354663,3.3340917685612617,3.648798699341915,3.6955267992639307,3.9960274672484815,4.386166116052357,4.75532567147408,4.841244198540116,5.115858360742324,5.307858912363384,5.453903857722857,5.688848485490568,4.042266336556571,5.26413903533704,4.960825278413312]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.649747691429396,1.6923911613712463,1.5941829342044902,1.62496470724396,1.5901894482095877,1.673151209499115,1.5946129703376668,1.673421128213455,1.9761455239714865,2.764565267176871,2.6366934188623765,2.915931946088748,2.6622404331670517,3.08897631247489,3.1936719251481716,3.165943762387368,3.810401887849122,3.917194436157454,3.8405563313871522,4.221132894319819,4.64157550523978,5.319442612951228,5.9984786778513195,5.939815405679758,6.110008624557031,7.268070873535038,6.127417548800447,4.9954484972618864,6.879770337498339,6.047711649756909]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.444132068570457,1.430181743088828,1.4630824838820584,1.4520752970907072,1.4288354823778073,1.4482794281904046,1.4998435146589255,1.522894699558486,1.4833382331123763,1.9142423407256421,1.5631370683325083,2.276678850674299,1.91013837147054,2.506977984536345,2.838994973026919,2.6934613260835647,2.8577816492734014,3.3804029625263756,3.550497267140709,3.770922040177144,4.130756726864934,4.191208729996932,3.6840097192289116,4.291901315804891,4.505709200169738,3.6397368419106755,5.2502794221806885,3.089084175851255,3.6485077331757405,3.8739389070697148]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5190024460223415,1.517652463298734,1.5583079276068497,1.527628719797852,1.5240605445319266,1.5901015412523456,1.5311867332137767,1.522490778819338,1.5408649310098828,1.9915389868545539,2.0076786238295883,2.483867755334643,1.9421838054463867,2.6969403975930644,2.9512632893167905,2.755848640795311,3.6089178544998566,3.743060787597724,3.6583902151124534,3.849099188266236,4.783221368796979,4.825591409865271,5.349417300003254,5.377480630596132,4.658824278892367,7.537146946281177,6.421251195635863,2.67602960465816,6.459722935036355,4.110145450324169],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5862330635413024,1.602978227971985,1.573245364711395,1.6042785542192064,1.5918527099428894,1.552346348684661,1.6089178096419097,1.5686278976904535,1.568705064925906,3.135642895381782,3.15465011896899,3.1849400624467417,2.7108103463038624,2.6198447917978065,3.34230031142118,2.968689827247689,3.801129299097442,3.5641632684179245,3.79048160877652,4.254647883143587,4.054784031314975,4.4232770213180075,4.261076410696673,3.515151375272582,4.511455156495997,3.811671361132677,5.708770055545354,4.372840769102196,5.770827300137595,3.638018946075766],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5909473001710912,1.7929125023357717,1.5876155141253423,1.6267611147870042,1.5724886515398389,1.5779866617091665,1.5969181437371842,1.5768323503933979,1.5928285156491133,2.2702292621392783,1.725561537101884,2.2469968804950318,2.7723154545269137,2.757819418473533,3.0361822381917984,2.795028725823534,2.7791842781071194,3.2663925261823623,3.4440215312085876,3.7420084377483622,4.209616476961902,4.753830498040749,5.368792107389733,5.712130790557555,6.344927997871624,6.142581061166996,5.058700054561678,5.127436032158673,4.416980128182327,5.260766057584325],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.672642751080316,1.4801501348088573,1.5179158107682253,1.5532621883458284,1.4959427920385562,1.7165628071890382,1.4909024754403708,1.7428638586712641,1.744331455295689,1.9599047650253645,1.7484709681947124,2.428923973721695,1.9269482645010392,3.3631436793476115,2.9349232525996647,2.753956003768937,2.7330190854317107,3.581795387823299,3.7124952647613587,4.279600527587061,4.338196785077849,5.735871695911926,2.922454211833688,5.7445494988999375,6.220146972611229,2.8510699061491587,5.5268819119589025,3.184421966596574,2.5692181760769963,6.788689024840226],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.3658738391845815,1.412738932734838,1.4060789280045594,1.380669433686776,1.363217628415277,1.366579235388587,1.5082160504582391,1.5799746838553987,2.2019794258290655,2.3397031103553045,1.863214969892036,2.636798319909506,2.0786891408157753,2.5521374553160725,2.8169981539082927,3.3749895235418617,3.7482083256701806,4.088581526688267,3.872245376460736,3.8547812994971604,4.545011918110084,4.0380577322344475,6.304480962777226,5.229979508385411,4.803940155945707,6.927050013884273,5.728639209751042,4.850603310267247,7.103946637251926,5.006506913242072],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=square<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB_log = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB_log,\n","                                 'n estimators', GridAB_log,\n","                                 BestParametresAB_log,\n","                                 'adaboostregressor__loss')\n","FigRMSEGRidAB_log.show()\n","if write_data is True:\n","    FigRMSEGRidAB_log.write_image('./Figures/ConsoGraphRMSEAB_log.pdf')\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()</th>\n","      <td>0.845165</td>\n","      <td>1.137523e+07</td>\n","      <td>3.431214e+06</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()</th>\n","      <td>0.826684</td>\n","      <td>1.203498e+07</td>\n","      <td>3.217855e+06</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()</th>\n","      <td>0.819662</td>\n","      <td>1.227637e+07</td>\n","      <td>3.230115e+06</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()</th>\n","      <td>0.304837</td>\n","      <td>2.410289e+07</td>\n","      <td>3.513263e+06</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()</th>\n","      <td>0.459721</td>\n","      <td>2.124882e+07</td>\n","      <td>2.905579e+06</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()</th>\n","      <td>0.475406</td>\n","      <td>2.093811e+07</td>\n","      <td>4.403702e+06</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               R²          RMSE           MAE\n","Lasso()                  0.845165  1.137523e+07  3.431214e+06\n","Ridge()                  0.826684  1.203498e+07  3.217855e+06\n","ElasticNet()             0.819662  1.227637e+07  3.230115e+06\n","KNeighborsRegressor()    0.304837  2.410289e+07  3.513263e+06\n","RandomForestRegressor()  0.459721  2.124882e+07  2.905579e+06\n","AdaBoostRegressor()      0.475406  2.093811e+07  4.403702e+06"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["Scores = ScoresLasso.append(\n","    [ScoresRidge, ScoresEN, ScoreskNN, ScoresRF, ScoresAB])\n","Scores\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()_log</th>\n","      <td>-0.136205</td>\n","      <td>2.358827</td>\n","      <td>1.130725</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()_log</th>\n","      <td>-0.169007</td>\n","      <td>2.392635</td>\n","      <td>1.124331</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()_log</th>\n","      <td>-0.149984</td>\n","      <td>2.373087</td>\n","      <td>1.125546</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()_log</th>\n","      <td>0.349275</td>\n","      <td>1.785116</td>\n","      <td>0.845344</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()_log</th>\n","      <td>0.349607</td>\n","      <td>1.784660</td>\n","      <td>0.782007</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()_log</th>\n","      <td>0.339668</td>\n","      <td>1.798246</td>\n","      <td>0.879046</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   R²      RMSE       MAE\n","Lasso()_log                 -0.136205  2.358827  1.130725\n","Ridge()_log                 -0.169007  2.392635  1.124331\n","ElasticNet()_log            -0.149984  2.373087  1.125546\n","KNeighborsRegressor()_log    0.349275  1.785116  0.845344\n","RandomForestRegressor()_log  0.349607  1.784660  0.782007\n","AdaBoostRegressor()_log      0.339668  1.798246  0.879046"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["ScoresLog = ScoresLasso_log.append(\n","    [ScoresRidge_log, ScoresEN_log, ScoreskNN_log, ScoresRF_log,\n","     ScoresAB_log]).rename('{}_log'.format)\n","ScoresLog\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R²</th>\n","      <th>RMSE</th>\n","      <th>MAE</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Lasso()</th>\n","      <td>0.845165</td>\n","      <td>1.137523e+07</td>\n","      <td>3.431214e+06</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()</th>\n","      <td>0.826684</td>\n","      <td>1.203498e+07</td>\n","      <td>3.217855e+06</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()</th>\n","      <td>0.819662</td>\n","      <td>1.227637e+07</td>\n","      <td>3.230115e+06</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()</th>\n","      <td>0.304837</td>\n","      <td>2.410289e+07</td>\n","      <td>3.513263e+06</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()</th>\n","      <td>0.459721</td>\n","      <td>2.124882e+07</td>\n","      <td>2.905579e+06</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()</th>\n","      <td>0.475406</td>\n","      <td>2.093811e+07</td>\n","      <td>4.403702e+06</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso()_log</th>\n","      <td>-0.136205</td>\n","      <td>2.358827e+00</td>\n","      <td>1.130725e+00</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge()_log</th>\n","      <td>-0.169007</td>\n","      <td>2.392635e+00</td>\n","      <td>1.124331e+00</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet()_log</th>\n","      <td>-0.149984</td>\n","      <td>2.373087e+00</td>\n","      <td>1.125546e+00</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor()_log</th>\n","      <td>0.349275</td>\n","      <td>1.785116e+00</td>\n","      <td>8.453436e-01</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor()_log</th>\n","      <td>0.349607</td>\n","      <td>1.784660e+00</td>\n","      <td>7.820072e-01</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor()_log</th>\n","      <td>0.339668</td>\n","      <td>1.798246e+00</td>\n","      <td>8.790455e-01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   R²          RMSE           MAE\n","Lasso()                      0.845165  1.137523e+07  3.431214e+06\n","Ridge()                      0.826684  1.203498e+07  3.217855e+06\n","ElasticNet()                 0.819662  1.227637e+07  3.230115e+06\n","KNeighborsRegressor()        0.304837  2.410289e+07  3.513263e+06\n","RandomForestRegressor()      0.459721  2.124882e+07  2.905579e+06\n","AdaBoostRegressor()          0.475406  2.093811e+07  4.403702e+06\n","Lasso()_log                 -0.136205  2.358827e+00  1.130725e+00\n","Ridge()_log                 -0.169007  2.392635e+00  1.124331e+00\n","ElasticNet()_log            -0.149984  2.373087e+00  1.125546e+00\n","KNeighborsRegressor()_log    0.349275  1.785116e+00  8.453436e-01\n","RandomForestRegressor()_log  0.349607  1.784660e+00  7.820072e-01\n","AdaBoostRegressor()_log      0.339668  1.798246e+00  8.790455e-01"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["CompareScores = Scores.append(ScoresLog)\n","if write_data is True:\n","    CompareScores.to_latex('./Tableaux/ConsoScoresModèles.tex')\n","CompareScores\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()"],"xaxis":"x","y":[0.8451651548527583,0.8266837066551648,0.8196616772469518,0.3048372839343573,0.4597212006724831,0.4754058925166845],"yaxis":"y"},{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()"],"xaxis":"x3","y":[11375230.184028259,12034984.568041615,12276366.760484876,24102887.69654305,21248819.363039464,20938113.302967895],"yaxis":"y3"},{"type":"bar","x":["Lasso()","Ridge()","ElasticNet()","KNeighborsRegressor()","RandomForestRegressor()","AdaBoostRegressor()"],"xaxis":"x5","y":[3431213.7507750085,3217854.979656839,3230115.192541209,3513263.208567951,2905579.3719053036,4403701.575562778],"yaxis":"y5"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log"],"xaxis":"x2","y":[-0.13620467920105894,-0.16900745367594738,-0.14998366919458284,0.34927507197044594,0.34960726833839983,0.3396676435516214],"yaxis":"y2"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log"],"xaxis":"x4","y":[2.3588268136194133,2.3926347680520297,2.3730867019508475,1.7851161997701304,1.7846604889199695,1.7982458153316956],"yaxis":"y4"},{"type":"bar","x":["Lasso()_log","Ridge()_log","ElasticNet()_log","KNeighborsRegressor()_log","RandomForestRegressor()_log","AdaBoostRegressor()_log"],"xaxis":"x6","y":[1.1307246383546765,1.1243313514677742,1.1255455230576976,0.8453436167333311,0.7820071827154659,0.8790455322973952],"yaxis":"y6"}],"layout":{"annotations":[{"font":{"size":16},"showarrow":false,"text":"Consommation brute","x":0.22,"xanchor":"center","xref":"paper","y":1,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Consommation log2","x":0.76,"xanchor":"center","xref":"paper","y":1,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"R²","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.8666666666666667,"yanchor":"middle","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"RMSE","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.5,"yanchor":"middle","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"MAE","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.13333333333333333,"yanchor":"middle","yref":"paper"}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Comparaison des scores des modèles de consommation"},"xaxis":{"anchor":"y","domain":[0,0.44],"matches":"x5","showticklabels":false},"xaxis2":{"anchor":"y2","domain":[0.54,0.98],"matches":"x6","showticklabels":false},"xaxis3":{"anchor":"y3","domain":[0,0.44],"matches":"x5","showticklabels":false},"xaxis4":{"anchor":"y4","domain":[0.54,0.98],"matches":"x6","showticklabels":false},"xaxis5":{"anchor":"y5","domain":[0,0.44]},"xaxis6":{"anchor":"y6","domain":[0.54,0.98]},"yaxis":{"anchor":"x","domain":[0.7333333333333333,1]},"yaxis2":{"anchor":"x2","domain":[0.7333333333333333,1]},"yaxis3":{"anchor":"x3","domain":[0.36666666666666664,0.6333333333333333]},"yaxis4":{"anchor":"x4","domain":[0.36666666666666664,0.6333333333333333]},"yaxis5":{"anchor":"x5","domain":[0,0.26666666666666666]},"yaxis6":{"anchor":"x6","domain":[0,0.26666666666666666]}}}},"metadata":{},"output_type":"display_data"}],"source":["fig = make_subplots(3,\n","                    2,\n","                    column_titles=(\"Consommation brute\", \"Consommation log2\"),\n","                    row_titles=('R²', 'RMSE', 'MAE'),\n","                    shared_xaxes=True)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['R²']), row=1, col=1)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['RMSE']), row=2, col=1)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['MAE']), row=3, col=1)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['R²']), row=1, col=2)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['RMSE']), row=2, col=2)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['MAE']), row=3, col=2)\n","fig.update_layout(\n","    title_text=\"Comparaison des scores des modèles de consommation\",\n","    showlegend=False)\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Modèle de prédiction sur la consommation énergétique (SiteEnergyUse) avec les données catégorielles"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["BEBCat = pd.read_csv('BEBCat.csv')"]}],"metadata":{"interpreter":{"hash":"117c35e9bc21f93c21d2b781ffd59753bc10bfa2757aefbea5ab108f880d10a5"},"kernelspec":{"display_name":"Python 3.9.9 64-bit ('.env': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
