{"cells":[{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","pd.options.plotting.backend = 'plotly'\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","from sklearn import metrics\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, \\\n","                             GradientBoostingRegressor\n","\n","from Pélec_04_fonctions import reg_modelGrid, visuRMSEGrid\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 17] File exists: './Figures/'\n","[Errno 17] File exists: './Tableaux/'\n"]}],"source":["write_data = True\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["BEBNum = pd.read_csv('BEBNum.csv')\n","\n","BEBNumM = BEBNum.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n","SiteEnergyUse = np.array(BEBNum['SiteEnergyUse(kBtu)']).reshape(-1, 1)\n","TotalGHGEmissions = np.array(BEBNum.TotalGHGEmissions).reshape(-1, 1)\n","\n","BEBNumM_train, BEBNumM_test, TotalGHGEmissions_train, TotalGHGEmissions_test = train_test_split(\n","    BEBNumM, TotalGHGEmissions, test_size=.2)\n","\n","score = 'neg_root_mean_squared_error'\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# Scaler moins sensible aux outlier d'après la doc\n","scaler = RobustScaler(quantile_range=(10, 90))\n"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Modèle de prédiction sur les émissions (TotalGHGEmissions)\n","avec les données numériques uniquement\n","## 1.1 Émissions brutes"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.3911637992791006\n","rmse : 399.56886073366894\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predLR=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[57.9404296875,39.259765625,57.0712890625,-0.443359375,21.150390625,34.4970703125,47.529296875,89.09765625,153.0908203125,42.3759765625,116.0810546875,63.1630859375,50.1416015625,141.7314453125,48.693359375,-9.1806640625,58.2763671875,44.427734375,95.314453125,99.322265625,25.310546875,51.9873046875,11.30859375,58.0009765625,8.6083984375,97.4345703125,367.50390625,49.7734375,83.1962890625,14.5361328125,240.3564453125,105.439453125,103.716796875,11.4716796875,81.5458984375,152.79296875,560.662109375,57.1435546875,55.0615234375,42.0859375,176.0654296875,61.8427734375,51.650390625,91.140625,107.4345703125,72.056640625,127.6005859375,173.5830078125,42.1591796875,100.1376953125,14.9599609375,22.630859375,752.4990234375,104.6357421875,63.6689453125,155.07421875,48.7802734375,41.4833984375,201.48828125,79.0322265625,141.3330078125,297.8525390625,46.3896484375,147.6123046875,131.0126953125,-5.259765625,131.771484375,113.880859375,33.6708984375,64.345703125,53.78515625,188.4248046875,54.12890625,54.3232421875,141.416015625,140.5849609375,158.01171875,7.8486328125,114.5283203125,57.591796875,94.994140625,146.7890625,46.2646484375,145.236328125,38.7685546875,25.5712890625,67.3095703125,58.3388671875,173.9736328125,153.7060546875,76.4765625,75.224609375,177.0244140625,14.5029296875,75.458984375,4.8837890625,2595.4267578125,61.4443359375,-34.0673828125,94.3818359375,222.0244140625,45.728515625,2.5390625,51.6416015625,-0.724609375,86.21484375,96.1162109375,243.455078125,35.1826171875,209.677734375,-15.5791015625,53.625,36.677734375,32.48046875,14.2119140625,73.4599609375,34.873046875,610.6455078125,224.962890625,135.849609375,31.734375,2.57421875,58.056640625,-14.4189453125,98.16796875,106.9287109375,41.46875,74.5087890625,29.8955078125,123.3828125,145.3916015625,42.4619140625,37.3740234375,16.328125,112.23046875,58.7353515625,78.8134765625,47.9384765625,155.0283203125,49.8349609375,39.88671875,70.4951171875,106.5810546875,13.9228515625,81.03515625,53.53125,-2.5078125,69.2998046875,-37.771484375,46.4423828125,84.8291015625,115.4931640625,27.552734375,91.2109375,71.4501953125,40.142578125,28.7978515625,37.97265625,464.5234375,93.646484375,25.9697265625,33.546875,46.90234375,63.48828125,77.44921875,52.8193359375,553.47265625,54.6171875,133.6875,3.2451171875,84.6796875,17.109375,73.4453125,86.78125,52.8798828125,91.2998046875,53.9462890625,253.330078125,8.541015625,25.619140625,118.92578125,127.7978515625,30.322265625,104.6552734375,41.2060546875,256.2744140625,25.041015625,63.2353515625,4.85546875,12.0341796875,94.150390625,-25.9287109375,63.5751953125,-14.0126953125,-36.3583984375,145.13671875,40.5712890625,78.4140625,4.634765625,0.1708984375,393.38671875,40.7392578125,18.419921875,20.9775390625,31.0361328125,36.078125,35.697265625,48.3798828125,89.9013671875,67.36328125,315.826171875,48.3369140625,38.9296875,30.2333984375,105.7666015625,418.6875,72.201171875,27.052734375,58.0263671875,33.830078125,74.9150390625,39.44921875,48.263671875,64.6025390625,25.6259765625,35.4990234375,102.4921875,42.3818359375,65.3935546875,56.4013671875,36.9423828125,120.1611328125,65.2412109375,137.767578125,37.1259765625,35.6787109375,53.8759765625,33.919921875,98.677734375,83.296875,288.6259765625,36.3134765625,34.875,52.02734375,31.0048828125,52.2431640625,-69.6806640625,102.697265625,286.7255859375,118.9345703125,58.6298828125,20.8359375,62.94140625,80.0654296875,251.4423828125,115.048828125,100.029296875,134.43359375,135.625,82.87890625,68.37109375,71.203125,-13.7685546875,64.2333984375,23.5048828125,182.943359375,33.7958984375,238.974609375,48.3271484375,79.38671875,113.263671875,104.2451171875,19.11328125,52.5947265625,48.4775390625,47.005859375,74.5498046875,1111.55859375,-19.1083984375,79.7275390625,125.3818359375,55.0546875,28.1123046875,126.22265625,103.1171875,42.357421875,39.076171875,64.0556640625,14.1064453125,59.416015625,621.90625,-12.5654296875,24.046875,60.921875,130.80078125,201.822265625,34.5107421875,90.0810546875,24.880859375,28.6171875,131.6689453125,132.74609375,184.533203125,48.294921875,-57.3095703125,34.3203125,249.6669921875,41.6875,39.5419921875,81.927734375,46.9609375,73.0126953125,46.708984375,186.619140625,41.1787109375,45.7529296875,28.3720703125,121.2587890625,36.2548828125,-20.189453125,149.98828125,3.7548828125,50.0361328125,62.0546875,84.798828125,38.7861328125,68.8330078125,97.462890625,408.1015625,48.029296875,73.0380859375,6.5,57.93359375,82.3046875,89.1376953125,103.25,305.8720703125,8.228515625,47.216796875,37.255859375,22.9912109375,31.119140625,39.623046875,60.640625,33.158203125,25.119140625,-10.404296875,69.8671875,26.1591796875,33.365234375,66.359375,60.541015625,29.7490234375,99.2998046875,245.818359375,332.7783203125,-7.8828125,255.818359375,29.5908203125,272.60546875,3.091796875,32.9423828125,59.9052734375,119.03125,37.03125,43.1162109375,258.82421875,89.443359375,256.88671875,10.4609375,3.845703125,69.8173828125,234.4189453125,11.5263671875,69.3720703125,41.9169921875,110.7568359375,-11.8310546875,229.791015625,39.849609375,20.8671875,56.7314453125,94.4892578125,136.751953125,38.2568359375,328.3623046875,136.2255859375,59.8525390625,106.591796875,69.1943359375,-25.3369140625,16.283203125,115.001953125,42.2490234375,57.8720703125,94.8798828125,66.4814453125,37.634765625,35.6083984375,58.703125,206.142578125,141.423828125,33.837890625,60.8037109375,57.6982421875,75.326171875,27.095703125,114.8115234375,-12.0537109375,65.6943359375,42.30859375,12.5869140625,66.6220703125,306.48046875,56.953125,-31.5517578125,1.87890625,87.1669921875,163.6669921875,54.876953125,72.3330078125,47.17578125,43.9345703125,79.1396484375,56.3408203125,90.0791015625,1108.435546875,61.1630859375,-117.8173828125,74.5947265625,95.931640625,-72.591796875,137.0810546875,1660.13671875,669.990234375,7.7900390625,325.8544921875,64.4267578125,77.044921875,68.318359375,33.498046875,13.625,18.8896484375,0.8671875,213.076171875,77.1748046875,106.9189453125,25.65234375,79.6474609375,153.13671875,79.333984375,53.3603515625,32.5048828125,55.076171875,128.296875,58.890625,15.2275390625,91.3056640625,45.90234375,24.498046875,64.455078125,52.2373046875,283.53125,5.5185546875,152.9140625,34.31640625,60.4912109375,33.9619140625,39.873046875,109.3720703125,23.05078125,66.1708984375,486.9326171875,136.7734375,1.09765625,37.5458984375,290.5634765625,148.1826171875,59.28515625,91.13671875,49.01171875,105.083984375,56.0869140625,56.494140625,44.0205078125,67.4453125,164.4638671875,8.2470703125,233.9384765625,66.9326171875,32.845703125,48.9873046875,32.36328125,38.2509765625,97.755859375,194.666015625,60.751953125,44.4755859375,666.3984375,19.1083984375,28.9658203125,-11.6513671875,6.8525390625,95.310546875,-23.1611328125,108.86328125,88.234375,99.541015625,145.5830078125,40.4501953125,33.5400390625,379.517578125,115.4560546875,66.6689453125,104.0166015625,7.796875,24.0322265625,1152.2509765625,24.380859375,93.89453125,116.52734375,518.9501953125,62.408203125,66.4453125,75.8896484375,25.2607421875,58.50390625,70.8544921875,21.4541015625,20.8798828125,131.74609375,72.3369140625,11.892578125,11.3505859375,-3.2958984375,5.6865234375,61.2978515625,36.5869140625,132.154296875,28.603515625,86.3203125,54.71875,53.6416015625,22.49609375,27.6904296875,-58.3642578125,126.6982421875,21.49609375,42.2236328125,-45.251953125,296.5029296875,32.8544921875,45.7451171875,91.5576171875,-24.5458984375,47.09765625,72.6455078125,122.369140625,49.822265625,146.25,74.0322265625,48.6494140625,20.5927734375,138.689453125,450.4619140625,73.7197265625,134.091796875,311.396484375,107.9755859375,37.63671875,33.203125,342.81640625,-10.6416015625,36.791015625,245.5703125,5.02734375,33.0703125,33.056640625,78.908203125,77.9853515625,132.2138671875,63.4033203125,36.9111328125,14.3447265625,43.919921875,133.912109375,1.2099609375,39.3095703125,150.24609375,10.796875,40.185546875,96.966796875,124.0947265625,451.623046875,118.23828125,-2.3095703125,443.373046875,64.263671875,130.986328125,104.5673828125,177.521484375,14.4072265625,290.798828125,240.8857421875,58.2333984375,471.2265625,67.642578125,260.716796875,47.6484375,0.62890625,84.3544921875,23.7353515625,56.267578125,101.1787109375,55.11328125,291.7451171875,14.58203125,154.259765625,50.71875,89.1328125,218.09375,30.44140625,104.572265625,45.078125,35.09375,21.08203125,60.5380859375,60.513671875,63.5947265625,43.109375,363.912109375,37.390625,28.357421875,143.31640625,38.41015625,81.677734375,72.4951171875,23.556640625,113.662109375,99.724609375,21.5634765625,861.7587890625,44.33984375,-1.3701171875,50.6630859375,166.4501953125,-3.4970703125,85.9345703125,102.53515625,50.48046875,19.20703125,229.84375,129.3583984375,46.248046875,34.720703125,160.369140625,81.107421875,54.7470703125,222.4111328125,49.9521484375,41.7744140625,35.7548828125,87.8662109375,92.92578125,121.517578125,-23.6611328125,24.94140625,106.2724609375,22.8984375,54.953125,49.53515625,50.2333984375,88.7685546875,72.2158203125,61.22265625,369.44140625,614.8173828125,65.9150390625,134.0322265625,41.96875,35.8994140625,113.0693359375,40.3662109375,78.970703125,204.8798828125,38.71875,-16.4189453125,28.0029296875,52.9384765625,140.3056640625,35.009765625,215.5087890625,-22.802734375,92.783203125,63.3818359375,220.03125,527.724609375,52.8046875,-16.943359375,44.48046875,71.822265625,30.91796875,36.208984375,58.9111328125,30.919921875,40.9921875,11.3994140625,37,35.48828125,56.8212890625,58.5458984375,2016.8193359375,46.73046875,1039.26171875,122.822265625,33.625,40.359375,40.7001953125,58.353515625,198.783203125,226.1416015625,138.2978515625,40.966796875,60.7529296875,70.140625,51.353515625,27.556640625,77.09765625,33.56640625,816.8212890625,32.95703125,42.7490234375,634.3408203125,125.1083984375,-4.2470703125,205.00390625,239.666015625,25.4423828125,85.234375,156.98046875,42.384765625,67.111328125,38.595703125,175.9111328125,126.4267578125,90.140625,393.1982421875,42.560546875,46.5400390625,202.14453125,158.783203125,274.4326171875,37.0458984375,57.11328125,27.755859375,59.0029296875,11.177734375,31.2529296875,141.349609375,158.392578125,33.1904296875,76.3818359375,36.3017578125,30.1455078125,42.6064453125,36.6103515625,112.521484375,158.8486328125,133.10546875,70.7333984375,129.1962890625,14.564453125,37.7109375,84.4599609375,21.5361328125,54.330078125,40.5634765625,75.978515625,-38.6689453125,407.6044921875,159.8056640625,41.6787109375,33.4462890625,62.5888671875,148.80859375,103.7177734375,158.3505859375,47.548828125,26.435546875,46.9970703125,221.751953125,74.6376953125,76.564453125,71.166015625,166.564453125,36.599609375,50.4306640625,-133.75,1475.041015625,63.1435546875,70.7265625,36.76171875,57.06640625,28.8681640625,38.0498046875,39.9287109375,40.6142578125,47.720703125,150.03515625,105.0927734375,29.498046875,232.0009765625,44.8564453125,33.84375,18.3115234375,175.494140625,46.74609375,118.46484375,33.755859375,97.7861328125,114.505859375,263.7880859375,11.849609375,120.921875,30.5146484375,41.0068359375,37.8544921875,413.90625,196.548828125,52.1708984375,-22.94140625,217.84375,61.486328125,39.0634765625,46.4150390625,47.8388671875,44.8056640625,55.3447265625,33.4384765625,1703.5166015625,50.376953125,38.67578125,195.4833984375,100.994140625,62.6904296875,33.361328125,41.48828125,66.703125,-216.2998046875,2.0380859375,20.62890625,59.5927734375,891.5380859375,64.1103515625,84.244140625,72.0791015625,47.8486328125,127.0458984375,242.8857421875,39.423828125,38.205078125,37.244140625,76.58984375,78.5244140625,49.3759765625,34.755859375,140.3828125,136.92578125,58.990234375,125.6259765625,51.9951171875,149.0712890625,10.0966796875,5.5390625,86.2919921875,88.8974609375,58.509765625,31.5107421875,64.6748046875,82.0185546875,178.2216796875,232.4814453125,32.634765625,79.1953125,15.2373046875,200.2314453125,41.36328125,18.9228515625,39.0146484375,59.5693359375,45.609375,12.744140625,24.6630859375,40.9130859375,74.3994140625,-12.9326171875,20.3759765625,23.181640625,72.0986328125,4.626953125,52.6201171875,39.689453125,-26.7353515625,64.9033203125,255.8486328125,108.818359375,232.5439453125,11.869140625,59.28515625,155.5869140625,110.4423828125,64.203125,30.716796875,703.83203125,74.0771484375,137.72265625,76.837890625,57.0927734375,11.8671875,536.9775390625,186.96875,79.9306640625,63.7001953125,1052.1640625,28.439453125,449.623046875,126.23046875,83.240234375,30.4462890625,419.986328125,26.978515625,81.26953125,60.876953125,6.7578125,61.4443359375,86.5205078125,84.27734375,337.306640625,4.22265625,202.693359375,44.2158203125,141.37890625,32.6982421875,27.5966796875,31.3583984375,296.087890625,77.775390625,99.177734375,52.9755859375,152.279296875,89.6943359375,25.8056640625,36.94140625,130.515625,61.0166015625,152.3876953125,2988.83984375,40.8701171875,37.798828125,39.57421875,261.7880859375,41.4873046875,24.5654296875,125.0263671875,200.919921875,188.9580078125,37.7783203125,28.7197265625,150.8671875,72.55078125,-5.2958984375,29.2216796875,381.3427734375,84.1826171875,16.478515625,44.7900390625,105.365234375,45.78515625,24.146484375,38.728515625,149.849609375,13.6533203125,158.0576171875,19.8134765625,116.8701171875,69.263671875,67.0625,187.0771484375,22.5029296875,29.408203125,249.205078125,29.1142578125,55.6943359375,231.9130859375,47.6904296875,322.833984375,54.21875,61.3408203125,60.33203125,62.0830078125,285.2841796875,146.7333984375,41.845703125,40.7177734375,231.8486328125,57.3193359375,12.6669921875,69.255859375,31.0390625,258.2119140625,47.6533203125,13.5205078125,77.09375,34.705078125,63.7626953125,66.1650390625,19.8984375,12.578125,10.8037109375,37.3427734375,389.7958984375,30.7880859375,147.58203125,46.76953125,67.318359375,133.8408203125,31.345703125,24.7509765625,39.2568359375,26.0087890625,67.9638671875,54.26953125,123.94140625,198.8427734375,226.07421875,168.46875,149.6181640625,50.9248046875,83.796875,82.462890625,150.525390625,84.5927734375,20.3837890625,83.046875,48.9658203125,77.7529296875,50.0869140625,-52.365234375,136.9619140625,32.857421875,144.5361328125,108.36328125,61.2490234375,56.208984375,167.728515625,35.2333984375,86.5078125,113.6708984375,131.060546875,99.66015625,80.4345703125,142.5322265625,115.8193359375,128.8671875,73.15234375,85.265625,4.0244140625,55.4677734375,1.04296875,247.755859375,27.080078125,69.443359375,71.7978515625,84.12890625,72.775390625,63.3173828125,25.15625,-13.0380859375,58.4892578125,710.1884765625,58.4140625,26.728515625,701.83203125,-23.9208984375,44.3271484375,-12.3603515625,181.6083984375,911.6416015625,-11.1181640625,48.8916015625,94.3583984375,26.1572265625,5.80859375,74.0419921875,174.5029296875,531.7451171875,-9.59375,-58.837890625,26.7822265625,482.8896484375,-113.490234375,44.1865234375,25.1494140625,53.8984375,48.470703125,700.7431640625,461.4609375,72.322265625,287.2216796875,123.5234375,92.2802734375,17.1435546875,239.4677734375,46.521484375,45.1376953125,136.3603515625,41.8603515625,35.673828125,44.150390625,35.6552734375,55.1552734375,62.5068359375,116.30078125,32.2275390625,431.44140625,36.5576171875,83.7734375,61.595703125,70.404296875,-83.6943359375,56.0810546875,65.0869140625,32.0732421875,36.8408203125,52.267578125,57.1845703125,294.8056640625,35.18359375,46.634765625,72.2998046875,9.74609375,46.533203125,-0.89453125,338.58984375,105.3154296875,125.4609375,189.318359375,60.6064453125,445.904296875,106.017578125,63.5361328125,175.8564453125,55.974609375,66.5439453125,64.73046875,106.5244140625,140.0048828125,83.6376953125,-19.0283203125,82.7353515625,2018.8193359375,38.841796875,123.4814453125,57.33203125,23.84375,175.8740234375,858.7890625,30.9853515625,15.5927734375,148.9736328125,71.7783203125,52.8623046875,16.328125,-9.2373046875,34.9990234375,176.39453125,266.650390625,35.0078125,33.4208984375,79.6640625,80.33203125,32.751953125,40.623046875,31.81640625,110.5859375,88.375,57.3037109375,31.9853515625,51.0751953125,-5.2333984375,26.201171875,39.8916015625,36.64453125,49.1787109375,44.720703125,99.44140625,390.857421875,32.0478515625,36.1884765625,38.921875,56.5517578125,78.560546875,118.9033203125,15.583984375,92.017578125,46.509765625,148.4306640625,60.0107421875,100.44140625,43.884765625,29.482421875,321.271484375,45.8486328125,-59.791015625,38.771484375,156.4794921875,84.6875,26.505859375,44.5263671875,48.4892578125,72.12109375,12.94140625,157.208984375,258.2392578125,87.6923828125,78.453125,155.23828125,32.4345703125,81.271484375,49.3662109375,102.7548828125,55.8291015625,139.0595703125,28.2294921875,16.53515625,27.1298828125,301.0859375,46.7880859375,168.619140625,76.1826171875,162.7919921875,5.6259765625,91.8388671875,-10.6865234375,61.53125,47.884765625,49.75,219.1162109375,473.32421875,27.9462890625,106.16015625,101.26171875,-112.77734375,54.3125,-3.482421875,44.32421875,41.9228515625,127.0712890625,-11.5234375,61.4560546875,120.671875,1303.896484375,102.6982421875,63.484375,51.9365234375,20.87109375,62.572265625,39.0751953125,59.6357421875,42.0546875,24.310546875,195.2294921875,143.5224609375,16.234375,-17.8427734375,411.421875,176.796875,6.115234375,167.8173828125,127.1513671875,679.6962890625,270.56640625,-16.5634765625,63.0234375,646.162109375,12.1533203125,53.96484375,204.08203125,62.384765625,101.615234375,83.6982421875,39.1083984375,165.1826171875,163.1748046875,113.525390625,65.9287109375,88.3017578125,69.0869140625,195.3203125,28.73046875,124.46484375,35.3369140625,235.037109375,29.560546875,86.638671875,26.251953125,65.54296875,86.181640625,36.072265625],"xaxis":"x","y":[30.29,47.7,6.73,5.66,152.14,7.53,17.11,9.67,73.77,5.21,150.69,47.42,17.51,11.08,4.76,73.31,30.34,59.24,33.58,31.96,232.12,9.43,8.12,51.2,14.69,33.77,10.29,32.33,61.62,7.67,21.08,35.56,105.32,41.15,287.23,85.28,139.34,18.63,25.15,10.04,42.74,29.93,17.38,263.57,11.17,8.59,570.98,13.65,13.22,2.91,6.53,49.15,1222.69,28.59,51.64,185.23,141.7,3.27,66.39,85.15,87.31,95.26,48.51,26.78,50.14,6.02,14.87,349.47,4,14.79,4.98,20.39,6.45,19.28,93.47,52.87,372.55,5.81,12.68,5.81,8.57,47.39,128.73,99.39,37.72,3.02,12,38.97,39.27,266.18,5.9,29.84,49.18,3.82,8.8,11.85,10734.57,266.78,12.25,6.04,18.07,7.21,63.89,8.53,138.68,18.34,29.39,218.53,69.3,587.16,11.97,8.46,4.36,5.16,46.57,32.22,5.46,181.38,7.08,65.03,5.87,24.25,23.17,178.21,39.57,16.5,4.48,61.36,4.03,8.8,77.29,6.05,45.3,8.77,256.57,2.17,37.97,11.11,74.45,17.67,19.83,406.2,209.92,45.81,2.36,22.02,198.38,6.22,8.31,5.96,13.61,262.75,152.36,15.43,7.23,5.94,43.45,60.94,159.16,31.92,9.71,69.36,18.1,8.37,23.58,48.3,147.82,71.47,18.89,53.36,17.65,4.5,92.59,62.57,4.8,25.37,90.31,100.42,6.37,2.9,63.61,64.19,22.15,192.63,5.55,109.79,3.29,67.15,31.92,79.3,8.81,6.71,12.33,29.33,12.15,16.83,116.21,6.43,187.92,39.56,30.85,78.29,4.33,4.63,16.89,3.42,10.21,5.97,49.16,20.1,190.58,7.67,4.79,1000.06,50.52,122.99,134.69,5.6,27.38,7.08,2.04,44.6,3.51,41.98,6.36,5.87,8.87,3.97,127.56,37.85,4.26,147.69,5.9,10.55,5.69,32.81,34.81,56.25,653.5,65.65,97.01,3.49,4.65,9.13,41.69,14.77,39.47,2.82,142.48,68.97,6.64,7.31,82.9,8.69,256.31,93.23,11.77,38.08,17.24,43.64,18.41,12.7,27.3,6.71,3.89,71.03,1.78,152.71,50.43,22.18,12.06,29.26,20.98,77.15,3.84,91.09,14.35,839.79,226.43,27.27,57.69,5.16,6.69,20.79,176.79,59.61,9.83,8.24,16.09,20.44,170.84,35.39,30.02,19.72,15.59,66.05,3.89,7.46,14.35,7.74,141.32,46.61,20.29,3.75,235.69,243.38,151.95,100.87,56.51,6.79,4.84,247.85,24.17,168.62,47.11,12.99,104.17,30.66,3.82,120.91,132.47,106.37,44.93,6.93,61.71,4.2,24.42,8.14,111.69,6.22,9.45,23.84,4.88,507.7,553.27,9.48,122.92,103.37,50,3.86,3.4,3.66,93.43,6.85,32.8,32.73,13.43,413.1,6.19,6.16,68.14,0.68,16.45,13.88,350.87,802.89,291.28,91.73,4.72,7.21,25.02,6.75,28.93,229.55,3.01,49.59,341.23,37.18,353.49,93.87,93.21,11.79,49.25,154.4,84.93,6.02,43.63,35.36,51.47,42.8,4.35,17.11,205.07,40.78,5.6,186.96,14.51,35.56,537.03,53.38,38.9,8.16,37.51,3.82,5.51,50.76,380.17,10.75,4.3,7.12,1.6,63.45,4.42,7.51,7.14,22.78,8.45,8.59,5.37,11.59,6.93,54.95,19.7,89.41,74.04,3.91,35.22,43.13,28.54,70.4,10.06,6.3,5.11,45.38,0,10.14,765.56,14.46,28.64,27.86,8.85,256.72,196.67,525.78,93.67,8.97,165.61,3.06,17.84,19.17,50.7,47.25,2.99,3.2,54.92,96.4,16.01,4.74,10.56,132.28,15.5,7.29,63.36,47.68,297.23,67.06,3.4,98.54,0.76,3.72,50.15,2.45,265.21,188.67,156.08,83.1,94.85,20.57,29.18,12.09,13.77,34.14,1231.19,39.41,7.57,113.25,237.36,93.23,33.52,57.03,12.79,12.97,5.72,6.38,10.93,16.2,57.16,18.62,346.62,44.19,190.45,6.1,75.67,3.69,73.48,1185.15,20.12,92.3,188.13,72.71,101.31,15.8,80.06,3.18,141.15,17.74,22.65,42.2,14.32,46.86,18.42,247.85,52.43,5.71,51.26,3.07,10.14,242.19,21.3,8.45,113.31,75.12,13.15,0.89,8.57,3.48,5.33,102.04,3.71,31.58,42.52,6.89,7.69,6.16,29.16,5.88,76.85,1.3,37.58,7.29,24.1,28.78,25.39,11.28,32.02,31.46,38.54,49.7,9.47,456.22,124.92,14.19,110.62,24.36,4.13,68.89,145.48,21.24,18.72,12.4,15.62,43.16,160.3,47.78,139.27,30.86,44.97,163.99,142.69,12.63,4,109.46,26.64,64.13,285.62,9.57,74.52,4.01,226.81,10.16,13.64,92.58,7.15,8.16,75.64,60.02,223.36,8,13.87,3.92,73.32,233.43,208.26,351.53,16.76,200.3,604.45,51.91,16.06,10.92,90.73,4.73,69.42,238.61,4.93,900.81,0.7,399.76,31.85,19.95,248.29,2.43,480.9,10.32,176.76,144.3,5.15,97.97,27.69,63.6,144.53,27.56,29.24,26.55,18.5,80.06,10.77,15.34,0.75,3.91,474.59,4.25,128.41,433.86,69.88,44.32,413.35,4.43,348.22,29.78,66.32,290.4,44.51,4.02,5.2,56.82,4.55,9.65,52.88,57.79,58.32,393.2,72.69,45.02,24.62,4.19,223.34,10.89,83.27,7.56,29.73,100.71,7.53,121.88,176.47,23.82,4.48,53.21,16.3,4.9,6.39,54.95,61.64,7.12,19.68,8.07,704.76,33.35,184.81,61.25,20.88,60.04,16.47,551.43,260.13,8.61,167.31,70.14,64,21.92,9.16,111.23,7.02,236.34,318.08,149.93,364.9,295.86,34.62,3.11,156.18,118.86,4.16,3.27,6.54,3.65,48.91,8.87,5.19,148.11,88.87,1793.9,37.34,275.54,30.87,5.53,56.96,82.38,21.52,1597.56,42.41,109.72,4.68,19.81,79.81,6.74,6.02,39.05,5.79,271.32,85.55,4.52,233,9.53,157.21,15.23,35.44,3.92,64.75,40.35,20.45,15.27,29.42,39.75,119.03,28.07,95.77,23.14,60.83,224.13,98.65,443.86,3.52,221.47,5.68,7.9,11.73,4.71,38.59,32.75,4.79,101.43,5.54,5.66,5.88,130.58,60.33,25.6,69.18,253.53,84.57,3.55,42.71,5.09,12.28,19.15,93.68,42.24,58.44,1084.79,243.37,7.47,4.43,8.09,69.67,82.72,101.64,19.04,4.02,5.6,46.49,183.51,98.22,6.54,69.3,41.96,3.71,5.97,913.07,145.42,7.56,5.3,45.96,8,71.16,13.1,53.88,53.02,379.34,43.35,7.94,322.79,4.43,14.81,3.41,18.97,3.9,115.56,61.68,29.92,9.99,530.66,3.7,49.17,27.39,96.62,6.57,51.93,21.38,63.48,90.34,423.93,183.79,5.86,92.45,23.85,45.32,77.75,5.31,2451.58,9.87,39.5,67.17,162.39,220.26,8.46,23,31.79,28.83,20.02,101.99,26.72,15.5,32.77,132.56,15.74,110.66,9.65,246.83,10.83,5.92,9.2,467.18,24.35,3.67,5.35,862.64,89.53,234.03,1.67,15.2,71.38,43.57,260.57,226.73,87.14,14.44,3.99,28.82,73,188.07,45.6,5.07,103.77,3.16,395.26,27.93,10.23,23.27,60.07,112.19,7.22,20.07,15.9,72.46,3.92,176.63,32.52,32.35,6.84,96.59,44.71,3.52,48.95,116.95,36.65,142.36,8.27,3.89,315.85,9.73,1.61,5.81,1638.46,8.61,14.99,49.18,26.73,17.42,202.49,59.21,9.99,42.57,481.1,29.62,399.54,72.88,52.79,20.67,381.89,3.56,4.07,15.74,26.94,27.84,32.72,15.64,582.28,13.36,23.83,27.06,411.09,4.74,15.18,73.14,185.82,13.97,12.15,3.49,19.84,93.67,93.55,59.18,74.98,43.56,312.6,12307.16,6.47,6.19,12.59,579.99,59.63,2.81,119.34,283.04,19.72,40.76,3.18,62.01,1.58,15.77,3.34,587.09,12.07,23.74,4.61,89.6,103.42,7.52,57.33,16.82,65.75,22.76,8.42,39.54,99.91,47.12,80.75,10.17,4.54,28.3,19.36,6.35,63.87,10.05,184.64,34.08,3.15,76.89,11.99,292.92,112.01,66.1,23.22,251.43,75.58,152.14,41.06,7.07,115.34,6.12,34.87,85.3,19.76,28.03,6.67,18.73,20.07,34.98,12.56,150.23,12.35,50.23,19.16,45.22,344.2,4.74,1.34,3.77,36.71,3.92,83.35,324.92,64.95,675.34,22.17,536.43,43.84,30.26,78.19,44.61,61.72,39.57,73.52,3.99,182.11,99.62,551.53,39.45,6.99,89.94,7.27,4.76,9.66,77.56,3.78,44.26,39.31,37.14,11.29,24.91,19.73,326.74,64.79,59.06,2.35,49.79,61.19,4.39,357.86,6.61,126.3,53.25,58.98,4.95,6.87,8.93,39.1,66.44,440.31,38.45,102.08,1558.85,37.17,124.96,5.3,260.94,3232.23,74.13,31.18,225.67,6.04,49.44,8.31,67.65,320.35,46.72,143.67,3.16,94.59,96.15,5.7,25.49,3.65,22.46,1727.11,49.26,173.75,295.48,53.56,136.07,5.03,250.05,74.28,3.59,123.14,0.12,24.64,4.98,46.95,27.76,5.53,17.23,154.13,740.97,78.06,125.42,7.53,131.64,129.05,6.67,99.65,298.91,30.14,46.32,5.88,0,14.3,45.07,1.14,147.24,21.28,4.44,249.09,72.02,47.25,1623.34,8.7,226.06,10.83,47.91,434.48,5.33,120.13,38.15,180.38,37.16,8.18,7.05,148.7,0,29.26,39.35,24.18,4.02,103.39,2089.28,4.5,77.48,143.3,46.61,46.85,80.08,108.04,23.01,47.82,67.62,69.53,4.2,25.48,7.74,4.33,22.61,3.63,11.25,58.79,58.85,13.34,7.25,26.41,5.94,5.02,4.44,4.72,23.65,96.41,529.47,7.1,14.15,3.72,48.23,63.06,96.39,5.29,10.08,4.49,22.08,1084.49,8.16,65.77,20.63,375.56,112.13,58.52,26.54,68.3,464.77,5.14,57.17,8.98,91.49,3.68,44.35,92.7,4.99,0.37,305.15,57.25,0,16.27,45.59,34.26,34.92,24.31,5.07,1.08,28.19,82.87,109.71,17.19,50.31,20.25,41.22,5.74,45.98,17.88,86.5,8.55,394.6,22.57,48.85,38.13,269.23,47.26,122.83,5.23,8.81,48.65,640.71,46,328.63,4725.43,73.9,65.65,6.98,3.5,8.54,4.15,6.98,4.69,40.38,150.81,8.15,4.38,37.13,328.97,132.71,3.15,229.2,10.97,122.54,84.18,6.46,26.59,500.93,148.02,6.39,222.02,12.75,50.57,9.28,3.86,369.14,156.11,46.38,14.99,69.77,7.21,100.59,5.3,39.61,102.73,23.42,4.26,127.53,3.91,12.5,117.11,8.47],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBNumM_train, TotalGHGEmissions_train)\n","\n","TotalGHGEmissions_predLR = pipeLR.predict(BEBNumM_test)\n","\n","LRr2 = metrics.r2_score(TotalGHGEmissions_test, TotalGHGEmissions_predLR)\n","print(\"r2 :\", LRr2)\n","LRrmse = metrics.mean_squared_error(TotalGHGEmissions_test,\n","                                    TotalGHGEmissions_predLR,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=TotalGHGEmissions_predLR.squeeze(),\n","    y=TotalGHGEmissions_test.squeeze(),\n","    labels={\n","        'x': f'{TotalGHGEmissions_predLR=}'.partition('=')[0],\n","        'y': f'{TotalGHGEmissions_test=}'.partition('=')[0]\n","    },\n","    title=\n","    \"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test\"\n",")\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.2 Modèle Ridge"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre      Ridge()\n","0  ridge__alpha  2456.916463\n","         Ridge()\n","R²      0.339807\n","RMSE  416.080075\n","MAE    94.693823\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predRidge=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[50.31899613667271,40.36805092275792,58.13620446666074,36.031816027043384,67.68619477255066,56.22760242812279,45.65022031054408,63.75429888274495,118.93987933231762,46.54175203008384,104.0925380797747,51.89195994119126,44.11469828539799,90.27589931816692,47.13789235591841,83.36877439676428,52.492774138205114,65.84503162999277,96.4767168966405,76.67482047781411,39.51185565708211,57.62720965807906,39.326387945011625,63.860259273813135,65.45499945788907,83.83881181353264,243.27066559348663,58.98162043387029,87.26826052949026,33.91626998101178,171.94086787408827,101.59692941124041,83.99412228845985,60.73203524551102,73.72311883532035,119.17219612790788,517.0657809399916,51.39825628604798,71.59711358771688,48.996496417873885,119.13948148325818,58.950053249522774,44.0843090591235,136.7409952116889,98.5804024499455,57.09080467504248,74.49131446389156,124.73798317973576,43.26347332104469,70.94659889674242,35.5034593774541,43.18670766014475,649.4489791451143,80.05800482394424,62.364783350878476,79.17541816343835,169.00356244189155,41.80146013696484,145.93827871201552,69.89810559905631,126.28878180148267,217.64768973319772,47.14308812731623,212.72425404954342,151.06162381072062,30.431577028502076,114.06432580223,81.76100021555266,43.23213464492648,52.17545260585258,47.66521392021283,163.50075996949232,49.84604610094901,45.766578864485936,99.29768971613547,121.07697894448232,241.58642759174043,43.00759938016769,105.09160484262804,54.291793332421065,78.10246981684071,193.7941481773644,79.36075984965406,153.92763358083812,44.25337629637948,39.01889252175167,55.25128588449878,60.90112268546943,163.37338649703935,331.63782795246334,62.129102959631574,63.284257484138976,124.77766657304778,46.86145975097091,63.80616360349858,58.887298438110655,1742.1550501097713,140.3566004655733,79.86498321625777,67.11490985542301,169.28282627628064,52.87064507989281,57.100481541504266,62.497138681486305,82.21786553863853,64.54581228564567,80.7770945418411,208.84149010584568,39.99861542973366,151.96407279082135,48.564391622089275,53.71867395200378,40.962216308969225,42.480159141804144,48.48187409825348,53.75822308152138,50.67391121531523,466.03839262579197,164.18609870095264,117.44303672431624,39.77397250198282,37.93120947489561,46.222045386937914,251.42084624379058,98.10479313811122,74.87752619429527,48.20392014774658,60.872048225519,40.81678368555417,176.72422522591214,185.29539052823418,48.833152932723245,41.40380792712549,39.27126011682002,149.07538204775068,50.8254862035037,63.79844433848928,72.75662488612133,117.94785823901344,47.113257077406864,44.96584839596068,147.82082081459748,145.5886257886874,61.481220575732955,60.56072060685631,76.38478587510794,39.225401948793525,65.96327209746144,40.98961327386612,43.003812302358796,134.99355635435964,105.6540133264195,41.34038893781961,67.32391705710825,64.4454531153487,53.49693966137067,52.72468945189892,39.55258780680161,344.25666970012946,73.06025905380017,36.1057993160202,43.14304470390294,43.291622813872785,62.22057422564604,58.56262648602061,49.01365295490365,384.6199949547172,90.0512006016786,87.78182864189436,60.59771777869359,64.0871472578929,42.30719968847869,71.92855147722699,82.75207758950604,44.86732295074244,146.484277921416,48.488940198144995,168.64448910214395,28.688150682503405,38.10233552794184,88.8221304066659,166.25287532994952,38.83384344914634,82.92398309385288,47.34036959138668,172.4382413725485,37.7411615527086,54.74035179261115,37.363284101261236,87.34354644074355,72.8796132181507,59.931842609234636,58.09540675909615,74.12587345015854,28.044468960667935,105.47529779637429,56.1104419048947,61.13708186632739,72.3879169037978,50.14003159239576,251.8407252548296,41.33165180199347,41.931539057293996,44.08759313655581,49.94797167130936,42.53921074986233,41.70097321110955,42.01179120905462,73.54556476590957,69.79882506989516,194.7848490444,42.774098463460845,42.415377224946724,244.56466574239133,86.91987379248772,553.9185943139332,78.8389317767926,47.775176679767526,163.2046514915678,47.239431317961085,56.94580861251555,82.29451738771704,46.115796137993186,54.56702285318556,46.11498771080716,40.215884149209906,78.69513740177506,49.02181117190518,59.25511382019664,61.89314377877361,40.16216775522386,98.32527422640347,52.04706085293847,94.4237832122151,43.05661276126989,42.28526029852646,49.85049740281901,46.5663714566879,101.1850260651436,63.69392309938167,193.10921229745475,40.730839231085426,43.99680767061561,42.15846425106497,46.360028633043285,48.649657307988896,88.82192131447798,72.41400927329823,281.40266426133405,65.59330696459041,63.454629111402554,42.126695272088384,83.0172866927938,72.78068049369611,175.44147431662748,88.64262088586669,109.99711120850058,100.9770710177003,86.78980754859018,63.975683577926155,98.47751924996726,54.58971275845495,60.56546221982531,66.43559779723209,36.766273986746995,148.3815897318771,47.74901001509046,159.18182431556238,46.15106703401205,57.57060539271643,88.88141235966773,82.95998526510678,86.22891949910807,61.53335551003339,46.759156347585986,100.22832917103415,133.8212762952615,668.5464308160481,163.41067537762171,59.095649660419795,99.41141432653117,56.85234385157383,38.21961747443899,127.26081812938168,76.86507981906249,72.73913497100003,66.19069017869842,64.58349057018027,39.57687152477491,65.97309385788469,415.6037104489865,35.82046931968465,54.59613494770964,48.04484529319319,108.7160916547096,139.71640568885041,43.76543059273182,76.33857358167549,51.080707663018785,42.65982363897922,93.0871903451326,160.99284559824656,124.36829090790296,51.40688032468623,180.92421522594128,77.11383594195661,262.2410808463892,50.33896493040477,71.81285961324211,68.36506469199162,49.459914031947534,52.590923653735004,54.330510282775215,191.19835302577616,46.367445023243235,67.73275997523808,98.61975789758966,98.90303098392324,44.87434756356513,35.99917090524248,142.70233588490393,169.04492397549012,47.244464810597066,61.30776343667765,61.015866515839775,41.87371426335183,55.03239518338115,77.44763848947694,576.0670328190203,58.429812020193545,68.30550920682171,35.90389395536428,58.2359582334047,121.4899067121207,134.62893704491057,89.92714793615514,246.2806511750139,128.14783684736284,80.73860354106525,45.51768870694934,34.438185680337206,41.60403008469538,46.85240945881857,58.18255855215213,90.01165849764274,80.27406818883155,45.28283158479308,63.031353817376484,39.211638567743165,40.88215901987512,49.26219312651111,53.93041585993431,40.73072269926813,83.42788862714258,165.81577793604524,370.7849446711534,227.19260750911556,307.29729649437866,41.21187295098184,195.43310514555995,40.49746865827437,49.63631671294074,59.942074342826956,154.10920512618708,38.96709093288835,45.37542392993906,198.09424880141975,95.77560793563778,199.08626989472393,69.5123461347002,58.28352212063904,58.8056836858611,156.33616842475985,196.7673519602573,141.98728172086948,63.438297713336155,79.22375338041128,59.573441126521125,189.4070852092271,95.24419334678419,33.15520212444078,51.99663588523119,78.06907167118794,100.50383782816355,45.092272805855906,222.531262202573,88.2291627652792,53.50980302497872,179.9343613012718,57.487453043548115,21.293155804815328,54.856784733824156,89.53984525980736,50.57537247223598,52.11021841718757,67.43516959127436,64.71430349539582,40.70895211780537,42.2401135516223,59.17457964545631,76.7156156687663,122.17349018581805,39.371461626435085,59.40595495141132,58.53688184543512,61.39546832243905,56.89760184950427,80.12845965054576,47.86297285595906,67.81301096658098,51.47383142917093,52.034039614447764,57.595559769570784,173.51024800869249,81.20525062465283,23.850150413184068,48.03638929689065,62.52405002960672,162.39386003884954,88.65673416813223,84.95994930043565,49.61031998372565,45.585525359122876,60.078832457638,78.03713007158036,74.97323592204646,746.3644823437672,58.28004482064625,43.33161646450327,56.75816744569516,68.1024949439514,90.77582802058112,100.71264153259527,1446.4678539918764,388.8178270493837,39.447497684506075,380.441165124587,48.03476594505439,62.493098606546326,52.7443394787476,53.25211347849141,42.7084787430648,33.26275666769935,27.760684599566517,244.6888849039881,68.71303726754635,103.17250414017067,40.092704870176114,65.13093385980885,80.16743925674254,59.75478277145846,59.960298676939466,43.38224660436151,89.52645230063527,90.38939119875127,80.21322953134865,33.45613307526529,148.8389209425856,42.65251417801084,37.38845259251988,62.82744519819016,52.827025700646715,183.07094812618203,32.08319479544488,214.58135154558934,45.149246666798334,53.90502081480072,39.741184496713856,44.23940239399951,97.5883813566413,40.657242703731576,82.98013333081173,362.16975822763015,108.72117178560909,32.141705132397846,47.61407994987922,192.11719120415054,92.86877253646104,44.76162679450313,78.13897853571864,76.2391363124327,82.30091378167364,56.78571254014798,64.40100423038814,88.05214866124734,49.034614280651724,126.54340187734127,37.001794748048134,213.73973463633692,70.55817748361909,45.364579286291665,54.51428394559956,46.141267760102515,39.73881813778125,69.59281615368565,194.1184500872599,62.43893375189222,62.662610329523815,454.50183584805734,118.90488803513674,58.933951396187965,126.07248818758896,61.54457536627559,60.73695020981481,157.16510741829427,120.3201495188046,71.57757923242505,194.6302925276113,106.13307283277541,43.369102253297,42.91353240325846,257.1274586604927,91.12871713453026,62.02285371068684,115.40531942460876,39.98564936781557,37.097820409324385,545.863111864606,37.49854002651339,81.5871178116879,132.7673082060061,345.2582259707017,53.16747369915676,49.321448231822814,67.89475247990853,35.65151017529714,67.41644956351156,60.50208920617041,39.64144089838808,67.03908419490011,111.85193170667287,60.57709276049011,45.72668273912879,36.018809173674015,75.46166951220391,40.319425740337266,55.73237288591533,45.0845621038376,80.37708651832918,49.92152421010857,59.665710775205895,90.14800539433452,49.676300828412224,91.17194744856054,45.67286502548326,45.17390446610706,106.66601918512885,84.35015544507314,42.26280345689168,171.40752205847986,198.8969495397338,43.32720871226519,86.04492922697696,71.72984085596835,24.543601839661818,45.36805314239311,57.83586455977799,96.92624069013017,47.10055955484007,124.9023670555755,54.15779349393044,65.32547431569556,97.5697502456662,99.51181673485938,247.7730903522294,106.21866229959761,79.385065425025,151.69464735266232,86.3713786795049,49.93631002837624,40.96494990950924,375.6000599576698,32.153399820518814,45.21718067286856,273.29973353495024,35.9329103984857,51.55567408796145,40.61200899139121,61.447465751293464,78.19929966643652,91.21198601551589,52.036017853806996,51.90291637011754,55.84880582712834,43.35055789011367,118.43505781762042,62.31629527034498,54.23713397326389,136.60091756747656,31.295009728823864,82.58739879552182,124.13953314595173,98.59595121057487,335.98020296080426,112.07738430814464,69.29907658379547,319.90163160797067,60.492096823467826,130.1524281428326,86.15378993095783,177.28806960782066,38.849344927875194,200.55717395374535,293.38804692007267,54.702346562080734,474.22337490999996,50.08329958583043,199.10437362687682,47.94585372070574,38.92323056819979,59.9802628938056,36.910179242503986,28.324689129063856,89.36815962764734,397.8027231777992,205.39763519155795,33.74312986881455,105.21939341726281,45.718260459971134,88.8952615536671,156.51813668183615,38.96274189354878,73.60616555873608,74.08490541004852,39.52598193660568,37.17086384397282,73.69008674201986,58.103551745069865,49.85662683619891,45.20762645175648,241.03449078589017,41.424970464736816,47.53043523038629,136.72603891075974,39.83757664663409,81.06271067094055,146.8287997212933,57.184818223435265,81.64436452218547,68.49037944658077,208.36972243479758,982.9464945449841,60.63308670438905,28.868684621680153,51.82815392539228,121.38953158562758,33.53539442730461,72.33356206399412,74.15768033511539,49.757366940234974,77.3750668635558,220.0852033113542,81.12783158254898,55.43151562811639,38.62660086420829,97.94190113231105,69.88296808800197,50.2495451514049,144.54447595758847,45.22446791570693,57.9819936607248,55.912686148837096,73.56381091713236,75.89651805823121,86.7107918634374,34.46604896639535,40.5515392607765,77.37394677179566,37.20248186453621,52.61148784608204,51.11144931828881,50.75110524693774,77.30057322172917,59.41465480201924,54.369185806409064,242.27864450018245,795.1866121399589,67.9526837148157,113.29944927928734,42.100557335204876,38.749163403409675,73.67580840953832,41.09604221658742,75.8458487937962,128.92725173617194,48.36049442753274,252.41286733709475,113.93530640826702,71.87419070982061,115.22005880633334,44.09133367194619,165.97920403656912,37.367545062018756,65.53228608775369,139.3645793722691,155.52611558853198,425.4206386531679,99.02221561459613,29.73736734701331,41.46402203643096,53.242705776062884,96.91252788239719,44.69251601511142,50.93835307076409,45.671353281774316,43.70770285507817,44.258185749594176,48.75903121106731,51.788390052925784,137.33265822808661,54.8970419081049,1727.3385668830088,48.56332448665478,832.84594238168,108.13010929047158,40.97600463830595,40.53640602497126,49.67086939306081,46.40263237455454,293.5036560002874,232.46036545480325,120.80986610170561,46.29114186564115,49.889898543829716,144.71206777672262,78.97959121228324,37.11031443463766,55.55715821552066,51.88476707632425,533.3934205186564,38.24829692372475,46.794291645578966,429.85182857020595,111.9362597212831,70.29109767709964,148.3255073558921,355.2118141749629,35.774252893442814,62.701902006077496,125.10005572168345,44.59794027059448,54.08949775989363,57.849649183926985,162.38136540373515,128.7580370744843,112.58759823037713,225.72377783895521,71.38350903750201,48.896570503321016,147.1899145885182,142.43418217939933,277.7163453324539,45.382248466236874,396.810702084495,57.724898166243776,61.32578427353664,44.906894206815174,41.69150190682218,152.78964988990103,109.55418525956519,40.69948081351801,108.70216906935113,48.52361433291658,44.29137896265985,45.73233175937726,51.24383482368516,94.77459959512123,99.29713508671227,137.01858710909113,54.1060619741004,93.47343933642861,35.25091976195898,39.394574192637066,60.79577612468202,42.006886878008785,49.97725383413921,45.391920819868645,58.115556706258126,99.77381165173516,365.1821576612856,123.54114320821495,61.723898125522155,40.868781114408584,53.27187555137262,108.18583898107373,77.24741633190706,108.95507388484205,57.05027183018963,36.3964314992157,55.908378142291525,152.3998198972365,59.167004412579004,65.64884815908623,60.69960622387609,190.55512803215214,63.28764971356743,45.767135254281804,40.64103561263272,998.4310425382625,66.36881605853438,75.40431347273288,40.39853868980624,96.9409933782212,52.73277580127852,39.60055622538727,74.54463320418951,40.82853361307337,45.763087177804465,69.20508701901599,87.96908808604414,61.60283087792385,214.7317557296411,50.35967731242,46.36307354711153,33.37109636302084,131.33157522360972,45.17355577160997,131.77528711270193,43.1114057605361,71.50845616078713,88.30402028408554,363.65014257508926,33.412185232842276,92.53706054878002,49.95963142530041,58.69635948088626,43.67634443751074,281.1372664957123,146.9666615081085,49.75908415363356,103.13167543637536,208.79407619452047,85.64931718388539,42.064591667965715,61.670589236219634,45.850558999931266,59.14267204946686,58.13931931166934,57.69469625083035,1149.2743473229004,48.891891888814165,40.00405527584316,136.0050230386627,111.83215277667861,268.19220115276426,45.131303070653075,33.429793321560155,48.66806528553195,208.92127809972672,37.265339243228496,46.65907440368468,47.189878773695746,266.52801745474864,47.831605583985684,86.58712182412874,55.14981458723462,42.771590212152816,110.9442386279789,292.3960258267685,44.66957286940216,54.48896075467485,52.355154345414654,120.7872041010946,88.30431081661179,42.63538065066827,40.47070662933919,297.69890105932257,94.06287286866954,57.26435473928541,87.01616022511095,47.91048642552796,117.58243988916571,88.33556753404774,133.259765002227,58.98824180050143,68.52659142861951,59.05042609146989,39.63288891790734,50.24135392994381,67.61129100773411,178.28509868479495,157.32818951806405,50.95058446606168,72.22763284412021,30.628309177139272,288.2687132950176,50.09412014528726,60.33613062098854,40.238254025408466,51.34890630042375,60.62115627400909,56.71818043226699,41.67304450907507,59.37762068325559,145.04611284553982,39.76832306784386,47.14305158514974,81.26608928213572,65.47940152053623,33.03986402359993,86.56623257776059,42.878131380714315,22.264052055789875,55.48418212389291,387.55803702842115,80.21577447371547,160.2931293539474,41.21639360281615,44.76162679450313,133.42369867725077,92.12041819876362,55.668133340873304,48.91773738778448,1004.0542405021612,65.78961180632749,98.87756978125245,58.16900328645001,47.55355439153968,46.65556355124493,393.96368361885743,159.93919979018324,77.20727857313233,77.02532952004181,677.7064657638758,38.37400422716867,336.97222405410844,91.77831679611154,99.92164120723415,45.06990154122825,383.0624587131018,36.749140459404416,60.99067496795971,48.01098523301508,54.85857884965973,60.91978358047007,60.740504006815634,65.53783337894986,380.95671511795706,39.16699444333433,145.13061444026263,54.36015726810714,137.71806000406391,39.84863929101664,39.36427283917879,130.77899391123253,253.30010856866673,89.27415441625988,102.64960807037056,50.340122434962694,138.89581258935513,75.35752582841403,51.818182815529184,38.90642499173589,93.63784709082586,73.20217081462991,227.55606229925647,2346.056198949238,58.47842753632859,41.67609924439118,48.94428893507206,364.64216366839344,70.82083851993792,37.43218850358328,86.63523454810715,237.74558110212556,144.07271716964948,39.431255924496696,37.85664659439698,197.69538049501537,67.99512532185565,72.31768420880138,40.39212209748696,523.5858850759871,53.1967555390043,53.48664469115086,46.12587548613199,93.201110432501,44.53424916050699,55.569273304759996,44.22515957956438,143.43913293045762,58.433360221474636,93.49077842224838,40.119717477999494,87.85710535163706,58.864418367432904,63.17469675592024,126.19334784780465,36.95979380002461,46.86401607143996,248.11396321615368,47.78045055200245,59.21658469160814,237.70044448855336,66.74073888193391,345.359411911994,49.913766221305245,55.51302368872578,51.84128800884719,67.46985782820673,199.26364098502668,102.07127285197512,67.13655097433667,46.546039700754335,319.5029550781344,47.69604881145592,53.08884010338029,64.87557870605363,46.338085142833,171.44622027924433,45.72640544594484,89.24480254468253,70.89012669236048,41.66449252859432,54.02667272617646,61.21784791988058,35.83286220858514,33.985400012801875,68.38544524048459,40.2433947707735,274.30858796282416,42.559026172961474,126.1069005098426,44.85773288744618,65.8675997993578,125.289514108627,45.87199497813579,47.553639006146106,40.38074844532471,93.34294207665661,65.59422573490721,54.93463606126446,169.3035239913781,135.88066306690877,281.43351311237416,113.22541403668515,257.8523560348848,61.082694516787896,77.11162649574268,72.08211325526894,118.35402094365975,91.17789451949471,35.78253847413177,59.89989536580269,44.61216516081933,58.75308932452252,76.45431535898426,155.24572573854186,104.82934398410063,44.67933218847014,105.78066032426075,80.20593908996997,46.0031623343674,57.861122112599816,109.96915185733725,35.536613373471695,66.28381101340109,87.76225932123566,122.4395889812987,75.64362897825838,74.78087361614752,114.9474697735743,80.76897912224847,89.07598055261303,74.44625644382766,65.49374294257838,31.398149897199723,59.3228020212866,29.829076391703182,164.82375684274106,47.41079734160775,69.24125312222009,54.969227599618,132.22738825650157,53.35926040806319,59.886848247783995,57.889622942808444,34.54571691374915,60.99847035848152,806.551319909999,46.436492434732656,68.93034959904779,1005.0462615954654,51.57334789510306,80.35278094295823,30.652656161487016,196.8754336608783,2597.8218697438942,84.36079549006845,44.551499219666866,64.61358175731738,46.81965712433147,33.0530999195157,64.76301694250358,118.12509051375545,347.8756781704144,42.37666492313441,114.54895036176458,38.84866768770115,405.00738996354187,81.99544968606301,49.04936926458471,37.796184150498036,46.76575104639566,55.88709388901405,370.7571601015344,321.91551254418505,53.24536686553979,198.2716198917225,96.52917096946662,219.4196683016938,34.662397719110686,368.3860129676117,47.229149113602276,46.350198384812,121.80188719500981,48.006331487102315,54.63322904430926,44.38706842878537,40.306177643018216,48.54557548484386,59.371893165908475,113.06940540144882,64.88041830474495,344.25193998706834,141.33393751747724,77.09751813733513,57.13454321182829,78.61403937552876,114.07617322074398,63.679702276511826,91.62896055492524,42.992872837212985,65.58053247396911,66.32973258867128,60.16748804827691,194.80542929462243,43.22751991744637,62.74813308706868,53.05734153814166,48.36394041116223,56.87911498231823,30.821097485007357,186.01757433317982,81.13782268958688,95.53714987616245,192.39866655347322,53.96991926347545,299.37634874773744,93.4958666116823,47.36619010938914,184.0208664837271,55.211338271522415,59.988748457389164,82.9684926287604,79.0363561779842,98.39193310637084,66.32088273109424,37.73658000691151,78.04588688902517,1726.3465457897046,45.09585316215746,88.0585948030214,45.753647887807304,35.37617690337587,126.03175657799433,865.0767175618736,41.53066662097613,57.441339128170455,94.15677621420512,54.423205230783765,60.090673423483715,36.4287569445068,74.49462543061179,42.851678350164896,150.87268052536174,144.622291777059,50.56365299465727,40.857494427682546,102.72493941661199,63.91929656704388,40.46235414650663,72.3755301308062,38.41252722274909,92.21917670761647,65.81346776929458,51.876238875795245,76.10006255993488,74.95641926264423,76.45369060550809,39.39774357087839,39.712662051240784,45.13111968658251,46.95707860521196,67.01271744942106,101.85048686518064,262.99303003567684,54.47510177535621,49.0249100752486,40.17617724841526,61.990491451785694,65.84789477164155,86.22479032075135,35.79582959446051,73.98121482874227,51.9668387725648,98.86548614401381,70.92481695326774,82.49980416553642,43.32093033745782,53.59148489421601,215.82958846173395,48.482669925406086,229.4201875883532,40.060488709473354,117.26021857699163,119.64265878162371,43.70043529618809,44.9849835331305,57.69503717116825,79.1218141359137,34.209722911481904,113.53996099673839,173.70093945002412,79.27743488314468,67.56775859392422,112.28149542678506,45.735487825766214,58.76276167815428,49.052025941769955,120.2075415899932,48.85847630951483,97.79091703820927,54.385321331326466,32.75110877551036,39.072990586381174,211.301983185117,45.1747686322097,163.80031210833135,53.14760476740377,154.71932295894172,74.93474558626298,72.5535436726054,32.870403165479914,48.43423598524153,48.09258064814425,80.54349090539543,148.77288097972664,335.79822418264905,92.35092098335244,87.17805675134585,75.68279938450992,19.523162355182166,67.35618949155592,139.91882337251786,45.83524330293649,48.488908987579066,89.43802798995466,117.58586090931614,50.05840744359538,154.6660369137233,850.9844783409432,88.82921236913056,47.44221489193735,54.863065316109484,39.840748855080975,63.597229977151954,40.26505990638281,57.54486075213094,42.159812440516575,52.46829802524546,198.23829980216098,102.598725554431,38.910243539074685,27.006161852404368,273.14761616408134,216.9511800788577,29.690761038495978,138.9539716757563,85.22510198962085,424.49049486469903,162.60715411850992,34.615015821200714,51.84883176907874,1283.7320249736847,64.119116615638,51.97379004606085,146.19789349521403,48.973175276409876,80.0794008156674,83.20256069637911,42.51554656964032,112.15013236217334,97.98696354198032,102.46302268370285,51.906630938400696,63.250630487595465,59.37200969772577,213.96376571205602,40.083149048361655,94.64290571659708,42.07081325073172,181.79541281611813,40.62632084705227,135.8291405903833,38.50865625007924,50.982205513644786,85.59510073082456,44.97664094698557],"xaxis":"x","y":[30.29,47.7,6.73,5.66,152.14,7.53,17.11,9.67,73.77,5.21,150.69,47.42,17.51,11.08,4.76,73.31,30.34,59.24,33.58,31.96,232.12,9.43,8.12,51.2,14.69,33.77,10.29,32.33,61.62,7.67,21.08,35.56,105.32,41.15,287.23,85.28,139.34,18.63,25.15,10.04,42.74,29.93,17.38,263.57,11.17,8.59,570.98,13.65,13.22,2.91,6.53,49.15,1222.69,28.59,51.64,185.23,141.7,3.27,66.39,85.15,87.31,95.26,48.51,26.78,50.14,6.02,14.87,349.47,4,14.79,4.98,20.39,6.45,19.28,93.47,52.87,372.55,5.81,12.68,5.81,8.57,47.39,128.73,99.39,37.72,3.02,12,38.97,39.27,266.18,5.9,29.84,49.18,3.82,8.8,11.85,10734.57,266.78,12.25,6.04,18.07,7.21,63.89,8.53,138.68,18.34,29.39,218.53,69.3,587.16,11.97,8.46,4.36,5.16,46.57,32.22,5.46,181.38,7.08,65.03,5.87,24.25,23.17,178.21,39.57,16.5,4.48,61.36,4.03,8.8,77.29,6.05,45.3,8.77,256.57,2.17,37.97,11.11,74.45,17.67,19.83,406.2,209.92,45.81,2.36,22.02,198.38,6.22,8.31,5.96,13.61,262.75,152.36,15.43,7.23,5.94,43.45,60.94,159.16,31.92,9.71,69.36,18.1,8.37,23.58,48.3,147.82,71.47,18.89,53.36,17.65,4.5,92.59,62.57,4.8,25.37,90.31,100.42,6.37,2.9,63.61,64.19,22.15,192.63,5.55,109.79,3.29,67.15,31.92,79.3,8.81,6.71,12.33,29.33,12.15,16.83,116.21,6.43,187.92,39.56,30.85,78.29,4.33,4.63,16.89,3.42,10.21,5.97,49.16,20.1,190.58,7.67,4.79,1000.06,50.52,122.99,134.69,5.6,27.38,7.08,2.04,44.6,3.51,41.98,6.36,5.87,8.87,3.97,127.56,37.85,4.26,147.69,5.9,10.55,5.69,32.81,34.81,56.25,653.5,65.65,97.01,3.49,4.65,9.13,41.69,14.77,39.47,2.82,142.48,68.97,6.64,7.31,82.9,8.69,256.31,93.23,11.77,38.08,17.24,43.64,18.41,12.7,27.3,6.71,3.89,71.03,1.78,152.71,50.43,22.18,12.06,29.26,20.98,77.15,3.84,91.09,14.35,839.79,226.43,27.27,57.69,5.16,6.69,20.79,176.79,59.61,9.83,8.24,16.09,20.44,170.84,35.39,30.02,19.72,15.59,66.05,3.89,7.46,14.35,7.74,141.32,46.61,20.29,3.75,235.69,243.38,151.95,100.87,56.51,6.79,4.84,247.85,24.17,168.62,47.11,12.99,104.17,30.66,3.82,120.91,132.47,106.37,44.93,6.93,61.71,4.2,24.42,8.14,111.69,6.22,9.45,23.84,4.88,507.7,553.27,9.48,122.92,103.37,50,3.86,3.4,3.66,93.43,6.85,32.8,32.73,13.43,413.1,6.19,6.16,68.14,0.68,16.45,13.88,350.87,802.89,291.28,91.73,4.72,7.21,25.02,6.75,28.93,229.55,3.01,49.59,341.23,37.18,353.49,93.87,93.21,11.79,49.25,154.4,84.93,6.02,43.63,35.36,51.47,42.8,4.35,17.11,205.07,40.78,5.6,186.96,14.51,35.56,537.03,53.38,38.9,8.16,37.51,3.82,5.51,50.76,380.17,10.75,4.3,7.12,1.6,63.45,4.42,7.51,7.14,22.78,8.45,8.59,5.37,11.59,6.93,54.95,19.7,89.41,74.04,3.91,35.22,43.13,28.54,70.4,10.06,6.3,5.11,45.38,0,10.14,765.56,14.46,28.64,27.86,8.85,256.72,196.67,525.78,93.67,8.97,165.61,3.06,17.84,19.17,50.7,47.25,2.99,3.2,54.92,96.4,16.01,4.74,10.56,132.28,15.5,7.29,63.36,47.68,297.23,67.06,3.4,98.54,0.76,3.72,50.15,2.45,265.21,188.67,156.08,83.1,94.85,20.57,29.18,12.09,13.77,34.14,1231.19,39.41,7.57,113.25,237.36,93.23,33.52,57.03,12.79,12.97,5.72,6.38,10.93,16.2,57.16,18.62,346.62,44.19,190.45,6.1,75.67,3.69,73.48,1185.15,20.12,92.3,188.13,72.71,101.31,15.8,80.06,3.18,141.15,17.74,22.65,42.2,14.32,46.86,18.42,247.85,52.43,5.71,51.26,3.07,10.14,242.19,21.3,8.45,113.31,75.12,13.15,0.89,8.57,3.48,5.33,102.04,3.71,31.58,42.52,6.89,7.69,6.16,29.16,5.88,76.85,1.3,37.58,7.29,24.1,28.78,25.39,11.28,32.02,31.46,38.54,49.7,9.47,456.22,124.92,14.19,110.62,24.36,4.13,68.89,145.48,21.24,18.72,12.4,15.62,43.16,160.3,47.78,139.27,30.86,44.97,163.99,142.69,12.63,4,109.46,26.64,64.13,285.62,9.57,74.52,4.01,226.81,10.16,13.64,92.58,7.15,8.16,75.64,60.02,223.36,8,13.87,3.92,73.32,233.43,208.26,351.53,16.76,200.3,604.45,51.91,16.06,10.92,90.73,4.73,69.42,238.61,4.93,900.81,0.7,399.76,31.85,19.95,248.29,2.43,480.9,10.32,176.76,144.3,5.15,97.97,27.69,63.6,144.53,27.56,29.24,26.55,18.5,80.06,10.77,15.34,0.75,3.91,474.59,4.25,128.41,433.86,69.88,44.32,413.35,4.43,348.22,29.78,66.32,290.4,44.51,4.02,5.2,56.82,4.55,9.65,52.88,57.79,58.32,393.2,72.69,45.02,24.62,4.19,223.34,10.89,83.27,7.56,29.73,100.71,7.53,121.88,176.47,23.82,4.48,53.21,16.3,4.9,6.39,54.95,61.64,7.12,19.68,8.07,704.76,33.35,184.81,61.25,20.88,60.04,16.47,551.43,260.13,8.61,167.31,70.14,64,21.92,9.16,111.23,7.02,236.34,318.08,149.93,364.9,295.86,34.62,3.11,156.18,118.86,4.16,3.27,6.54,3.65,48.91,8.87,5.19,148.11,88.87,1793.9,37.34,275.54,30.87,5.53,56.96,82.38,21.52,1597.56,42.41,109.72,4.68,19.81,79.81,6.74,6.02,39.05,5.79,271.32,85.55,4.52,233,9.53,157.21,15.23,35.44,3.92,64.75,40.35,20.45,15.27,29.42,39.75,119.03,28.07,95.77,23.14,60.83,224.13,98.65,443.86,3.52,221.47,5.68,7.9,11.73,4.71,38.59,32.75,4.79,101.43,5.54,5.66,5.88,130.58,60.33,25.6,69.18,253.53,84.57,3.55,42.71,5.09,12.28,19.15,93.68,42.24,58.44,1084.79,243.37,7.47,4.43,8.09,69.67,82.72,101.64,19.04,4.02,5.6,46.49,183.51,98.22,6.54,69.3,41.96,3.71,5.97,913.07,145.42,7.56,5.3,45.96,8,71.16,13.1,53.88,53.02,379.34,43.35,7.94,322.79,4.43,14.81,3.41,18.97,3.9,115.56,61.68,29.92,9.99,530.66,3.7,49.17,27.39,96.62,6.57,51.93,21.38,63.48,90.34,423.93,183.79,5.86,92.45,23.85,45.32,77.75,5.31,2451.58,9.87,39.5,67.17,162.39,220.26,8.46,23,31.79,28.83,20.02,101.99,26.72,15.5,32.77,132.56,15.74,110.66,9.65,246.83,10.83,5.92,9.2,467.18,24.35,3.67,5.35,862.64,89.53,234.03,1.67,15.2,71.38,43.57,260.57,226.73,87.14,14.44,3.99,28.82,73,188.07,45.6,5.07,103.77,3.16,395.26,27.93,10.23,23.27,60.07,112.19,7.22,20.07,15.9,72.46,3.92,176.63,32.52,32.35,6.84,96.59,44.71,3.52,48.95,116.95,36.65,142.36,8.27,3.89,315.85,9.73,1.61,5.81,1638.46,8.61,14.99,49.18,26.73,17.42,202.49,59.21,9.99,42.57,481.1,29.62,399.54,72.88,52.79,20.67,381.89,3.56,4.07,15.74,26.94,27.84,32.72,15.64,582.28,13.36,23.83,27.06,411.09,4.74,15.18,73.14,185.82,13.97,12.15,3.49,19.84,93.67,93.55,59.18,74.98,43.56,312.6,12307.16,6.47,6.19,12.59,579.99,59.63,2.81,119.34,283.04,19.72,40.76,3.18,62.01,1.58,15.77,3.34,587.09,12.07,23.74,4.61,89.6,103.42,7.52,57.33,16.82,65.75,22.76,8.42,39.54,99.91,47.12,80.75,10.17,4.54,28.3,19.36,6.35,63.87,10.05,184.64,34.08,3.15,76.89,11.99,292.92,112.01,66.1,23.22,251.43,75.58,152.14,41.06,7.07,115.34,6.12,34.87,85.3,19.76,28.03,6.67,18.73,20.07,34.98,12.56,150.23,12.35,50.23,19.16,45.22,344.2,4.74,1.34,3.77,36.71,3.92,83.35,324.92,64.95,675.34,22.17,536.43,43.84,30.26,78.19,44.61,61.72,39.57,73.52,3.99,182.11,99.62,551.53,39.45,6.99,89.94,7.27,4.76,9.66,77.56,3.78,44.26,39.31,37.14,11.29,24.91,19.73,326.74,64.79,59.06,2.35,49.79,61.19,4.39,357.86,6.61,126.3,53.25,58.98,4.95,6.87,8.93,39.1,66.44,440.31,38.45,102.08,1558.85,37.17,124.96,5.3,260.94,3232.23,74.13,31.18,225.67,6.04,49.44,8.31,67.65,320.35,46.72,143.67,3.16,94.59,96.15,5.7,25.49,3.65,22.46,1727.11,49.26,173.75,295.48,53.56,136.07,5.03,250.05,74.28,3.59,123.14,0.12,24.64,4.98,46.95,27.76,5.53,17.23,154.13,740.97,78.06,125.42,7.53,131.64,129.05,6.67,99.65,298.91,30.14,46.32,5.88,0,14.3,45.07,1.14,147.24,21.28,4.44,249.09,72.02,47.25,1623.34,8.7,226.06,10.83,47.91,434.48,5.33,120.13,38.15,180.38,37.16,8.18,7.05,148.7,0,29.26,39.35,24.18,4.02,103.39,2089.28,4.5,77.48,143.3,46.61,46.85,80.08,108.04,23.01,47.82,67.62,69.53,4.2,25.48,7.74,4.33,22.61,3.63,11.25,58.79,58.85,13.34,7.25,26.41,5.94,5.02,4.44,4.72,23.65,96.41,529.47,7.1,14.15,3.72,48.23,63.06,96.39,5.29,10.08,4.49,22.08,1084.49,8.16,65.77,20.63,375.56,112.13,58.52,26.54,68.3,464.77,5.14,57.17,8.98,91.49,3.68,44.35,92.7,4.99,0.37,305.15,57.25,0,16.27,45.59,34.26,34.92,24.31,5.07,1.08,28.19,82.87,109.71,17.19,50.31,20.25,41.22,5.74,45.98,17.88,86.5,8.55,394.6,22.57,48.85,38.13,269.23,47.26,122.83,5.23,8.81,48.65,640.71,46,328.63,4725.43,73.9,65.65,6.98,3.5,8.54,4.15,6.98,4.69,40.38,150.81,8.15,4.38,37.13,328.97,132.71,3.15,229.2,10.97,122.54,84.18,6.46,26.59,500.93,148.02,6.39,222.02,12.75,50.57,9.28,3.86,369.14,156.11,46.38,14.99,69.77,7.21,100.59,5.3,39.61,102.73,23.42,4.26,127.53,3.91,12.5,117.11,8.47],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","TotalGHGEmissions_predRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=TotalGHGEmissions_train,\n","                            y_test=TotalGHGEmissions_test,\n","                            y_test_name='TotalGHGEmissions_test',\n","                            y_pred_name='TotalGHGEmissions_predRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","print(ScoresRidge)\n","figRidge.show()"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[404.10515691226357,404.1051546964016,404.1051524393023,404.10515014019865,404.10514779830885,404.10514541283675,404.1051429829712,404.1051405078857,404.1051379867393,404.1051354186744,404.1051328028184,404.1051301382813,404.1051274241577,404.1051246595242,404.10512184344134,404.10511897495155,404.10511605307966,404.10511307683186,404.1051100451965,404.10510695714294,404.1051038116211,404.10510060756167,404.10509734387523,404.1050940194518,404.10509063316175,404.1050871838532,404.1050836703539,404.1050800914689,404.10507644598147,404.1050727326525,404.1050689502189,404.10506509739525,404.1050611728714,404.10505717531305,404.1050531033611,404.1050489556311,404.1050447307128,404.1050404271702,404.10503604353954,404.10503157833074,404.10502703002584,404.1050223970782,404.1050176779128,404.10501287092524,404.10500797448117,404.10500298691596,404.104997906534,404.10499273160775,404.1049874603778,404.1049820910524,404.10497662180586,404.1049710507788,404.10496537607685,404.1049595957715,404.10495370789687,404.1049477104515,404.10494160139615,404.1049353786542,404.1049290401099,404.1049225836083,404.1049160069544,404.1049093079122,404.10490248420444,404.10489553351096,404.1048884534688,404.104881241671,404.1048738956658,404.10486641295563,404.1048587909967,404.1048510271976,404.10484311891935,404.1048350634729,404.10482685811985,404.10481850007073,404.10480998648393,404.1048013144652,404.1047924810665,404.1047834832849,404.10477431806123,404.1047649822796,404.10475547276644,404.1047457862891,404.10473591955406,404.10472586920747,404.10471563183256,404.1047052039491,404.1046945820121,404.10468376241045,404.10467274146635,404.1046615154328,404.1046500804937,404.10463843276204,404.1046265682777,404.10461448300794,404.10460217284407,404.10458963360145,404.10457686101745,404.1045638507503,404.1045505983768,404.10453709939264,404.1045233492086,404.10450934315077,404.10449507645774,404.1044805442799,404.1044657416773,404.1044506636184,404.10443530497713,404.1044196605333,404.1044037249686,404.10438749286646,404.1043709587089,404.1043541168759,404.1043369616424,404.10431948717735,404.10430168754067,404.10428355668193,404.1042650884386,404.10424627653293,404.10422711457073,404.1042075960386,404.10418771430216,404.10416746260364,404.1041468340597,404.1041258216583,404.1041044182578,404.104082616583,404.1040604092242,404.1040377886329,404.1040147471209,404.1039912768566,404.10396736986314,404.1039430180149,404.1039182130352,404.1038929464935,404.10386720980216,404.10384099421464,404.1038142908207,404.10378709054504,404.103759384143,404.1037311621987,404.1037024151209,404.10367313313924,404.10364330630256,404.1036129244745,404.1035819773298,404.10355045435114,404.1035183448261,404.10348563784294,404.1034523222863,404.1034183868352,404.103383819957,404.1033486099058,404.1033127447161,404.1032762122006,404.1032389999452,404.10320109530494,404.10316248539965,404.10312315711,404.10308309707216,404.1030422916742,404.1030007270513,404.10295838908013,404.1029152633753,404.10287133528374,404.10282658988046,404.1027810119621,404.10273458604337,404.1026872963506,404.1026391268172,404.1025900610778,404.1025400824627,404.10248917399264,404.10243731837187,404.1023844979843,404.1023306948854,404.1022758907976,404.10222006710336,404.10216320483926,404.10210528468923,404.1020462869783,404.10198619166573,404.1019249783384,404.1018626262036,404.1017991140824,404.10173442040224,404.1016685231894,404.1016014000623,404.1015330282228,404.1014633844495,404.1013924450898,404.10132018605077,404.10124658279267,404.10117161031906,404.1010952431694,404.1010174554106,404.1009382206268,404.10085751191184,404.10077530186,404.10069156255616,404.10060626556617,404.1005193819286,404.1004308821435,404.1003407361628,404.10024891338105,404.1001553826237,404.1000601121371,404.09996306957845,404.0998642220042,404.0997635358586,404.099660976963,404.09955651050416,404.09945010102257,404.09934171239985,404.0992313078476,404.0991188498939,404.0990043003715,404.0988876204045,404.0987687703954,404.0986477100114,404.0985243981714,404.0983987930316,404.09827085197185,404.09814053158067,404.0980077876412,404.09787257511596,404.09773484813184,404.09759455996453,404.09745166302264,404.09730610883196,404.0971578480193,404.0970068302951,404.0968530044371,404.096696318273,404.09653671866283,404.0963741514812,404.0962085615988,404.09603989286416,404.0958680880844,404.0956930890067,404.0955148362976,404.0953332695241,404.0951483271329,404.09495994642947,404.0947680635581,404.09457261347853,404.09437352994576,404.09417074548736,404.09396419138056,404.0937537976295,404.0935394929412,404.0933212047027,404.0930988589554,404.09287238037143,404.09264169222774,404.0924067163804,404.09216737323874,404.09192358173885,404.0916752593154,404.0914223218757,404.09116468377,404.09090225776345,404.09063495500715,404.0903626850083,404.0900853556001,404.08980287291024,404.08951514133054,404.0892220634842,404.0889235401944,404.0886194704499,404.0883097513721,404.0879942781809,404.0876729441592,404.087345640618,404.08701225685934,404.0866726801408,404.0863267956367,404.08597448640023,404.0856156333254,404.08525011510636,404.0848778081976,404.0844985867731,404.08411232268446,404.08371888541797,404.0833181420523,404.082909957213,404.08249419302985,404.0820707090887,404.0816393623866,404.0812000072842,404.08075249545794,404.0802966758497,404.07983239461873,404.07935949509005,404.07887781770194,404.0783871999553,404.0778874763581,404.07737847837217,404.0768600343575,404.07633196951554,404.07579410583196,404.07524626201786,404.0746882534512,404.07411989211516,404.07354098653707,404.07295134172534,404.0723507591065,404.07173903645906,404.07111596784864,404.07048134355983,404.06983495002794,404.06917656976975,404.0685059813122,404.06782295912075,404.0671272735256,404.06641869064714,404.0656969723203,404.06496187601726,404.06421315476837,404.06345055708226,404.0626738268655,404.0618827033383,404.06107692095145,404.06025620930006,404.0594202930366,404.0585688917824,404.05770172003724,404.05681848708787,404.0559188969144,404.0550026480958,404.0540694337127,404.0531189412509,404.0521508524986,404.05116484344836,404.05016058419085,404.0491377388122,404.0480959652849,404.0470349153614,404.04595423446244,404.044853561565,404.0437325290883,404.04259076277765,404.0414278815865,404.0402434975569,404.03903721569594,404.03780863385396,404.03655734259684,404.0352829250789,404.033984956912,404.0326630060334,404.0313166325712,404.029945388708,404.0285488185412,404.027126457942,404.0256778344127,404.02420246693936,404.02269986584537,404.02116953263885,404.0196109598618,404.01802363093327,404.0164070199918,404.0147605917352,404.0130838012576,404.011376093884,404.0096369050022,404.00786565989165,404.00606177355064,404.0042246505203,404.0023536847055,404.00044825919275,403.99850774606807,403.9965315062271,403.9945188891874,403.9924692328942,403.99038186352544,403.98825609529365,403.98609123024266,403.98388655804473,403.981641355792,403.97935488778694,403.97702640532736,403.97465514648997,403.9722403359118,403.9697811845657,403.9672768895341,403.96472663378074,403.96212958591764,403.95948489996874,403.95679171513075,403.9540491555323,403.951256329986,403.9484123317413,403.9455162382315,403.94256711081727,403.93956399452895,403.93650591780334,403.93339189221786,403.9302209122218,403.92699195486296,403.92370397951197,403.92035592758293,403.91694672225015,403.91347526816196,403.9099404511508,403.9063411379398,403.9026761758467,403.8989443924831,403.89514459545114,403.89127557203653,403.8873360888987,403.88332489175616,403.8792407050701,403.8750822317241,403.8708481527001,403.86653712675115,403.86214779007196,403.8576787559649,403.8531286145034,403.84849593219235,403.843779251625,403.83897709113774,403.8340879444606,403.8291102803654,403.82404254231193,403.8188831480897,403.8136304894583,403.8082829317847,403.8028388136775,403.79729644661995,403.79165411459917,403.78591007373444,403.7800625519021,403.77410974835936,403.76804983336643,403.7618809478051,403.7556012027984,403.749208679326,403.74270142784036,403.7360774678811,403.729334787687,403.72247134380984,403.715485060725,403.70837383044284,403.7011355121205,403.6937679316711,403.6862688813775,403.67863611950077,403.6708673698946,403.66296032161756,403.65491262854755,403.6467219089969,403.6383857453309,403.62990168358647,403.62126723309467,403.6124798661041,403.6035370174097,403.59443608398277,403.58517442460635,403.575749359513,403.56615817002864,403.55639809822026,403.54646634655035,403.5363600775343,403.5260764134076,403.51561243579636,403.50496518539785,403.4941316616663,403.48310882250837,403.47189358398697,403.4604828200336,403.44887336217226,403.4370619992509,403.42504547718715,403.4128204987231,403.400383723194,403.3877317663076,403.37486119994037,403.36176855194526,403.3484503059758,403.3349029013244,403.32112273277926,403.30710615049713,403.2928494598936,403.2783489215538,403.2636007511604,403.24860111944565,403.23334615215964,403.21783193006615,403.20205448895723,403.186009819695,403.1696938682756,403.15310253592133,403.13623167919843,403.11907711016227,403.10163459653313,403.08389986189934,403.06586858595347,403.0475364047594,403.0288989110519,403.009951654571,402.9906901424318,402.9711098395284,402.9512061689782,402.930974512601,402.9104102114411,402.88950856632727,402.8682648384758,402.8466742501358,402.82473198527896,402.80243319033355,402.7797729749659,402.7567464129071,402.7333485428306,402.70957436927677,402.6854188636305,402.6608769651481,402.6359435820382,402.61061359259617,402.5848818463927,402.5587431655193,402.53219234588767,402.50522415859075,402.47783335131805,402.4500146498328,402.4217627595079,402.3930723669235,402.36393814152495,402.33435473734414,402.3043167947823,402.2738189424569,402.242855799111,402.21142197558856,402.17951207687133,402.1471207041832,402.1142424571562,402.0808719360633,402.0470037441147,402.0126324898182,401.9777527894051,401.94235926931725,401.90644656875963,401.87000934231367,401.8330422626128,401.7955400230787,401.757497340717,401.71890895897116,401.67976965063326,401.64007422081096,401.5998175099465,401.55899439688966,401.51759980201774,401.4756286904055,401.43307607503857,401.389937020069,401.34620664411193,401.3018801235763,401.2569526960312,401.21141966359903,401.1652763963783,401.1185183358853,401.07114099851503,401.02313997901626,400.97451095397554,400.9252496853036,400.87535202372464,400.82481391225616,400.77363138967934,400.72180059399153,400.6693177658364,400.6161792519041,400.5623815082965,400.5079211038499,400.4527947234101,400.39699917104974,400.3405313732248,400.28338838185954,400.2255673773551,400.1670656715105,400.1078807103533,400.0480100768674,399.98745149361247,399.9262028252259,399.86426208079934,399.80162741612173,399.7382971357807,399.6742696951116,399.60954370199033,399.544117918457,399.4779912621652,399.4111628076467,399.343631787385,399.27539759268836,399.206459774355,399.13681804312336,399.0664722698989,398.99542248575017,398.92366888166873,398.85121180808386,398.77805177412904,398.7041894466496,398.62962564895133,398.554361359281,398.4783977090357,398.4017359806973,398.32437760548845,398.24632416074627,398.16757736701203,398.088139084837,398.00801131130004,397.92719617624005,397.8456959382024,397.7635129801016,397.68064980460105,397.59710902921455,397.5128933811342,397.4280056917869,397.34244889112824,397.2562260016798,397.16934013231685,397.0817944718166,396.99359228217617,396.90473689171233,396.81523168795286,396.7250801103352,396.6342856427242,396.5428518057662,396.45078214909324,396.3580802433968,396.2647496723877,396.1707940246632,396.0762168854979,395.9810218285843,395.8852124077408,395.7887921486135,395.69176454039183,395.5941330275665,395.495901001752,395.39707179360005,395.29764866483373,395.19763480042457,395.0970333009435,394.9958471751139,394.8940793325929,394.7917325770134,394.6888095993121,394.58531297137563,394.4812451400323,394.3766084214204,394.271404995761,394.16563690256413,394.0593060362993,393.95241414255514,393.84496281472065,393.7369534912107,393.6283874532668,393.5192658233568,393.40958956419973,393.29935947843984,393.1885762089943,393.0772402400956,392.96535189905046,392.85291135873683,392.7399186408554,392.6263736199543,392.51227602824383,392.39762546121426,392.2824213840711,392.16666313899725,392.0503499532556,391.93348094813535,391.81605514875235,391.6980714947058,391.57952885159546,391.46042602339855,391.3407617657078,391.22053479982543,391.0997438277102,390.97838754776984,390.85646467149127,390.73397394089716,390.61091414681766,390.4872841479631,390.36308289078,390.2383094300756,390.112962950389,389.9870427880878,389.8605484541686,389.7334796577345,389.6058363301252,389.4776186496697,389.3488270670338,389.2194623311294,389.08952551555456,388.95901804552875,388.8279417252901,388.6962987659143,388.5640918135208,388.43132397782523,388.2979988609972,388.1641205867835,388.0296938298532,387.8947238453211,387.75921649840546,387.6231782941751,387.48661640733843,387.349538712029,387.21195381153956,387.0738710679555,386.9353006316396,386.79625347052,386.656741399128,386.51677710733895,386.3763741887631,386.2355471687368,386.09431153186017,385.9526837490334,385.81068130393504,385.66832271889325,385.5256275800969,385.3826165620927,385.23931145151806,385.0957351700139,384.95191179626806,384.8078665871345,384.66362599777574,384.51921770077786,384.3746706041835,384.2300148683921,384.0852819218763,383.9405044756606,383.79571653651385,383.65095341880277,383.50625175495793,383.36164950450075,383.21718596158433,383.0729017609988,382.9288388825924,382.7850406540644,382.6415517520812,382.4984182016723,382.3556873738616,382.21340798149316,382.0716300732073,381.9304050255308,381.78978553303995,381.64982559656255,381.51058050938246,381.3721068414142,381.2344624213186,381.0977063165292,380.96189881116436,380.8271013818027,380.69337667109954,380.56078845922787,380.4294016331296,380.29928215356506,380.17049701995313,380.0431142329984,379.91720275510477,379.79283246857983,379.6700741316361,379.5489993322045,379.42968043957535,379.3121905538882,379.196603453498,379.0829935402513,378.97143578270544,378.86200565733753,378.7547790877895,378.6498323822033,378.54724216870693,378.44708532911545,378.3494389309217,378.25438015765104,378.1619862376673,378.0723343715187,377.9855016579202,377.90156501847554,377.82060112124725,377.7426863032889,377.66789649226155,377.5963071272587,377.5279930789735,377.4630285693446,377.4014870908225,377.34344132540474,377.2889630635906,377.23812312341136,377.19099126969655,377.147636133737,377.10812513351266,377.07252439465253,377.0408986722958,377.01331127402716,376.98982398405735,376.97049698882137,376.95538880416524,376.94455620429454,376.9380541526499,376.9359357348785,376.93825209406384,376.94505236837233,376.9563836312736,376.97229083448184,376.99281675376346,377.01800193774733,377.0478846598692,377.0825008735698,377.121884170864,377.16606574438254,377.21507435298446,377.26893629102403,377.3276753613476,377.3913128520856,377.4598675172912,377.5333555614685,377.61179062802023,377.69518379163173,377.7835435546,377.87687584709846,377.9751840313631,378.0784689097677,378.1867287367474,378.299959234517,378.41815361252003,378.5413025905309,378.66939442532686,378.80241494082827,378.94034756160517,379.0831733496313,379.23087104416055,379.38341710459434,379.54078575619843,379.7029490385209,379.8698768563586,380.04153703311204,380.2178953663647,380.3989156855181,380.58455991131103,380.7747881170497,380.9695585913699,381.1688279023576,381.3725509628456,381.58068109671376,381.7931701060103,382.00996833872483,382.23102475703547,382.4562870058627,382.6857014815626,382.9192134005958,383.15676686801464,383.3983049456146,383.6437697196021,383.8931023676358,384.1462432251048,384.4031318505148,384.66370708985585,384.92790713983953,385.1956696098913,385.46693158279885,385.7416296739201,386.0197000888628,386.30107867955707,386.585700998646,386.8735023521308,387.16441785021107,387.45838245627135,387.7553310339682,388.0551983923842,388.3579193292162,388.66342867197926,388.9716613172063,389.282552267637,389.5960366673902,389.9120498351242,390.23052729519173,390.55140480680456,390.8746183912259,391.2001043570141,391.52779932334835,391.85764024146647,392.1895644142543,392.52350951402667,392.85941359854553,393.19721512532215,393.5368529642553,393.87826640866007,394.22139518474245,394.5661794595802,394.9125598476698,395.26047741610273,395.6098736884366,395.9606906473253,396.31287073597605,396.6663568585025,397.0210923792398,397.37702112109315,397.73408736298984,398.0922358365018,398.4514117217109,398.81156064238553,399.1726286605373,399.534562270427,399.897308392087,400.26081436442746,400.62502793799297,400.9898972674322,401.35537090374766,401.72139778638336,402.0879272352161,402.454908942504,402.8222929648539,403.1900297152603,403.558069955271,403.9263647873289,404.29486564734145,404.6635242975225,405.03229281955277,405.4011236081013,405.7699693647455,406.13878309233104,406.507518089802,406.8761279475374,407.2445665432204,407.6127880382693,407.98074687485325,408.34839777351425,408.7156957314143,409.0825960212231,409.4490541906598,409.81502606269885,410.1804677364471,410.5453355886975,410.90958627616226,411.2731767383817,411.6360642013099,411.9982061815675,412.3595604913562,412.7200852440218,413.07973886025377,413.4384800749058,413.79626794441947,414.1530618548301,414.508821530334,414.86350704239265,415.2170788193477,415.5694976565204,415.9207247267639,416.27072159144063,416.619450211791,416.9668729606595,417.31295263454615,417.65765246594594,418.00093613594,418.3427677870033,418.68311203598944,419.0219339872557,419.3591992458896,419.69487393099973,420.02892468902945,420.36131870706004,420.6920237260597,421.02100805404507,421.3482405791159,421.6736907823266,421.99732875035863,422.31912518796014,422.63905143011596,422.9570794539176,423.2731818900996,423.58733203421235,423.89950385740167,424.20967201676666,424.51781186527035,424.8238994611762,425.1279115769895,425.429825707879,425.72962007956005,426.0272736556203,426.32276614427155,426.6160780045102,426.9071904516768,427.1960854623993,427.4827457789117,427.7671549127398,428.04929714774715,428.3291575425361,428.6067219322016,428.8819769294355,429.1549099249811,429.42550908744033,429.6937633624375,429.95966247114075,430.2231969081521,430.48435793876934,430.7431375956311,430.9995286747538,431.2535247309717,431.5051200727935,431.7543097566862,432.00108958080443]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[542.3287889113676,542.3287861688596,542.3287833753133,542.3287805297794,542.32877763129,542.32877467886,542.3287716714851,542.3287686081433,542.3287654877927,542.3287623093726,542.3287590718023,542.328755773981,542.3287524147875,542.3287489930793,542.3287455076936,542.3287419574451,542.328738341127,542.3287346575091,542.3287309053396,542.3287270833425,542.3287231902184,542.3287192246438,542.3287151852702,542.3287110707241,542.3287068796071,542.3287026104937,542.3286982619329,542.3286938324458,542.3286893205263,542.3286847246411,542.3286800432269,542.3286752746923,542.3286704174162,542.328665469747,542.3286604300023,542.3286552964689,542.3286500674016,542.3286447410225,542.3286393155201,542.3286337890505,542.3286281597345,542.3286224256583,542.3286165848722,542.3286106353901,542.3286045751898,542.328598402211,542.3285921143545,542.3285857094826,542.3285791854178,542.3285725399421,542.3285657707959,542.3285588756781,542.3285518522439,542.3285446981064,542.3285374108326,542.328529987945,542.32852242692,542.328514725187,542.3285068801277,542.3284988890747,542.3284907493114,542.3284824580703,542.3284740125328,542.328465409827,542.3284566470286,542.3284477211583,542.328438629182,542.3284293680081,542.328419934488,542.3284103254149,542.3284005375216,542.3283905674806,542.3283804119023,542.3283700673344,542.3283595302597,542.3283487970958,542.328337864194,542.3283267278379,542.3283153842406,542.3283038295458,542.3282920598256,542.3282800710786,542.3282678592287,542.328255420125,542.3282427495382,542.3282298431607,542.3282166966051,542.3282033054016,542.3281896649985,542.3281757707579,542.3281616179563,542.3281472017832,542.3281325173366,542.3281175596253,542.328102323564,542.3280868039733,542.328070995577,542.3280548930013,542.328038490772,542.3280217833131,542.328004764945,542.3279874298826,542.3279697722326,542.3279517859922,542.3279334650476,542.3279148031706,542.327895794017,542.3278764311251,542.3278567079125,542.3278366176751,542.3278161535826,542.3277953086795,542.327774075879,542.3277524479646,542.3277304175833,542.3277079772465,542.3276851193266,542.3276618360532,542.327638119512,542.3276139616412,542.3275893542291,542.3275642889108,542.3275387571667,542.327512750318,542.3274862595244,542.3274592757818,542.327431789918,542.32740379259,542.3273752742818,542.3273462252996,542.3273166357701,542.3272864956352,542.3272557946507,542.3272245223815,542.3271926681978,542.3271602212736,542.3271271705798,542.3270935048834,542.3270592127413,542.3270242824989,542.326988702284,542.3269524600032,542.3269155433392,542.3268779397448,542.3268396364398,542.3268006204061,542.3267608783833,542.3267203968651,542.3266791620929,542.3266371600537,542.326594376472,542.3265507968085,542.3265064062526,542.326461189718,542.3264151318388,542.3263682169627,542.326320429146,542.3262717521493,542.3262221694308,542.3261716641406,542.3261202191165,542.3260678168765,542.3260144396131,542.3259600691886,542.3259046871276,542.3258482746106,542.3257908124693,542.325732281178,542.3256726608482,542.3256119312219,542.3255500716642,542.3254870611568,542.3254228782896,542.3253575012559,542.3252909078425,542.3252230754237,542.3251539809528,542.325083600955,542.3250119115193,542.324938888289,542.3248645064562,542.3247887407507,542.324711565433,542.3246329542845,542.3245528806005,542.3244713171788,542.324388236312,542.3243036097774,542.3242174088281,542.324129604183,542.324040166016,542.3239490639475,542.3238562670329,542.3237617437524,542.3236654620008,542.3235673890761,542.3234674916682,542.3233657358489,542.3232620870585,542.3231565100959,542.3230489691055,542.3229394275656,542.3228278482758,542.3227141933451,542.3225984241776,542.322480501461,542.3223603851524,542.3222380344654,542.3221134078553,542.3219864630063,542.3218571568158,542.3217254453814,542.3215912839847,542.3214546270772,542.321315428264,542.3211736402886,542.321029215017,542.3208821034217,542.3207322555639,542.3205796205782,542.3204241466547,542.3202657810207,542.3201044699247,542.3199401586169,542.3197727913307,542.3196023112648,542.3194286605631,542.3192517802955,542.3190716104383,542.3188880898537,542.318701156269,542.3185107462562,542.3183167952091,542.318119237324,542.3179180055755,542.3177130316943,542.3175042461453,542.3172915781018,542.3170749554253,542.3168543046368,542.3166295508952,542.3164006179722,542.316167428224,542.3159299025683,542.3156879604553,542.3154415198412,542.3151904971617,542.314934807303,542.314674363573,542.314409077672,542.3141388596647,542.3138636179474,542.3135832592197,542.3132976884514,542.3130068088508,542.3127105218334,542.312408726988,542.3121013220427,542.3117882028307,542.3114692632563,542.3111443952579,542.310813488773,542.3104764317003,542.3101331098634,542.309783406971,542.3094272045792,542.3090643820512,542.3086948165173,542.3083183828337,542.3079349535403,542.3075443988182,542.3071465864472,542.3067413817603,542.3063286475988,542.3059082442685,542.3054800294901,542.3050438583537,542.3045995832704,542.3041470539217,542.303686117211,542.3032166172118,542.3027383951156,542.3022512891794,542.301755134672,542.3012497638185,542.3007350057454,542.3002106864232,542.2996766286083,542.2991326517849,542.2985785721052,542.2980142023267,542.2974393517522,542.2968538261662,542.2962574277681,542.2956499551107,542.2950312030297,542.2944009625788,542.2937590209585,542.293105161447,542.292439163328,542.2917608018178,542.2910698479916,542.2903660687075,542.2896492265302,542.2889190796515,542.2881753818128,542.2874178822217,542.2866463254715,542.2858604514563,542.2850599952855,542.2842446871967,542.2834142524688,542.2825684113293,542.2817068788651,542.2808293649276,542.2799355740389,542.2790252052945,542.2780979522656,542.2771535028976,542.2761915394111,542.2752117381958,542.2742137697063,542.2731972983545,542.2721619824008,542.2711074738431,542.2700334183038,542.2689394549146,542.2678252162002,542.2666903279587,542.2655344091412,542.2643570717289,542.2631579206063,542.2619365534367,542.2606925605293,542.2594255247104,542.2581350211871,542.2568206174117,542.255481872944,542.2541183393085,542.252729559852,542.2513150695975,542.2498743950954,542.2484070542731,542.2469125562808,542.2453904013358,542.2438400805644,542.2422610758389,542.2406528596154,542.2390148947649,542.2373466344047,542.2356475217264,542.2339169898196,542.232154461493,542.230359349095,542.2285310543275,542.2266689680599,542.2247724701375,542.2228409291888,542.2208737024289,542.2188701354581,542.2168295620601,542.2147513039952,542.21263467079,542.2104789595242,542.2082834546139,542.2060474275913,542.2037701368813,542.2014508275743,542.1990887311931,542.1966830654618,542.1942330340645,542.1917378264048,542.1891966173595,542.1866085670276,542.1839728204794,542.1812885074957,542.1785547423083,542.1757706233334,542.1729352329028,542.170047636989,542.1671068849274,542.1641120091348,542.1610620248225,542.1579559297045,542.1547927037041,542.1515713086533,542.1482906879888,542.1449497664439,542.1415474497361,542.1380826242478,542.134554156706,542.1309608938554,542.1273016621253,542.1235752672953,542.1197804941537,542.115916106152,542.1119808450543,542.107973430582,542.1038925600533,542.0997369080185,542.0955051258901,542.0911958415674,542.0868076590561,542.082339158084,542.0777888937108,542.0731553959325,542.0684371692812,542.0636326924202,542.0587404177338,542.0537587709105,542.0486861505232,542.0435209276036,542.0382614452108,542.032906017995,542.0274529317567,542.021900443,542.0162467784814,542.0104901347529,542.0046286776994,541.998660542075,541.9925838310264,541.9863966156205,541.9800969343596,541.973682792696,541.9671521625396,541.9605029817616,541.9537331536925,541.9468405466168,541.9398229932617,541.9326782902807,541.925404197735,541.9179984385677,541.9104586980754,541.9027826233747,541.8949678228646,541.8870118656855,541.8789122811727,541.8706665583077,541.8622721451646,541.8537264483538,541.8450268324609,541.8361706194842,541.8271550882677,541.8179774739303,541.808634967296,541.7991247143162,541.789443815494,541.7795893253037,541.7695582516103,541.759347555085,541.7489541486218,541.7383748967508,541.7276066150511,541.7166460695639,541.7054899762047,541.6941350001746,541.6825777553731,541.6708148038107,541.6588426550231,541.6466577654853,541.6342565380301,541.6216353212649,541.608790408995,541.5957180396457,541.5824143956913,541.5688756030864,541.5550977307005,541.5410767897589,541.5268087332879,541.5122894555674,541.4975147915856,541.4824805165069,541.4671823451413,541.4516159314261,541.4357768679124,541.4196606852648,541.4032628517688,541.3865787728487,541.3696037905971,541.3523331833172,541.3347621650777,541.3168858852786,541.2986994282354,541.2801978127749,541.2613759918494,541.2422288521645,541.222751213828,541.2029378300141,541.1827833866494,541.1622825021162,541.1414297269799,541.1202195437364,541.0986463665829,541.0767045412134,541.0543883446384,541.0316919850303,541.0086096015973,540.9851352644836,540.9612629746994,540.9369866640822,540.9123001952864,540.8871973618095,540.8616718880471,540.8357174293875,540.8093275723365,540.7824958346843,540.7552156657069,540.7274804464087,540.6992834898031,540.6706180412383,540.6414772787614,540.6118543135294,540.5817421902643,540.551133887754,540.5200223193999,540.4884003338145,540.456260715467,540.4235961853797,540.3903994018772,540.3566629613863,540.3223793992918,540.2875411908452,540.2521407521313,540.2161704410883,540.1796225585889,540.1424893495783,540.1047630042707,540.066435659408,540.0274993995772,539.9879462585903,539.9477682209279,539.9069572232411,539.8655051559219,539.8234038647329,539.7806451525046,539.7372207808922,539.6931224722022,539.6483419112794,539.6028707474599,539.5567005965885,539.5098230430997,539.462229642163,539.4139119218914,539.3648613856124,539.3150695142015,539.2645277684762,539.2132275916534,539.1611604118605,539.1083176447107,539.0546906959305,539.0002709640459,538.9450498431188,538.8890187255374,538.8321690048576,538.7744920786874,538.7159793516219,538.6566222382174,538.5964121660086,538.5353405785609,538.4733989385577,538.4105787309203,538.3468714659546,538.282268682521,538.2167619512286,538.1503428776422,538.0830031055045,538.0147343199669,537.9455282508235,537.8753766757454,537.8042714235106,537.732204377223,537.6591674775175,537.5851527257449,537.5101521871318,537.4341579939098,537.3571623484096,537.2791575261115,537.2001358786515,537.1200898367726,537.0390119132176,536.9568947055579,536.873730898952,536.7895132688262,536.704234683476,536.6178881065754,536.5304665995969,536.4419633241287,536.3523715440874,536.2616846278214,536.1698960500958,536.0769993939581,535.9829883524757,535.8878567303439,535.7915984453564,535.6942075297362,535.595678131323,535.4960045146106,535.3951810616363,535.293202272713,535.190062767005,535.085757282947,534.9802806785001,534.8736279312473,534.7657941383261,534.6567745162005,534.5465644002684,534.4351592443116,534.3225546197849,534.2087462149503,534.0937298338574,533.9775013951763,533.8600569308843,533.7413925848155,533.6215046110772,533.5003893723411,533.3780433380173,533.2544630823181,533.1296452822255,533.0035867153672,532.8762842578163,532.7477348818275,532.6179356535174,532.4868837305117,532.3545763595638,532.2210108741706,532.0861846921931,531.9500953135055,531.8127403176879,531.6741173617834,531.5342241781341,531.393058572326,531.2506184212538,531.1069016713337,530.9619063368846,530.8156304987002,530.6680723028364,530.5192299596368,530.3691017430226,530.2176859900695,530.0649811008973,529.9109855388983,529.7556978313285,529.5991165702885,529.441240414119,529.2820680892354,529.1215983924284,528.9598301936553,528.7967624393461,528.6323941562493,528.4667244558425,528.2997525393273,528.1314777032386,527.9618993456814,527.791016973224,527.6188302084628,527.4453387982816,527.2705426228208,527.0944417051768,526.9170362218432,526.7383265139126,526.5583130990501,526.3769966842503,526.1943781793861,526.0104587115632,525.8252396402787,525.6387225733976,525.4509093839414,525.2618022276981,525.0714035616459,524.8797161631908,524.6867431502135,524.4924880019166,524.2969545804622,524.1001471533903,523.9020704168014,523.7027295192888,523.5021300865998,523.300278247006,523.0971806573584,522.892844529801,522.6872776591144,522.4804884506578,522.2724859488767,522.0632798663394,521.8528806132642,521.6412993274964,521.4285479048898,521.2146390300502,520.9995862073897,520.7834037924422,520.5661070233872,520.3477120527266,520.1282359790565,519.9076968788764,519.6861138383699,519.4635069850974,519.2398975195342,519.0153077463833,518.789761105598,518.5632822030398,518.3358968407014,518.1076320464185,517.878516102998,517.6485785766819,517.4178503448735,517.1863636230448,516.954151990746,516.7212504166372,516.4876952824643,516.2535244058932,516.0187770621276,515.7834940042248,515.5477174820335,515.3114912596697,515.0748606314559,514.8378724362424,514.6005750700346,514.3630184968538,514.1252542577535,513.8873354779238,513.6493168718106,513.4112547461872,513.17320700111,512.9352331286998,512.6973942096911,512.4597529076932,512.2223734611148,511.98532167270594,511.7486648966743,511.51247202334093,511.2768134613003,511.04176111706255,510.8073883721514,510.57377005764715,510.34098242616216,510.1091031212451,509.8782111442206,509.64838681846953,509.4197117511696,509.1922687925175,508.96614199246505,508.74141655500387,508.5181787900468,508.29651606295556,508.07651674177475,507.8582701422401,507.6418664706315,507.4273967645569,507.2149528317508,507.0046271869847,506.79651298719165,506.59070396491194,506.38729436017644,506.18637885094756,505.98805248224426,505.79241059408326,505.5995487483736,505.40956265490547,505.22254809657915,505.0386008540213,504.8578166297435,504.68029097199513,504.50611919846693,504.335396320003,504.1682169644806,504.0046753010135,503.84486496463734,503.6888789816331,503.53680969564215,503.38874869472465,503.24478673950694,503.1050136925654,502.9695184491819,502.8383888696054,502.7117117129499,502.5895725728448,502.47205581495575,502.3592445164798,502.25122040771305,502.1480638157803,502.04985361060676,501.9566671532024,501.86858024632045,501.78566708754033,501.708000224817,501.6356505145268,501.56868708203086,501.50717728476576,501.4511866778608,501.40077898227264,501.35601605541854,501.31695786427343,501.2836624608983,501.2561859603469,501.23458252089694,501.21890432653845,501.20920157165153,501.20552244778924,501.20791313248435,501.2164177799872,501.23107851383844,501.2519354211783,501.2790265486863,501.31238790004727,501.35205343483307,501.3980550686939,501.450422674744,501.50918408603815,501.5743650990241,501.64598947786953,501.72407895955496,501.8086532596328,501.89973007855554,501.99732510847764,502.1014520404446,502.21212257188483,502.3293464143263,502.45313130126806,502.5834829961399,502.72040530029346,502.86390006097304,503.0139671792207,503.17060461768017,503.33380840827004,503.5035726597007,503.6798895648243,503.8627494078035,504.0521405711026,504.2480495423033,504.45046092075506,504.65935742408044,504.87471989455565,505.0965273053958,505.32475676697675,505.5593835330315,505.80038100686335,506.0477207476183,506.3013724766665,506.5613040841434,506.82748163570085,507.099869379525,507.37842975367266,507.66312339378715,507.9539091412416,508.250744051775,508.55358340466694,508.8623807125105,509.17708773163463,509.4976544732243,509.82402921518985,510.15615851482943,510.4939872223293,510.8374584951416,511.18651381327686,511.5410929955462,511.90113421678166,512.2665740260657,512.6373473659883,513.0133875929563,513.3946264985672,513.7809943320623,514.1724198238668,514.5688302102233,514.970151258917,515.376307296095,515.787221234172,516.2028146008128,516.623007568983,517.0477189880488,517.4768664159146,517.9103661521704,518.3481332722326,518.7900816624499,519.2361240561471,519.6861720705779,520.1401362447546,520.5979260781234,521.059450070047,521.5246157600633,521.9933297688775,522.4654978400542,522.9410248823665,523.419815012763,523.9017715999131,524.3867973082858,524.874794142724,525.3656634934684,525.8593061815908,526.3556225047926,526.8545122835252,527.3558749073919,527.8596093817848,528.3656143747166,528.8737882638047,529.3840291833625,529.8962350715583,530.4103037176017,530.926132808911,531.4436199782247,531.9626628506157,532.4831590903682,533.0050064476759,533.5281028051262,534.0523462239275,534.577634989847,535.1038676588139,535.6309431021632,536.158760551469,536.6872196429474,537.2162204613821,537.7456635835454,538.2754501210807,538.805481762811,539.3356608164462,539.8658902496537,540.3960737304662,540.9261156669952,541.4559212464235,541.9853964732465,542.5144482067396,543.0429841976234,543.570913123902,544.0981446258536,544.6245893401466,545.1501589330626,545.674766132805,546.1983247608728,546.7207497624813,547.2419572360142,547.7618644614868,548.2803899280087,548.7974533602309,549.3129757437629,549.8268793495495,550.3390877571957,550.8495258772308,551.3581199723023,551.8647976772941,552.3694880183587,552.8721214308648,553.3726297762512,553.8709463577856,554.3670059352273,554.8607447383941,555.3521004796316,555.8410123651897,556.3274211055052,556.8112689243992,557.2924995671905,557.771058307732,558.2468919543783,558.7199488548908,559.1901789002915,559.657533527672,560.1219657219732,560.5834300167437,561.0418824938915,561.4972807824425,561.9495840563194,562.3987530311576,562.8447499601733,563.2875386291,563.7270843502137,564.1633539554606,564.5963157887109,565.0259396971554,565.452197021866,565.8750605875405,566.2945046914534,566.7105050916351,567.1230389943005,567.5320850405504,567.9376232923698,568.3396352179444,568.738103676322,569.1330129014414,569.5243484855523,569.9120973620543,570.2962477877763,570.6767893247232,571.0537128213159,571.4270103931476,571.7966754032849,572.1627024421364,572.5250873069157,572.8838269807242,573.2389196112791,573.5903644893114,573.9381620266586,574.2823137340795,574.6228221988134,574.9596910619082,575.292924995345,575.6225296789781,575.9485117773204,576.2708789161908,576.5896396592527,576.9048034844627,577.2163807604525,577.5243827228669]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[265.8815249131596,265.88152322394365,265.8815215032913,265.881519750618,265.8815179653277,265.8815161468135,265.88151429445725,265.8815124076282,265.88151048568585,265.8815085279763,265.88150653383445,265.8815045025816,265.8815024335279,265.8815003259691,265.88149817918907,265.881495992458,265.88149376503236,265.88149149615464,265.88148918505334,265.88148683094335,265.8814844330238,265.88148199047953,265.88147950248026,265.8814769681795,265.8814743867164,265.8814717572127,265.88146907877496,265.881466350492,265.8814635714366,265.88146074066384,265.8814578572109,265.88145492009824,265.88145192832656,265.88144888087913,265.88144577671994,265.8814426147933,265.881439394024,265.88143611331793,265.88143277155893,265.88142936761096,265.88142590031714,265.8814223684981,265.88141877095353,265.8814151064604,265.8814113737725,265.8814075716209,265.8814036987135,265.8813997537329,265.8813957353379,265.88139164216267,265.8813874728159,265.8813832258794,265.8813788999098,265.8813744934366,265.8813700049611,265.881365432958,265.88136077587234,265.8813560321214,265.881351200092,265.8813462781419,265.8813412645974,265.8813361577541,265.88133095587614,265.88132565719496,265.881320259909,265.8813147621837,265.8813091621496,265.88130345790313,265.88129764750533,265.8812917289803,265.8812857003171,265.88127955946516,265.8812733043373,265.881266932807,265.88126044270814,265.88125383183456,265.88124709793897,265.88124023873195,265.88123325188195,265.8812261350135,265.8812188857073,265.8812115014996,265.8812039798794,265.88119631829,265.88118851412696,265.88118056473746,265.88117246741916,265.88116421941925,265.8811558179342,265.8811472601077,265.88113854303106,265.8811296637409,265.8811206192188,265.8811114063906,265.8811020221241,265.88109246322955,265.8810827264579,265.8810728084993,265.88106270598166,265.88105241547214,265.8810419334722,265.881031256419,265.8810203806829,265.8810093025676,265.880998018307,265.8809865240662,265.8809748159373,265.88096288994154,265.88095074202477,265.88093836805785,265.8809257638352,265.88091292507244,265.8808998474057,265.8808865263901,265.880872957498,265.88085913611735,265.8808450575506,265.8808307170127,265.88081610962945,265.88080123043596,265.8807860743753,265.88077063629646,265.88075491095265,265.8807388929986,265.88072257699116,265.8807059573842,265.88068902853036,265.8806717846759,265.88065421996,265.88063632841363,265.88061810395624,265.8805995403945,265.8805806314197,265.8805613706055,265.8805417514065,265.88052176715564,265.8805014110616,265.88048067620673,265.8804595555447,265.8804380418985,265.8804161279578,265.8803938062752,265.88037106926595,265.88034790920415,265.8803243182198,265.8803002882962,265.880275811269,265.8802508788208,265.88022548247966,265.8801996136167,265.88017326344203,265.8801464230031,265.8801190831796,265.8800912346833,265.8800628680516,265.8800339736472,265.8800045416533,265.87997456207074,265.87994402471355,265.87991291920787,265.8798812349861,265.87984896128376,265.87981608713744,265.8797826013789,265.8797484926333,265.8797137493136,265.8796783596174,265.8796423115232,265.8796055927862,265.8795681909337,265.8795300932612,265.8794912868285,265.87945175845414,265.87941149471266,265.8793704819283,265.8793287061716,265.879286153254,265.87924280872346,265.8791986578592,265.8791536856676,265.8791078768753,265.8790612159261,265.8790136869743,265.8789652738802,265.8789159602039,265.8788657292,265.8788145638126,265.87876244666813,265.87870936007096,265.8786552859966,265.8786002060856,265.8785441016379,265.87848695360526,265.8784287425865,265.8783694488204,265.87830905217754,265.87824753215546,265.87818486787097,265.87812103805373,265.8780560210364,265.8779897947518,265.8779223367213,265.87785362404986,265.8777836334171,265.8777123410698,265.8776397228132,265.87756575400454,265.87749040954304,265.87741366386183,265.8773354909197,265.87725586419253,265.87717475666375,265.87709214081497,265.87700798861806,265.87692227152377,265.87683496045446,265.876746025792,265.8766554373691,265.87656316445884,265.87646917576456,265.87637343940855,265.87627592292296,265.87617659323666,265.87607541666557,265.87597235890115,265.8758673849989,265.875760459366,265.87565154574975,265.8755406072256,265.875427606185,265.8753125043212,265.8751952626181,265.8750758413369,265.8749542000017,265.8748302973869,265.8747040915033,265.8745755395831,265.87444459806693,265.8743112225882,265.87417536795846,265.8740369881529,265.87389603629356,265.8737524646349,265.8736062245478,265.8734572665018,265.8733055400503,265.873150993813,265.87299357545817,265.87283323168606,265.8726699082104,265.8725035497407,265.8723340999633,265.87216150152307,265.8719856960041,265.87180662391,265.8716242246441,265.8714384364897,265.8712491965881,265.87105644092077,265.8708601042837,265.870660120269,265.8704564212414,265.8702489383163,265.8700376013368,265.86982233884953,265.86960307808187,265.8693797449172,265.8691522638714,265.86892055806607,265.86868454920386,265.86844415754354,265.8681993018712,265.8679498994758,265.8676958661199,265.8674371160131,265.8671735617833,265.86690511444675,265.86663168338043,265.866353176291,265.86606949918416,265.86578055633447,265.8654862502533,265.86518648165656,265.86488114943256,265.8645701506075,265.86425338031427,265.8639307317543,265.863602096165,265.8632673627835,265.86292641881073,265.86257914937266,265.86222543748517,265.86186516401386,265.8614982076358,265.8611244447999,265.86074374968655,265.8603559941655,265.8599610477564,265.8595587775841,265.8591490483359,265.85873172221795,265.8583066589108,265.8578737155228,265.85743274654396,265.8569836037992,265.8565261364002,265.85606019069644,265.85558561022583,265.85510223566337,265.8546099047704,265.85410845234276,265.85359771015567,265.8530775069122,265.85254766818616,265.85200801636665,265.8514583706018,265.85089854674004,265.85032835727117,265.8497476112669,265.84915611431984,265.8485536684808,265.8479400721966,265.8473151202456,265.84667860367244,265.84603030972164,265.8453700217707,265.8446975192611,265.8440125776286,265.84331496823285,265.8426044582842,265.8418808107729,265.84114378439085,265.84039313346005,265.83962860785243,265.83884995291396,265.8380569093828,265.8372492133111,265.8364265959809,265.83558878382144,265.8347354983245,265.83386645595783,265.83298136807764,265.8320799408407,265.8311618751111,265.8302268663721,265.8292746046293,265.8283047743189,265.8273170542086,265.8263111173019,265.8252866307377,265.82424325568957,265.8231806472628,265.82209845439104,265.8209963197304,265.8198738795512,265.8187307636308,265.8175665951402,265.8163809905347,265.81517355943765,265.81394390452544,265.8126916214103,265.81141629852004,265.81011751697804,265.80879485048024,265.80744786516937,265.80607611950995,265.80467916415927,265.8032565418368,265.80180778719233,265.80033242667434,265.7988299783897,265.79729995197,265.795741848429,265.79415516002325,265.7925393701079,265.7908939529897,265.7892183737812,265.78751208825065,265.7857745426711,265.7840051736657,265.7822034080525,265.78036866268883,265.7785003443089,265.77659784936367,265.7746605638574,265.77268786318206,265.77067911194865,265.7686336638176,265.7665508613285,265.7644300357242,265.76227050677653,265.7600715826076,265.75783255950915,265.75555272176257,265.75323134145304,265.7508676782837,265.7484609793893,265.746010479144,265.7435153989707,265.7409749471474,265.7383883186102,265.73575469475657,265.7330732432455,265.73034311779566,265.72756345798257,265.72473338903364,265.72185202162115,265.7189184516528,265.71593176006354,265.71289101260186,265.70979525961707,265.70664353584465,265.70343486018936,265.70016823550736,265.6968426483872,265.6934570689298,265.69001045052534,265.6865017296318,265.68292982555056,265.67929364020057,265.6755920578947,265.6718239451103,265.66798815026425,265.6640835034834,265.66010881637703,265.65606288180777,265.6519444736625,265.64775234662307,265.6434852359366,265.6391418571882,265.6347209060692,265.63022105815105,265.6256409686574,265.62097927223545,265.6162345827322,265.6114054929664,265.60649057450803,265.60148837745453,265.5963974302093,265.5912162392659,265.585943288989,265.5805770414015,265.5751159359734,265.56955838941195,265.56390279545894,265.5581475246853,265.55229092429516,265.54633131793133,265.54026700548474,265.53409626290875,265.5278173420399,265.52142847042217,265.5149278511383,265.5083136626443,265.5015840586146,265.49473716779096,265.4877710938396,265.4806839152153,265.47347368503415,265.4661384309552,265.4586761550706,265.45108483380375,265.4433624178203,265.435506831947,265.4275159751044,265.41938772024616,265.41111991431626,265.40271037821503,265.3941569067792,265.38545726877715,265.3766092069162,265.36761043786737,265.3584586523049,265.34915151496193,265.33968666470275,265.3300617146159,265.32027425212175,265.310321839103,265.3002020120517,265.2899122822413,265.2794501359166,265.26881303450864,265.2579984148722,265.247003689546,265.2358262470419,265.22446345215474,265.2129126463043,265.20117114790037,265.18923625274067,265.177105234435,265.16477534486273,265.15224381466044,265.13950785374163,265.12656465185285,265.1134113791603,265.1000451868766,265.08646320792155,265.0726625576202,265.05864033444266,265.0443936207814,265.0299194837704,265.015214976147,265.0002771371548,264.9851029934947,264.96968956031805,264.95403384226734,264.9381328345646,264.92198352414925,264.905582890864,264.8889279086935,264.87201554705274,264.8548427721318,264.83740654828915,264.81970383950704,264.8017316108962,264.78348683026184,264.7649664697253,264.7461675074058,264.7270869291614,264.70772173038904,264.6880689178898,264.6681255117909,264.6478885475343,264.6273550779276,264.6065221752581,264.58538693347157,264.5639464704176,264.5421979301567,264.52013848533664,264.49776533963166,264.4750757302493,264.4520669305016,264.4287362524445,264.40508104957945,264.38109871962206,264.35678670733716,264.33214250743424,264.30716366753074,264.28184779117464,264.2561925409308,264.23019564152764,264.2038548830626,264.1771681242659,264.15013329582166,264.12274840374084,264.0950115327903,264.0669208499685,264.0384746080326,264.0096711490686,263.9805089081049,263.9509864167651,263.9211023069584,263.8908553146005,263.8602442833662,263.8292681684652,263.79792604044053,263.7662170889806,263.73414062674806,263.7016960932098,263.6688830584724,263.63570122711224,263.60215044199646,263.56823068808615,263.5339420962207,263.49928494687015,263.46425967385414,263.42886686801614,263.39310728084934,263.3569818280627,263.32049159308235,263.2836378304769,263.2464219693027,263.20884561635455,263.17091055931775,263.1326187698092,263.0939724063006,263.05497381690947,263.0156255420551,262.9759303169622,262.93589107400743,262.8955109448939,262.8547932626467,262.81374156341724,262.7723595880855,262.73065128364783,262.68862080438373,262.64627251278534,262.60361098024293,262.56064098747197,262.5173675246742,262.47379579141864,262.4299311962343,262.38577935590286,262.3413460944414,262.2966374417641,262.2516596320145,262.2064191015571,262.1609224866217,262.1151766205863,262.06918853089775,262.022965435615,261.9765147395712,261.92984403014736,261.8829610726508,261.83587380529207,261.7885903337557,261.74111892536234,261.69346800281517,261.64564613752975,261.5976620425474,261.5495245650269,261.5012426783178,261.45282547361364,261.4042821511912,261.35562201123264,261.3068544442392,261.2579889210415,261.2090349824082,261.16000222826597,261.110900306536,261.0617389015972,261.0125277223883,260.9632764901587,260.9139949258846,260.8646927373618,260.81537960599337,260.76606517328815,260.7167590270875,260.66747068754313,260.6182095928618,260.56898508484255,260.5198063942279,260.47068262589323,260.4216227438991,260.3726355564328,260.3237297006675,260.27491362756325,260.22619558664485,260.1775836107796,260.1290855009897,260.08070881132943,260.0324608338574,259.9843485837383,259.9363787845052,259.88855785351586,259.8408918876362,259.79338664918555,259.7460475521758,259.69887964887886,259.65188761675614,259.605075745783,259.5584479262027,259.51200763674,259.46575793330953,259.41970143825085,259.3738403301179,259.32817633405887,259.28271071281176,259.237444258348,259.19237728418835,259.14750961842356,259.1028405974605,259.0583690605224,259.0140933449245,258.9700112821498,258.9261201947446,258.8824168940531,258.838897678813,258.79555833462484,258.75239413431393,258.70939983919817,258.66656970127434,258.62389746633494,258.5813763780253,258.53899918284947,258.4967581361316,258.4546450089399,258.41265109597657,258.37076722443595,258.32898376383434,258.28729063681175,258.24567733090214,258.2041329112746,258.1626460344386,258.1212049629114,258.0797975808408,258.0384114105792,257.9970336302002,257.9556510919497,257.9142503416254,257.87281763887165,257.8313389783825,257.789800112001,257.74818657170374,257.7064836934587,257.6646766419442,257.62275043611623,257.58068997561105,257.538480067969,257.49610545666667,257.4535508499408,257.4108009503924,257.3678404853522,257.324654237995,257.28122707918453,257.23754400003435,257.193590145165,257.14935084664205,257.1048116585757,257.0599583923628,257.0147771525503,256.9692543733014,256.92337685544015,256.8771318040507,256.8305068666109,256.78349017162776,256.73607036775195,256.68823666334004,256.639978866432,256.59128742511234,256.5421534682172,256.49256884634895,256.4425261731591,256.3920188668517,256.34104119186463,256.2895883006738,256.23765627566945,256.18524217104664,256.13234405464686,256.07896104968677,256.0250933763052,255.9707423928533,255.91591063685004,255.8606018655215,255.8048210958354,255.7485746439397,255.69187016390822,255.63471668569292,255.57712465217497,255.5191059552057,255.46067397052127,255.40184359141074,255.34263126101476,255.28305500312433,255.22313445135032,255.16289087652484,255.10234721219695,255.04152807808032,254.9804598013076,254.91917043534403,254.8576897764138,254.79604937728715,254.7342825582793,254.6724244153116,254.61051182488586,254.548583445823,254.4866797176232,254.42484285530406,254.36311684058052,254.30154740925093,254.2401820346655,254.17906990715557,254.1182619093094,254.05781058699307,253.99777011602202,253.9381962643974,253.87914635003773,253.82067919394586,253.7628550687644,253.7057356426892,253.649383918724,253.5938641692779,253.53924186612016,253.48558360572918,253.4329570300875,253.38143074299563,253.33107422199535,253.2819577260147,253.2341521988648,253.1877291687428,253.14276064391063,253.09931900474461,253.05747689236873,253.0173070941047,252.97888242599248,252.94227561265436,252.90755916479185,252.87480525462735,252.84408558961326,252.81547128475268,252.78903273388653,252.76483948031813,252.742960087156,252.72346200776784,252.7064114567458,252.69187328179203,252.67991083693755,252.6705858575106,252.6639583372727,252.66008640814047,252.65902622290622,252.66083184136886,252.66555512027736,252.67324560747969,252.6839504406616,252.69771425104454,252.7145790723956,252.73458425568987,252.757766389741,252.78415922809938,252.81379362249314,252.84669746306236,252.8828956256157,252.9224099261047,252.96525908249242,253.0114586841556,253.06102116893715,253.11395580793192,253.17026869805701,253.2299627624327,253.29303775856238,253.35949029427414,253.4293138513539,253.50249881677001,253.57903252136114,253.6588992858294,253.74208047385304,253.82855455210773,253.91829715695928,254.011281167566,254.10747678510828,254.2068516178412,254.309370771646,254.41499694574043,254.52369053319256,254.63540972586605,254.7501106234179,254.8677473459556,254.988272149956,255.11163554703893,255.23778642519017,255.36667217201853,255.4982387996404,255.63243107077898,255.76919262567466,255.908466109404,256.05019329921487,256.1943152314906,256.34077232796733,256.48950452083943,256.64045137639977,256.79355221687496,256.94874624012994,257.10597263693273,257.2651707054834,257.42627996293004,257.5892402536134,257.75399185379433,257.9204755726414,258.08863284927304,258.25840584566333,258.42973753524734,258.6025717870688,258.77685344534467,258.95252840432715,259.12954367837074,259.3078474671237,259.48738921578547,259.6681196703836,259.84999092804384,260.0329564822423,260.2169712630414,260.4019916723305,260.5879756141013,260.77488251980566,260.9626733688546,261.15131070432847,261.34075864398125,261.5309828866334,261.7219507140554,261.91363098845426,262.10599414568685,262.29901218432803,262.4926586507312,262.68690862022476,262.88173867459614,263.0771268760165,263.2730527375696,263.469497190547,263.66644254868027,263.8638724694813,264.06177191286577,264.26012709723545,264.45892545320027,264.6581555751171,264.857807170628,265.05787100837796,265.2583388640926,265.4592034651971,265.6604584341553,265.86209823070647,266.0641180931781,266.2665139790478,266.4692825049273,266.67242088613904,266.87592687605047,267.0797987053321,267.2840350212977,267.48863482748476,267.69359742362593,267.8989223461624,268.10460930944,268.3106581477309,268.5170687582116,268.7238410450292,268.9309748645788,269.13846997211033,269.346325969779,269.55454225624453,269.76311797792243,269.97205198198066,270.1813427711728,270.3909884605872,270.600986736392,270.8113348166439,271.0220294142235,271.23306670195586,271.44444227996485,271.65615114530544,271.868187663911,272.08054554488547,272.2932178171642,272.50619680856164,272.719474127214,272.93304064542417,273.14688648590425,273.36100101041,273.5753728107495,273.78998970214883,274.00483871894676,274.21990611258775,274.4351773518747,274.6506371254407,274.86626934639116,275.0820571590637,275.2979829478511,275.51402834802263,275.73017425848207,275.9464008563915,276.16268761358697,276.37901331471403,276.59535607700104,276.8116933715885,277.0280020463347,277.2442583500057,277.4604379577677,277.6765159978877,277.8924670795569,278.1082653217395,278.32388438296243,278.53929749194606,278.7544774789901,278.96939680801813,279.18402760919264,279.3983417120064,279.61231067876486,279.8259058383659,280.03909832029467,280.25185908874573,280.4641589767896,280.67596872050285,280.8872589929829,281.09800043817086,281.308163704408,281.517719477657,281.7266385143167,281.9348916735678,282.14244994918636,282.34928450076677,282.55536668429716,282.7606680820377,282.965160531651,283.1688161545385,283.3716073833432,283.57350698857863,283.77448810434805,283.97452425312406,284.17358936955964,284.37165782330356,284.5687044408012,284.76470452606156,284.9596338803733,285.1534688209593,285.34618619856053,285.53776341394183,285.72817843331677,285.91740980269077,286.10543666112426,286.29223875292,286.477796438742]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[257.92146938305154,257.92146576433424,257.9214620782727,257.92145832361393,257.92145449908173,257.9214506033756,257.92144663517087,257.9214425931173,257.9214384758426,257.92143428194527,257.92143001000096,257.9214256585557,257.9214212261315,257.92141671121976,257.9214121122865,257.92140742776814,257.9214026560717,257.92139779557465,257.9213928446244,257.9213878015387,257.92138266460165,257.9213774320676,257.9213721021573,257.92136667305823,257.92136114292487,257.921355509877,257.92134977200027,257.92134392734226,257.9213379739172,257.92133190970054,257.9213257326298,257.9213194406064,257.92131303148955,257.9213065031016,257.92129985322237,257.921293079591,257.9212861799041,257.9212791518175,257.9212719929398,257.9212647008378,257.9212572730333,257.92124970699933,257.92124200016485,257.92123414990965,257.92122615356396,257.92121800840965,257.9212097116781,257.9212012605479,257.92119265214546,257.92118388354487,257.9211749517655,257.9211658537693,257.9211565864643,257.9211471467001,257.92113753126597,257.92112773689473,257.9211177602551,257.921107597956,257.92109724654256,257.9210867024954,257.92107596223025,257.92106502209504,257.9210538783715,257.9210425272707,257.92103096493304,257.921019187429,257.9210071907531,257.92099497082773,257.92098252349894,257.92096984453383,257.9209569296246,257.92094377437815,257.92093037432346,257.9209167249045,257.9209028214814,257.9208886593275,257.9208742336289,257.92085953948066,257.9208445718882,257.9208293257627,257.9208137959213,257.92079797708556,257.92078186387664,257.9207654508175,257.9207487323285,257.9207317027266,257.9207143562224,257.920696686919,257.9206786888104,257.9206603557775,257.92064168158896,257.9206226598966,257.92060328423406,257.92058354801543,257.9205634445309,257.92054296694715,257.9205221083037,257.92050086150977,257.9204792193418,257.92045717444535,257.9204347193254,257.9204118463495,257.92038854774205,257.9203648155844,257.92034064180876,257.9203160181989,257.92029093638376,257.92026538783927,257.9202393638803,257.9202128556613,257.92018585417225,257.92015835023506,257.92013033450087,257.9201017974476,257.92007272937565,257.9200431204044,257.9200129604704,257.91998223932285,257.9199509465196,257.9199190714241,257.91988660320357,257.9198535308223,257.91981984304033,257.9197855284063,257.91975057525946,257.91971497171835,257.9196787056844,257.9196417648311,257.9196041366031,257.9195658082133,257.9195267666344,257.9194869985985,257.9194464905909,257.9194052288435,257.91936319933444,257.919320387781,257.91927677963315,257.9192323600716,257.9191871140009,257.91914102604534,257.9190940805437,257.91904626154127,257.9189975527893,257.9189479377359,257.91889739952114,257.9188459209702,257.9187934845927,257.9187400725695,257.9186856667513,257.91863024865137,257.9185737994397,257.9185162999359,257.91845773060146,257.91839807153787,257.91833730247254,257.9182754027593,257.9182123513666,257.918148126873,257.91808270745616,257.91801607089195,257.91794819454003,257.9178790553395,257.91780862980227,257.9177368940023,257.91766382356974,257.91758939368117,257.9175135790513,257.91743635392515,257.9173576920702,257.91727756676437,257.9171959507903,257.91711281642455,257.91702813542724,257.9169418790361,257.91685401795144,257.916764522331,257.9166733617773,257.91658050532794,257.9164859214452,257.91638957800546,257.9162914422879,257.91619148096385,257.9160896600854,257.91598594507434,257.91588030070875,257.91577269111286,257.9156630797458,257.9155514293852,257.9154377021191,257.91532185933096,257.91520386168685,257.9150836691217,257.91496124082596,257.9148365352329,257.91470951000485,257.9145801220149,257.9144483273368,257.9143140812285,257.9141773381187,257.91403805158507,257.9138961743487,257.91375165824905,257.913604454233,257.9134545123369,257.91330178166845,257.9131462103898,257.912987745703,257.91282633382895,257.9126619199897,257.9124944483905,257.912323862202,257.9121501035396,257.91197311344365,257.9117928318617,257.9116091976247,257.9114221484318,257.9112316208231,257.9110375501625,257.91083987061467,257.9106385151226,257.9104334153851,257.9102245018356,257.9100117036152,257.90979494855077,257.9095741631319,257.9093492724841,257.90912020034443,257.90888686903554,257.90864919944005,257.9084071109746,257.9081605215601,257.9079093475966,257.9076535039356,257.90739290384863,257.9071274590007,257.9068570794201,257.9065816734664,257.9063011478029,257.9060154073622,257.90572435531624,257.905427893044,257.9051259200954,257.9048183341617,257.9045050310397,257.9041859045944,257.90386084672747,257.90352974733855,257.90319249428876,257.902848973364,257.9024990682347,257.9021426604203,257.901779629246,257.90140985180426,257.90103320291473,257.9006495550802,257.9002587784444,257.8998607407514,257.8994553072962,257.89904234088675,257.89862170179185,257.8981932476972,257.8977568336583,257.89731231205195,257.8968595325275,257.89639834195486,257.89592858437567,257.8954501009493,257.89496272990357,257.8944663064776,257.8939606628673,257.8934456281731,257.89292102833787,257.8923866860942,257.891842420902,257.89128804889054,257.89072338279874,257.89014823190865,257.88956240198746,257.88896569522,257.8883579101446,257.8877388415861,257.8871082805887,257.88646601434624,257.8858118261322,257.88514549522773,257.88446679685205,257.8837755020829,257.8830713777854,257.8823541865343,257.88162368653667,257.88087963154913,257.8801217708022,257.87934984891365,257.8785636058073,257.8777627766263,257.87694709164816,257.87611627619333,257.87527005054017,257.8744081298293,257.8735302239723,257.87263603755673,257.8717252697518,257.87079761420773,257.8698527589559,257.86889038631034,257.86791017276187,257.8669117888743,257.86589489917816,257.8648591620598,257.8638042296528,257.8627297477257,257.8616353555653,257.8605206858628,257.8593853645946,257.85822901089995,257.85705123696147,257.85585164787835,257.8546298415407,257.8533854084992,257.85211793183635,257.8508269870304,257.8495121418201,257.84817295606985,257.84680898162486,257.84541976217065,257.8440048330889,257.84256372130795,257.8410959451533,257.83960101419655,257.8380784290964,257.83652768144697,257.83494825360987,257.83333961855874,257.8317012397069,257.8300325707443,257.8283330554613,257.8266021275787,257.82483921056775,257.8230437174717,257.82121505072257,257.81935260195513,257.8174557518176,257.8155238697824,257.81355631394575,257.81155243083475,257.80951155520233,257.80743300982556,257.8053161052938,257.803160139802,257.8009643989328,257.79872815543996,257.79645066902526,257.7941311861157,257.79176893963455,257.7893631487673,257.7869130187296,257.7844177405228,257.78187649069673,257.7792884310987,257.7766527086255,257.7739684549669,257.7712347863494,257.76845080327314,257.7656155902469,257.7627282155159,257.7597877307901,257.75679317096524,257.7537435538406,257.7506378798313,257.7474751316837,257.74425427417174,257.74097425380626,257.73763399852555,257.73423241739306,257.7307684002836,257.7272408175654,257.7236485197821,257.7199903373273,257.71626508011485,257.7124715372449,257.7086084766646,257.70467464482914,257.70066876634957,257.69658954364235,257.692435656575,257.6882057621033,257.68389849390513,257.67951246200977,257.67504625242583,257.6704984267582,257.66586752182667,257.6611520492751,257.656350495178,257.651461319645,257.64648295641535,257.64141381245116,257.6362522675269,257.63099667381204,257.6256453554488,257.6201966081304,257.61464869866654,257.6089998645523,257.60324831352784,257.59739222313533,257.59142974027185,257.585358980738,257.57917802878137,257.5728849366349,257.56647772405705,257.55995437785924,257.5533128514367,257.546551064291,257.53966690155255,257.53265821349487,257.5255228150504,257.5182584853205,257.5108629670803,257.50333396628474,257.49566915156873,257.487866153744,257.47992256529864,257.47183593988404,257.4636037918112,257.4552235955367,257.4466927851502,257.43800875386046,257.4291688534786,257.4201703939009,257.4110106425909,257.4016868240617,257.3921961193545,257.3825356655218,257.37270255510833,257.362693835631,257.35250650906573,257.34213753132536,257.3315838117514,257.3208422125997,257.30990954852814,257.29878258609494,257.28745804325064,257.27593258884144,257.2642028421121,257.2522653722153,257.2401166977307,257.22775328617826,257.2151715535513,257.20236786384964,257.18933852861977,257.1760798065051,257.1625879028068,257.1488589690518,257.13488910257246,257.1206743460941,257.1062106873401,257.09149405864616,257.07652033658786,257.0612853416231,257.0457848377499,257.03001453218104,257.01397007503584,256.99764705904613,256.9810410192881,256.9641474329268,256.94696171899085,256.9294792381574,256.9116952925725,256.8936051256894,256.87520392213247,256.856486807592,256.83744884874386,256.81808505319947,256.7983903694903,256.7783596870791,256.7579878364071,256.73726958898135,256.71619965749056,256.6947726959666,256.67298329998033,256.6508260068842,256.62829529609394,256.60538558941795,256.5820912514325,256.5584065899037,256.534325856265,256.50984324614,256.4849528999256,256.4596489034268,256.43392528855327,256.40777603407196,256.3811950664264,256.3541762606169,256.3267134411477,256.29880038304486,256.2704308129406,256.2415984102372,256.2122968083392,256.18251959596694,256.15226031854957,256.12151247970036,256.0902695427754,256.0585249325205,256.02627203680566,255.9935042084537,255.96021476716234,255.92639700152088,255.89204417112825,255.85714950881217,255.82170622295118,255.78570749990348,255.74914650654537,255.712016392921,255.67431029500156,255.6360213375694,255.59714263720943,255.557667305431,255.5175884519045,255.47689918782942,255.43559262942642,255.39366190155772,255.35110014148384,255.3079005027472,255.26405615919427,255.21956030913637,255.1744061796462,255.1285870309939,255.0820961612299,255.0349269109042,254.98707266793502,254.93852687261892,254.88928302279086,254.83933467912732,254.78867547059983,254.737299100075,254.6851993500591,254.63237008859875,254.57880527532012,254.5244989676214,254.46944532700718,254.41363862557114,254.35707325262032,254.29974372144355,254.2416446762169,254.18277089905223,254.12311731717486,254.06267901023975,254.00145121777197,253.9394293467367,253.8766089792277,253.81298588027514,253.74855600576268,253.68331551045637,253.61726075612958,253.55038831978678,253.48269500197327,253.41417783517008,253.34483409225479,253.27466129504026,253.20365722285754,253.1318199211968,253.05914771038215,252.98563919427528,252.9112932689969,252.83610913165646,252.7600862890741,252.68322456648724,252.6055241162245,252.52698542633945,252.44760932918385,252.3673970099103,252.28635001488684,252.20447026001108,252.12176003890454,252.0382220309712,251.95385930930703,251.86867534843898,251.78267403187272,251.6958596594433,251.60823695443244,251.51981107045003,251.4305875980463,251.34057257104627,251.24977247257803,251.15819424078134,251.06584527416734,250.972733436619,250.8788670620034,250.78425495837865,250.68890641177256,250.5928311895154,250.4960395431008,250.39854221055833,250.30035041831647,250.20147588253312,250.10193080987708,250.00172789773896,249.90088033385246,249.79940179530936,249.69730644694633,249.5946089390917,249.49132440465158,249.38746845552024,249.28305717830153,249.17810712932825,249.07263532896278,248.96665925517115,248.8601968363618,248.75326644347405,248.64588688131352,248.5380773791295,248.42985758042514,248.32124753200029,248.2122676722262,248.10293881855293,247.99328215424816,247.88331921437688,247.77307187102528,247.66256231777552,247.5518130534456,247.44084686510345,247.32968681036965,247.21835619902788,247.10687857395774,246.99527769141525,246.88357750067817,246.77180212308855,246.65997583051194,246.54812302324848,246.43626820742483,246.32443597190016,246.21265096472217,246.10093786917105,245.98932137943078,245.87782617592723,245.76647690037802,245.655298130598,245.5443143551056,245.43354994757956,245.32302914121198,245.21277600301144,245.10281440810363,244.99316801408435,244.88386023547633,244.77491421834347,244.66635281511742,244.55819855968866,244.45047364281942,244.3431998879298,244.23639872731323,244.1300911788341,244.02429782316108,243.91903878158834,243.81433369449613,243.71020170050122,243.6066614163466,243.50373091757612,243.40142772004324,243.29976876229497,243.19877038887515,243.0984483345853,242.9988177097425,242.899892986469,242.80168798604464,242.70421586735503,242.60748911646013,242.51151953730917,242.4163182436214,242.3218956519533,242.22826147596456,242.135424721897,242.0433936852728,241.95217594881865,241.86177838161757,241.77220713948535,241.68346766656967,241.59556469815996,241.50850226470033,241.42228369698827,241.3369116325421,241.25238802311623,241.16871414334045,241.0858906004553,241.00391734511476,240.92279368322318,240.84251828877117,240.76308921763328,240.68450392228706,240.60675926741175,240.529851546322,240.45377649819125,240.3785293260164,240.3041047152753,240.23049685322553,240.15769944879418,240.08570575300453,240.0145085798874,239.94410032782238,239.87447300125478,239.8056182327338,239.737527305217,239.6701911745866,239.6036004923234,239.53774562828443,239.47261669353148,239.40820356315814,239.34449589906365,239.2814831726243,239.2191546872125,239.15749960051667,239.0965069466158,239.0361656577642,238.97646458584435,238.91739252344686,238.85893822453872,238.80109042468385,238.74383786078076,238.68716929028517,238.63107350988793,238.5755393736201,238.5205558103598,238.46611184071767,238.4121965932808,238.35879932019614,238.30590941207797,238.25351641222565,238.2016100301407,238.15018015433438,238.099216864419,238.04871044247918,237.99865138372067,237.949030406397,237.89983846101632,237.8510667388325,237.80270667962677,237.75474997878817,237.7071885937023,237.66001474946046,237.6132209439018,237.56679995200375,237.52074482963604,237.47504891669655,237.42970583964686,237.38470951346756,237.3400541430537,237.29573422407168,237.2517445433007,237.20808017848032,237.16473649768835,237.12170915827323,237.07899410536373,237.03658756998175,236.99448606678183,236.9526863914422,236.9111856177313,236.8699810942749,236.82907044104658,236.78845154560648,236.74812255911027,236.70808189211195,236.6683282101824,236.62886042936492,236.58967771148892,236.55077945936202,236.51216531185935,236.4738351389294,236.43578903653358,236.39802732153663,236.3605505265638,236.32335939484022,236.28645487502615,236.24983811606202,236.21351046203574,236.17747344708343,236.1417287903349,236.10627839091347,236.07112432299962,236.0362688309667,236.0017143245964,235.96746337438097,235.93351870691902,235.8998832004099,235.86655988025262,235.83355191475377,235.80086261094877,235.76849541054045,235.73645388595875,235.70474173654438,235.67336278486025,235.64232097313288,235.61162035982744,235.58126511635808,235.5512595239373,235.52160797056638,235.49231494816988,235.4633850498771,235.43482296745296,235.406633488882,235.37882149610815,235.35139196293395,235.32434995308253,235.29770061842612,235.27144919738504,235.24560101350093,235.22016147418884,235.19513606967172,235.17053037210275,235.14635003487905,235.12260079215173,235.09928845853682,235.07641892903138,235.0539981791393,235.0320322652112,235.01052732500182,234.98948957844945,234.96892532868077,234.9488409632434,234.92924295556983,234.9101378666742,234.8915323470835,234.87343313900465,234.85584707872658,234.8387810992578,234.8222422331973,234.80623761583644,234.79077448848878,234.77586020204342,234.761502220736,234.74770812613116,234.73448562130875,234.72184253524415,234.70978682737336,234.6983265923307,234.68747006484622,234.67722562478937,234.66760180234235,234.65860728328732,234.65025091438864,234.64254170885064,234.63548885183064,234.62910170598425,234.62338981702015,234.6183629192397,234.61403094103505,234.6104040103197,234.6074924598625,234.60530683249672,234.60385788617418,234.6031565988334,234.60321417305028,234.60404204043905,234.60565186577057,234.60805555077496,234.61126523759395,234.6152933118503,234.62015240529865,234.62585539802458,234.6324154201578,234.6398458530645,234.64816032998675,234.65737273609457,234.66749720791896,234.67854813213364,234.690540143655,234.70348812302964,234.71740719308187,234.73231271479213,234.7482202823818,234.7651457175784,234.78310506303927,234.80211457491234,234.82219071451436,234.84335013910987,234.86560969177592,234.88898639033934,234.9134974153769,234.93916009726976,234.9659919023073,234.99401041783773,235.02323333646544,235.05367843929827,235.08536357825008,235.11830665740828,235.15252561347774,235.1880383953152,235.22486294257445,235.26301716348064,235.30251891176033,235.34338596275373,235.38563598873958,235.42928653350717,235.47435498621158,235.5208585545528,235.56881423732094,235.6182387963543,235.669148727958,235.72156023383593,235.77548919158926,235.83095112483912,235.8879611730322,235.94653406099144,236.00668406827535,236.06842499841147,236.13177014807258,236.19673227626419,236.2633235735946,236.33155563169944,236.40143941289455,236.47298522013,236.54620266732184,236.62110065013485,236.6976873172928,236.77597004249137,236.85595539698787,236.93764912294293,237.02105610758744,237.106180358287,237.19302497857493,237.2815921452239,237.37188308642308,237.46389806112708,237.5576363396393,237.65309618549097,237.75027483867353,237.84916850027864,237.9497723185986,238.0520803767341,238.15608568175477,238.26178015545267,238.36915462672582,238.4781988256238,238.58890137908466,238.70124980838605,238.81523052833063,238.93082884818043,239.0480289743502,239.16681401486545,239.2871659855856,239.40906581818862,239.53249336990766,239.65742743500707,239.78384575797887,239.911725048437,240.04104099768233,240.1717682969058,240.3038806569947,240.43735082989997,240.57215063152236,240.70825096606671,240.84562185181414,240.9842324482554,241.12405108452717,241.26504528908887,241.40718182057537,241.5504266997575,241.694745242541,241.84010209393165,241.98646126289216,242.13378615801548,242.28203962393738,242.43118397841027,242.58118104995864,242.73199221603772,242.88357844161368,243.035900318087,243.18891810247797,243.34259175679603,243.49688098751284,243.65174528506262,243.80714396329213,243.9630361987857,244.1193810699907,244.27613759607257,244.43326477542826,244.59072162379076,244.7484672118592,244.906460702391,245.06466138669543,245.22302872047086,245.38152235893062,245.5401021911648,245.69872837368936,245.8573613631358,246.01596194803884,246.17449127968163,246.3329109019621,246.49118278024693,246.64926932918286,246.80713343943762,246.9647385033474,247.1220484394497,247.27902771588353,247.43564137264312,247.59185504267316,247.7476349717966,247.90294803747022,248.05776176636425,248.21204435076595,248.3657646638099,248.51889227353956,248.6713974558077],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[298.5321485639681,298.5321470458939,298.53214549956834,298.53214392446563,298.53214232005047,298.5321406857773,298.53213902109053,298.5321373254238,298.5321355982008,298.5321338388345,298.5321320467264,298.5321302212676,298.532128361837,298.5321264678027,298.5321245385205,298.5321225733347,298.532120571577,298.53211853256687,298.5321164556109,298.532114340003,298.5321121850242,298.53210998994143,298.5321077540086,298.53210547646523,298.53210315653723,298.53210079343575,298.5320983863574,298.5320959344838,298.53209343698126,298.53209089300077,298.5320883016772,298.53208566212965,298.5320829734608,298.5320802347564,298.53207744508535,298.53207460349915,298.53207170903164,298.5320687606989,298.5320657574987,298.5320626984095,298.5320595823916,298.53205640838536,298.53205317531206,298.53204988207204,298.53204652754596,298.5320431105932,298.5320396300521,298.5320360847395,298.53203247344953,298.532028794955,298.53202504800504,298.5320212313258,298.53201734361954,298.5320133835647,298.53200934981476,298.53200524099856,298.532001055719,298.53199679255346,298.53199245005214,298.53198802673904,298.5319835211101,298.5319789316337,298.53197425674955,298.53196949486795,298.5319646443705,298.5319597036079,298.53195467090035,298.53194954453704,298.5319443227752,298.53193900383945,298.5319335859215,298.5319280671795,298.53192244573705,298.53191671968386,298.5319108870721,298.5319049459198,298.5318988942067,298.53189272987566,298.53188645083105,298.5318800549378,298.5318735400222,298.5318669038691,298.5318601442226,298.5318532587844,298.53184624521435,298.5318391011276,298.53183182409595,298.53182441164506,298.5318168612553,298.53180917035974,298.53180133634436,298.5317933565455,298.5317852282507,298.53177694869663,298.53176851506873,298.5317599245,298.5317511740704,298.53174226080483,298.5317331816736,298.5317239335904,298.53171451341154,298.53170491793446,298.53169514389737,298.53168518797804,298.53167504679186,298.5316647168918,298.5316541947661,298.5316434768382,298.53163255946464,298.53162143893456,298.5316101114677,298.5315985732137,298.5315868202505,298.53157484858286,298.5315626541416,298.5315502327816,298.5315375802807,298.53152469233845,298.53151156457346,298.5314981925241,298.53148457164497,298.53147069730625,298.53145656479217,298.5314421692988,298.5314275059336,298.5314125697122,298.53139735555806,298.5313818583001,298.53136607267095,298.5313499933051,298.53133361473755,298.5313169314016,298.531299937627,298.5312826276381,298.5312649955511,298.53124703537384,298.53122874100245,298.5312101062187,298.53119112468966,298.5311717899642,298.5311520954715,298.5311320345177,298.53111160028544,298.5310907858303,298.53106958407847,298.5310479878245,298.5310259897293,298.53100358231717,298.53098075797317,298.530957508941,298.5309338273199,298.5309097050625,298.5308851339715,298.5308601056972,298.53083461173475,298.53080864342087,298.53078219193173,298.530755248279,298.53072780330723,298.5306998476907,298.5306713719305,298.53064236635106,298.5306128210969,298.5305827261289,298.5305520712217,298.53052084596015,298.53048903973473,298.530456641739,298.53042364096564,298.53039002620284,298.5303557860299,298.5303209088142,298.5302853827067,298.53024919563825,298.5302123353151,298.5301747892153,298.5301365445839,298.5300975884287,298.5300579075163,298.53001748836755,298.5299763172521,298.5299343801848,298.5298916629208,298.5298481509498,298.5298038294926,298.52975868349483,298.5297126976228,298.5296658562572,298.529618143489,298.52956954311355,298.52952003862447,298.5294696132097,298.5294182497438,298.5293659307839,298.5293126385623,298.52925835498166,298.5292030616085,298.5291467396667,298.52908937003133,298.529030933222,298.5289714093968,298.528910778345,298.52884901948033,298.5287861118349,298.5287220340504,298.5286567643721,298.5285902806418,298.528522560289,298.52845358032454,298.5283833173321,298.5283117474606,298.52823884641646,298.5281645894543,298.52808895137,298.52801190649075,298.52793342866767,298.52785349126634,298.5277720671577,298.52768912870937,298.5276046477756,298.52751859568906,298.5274309432495,298.52734166071485,298.5272507177916,298.5271580836234,298.52706372678216,298.5269676152557,298.52686971643845,298.52676999711935,298.5266684234721,298.526564961042,298.5264595747357,298.5263522288088,298.5262428868536,298.5261315117877,298.52601806584084,298.52590251054147,298.5257848067063,298.525664914424,298.5255427930438,298.52541840116174,298.5252916966054,298.52516263642144,298.52503117686,298.5248972733603,298.5247608805356,298.5246219521591,298.52448044114567,298.5243362995394,298.52418947849515,298.524039928263,298.52388759817137,298.52373243661077,298.52357439101496,298.5234134078448,298.5232494325702,298.5230824096504,298.5229122825167,298.52273899355424,298.5225624840803,298.5223826943271,298.5221995634201,298.52201302935896,298.5218230289957,298.5216294980143,298.5214323709091,298.52123158096276,298.52102706022407,298.5208187394856,298.52060654825954,298.520390414756,298.52017026585725,298.51994602709465,298.5197176226231,298.5194849751961,298.51924800614034,298.51900663532865,298.5187607811542,298.51851036050317,298.5182552887269,298.51799547961406,298.51773084536154,298.51746129654623,298.5171867420947,298.5169070892528,298.51662224355584,298.51633210879606,298.5160365869921,298.515735578355,298.515428981258,298.5151166921989,298.51479860577,298.51447461462,298.51414460942084,298.51380847883047,298.51346610945654,298.51311738581904,298.51276219031223,298.512400403166,298.5120319024066,298.5116565638161,298.51127426089243,298.51088486480705,298.51048824436333,298.51008426595297,298.5096727935129,298.5092536884804,298.5088268097478,298.50839201361623,298.5079491537494,298.5074980811251,298.5070386439867,298.50657068779424,298.5060940551739,298.50560858586636,298.50511411667577,298.5046104814155,298.5040975108553,298.50357503266594,298.5030428713639,298.5025008482537,298.50194878137114,298.50138648542395,298.5008137717327,298.500230448169,298.4996363190945,298.4990311852983,298.49841484393164,298.49778708844383,298.497147708516,298.4964964899944,298.4958332148194,298.49515766095965,298.494469602338,298.49376880876156,298.49305504584703,298.49232807494735,298.4915876530757,298.49083353282776,298.49006546230464,298.48928318503226,298.48848643988066,298.487674960983,298.48684847764883,298.4860067142827,298.4851493902952,298.4842762200161,298.48338691260494,298.4824811719597,298.4815586966253,298.48061917969926,298.4796623087365,298.4786877656527,298.4776952266254,298.4766843619942,298.47565483615926,298.4746063074778,298.4735384281594,298.4724508441587,298.47134319506796,298.4702151140068,298.4690662275096,298.46789615541326,298.46670451074107,298.46549089958535,298.4642549209896,298.4629961668266,298.4617142216762,298.4604086627002,298.4590790595178,298.4577249740745,298.4563459605138,298.45494156504355,298.4535113258033,298.45205477272606,298.45057142740154,298.4490608029347,298.44752240380336,298.4459557257154,298.44436025545934,298.44273547075727,298.4410808401141,298.43939582266347,298.4376798680132,298.43593241608755,298.4341528969681,298.4323407307309,298.4304953272828,298.4286160861958,298.42670239653734,298.4247536366995,298.4227691742268,298.4207483656394,298.4186905562558,298.4165950800132,298.41446125928366,298.41228840469086,298.4100758149219,298.40782277653796,298.40552856378264,298.4031924383875,298.4008136493756,298.39839143286235,298.39592501185433,298.39341359604606,298.3908563816129,298.3882525510036,298.38560127272973,298.3829017011516,298.3801529762642,298.37735422347816,298.37450455340166,298.3716030616168,298.36864882845515,298.36564091877216,298.36257838171764,298.35946025050464,298.3562855421766,298.3530532573723,298.34976238008767,298.34641187743784,298.3430006994146,298.3395277786438,298.33599203014006,298.3323923510596,298.3287276204519,298.32499669900926,298.3211984288141,298.3173316330863,298.3133951159271,298.30938766206293,298.30530803658746,298.3011549847027,298.2969272314577,298.2926234814888,298.28824241875554,298.2837827062787,298.2792429858759,298.27462187789706,298.2699179809592,298.26512987168206,298.26025610442093,298.2552952110026,298.25024570045906,298.24510605876225,298.23987474855966,298.23455020890907,298.22913085501676,298.2236150779735,298.2180012444938,298.212287696657,298.20647275164714,298.20055470149845,298.1945318128395,298.18840232664223,298.1821644579718,298.1758163957407,298.1693563024646,298.1627823140231,298.15609253942256,298.14928506056447,298.14235793201743,298.13530918079385,298.12813680613147,298.12083877928126,298.1134130432994,298.1058575128475,298.0981700739971,298.09034858404306,298.0823908713237,298.07429473504783,298.0660579451309,298.05767824204105,298.0491533366519,298.040480910106,298.0316586136902,298.022684068719,298.0135548664321,298.00426856790017,297.994822703947,297.9852147750821,297.9754422514477,297.9655025727796,297.9553931483834,297.94511135712713,297.9346545474467,297.924020037373,297.91320511457303,297.90220703641,297.8910230300239,297.8796502924311,297.8680859906435,297.85632726180995,297.84437121338107,297.83221492329415,297.8198554401843,297.8072897836199,297.7945149443619,297.7815278846517,297.76832553852574,297.7549048121562,297.7412625842248,297.7273957063222,297.7133010033822,297.6989752741457,297.684415291658,297.66961780380007,297.654579533855,297.63929718110995,297.62376742149513,297.60798690825993,297.5919522726894,297.57566012485876,297.55910705443057,297.5422896314922,297.52520440743723,297.50784791589047,297.49021667367737,297.4723071818399,297.4541159266991,297.4356393809658,297.416874004899,297.3978162475147,297.3784625478461,297.35880933625344,297.3388530357887,297.31859006361054,297.2980168324557,297.2771297521652,297.25592523126363,297.2343996785978,297.2125495050304,297.1903711251922,297.16786095929103,297.14501543498136,297.12183098929194,297.0983040706116,297.07443114073806,297.0502086769834,297.02563317434215,297.00070114771876,296.9754091342153,296.94975369547984,296.9237314201151,296.89733892614595,296.8705728635482,296.8434299168342,296.81590680769904,296.78800029772253,296.75970719112996,296.7310243376067,296.7019486351705,296.6724770330936,296.64260653488145,296.61233420129804,296.58165715344404,296.55057257587913,296.5190777197915,296.4871699062102,296.45484652925774,296.4221050594411,296.3889430469796,296.355358125164,296.3213480137462,296.28691052235564,296.252043553939,296.2167451082186,296.18101328516815,296.14484628849857,296.10824242915317,296.0712001288052,296.03371792335435,295.9957944664173,295.9574285328077,295.9186190219993,295.87936496156914,295.83966551061195,295.7995199631237,295.7589277513462,295.71788844906735,295.6764017748706,295.6344675953276,295.59208592812826,295.54925694513963,295.5059809753895,295.46225850796486,295.41809019482156,295.37347685349505,295.3284194697078,295.282919199864,295.2369773734258,295.19059549516345,295.1437752472711,295.09651849134383,295.04882727020487,295.00070380957914,294.9521505196043,294.9031699961732,294.85376502209954,294.80393856810304,294.75369379360313,294.70303404731925,294.6519628676694,294.6004839829601,294.54860131136587,294.49631896068837,294.4436412278945,294.3905725984256,294.3371177452757,294.28328152783405,294.2290689904894,294.1744853609926,294.11953604857473,294.0642266418205,294.008562906295,293.95255078192156,293.89619638011413,293.8395059806612,293.78248602836464,293.72514312943514,293.667484047646,293.6095157002493,293.5512451536586,293.4926796189022,293.4338264468533,293.3746931232428,293.31528726346227,293.2556166071637,293.19568901266643,293.13551245117947,293.07509500084905,293.01444484064257,292.9535702440813,292.8924795728327,292.8311812701765,292.76968385435754,292.7079959118401,292.64612609047873,292.58408309262074,292.5218756681571,292.4595126075379,292.39700273476933,292.3343549004117,292.2715779745929,292.2086808400596,292.1456723852825,292.08256149763434,292.01935705666153,291.9560679274672,291.892702954225,291.8292709538436,291.76578070980105,291.70224096616647,291.6386604218309,291.5750477249629,291.511411467709,291.4477601811564,291.3841023305749,291.3204463109551,291.25680044286,291.19317296860356,291.12957204877233,291.0660057591047,291.002482087739,290.9390089328442,290.8755941006438,290.81224530384236,290.7489701604655,290.68577619311844,290.622670828673,290.55966139838534,290.4967551384515,290.4339591910006,290.3712806055303,290.308726340782,290.2463032670568,290.18401816896744,290.1218777486244,290.0598886292491,289.99805735920853,289.93639041646236,289.8748942134134,289.8135751021506,289.75243938007173,289.6914932958732,289.630743055891,289.57019483077784,289.50985476249804,289.44972897162313,289.38982356490686,289.33014464312106,289.270698309129,289.21149067617455,289.1525278763645,289.093816069318,289.0353614509601,288.977170262433,288.9192487990988,288.861603419608,288.80424055500566,288.74716671784887,288.6903885113073,288.6339126382199,288.5777459100794,288.5218952559164,288.46636773105746,288.4111705257269,288.3563109734676,288.3017965593523,288.24763492796046,288.1938338910938,288.1404014352053,288.08734572851836,288.0346751278105,287.98239818484035,287.93052365239527,287.87906048993773,287.828017868832,287.77740517713113,287.7272320239069,287.6775082431061,287.6282438969178,287.5794492786382,287.5311349150199,287.483311568094,287.43599023645714,287.3891821560128,287.3428988001618,287.29715187943606,287.2519533405715,287.20731536501887,287.1632503668906,287.1197709903458,287.07689010641434,287.03462080926533,286.9929764119246,286.95197044144834,286.9116166335617,286.87192892677115,286.83292145596346,286.7946085455027,286.7570047018402,286.7201246056518,286.68398310352035,286.64859519918014,286.61397604434285,286.5801409291248,286.5471052720967,286.51488460997837,286.4834945870007,286.4529509439588,286.423269506982,286.39446617604466,286.36655691324347,286.3395577308697,286.31348467930104,286.2883538347417,286.2641812868373,286.2409831261939,286.2187754318271,286.1975742585713,286.1773956244755,286.15825549821466,286.1401697865435,286.12315432182174,286.10722484963577,286.0923970165458,286.07868635798405,286.06610828632904,286.05467807918285,286.0444108678751,286.03532162621764,286.0274251595338,286.0207360939838,286.01526886620917,286.01103771331526,286.00805666321304,286.00633952533855,286.00589988176773,286.0067510787434,286.0089062186303,286.01237815231246,286.0171794720476,286.02332250478906,286.0308193059887,286.03968165388886,286.04992104431454,286.06154868597145,286.07457549625866,286.08901209760006,286.10486881429904,286.122155669921,286.1408823852036,286.16105837649843,286.1826927547408,286.2057943249495,286.23037158625186,286.25643273243213,286.28398565299744,286.3130379347575,286.34359686390934,286.37566942862134,286.4092623221071,286.44438194617976,286.4810344152754,286.51922556093695,286.55896093674147,286.6002458236627,286.64308523585083,286.68748392681607,286.73344639600094,286.78097689572246,286.83007943846934,286.8807578045334,286.93301554995855,286.9868560147859,287.04228233157494,287.0992974341811,287.1579040667654,287.21810479301564,287.27990200555575,287.3432979355189,287.40829466226097,287.4748941231893,287.54309812368194,287.6129083470698,287.684326364659,287.7573536457633,287.831991567722,287.9082414258744,287.9861044434648,288.0655817814491,288.1466745481751,288.2293838089085,288.3137105951767,288.39965591390063,288.4872207562874,288.5764061064554,288.66721294976213,288.7596422808087,288.8536951110911,288.9493724762713,289.0466754430419,289.14560511555476,289.2461626413896,289.34834921703566,289.45216609286,289.55761457753965,289.66469604193173,289.7734119223594,289.88376372329094,289.99575301939063,290.10938145692046,290.2246507544747,290.3415627030272,290.4601191652764,290.58032207427095,290.7021734313024,290.82567530305215,290.95082981798026,291.07763916194796,291.20610557306395,291.33623133574963,291.46801877401674,291.60147024395616,291.7365881254354,291.87337481300585,292.01183270602394,292.15196419798787,292.2937716650999,292.4372574540607,292.5824238691081,292.72927315831134,292.87780749913856,293.028028983312,293.17993960097266,293.33354122417376,293.4888355897292,293.64582428144035,293.8045087117311,293.9648901027197,294.12696946676135,294.29074758649415,294.4562249944253,294.62340195209606,294.79227842886485,294.9628540803514,295.1351282265848,295.30909982990136,295.4847674726392,295.6621293346783,295.8411831708761,296.02192628844927,296.20435552435555,296.3884672227279,296.5742572124167,296.7617207846949,296.9508526711823,297.1416470220462,297.3340973845347,297.52819668190125,297.723937192777,297.92131053104873,298.12030762629996,298.3209187048713,298.5231332715977,298.72694009227655,298.9323271769221,299.1392817638608,299.34779030471753,299.5578384503456,299.76941103774953,299.9824920780468,300.19706474551595,300.41311136777364,300.6306134171211,300.84955150310043,301.0699053662958,301.29165387341334,301.51477501367043,301.7392458965215,301.9650427507469,302.1921409249231,302.4205148892952,302.650138239065,302.8809836991057,303.11302313011225,303.34622753619016,303.58056707388357,303.8160110626403,304.0525279967058,304.29008555843666,304.5286506330188,304.7681893245728,305.00866697362494,305.2500481759196,305.4922968025436,305.7353760213316,305.9792483195171,306.2238755275907,306.4692188443237,306.7152388629145,306.9618955982081,307.2091485149405,307.45695655695476,307.7052781773344,307.9540713693971,308.2032936984891,308.45290233452,308.7028540851752,308.9531054297429,309.20361255348854,309.4543313825133,309.7052176190264,309.9562267769666,310.2073142179035,310.45843518714935,310.70954485001556,310.96059832814376,311.21155073584504,311.4623572163798,311.7129729781122,311.9633533304739,312.21345371967334,312.46322976408703,312.7126372892729,312.96163236254483,313.2101713270514,313.45821083530285,313.7057078820923,313.95261983675994,314.1989044747507,314.44452000841886,314.68942511703443,314.93357897595087,315.1769412848933,315.419472295332,315.66113283690714,315.9018843428728,316.14168887453343,316.38050914464685,316.6183085397706,316.8550511415323,317.0907017468075,317.32522588679,317.55858984494324,317.7907606738246,318.0217062107753,318.25139509247356,318.47979676834905,318.70688151286004,318.9326204366381,319.1569854965045,319.3799495043683,319.601486135016,319.8215699328047,320.0401763172737,320.25728158769033,320.47286292654866,320.68689840203945],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[576.774602966089,576.7745953435732,576.7745875792017,576.7745796703355,576.7745716142853,576.7745634083118,576.774555049625,576.7745465353829,576.7745378626912,576.7745290286003,576.7745200301076,576.7745108641527,576.7745015276197,576.7744920173337,576.7744823300616,576.77447246251,576.7744624113235,576.7744521730842,576.7744417443122,576.7744311214608,576.7744203009189,576.7744092790072,576.7743980519783,576.7743866160149,576.774374967229,576.7743631016599,576.7743510152737,576.7743387039606,576.7743261635347,576.7743133897329,576.7743003782117,576.7742871245472,576.7742736242334,576.7742598726803,576.7742458652118,576.7742315970662,576.774217063392,576.7742022592485,576.7741871796011,576.7741718193239,576.774156173194,576.7741402358921,576.7741240019994,576.7741074659962,576.7740906222609,576.7740734650669,576.7740559885802,576.7740381868595,576.7740200538518,576.7740015833931,576.7739827692023,576.7739636048841,576.7739440839214,576.7739241996786,576.7739039453942,576.7738833141829,576.7738622990295,576.7738408927895,576.7738190881853,576.7737968778031,576.773774254092,576.7737512093602,576.7737277357733,576.7737038253493,576.7736794699599,576.7736546613249,576.7736293910093,576.7736036504225,576.7735774308118,576.7735507232638,576.7735235186981,576.7734958078659,576.7734675813459,576.773438829542,576.7734095426787,576.7733797107994,576.7733493237619,576.7733183712353,576.773286842696,576.7732547274256,576.7732220145054,576.7731886928141,576.7731547510225,576.7731201775923,576.7730849607689,576.7730490885796,576.7730125488287,576.7729753290939,576.772937416722,576.7728987988232,576.7728594622689,576.7728193936865,576.7727785794531,576.7727370056939,576.7726946582752,576.7726515228001,576.7726075846042,576.7725628287501,576.7725172400228,576.7724708029236,576.7724235016658,576.772375320169,576.7723262420539,576.7722762506351,576.7722253289182,576.7721734595924,576.7721206250242,576.7720668072523,576.772011987981,576.7719561485754,576.7718992700517,576.771841333075,576.7717823179491,576.7717222046131,576.7716609726305,576.7715986011863,576.7715350690779,576.7714703547081,576.7714044360781,576.7713372907792,576.7712688959866,576.7711992284501,576.7711282644873,576.7710559799755,576.7709823503425,576.77090735056,576.7708309551329,576.7707531380917,576.7706738729856,576.7705931328697,576.7705108902993,576.7704271173177,576.7703417854489,576.7702548656877,576.7701663284877,576.7700761437553,576.7699842808341,576.7698907085002,576.7697953949466,576.7696983077759,576.7695994139883,576.7694986799685,576.7693960714777,576.7692915536393,576.7691850909279,576.7690766471569,576.7689661854679,576.7688536683158,576.7687390574572,576.7686223139381,576.7685033980783,576.7683822694618,576.7682588869189,576.7681332085147,576.7680051915346,576.7678747924698,576.767741967001,576.7676066699861,576.7674688554421,576.7673284765314,576.7671854855456,576.7670398338882,576.7668914720596,576.7667403496386,576.7665864152674,576.7664296166325,576.7662699004475,576.7661072124348,576.7659414973075,576.7657726987514,576.7656007594038,576.7654256208361,576.7652472235318,576.7650655068701,576.7648804091003,576.7646918673254,576.7644998174771,576.7643041942976,576.7641049313145,576.763901960819,576.7636952138444,576.7634846201415,576.7632701081543,576.7630516049963,576.7628290364266,576.7626023268239,576.7623713991601,576.7621361749758,576.7618965743525,576.7616525158866,576.7614039166601,576.7611506922145,576.7608927565204,576.7606300219497,576.7603623992455,576.7600897974913,576.7598121240802,576.7595292846855,576.7592411832252,576.7589477218328,576.7586488008221,576.7583443186548,576.758034171904,576.7577182552229,576.7573964613046,576.7570686808485,576.7567348025235,576.7563947129294,576.756048296558,576.7556954357553,576.7553360106803,576.7549698992664,576.7545969771777,576.7542171177687,576.7538301920408,576.753436068599,576.7530346136057,576.7526256907389,576.7522091611424,576.7517848833813,576.7513527133935,576.7509125044405,576.7504641070575,576.7500073690051,576.7495421352145,576.7490682477379,576.7485855456941,576.7480938652129,576.7475930393822,576.7470828981895,576.7465632684657,576.7460339738263,576.7454948346107,576.7449456678233,576.7443862870714,576.7438165025005,576.743236120733,576.7426449448001,576.742042774078,576.7414294042175,576.7408046270771,576.7401682306524,576.7395199990037,576.7388597121851,576.7381871461672,576.7375020727643,576.7368042595566,576.7360934698121,576.7353694624065,576.7346319917411,576.7338808076629,576.7331156553763,576.7323362753616,576.7315424032837,576.730733769906,576.7299101009983,576.7290711172454,576.7282165341528,576.7273460619489,576.726459405493,576.7255562641699,576.7246363317919,576.7236992964969,576.7227448406412,576.721772640694,576.7207823671293,576.7197736843137,576.7187462503953,576.7176997171878,576.7166337300545,576.7155479277887,576.7144419424934,576.7133153994573,576.7121679170281,576.7109991064881,576.7098085719198,576.7085959100763,576.7073607102448,576.7061025541096,576.7048210156119,576.703515660808,576.702186047723,576.700831726204,576.6994522377694,576.6980471154551,576.69661588366,576.6951580579856,576.6936731450752,576.6921606424488,576.690620038337,576.6890508115079,576.6874524310946,576.6858243564199,576.6841660368128,576.6824769114289,576.6807564090608,576.6790039479499,576.6772189355919,576.6754007685405,576.6735488322072,576.6716625006561,576.6697411363971,576.6677840901739,576.6657907007493,576.6637602946847,576.6616921861197,576.6595856765404,576.6574400545529,576.6552545956463,576.6530285619513,576.6507612019989,576.6484517504713,576.6460994279482,576.6437034406517,576.6412629801827,576.6387772232556,576.6362453314265,576.6336664508179,576.6310397118361,576.6283642288884,576.6256391000893,576.622863406965,576.6200362141532,576.6171565690948,576.6142235017238,576.6112360241472,576.6081931303232,576.6050937957328,576.6019369770424,576.5987216117668,576.5954466179202,576.5921108936632,576.5887133169463,576.5852527451409,576.5817280146723,576.5781379406379,576.5744813164233,576.570756913313,576.5669634800897,576.5630997426302,576.5591644034936,576.5551561415011,576.5510736113107,576.5469154429818,576.5426802415337,576.5383665864988,576.5339730314639,576.529498103606,576.5249403032193,576.5202981032345,576.5155699487318,576.5107542564413,576.5058494142372,576.5008537806262,576.495765684221,576.4905834232111,576.4853052648192,576.4799294447544,576.4744541666496,576.4688776014951,576.4631978870585,576.4574131272982,576.4515213917647,576.445520714993,576.4394090958849,576.4331844970803,576.426844844319,576.4203880257924,576.4138118914815,576.4071142524878,576.4002928803519,576.3933455063599,576.3862698208394,576.3790634724435,576.3717240674256,576.3642491688968,576.3566362960787,576.3488829235381,576.3409864804133,576.332944349626,576.3247538670801,576.3164123208498,576.3079169503562,576.2992649455233,576.2904534459312,576.281479539949,576.2723402638552,576.2630326009466,576.2535534806327,576.2438997775135,576.2340683104469,576.224055841601,576.2138590754886,576.2034746579916,576.1928991753682,576.1821291532473,576.1711610556035,576.1599912837222,576.1486161751446,576.1370320026032,576.1252349729333,576.113221225978,576.1009868334701,576.0885277979005,576.0758400513728,576.0629194544371,576.0497617949104,576.0363627866802,576.022718068491,576.0088232027126,575.994673674094,575.9802648885,575.965592171628,575.9506507677102,575.9354358381992,575.9199424604321,575.9041656262833,575.8881002407934,575.871741120784,575.855082993458,575.8381204949728,575.8208481690061,575.8032604652968,575.7853517381732,575.7671162450589,575.7485481449635,575.7296414969552,575.7103902586152,575.6907882844757,575.6708293244361,575.6505070221687,575.6298149134986,575.6087464247729,575.5872948712071,575.56545345522,575.5432152647459,575.5205732715311,575.4975203294182,575.4740491726058,575.4501524138977,575.4258225429347,575.401051924407,575.3758327962557,575.3501572678534,575.3240173181758,575.2974047939514,575.2703114078021,575.2427287363663,575.2146482184094,575.186061152921,575.1569586971978,575.127331864915,575.0971715241845,575.0664683956039,575.0352130502913,575.0033959079101,574.9710072346886,574.938037141422,574.9044755814743,574.8703123487654,574.8355370757573,574.8001392314281,574.7641081192468,574.7274328751362,574.6901024654375,574.6521056848721,574.6134311544976,574.574067319667,574.5340024479851,574.4932246272699,574.4517217635121,574.4094815788426,574.3664916095012,574.3227392038172,574.2782115201886,574.23289552508,574.1867779910257,574.1398454946454,574.0920844146741,574.0434809300073,573.9940210177642,573.9436904513656,573.892474798637,573.8403594199308,573.7873294662746,573.7333698775435,573.6784653806616,573.6226004878347,573.5657594948145,573.5079264791959,573.449085298754,573.3892195898176,573.3283127656857,573.2663480150876,573.2033083006905,573.139176357653,573.0739346922346,573.0075655804566,572.9400510668204,572.8713729630884,572.8015128471231,572.730452061797,572.6581717139679,572.5846526735273,572.509875572523,572.4338208043619,572.3564685230905,572.2777986427641,572.1977908369,572.1164245380257,572.0336789373181,571.9495329843428,571.8639653868969,571.7769546109507,571.6884788807052,571.5985161787574,571.5070442463817,571.4140405839319,571.3194824513666,571.2233468688979,571.1256106177743,571.0262502411965,570.9252420453677,570.8225621006882,570.7181862430938,570.6120900755421,570.5042489696494,570.3946380674861,570.2832322835264,570.170006306762,570.0549346029836,569.9379914172246,569.8191507763835,569.6983864920142,569.5756721633006,569.4509811802047,569.3242867268035,569.1955617848078,569.0647791372706,568.931911372485,568.7969308880747,568.6598098952796,568.5205204234345,568.3790343246471,568.2353232786769,568.0893587980052,567.9411122331178,567.7905547779759,567.637657475698,567.482391224436,567.324726783455,567.1646347794134,567.0020857128384,566.837049964804,566.669497803799,566.4993993927939,566.3267247964945,566.1514439887876,565.9735268603687,565.7929432265513,565.6096628352553,565.4236553751659,565.2348904840594,565.0433377572946,564.8489667564552,564.651747018145,564.4516480629247,564.2486394043846,564.042690558342,563.8337710521612,563.6218504341804,563.4068982832401,563.1888842183015,562.9677779081462,562.7435490811434,562.5161675350772,562.2856031470162,562.0518258832204,561.8148058090671,561.5745130989836,561.3309180463751,561.083991073531,560.8337027414983,560.5800237599028,560.3229249967071,560.0623774878891,559.7983524470216,559.5308212747427,559.2597555680967,558.9851271297289,558.7069079769219,558.4250703504505,558.1395867232457,557.8504298088451,557.5575725696146,557.2609882247297,556.9606502578904,556.6565324247649,556.3486087601384,556.0368535847523,555.7212415118252,555.4017474532303,555.0783466253297,554.7510145544338,554.4197270818958,554.0844603688057,553.745190900295,553.4018954894227,553.0545512806513,552.7031357528892,552.3476267221072,551.9880023435104,551.6242411132732,551.2563218698274,550.8842237947047,550.5079264129345,550.1274095929998,549.7426535463518,549.3536388264943,548.9603463276407,548.5627572829545,548.1608532623877,547.7546161701262,547.3440282416608,546.9290720405012,546.5097304545517,546.0859866921727,545.657824277953,545.2252270482156,544.7881791462917,544.3466650175902,543.9006694044997,543.4501773411563,542.9951741481165,542.5356454269801,542.0715770549986,541.6029551797194,541.1297662137165,540.6519968294479,540.1696339543024,539.6826647658818,539.1910766875782,538.6948573845053,538.1939947598393,537.6884769516353,537.1782923301793,536.6634294959426,536.1438772782024,535.6196247343971,535.0906611502868,534.5569760409827,534.0185591529229,533.4754004668584,532.9274902019279,532.3748188208871,531.8173770365681,531.2551558196398,530.6881464077412,530.1163403160605,529.5397293494275,528.9583056159946,528.3720615425716,527.7809898916832,527.1850837804197,526.5843367011395,525.9787425440954,525.3682956220382,524.752990696864,524.1328230083562,523.5077883050848,522.8778828775048,522.2431035933139,521.6034479351052,520.958914040365,520.3095007438512,519.6552076223874,518.9960350421075,518.3319842081723,517.6630572169889,516.9892571109451,516.310587935675,515.6270547998677,514.9386639376195,514.2454227733319,513.5473399891474,512.8444255949148,512.1366910006624,511.42414909156247,510.7068143053535,509.9847027121889,509.2578320968715,508.5262220434267,507.7898940219611,507.0488714777508,506.30317992248746,505.55284702761566,504.79790271968136,504.0383792776023,503.2743114317732,502.5057364649012,501.7326943144687,500.9552276767083,500.1733821119704,499.38720615135355,498.5967514044661,497.8020726681734,497.00322803618496,496.2002790093205,495.3932906062968,494.58233147485805,493.7674740030779,492.948794430643,492.1263729599274,491.30029386666075,490.4706456099787,489.63752094164767,488.8010170142402,487.9612354880361,487.1182826364163,486.2722694495088,485.4233117358418,484.57153022175254,483.7170506482948,482.86000386538143,482.0005259228946,481.13875815849104,480.27484728182344,479.40894545489914,478.5412103682876,477.67180531288847,476.80089924696995,475.92866685818024,475.05528862023755,474.1809508439997,473.30584572261455,472.4301713704504,471.55413185551106,470.6779372250324,469.80180352396843,468.92595280607037,468.05061313726867,467.1760185910714,466.30240923569676,465.43003111266364,464.5591362065706,463.68998240580254,462.82283345391,461.9579588914189,461.09563398783666,460.23613966363473,459.3797624019981,458.52679415014745,457.67753221005495,456.83227911838986,455.991342515549,455.15503500364383,454.3236739933369,453.4975815394419,452.6770841652206,451.86251267533686,451.05420195745086,450.2524907724605,449.457721533425,448.67024007323465,447.89039540111554,447.11853944809127,446.3550268015517,445.60021442911125,444.8544613919699,444.11812854802486,443.3915782450114,442.6751740039861,441.96928019350287,441.2742616948575,440.59048355882345,439.9183106543232,439.25810730952435,438.6102369458749,437.9750617056313,437.3529420734634,436.7442364927512,436.14930097722254,435.56848871860615,435.00214969100716,434.45063025273805,433.9142727463587,433.39341509771185,432.8883904147528,432.3995265869941,431.92714588640735,431.4715645706303,431.03309248934505,430.6120326947013,430.20868105665875,429.82332588413425,429.4562475528297,429.1077181406206,428.7780010713709,428.4673507680346,428.176012315887,427.9042211367124,427.6522026747534,427.42017209520264,427.20833399598877,427.0168821335795,426.84599916348736,426.69585639612797,426.5666135686397,426.4584186332293,426.3714075625636,426.3057041726773,426.26141996381784,426.23865397959474,426.23749268474626,426.25800986178155,426.30026652669943,426.3643108639259,426.450178180556,426.55789087992576,426.6874584544819,426.83887749786,427.0121317360208,427.2071920772431,427.42401668071096,427.6625510433826,427.922728104776,428.2044683692543,428.50768004534876,428.83225920161027,429.17808993843926,429.54504457530294,429.93298385271254,430.34175714830246,430.7712027063183,431.2211478797991,431.69140938471327,432.1817935652876,432.69209666975667,433.22210513574055,433.77159588445784,434.3403366229672,434.928086153636,435.5345946900307,436.1596041784284,436.8028486241592,437.46405442199716,438.14294068982895,438.83921960485156,439.55259674155803,440.28277141080235,441.0294369992474,441.7922813085293,442.57098689349766,443.36523139891506,444.174687894031,444.9990252044752,445.8379082409434,446.6909983241841,447.55795350582133,448.4384288845864,449.33207691755945,450.23854772605495,451.15748939582113,452.0885482712472,453.0313692433124,453.98559603103433,454.9508714562088,455.926837711259,456.91313662004313,457.9094098914941,458.9152993659913,459.9304472543924,460.9544963696716,461.98709035113706,463.0278738812203,464.0764928948489,465.132594781435,466.1958285795247,467.265845164174,468.3422974271282,469.4248404498951,470.5131316698163,471.60683103924646,472.7056011779662,473.8091075189561,474.9170184476703,476.02900543495116,477.1447431637341,478.26390964969113,479.3861863559692,480.511258302176,481.63881416777286,482.7685463900275,483.9001512566855,485.03332899351284,486.16778384686245,487.3032241614184,488.439362453259,489.57591547838734,490.71260429686964,491.84915433271397,492.98529542962604,494.1207619027663,495.2552925866317,496.3886308791821,497.5205247823211,498.65072693884406,499.77899466595125,500.90508998543015,502.0287796505964,503.1498351700842,504.26803282857145,505.383153704518,506.4949836849935,507.6033134776671,508.7079386200263,509.8086594858905,510.90528128927787,511.99761408568656,513.0854727708426,514.1686770769682,515.2470515666197,516.3204256241451,517.3886334448032,518.4515140215932,519.5089111298344,520.5606733095407,521.6066538456284,522.6467107459994,523.680706717539,524.7085091400678,525.729990038288,526.7450260517634,527.7534984029714,528.7552928634708,529.7502997182224,530.7384137281064,531.7195340906782,532.6935643992048,533.6604126000274,534.6199909482934,535.5722159621057,536.5170083751339,537.4542930877383,538.3839991166537,539.3060595432854,540.220411460666,541.1269959191279,542.0257578707447,542.9166461125933,543.7996132288972,544.6746155321003,545.5416130029353,546.4005692295368,547.2514513456664,548.0942299680995,548.9288791332386,549.7553762330123,550.5737019501162,551.3838401926613,552.1857780282847,552.9795056177852,553.7650161483439,554.542305766388,555.3113735101581,556.0722212420362,556.8248535806961,557.5692778331284,558.3055039266017,559.033544340614,559.7534140388884,560.4651304014699,561.1687131569721,561.8641843150306,562.5515680990086,563.2308908790088,563.9021811052357,564.5654692417562,565.2207877007042,565.8681707769722,566.5076545834313,567.1392769867209,567.7630775436468,568.3790974382234,568.9873794193984,569.5879677394901,570.180908093373,570.7662475584397,571.344034535368,571.9143186897219,572.477150894409,573.0325831730211,573.5806686440772,574.1214614661895,574.6550167841743,575.1813906761186,575.7006401014233,576.2128228498352,576.7179974914771,577.2162233278917],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[321.028641796254,321.0286418349869,321.02864187444084,321.02864191462936,321.02864195556606,321.02864199726486,321.0286420397401,321.028642083006,321.02864212707743,321.0286421719694,321.0286422176972,321.0286422642763,321.02864231172265,321.0286423600524,321.028642409282,321.02864245942817,321.028642510508,321.0286425625389,321.02864261553856,321.028642669525,321.0286427245167,321.0286427805324,321.02864283759106,321.0286428957121,321.0286429549154,321.0286430152211,321.02864307664976,321.02864313922225,321.02864320296,321.02864326788455,321.02864333401806,321.0286434013831,321.0286434700027,321.02864353990003,321.028643611099,321.02864368362395,321.02864375749954,321.0286438327509,321.0286439094036,321.02864398748403,321.0286440670185,321.02864414803423,321.0286442305588,321.0286443146204,321.0286444002476,321.02864448746965,321.0286445763162,321.0286446668177,321.02864475900475,321.0286448529088,321.0286449485622,321.0286450459973,321.02864514524714,321.0286452463459,321.0286453493279,321.02864545422824,321.02864556108267,321.02864566992776,321.02864578080045,321.02864589373877,321.02864600878104,321.02864612596653,321.0286462453354,321.02864636692806,321.0286464907863,321.02864661695213,321.02864674546873,321.02864687638,321.0286470097305,321.02864714556574,321.0286472839321,321.0286474248769,321.02864756844815,321.0286477146949,321.02864786366695,321.02864801541534,321.02864816999187,321.02864832744916,321.0286484878411,321.02864865122245,321.028648817649,321.02864898717763,321.02864915986623,321.02864933577376,321.02864951496036,321.0286496974873,321.02864988341685,321.02865007281264,321.0286502657393,321.0286504622629,321.02865066245056,321.02865086637075,321.0286510740931,321.0286512856888,321.02865150123006,321.02865172079066,321.02865194444576,321.0286521722718,321.0286524043468,321.02865264075007,321.0286528815626,321.0286531268668,321.02865337674666,321.0286536312876,321.02865389057695,321.02865415470336,321.0286544237574,321.02865469783114,321.02865497701856,321.02865526141517,321.02865555111856,321.028655846228,321.0286561468446,321.0286564530714,321.02865676501347,321.0286570827777,321.0286574064732,321.0286577362109,321.028658072104,321.02865841426774,321.0286587628194,321.0286591178789,321.02865947956803,321.02865984801093,321.0286602233342,321.02866060566674,321.02866099513994,321.0286613918876,321.02866179604615,321.0286622077545,321.0286626271542,321.0286630543896,321.0286634896075,321.02866393295767,321.02866438459284,321.02866484466836,321.02866531334263,321.0286657907771,321.0286662771362,321.0286667725875,321.0286672773018,321.02866779145296,321.02866831521834,321.0286688487785,321.02866939231774,321.02866994602346,321.02867051008667,321.0286710847023,321.0286716700688,321.02867226638824,321.0286728738667,321.02867349271435,321.028674123145,321.02867476537676,321.0286754196317,321.0286760861365,321.02867676512165,321.0286774568225,321.0286781614786,321.02867887933405,321.0286796106379,321.0286803556436,321.0286811146097,321.0286818877995,321.0286826754816,321.0286834779294,321.0286842954219,321.02868512824296,321.0286859766825,321.02868684103544,321.0286877216027,321.02868861869075,321.0286895326122,321.0286904636853,321.0286914122348,321.02869237859153,321.0286933630927,321.0286943660819,321.02869538790947,321.0286964289325,321.028697489515,321.0286985700279,321.0286996708494,321.0287007923648,321.02870193496716,321.028703099057,321.02870428504247,321.0287054933399,321.0287067243735,321.02870797857565,321.0287092563874,321.02871055825824,321.02871188464627,321.0287132360186,321.02871461285145,321.02871601563044,321.02871744485054,321.0287189010163,321.0287203846424,321.0287218962533,321.02872343638387,321.0287250055794,321.0287266043961,321.0287282334007,321.0287298931715,321.0287315842979,321.0287333073808,321.02873506303325,321.02873685188024,321.0287386745591,321.0287405317198,321.028742424025,321.02874435215074,321.0287463167862,321.0287483186345,321.02875035841254,321.02875243685156,321.0287545546973,321.02875671271045,321.0287589116669,321.0287611523578,321.0287634355904,321.02876576218796,321.0287681329903,321.02877054885425,321.02877301065354,321.02877551927975,321.02877807564226,321.02878068066894,321.02878333530634,321.02878604052006,321.0287887972955,321.02879160663787,321.02879446957274,321.02879738714677,321.02880036042774,321.02880339050535,321.0288064784914,321.0288096255207,321.02881283275104,321.02881610136416,321.028819432566,321.02882282758736,321.02882628768447,321.0288298141393,321.02883340826065,321.02883707138426,321.0288408048736,321.0288446101205,321.02884848854575,321.02885244159995,321.0288564707637,321.028860577549,321.0288647634991,321.02886903019015,321.0288733792312,321.02887781226536,321.02888233097025,321.0288869370592,321.02889163228195,321.0288964184252,321.02890129731384,321.02890627081166,321.0289113408224,321.0289165092903,321.0289217782017,321.02892714958546,321.0289326255142,321.02893820810533,321.02894389952206,321.0289497019747,321.0289556177214,321.02896164906963,321.0289677983772,321.02897406805374,321.02898046056134,321.0289869784166,321.0289936241914,321.0290004005146,321.029007310073,321.02901435561324,321.0290215399431,321.02902886593284,321.0290363365169,321.02904395469574,321.02905172353695,321.0290596461772,321.02906772582423,321.0290759657581,321.0290843693337,321.02909293998187,321.0291016812119,321.0291105966134,321.02911968985813,321.02912896470247,321.0291384249891,321.02914807465004,321.02915791770795,321.02916795827923,321.0291782005761,321.02918864890944,321.02919930769093,321.0292101814358,321.0292212747662,321.029232592413,321.0292441392193,321.02925592014356,321.029267940262,321.0292802047725,321.02929271899745,321.0293054883871,321.0293185185231,321.0293318151223,321.0293453840398,321.0293592312734,321.02937336296725,321.0293877854155,321.0294025050671,321.02941752852934,321.029432862573,321.0294485141362,321.02946449032953,321.0294807984405,321.02949744593894,321.0295144404816,321.02953178991777,321.02954950229457,321.0295675858627,321.029586049082,321.0296049006276,321.0296241493959,321.02964380451107,321.0296638753315,321.0296843714566,321.02970530273353,321.0297266792649,321.0297485114154,321.0297708098203,321.02979358539267,321.0298168493318,321.0298406131315,321.029864888589,321.0298896878136,321.02991502323584,321.0299409076175,321.029967354061,321.0299943760195,321.0300219873078,321.03005020211265,321.03007903500463,321.0301085009486,321.0301386153167,321.0301693938999,321.0302008529211,321.03023300904806,321.0302658794069,321.0302994815961,321.03033383370115,321.0303689543092,321.03040486252434,321.030441577984,321.0304791208748,321.03051751194994,321.03055677254645,321.03059692460357,321.03063799068116,321.03067999397916,321.03072295835744,321.0307669083568,321.03081186921963,321.03085786691247,321.03090492814863,321.0309530804113,321.03100235197843,321.0310527719474,321.0311043702609,321.03115717773414,321.0312112260822,321.0312665479484,321.0313231769345,321.0313811476305,321.0314404956469,321.03150125764665,321.0315634713795,321.0316271757164,321.0316924106853,321.03175921750926,321.0318276386437,321.03189771781706,321.0319695000714,321.0320430318051,321.0321183608162,321.0321955363487,321.0322746091383,321.03235563146166,321.0324386571859,321.0325237418206,321.03261094257124,321.03270031839423,321.0327919300543,321.03288584018327,321.03298211334095,321.0330808160785,321.0331820170033,321.0332857868462,321.0333921985312,321.03350132724745,321.03361325052305,321.03372804830235,321.03384580302514,321.0339665997081,321.0340905260304,321.0342176724203,321.03434813214636,321.0344820014101,321.0346193794433,321.03476036860724,321.03490507449584,321.03505360604237,321.0352060756292,321.0353625992015,321.0355232963846,321.03568829060544,321.0358577092175,321.0360316836306,321.0362103494441,321.03639384658544,321.0365823194522,321.0367759170596,321.03697479319266,321.03717910656303,321.03738902097103,321.03760470547354,321.0378263345562,321.0380540883124,321.03828815262733,321.0385287193681,321.03877598658045,321.03903015869093,321.03929144671633,321.0395600684797,321.03983624883284,321.04012021988655,321.04041222124755,321.04071250026357,321.0410213122755,321.0413389208784,321.0416655981897,321.0420016251269,321.0423472916931,321.0427028972721,321.0430687509321,321.0434451717395,321.04383248908186,321.04423104300133,321.04464118453774,321.0450632760829,321.0454976917451,321.0459448177255,321.0464050527049,321.04687880824326,321.0473665091903,321.0478685941092,321.04838551571294,321.0489177413125,321.04946575328,321.05003004952414,321.05061114398035,321.0512095671149,321.05182586644395,321.05246060706725,321.0531143722174,321.0537877638249,321.0544814030991,321.0551959311255,321.05593200948056,321.0566903208622,321.0574715697394,321.05827648301914,321.05910581073095,321.0599603267311,321.0608408294252,321.06174814251045,321.0626831157374,321.0636466256923,321.064639576599,321.0656629011431,321.066717561316,321.0678045492808,321.0689248882609,321.07007963344967,321.07126987294373,321.07249672869824,321.07376135750565,321.0750649519987,321.0764087416757,321.0777939939514,321.07922201523104,321.0806941520094,321.0822117919951,321.08377636525853,321.0853893454065,321.08705225078086,321.0887666456838,321.09053414162764,321.09235639861083,321.09423512641945,321.0961720859545,321.0981690905847,321.1002280075252,321.1023507592414,321.10453932487866,321.10679574171667,321.10912210664907,321.11152057768817,321.1139933754933,321.1165427849234,321.11917115661356,321.12188090857427,321.1246745278128,321.12755457197744,321.13052367102165,321.1335845288898,321.1367399252215,321.13999271707496,321.143345840668,321.14680231313565,321.15036523430257,321.15403778847013,321.1578232462159,321.1617249662043,321.16574639700724,321.16989107893244,321.17416264585796,321.1785648270714,321.1831014491109,321.18777643760717,321.19259381912207,321.1975577229847,321.20267238311897,321.2079421398622,321.21337144177204,321.21896484741757,321.224727027152,321.23066276486566,321.2367769597122,321.2430746278088,321.24956090390344,321.2562410430077,321.26312042199004,321.27020454112636,321.2774990256032,321.285009626969,321.2927422245306,321.3007028266881,321.3088975722056,321.31733273141134,321.3260147073241,321.3349500366983,321.3441453909854,321.35360757720434,321.3633435387168,321.37336035590045,321.38366524671693,321.39426556716575,321.4051688116217,321.4163826130471,321.42791474307523,321.43977311195744,321.45196576836923,321.4645008990682,321.47738682839946,321.4906320176405,321.504245064182,321.5182347005363,321.5326097931697,321.5473793411519,321.5625524746166,321.5781384530295,321.5941466632555,321.61058661742277,321.6274679505769,321.6448004181208,321.6625938930358,321.68085836287986,321.6996039265581,321.71884079086266,321.7385792667786,321.75882976555096,321.77960279451366,321.80090895267443,321.82275892605645,321.8451634827937,321.8681334679804,321.89167979827374,321.915813456249,321.94054548450964,321.96588697955275,321.99184908539115,322.0184429869361,322.0456799031431,322.0735710799238,322.1021277828311,322.13136128952027,322.1612828819932,322.19190383863247,322.22323542603345,322.2552888906412,322.2880754502031,322.3216062850453,322.3558925291851,322.3909452612905,322.42677549549757,322.46339417210083,322.5008121481293,322.53904018782197,322.57808895302014,322.6179689934898,322.6586907371927,322.70026448052323,322.74270037852756,322.78600843512555,322.8301984933531,322.8752802256454,322.92126312418037,322.96815649130394,323.0159694300576,323.0647108348282,323.114389382144,323.16501352163715,323.216591467194,323.26913118831726,323.3226404017193,323.3771265631709,323.43259685962613,323.489058201645,323.546517216135,323.60498023943364,323.66445331075107,323.7249421659935,323.7864522319873,323.8489886211214,323.9125561264273,323.97715921711324,324.04280203456824,324.1094883888531,324.17722175569065,324.24600527396996,324.31584174377605,324.38673362495496,324.45868303622575,324.53169175484555,324.6057612168348,324.68089251776945,324.75708641414155,324.8343433252925,324.91266333591796,324.9920461991455,325.0724913401798,325.15399786051387,325.2365645426986,325.32018985566407,325.4048719605839,325.49060871727,325.5773976910887,325.66523616038046,325.75412112437095,325.8440493115547,325.93501718853344,326.0270209692886,326.12005662486814,326.21411989346274,326.30920629085017,326.4053111211798,326.5024294880744,326.6005563060184,326.69968631200715,326.79981407742696,326.90093402013594,327.00304041671535,327.10612741486085,327.2101890458802,327.315219237268,327.4212118253225,327.5281605677739,327.636059156391,327.7449012295321,327.8546803846107,327.9653901904403,328.07702419942996,328.189575959595,328.30303902635586,328.4174069740924,328.5326734074242,328.64883197218916,328.7658763660905,328.8838003489883,329.00259775280585,329.1222624910293,329.2427885677753,329.3641700864052,329.4864012576642,329.609476407327,329.7333899833307,329.8581365623794,329.98371085600553,330.11010771607397,330.2373221397171,330.3653492736925,330.4941844181526,330.6238230298211,330.7542607245706,330.88549327939944,331.0175166338036,331.15032689054766,331.28392031583326,331.41829333887074,331.5534425508596,331.6893647033823,331.8260567062239,331.9635156246241,332.10173867597587,332.24072322598306,332.38046678429197,332.5209669996135,332.66222165435255,332.80422865876466,332.9469860446597,333.0904919586739,333.2347446551324,333.37974248852714,333.5254839056326,333.6719674372873,333.81919168986514,333.96715533646505,334.11585710784607,334.26529578313637,334.415470180345,334.5663791467056,334.71802154888144,334.8703962630626,335.0235021649838,335.17733811989405,335.3319029725082,335.48719553697026,335.6432145868584,335.79995884526033,335.95742697495086,336.11561756869753,336.27452913972536,336.4341601123662,336.59450881292264,336.7555734607705,336.9173521597273,337.0798428897112,337.2430434987147,337.40695169511605,337.5715650403501,337.7368809419616,337.9028966470581,338.06960923618414,338.2370156176326,338.40511252221063,338.5738964984751,338.74336390845207,338.91351092385224,339.0843335227947,339.2558274870484,339.4279883998013,339.6008116439624,339.7742924010058,339.94842565035964,340.12320616934323,340.2986285336563,340.4746871184199,340.6513760997688,340.8286894569936,341.006620975231,341.1851642486965,341.3643126844547,341.54405950672185,341.7243977616913,341.9053203228758,342.08681989695384,342.26888903011246,342.4515201148729,342.6347053973871,342.81843698519276,343.00270685540994,343.1875068633682,343.372828751644,343.5586641594961,343.745004632678,343.93184163361263,344.119166551909,344.30697071520234,344.49524540029904,344.6839818446053,344.87317125781925,345.06280483386735,345.2528737630611,345.4433692444554,345.6342824983852,345.82560477915825,346.01732738788286,346.20944168540717,346.40193910534686,346.59481116718024,346.7880494893854,346.98164580259794,347.1755919627653,347.3698799642747,347.5645019530309,347.7594502394612,347.95471731142277,348.1502958469904,348.3461787271004,348.5423590480277,348.73883013367185,348.9355855476298,349.13261910503155,349.3299248841159,349.52749723752237,349.7253308032784,349.9234205154565,350.12176161448093,350.3203496570604,350.51918052572535,350.71825043794723,350.91755595481834,351.1170939892714,351.3168618138171,351.51685706777965,351.7170777640085,351.9175222950488,352.1181894387492,352.3190783632872,352.52018863159714,352.7215202051788,352.923073447271,353.1248491253741,353.3268484131038,353.52907289136164,353.73152454880716,353.9342057816183,354.13711939252534,354.3402685891088,354.5436569813473,354.747288578406,354.951167784657,355.1552993949213,355.3596885889275,355.56434092497864,355.76926233282416,355.9744591057314,356.1799378917557,356.38570568420647,356.59176981131054,356.798137925073,357.0048179893394,357.2118182670632,357.41914730678417,357.62681392832536,357.83482720771724,358.04319646135997,358.2519312294348,358.46104125857994,358.6705364838445,358.8804270099388,359.09072309179874,359.30143511448625,359.5125735724453,359.72414904814053,359.93617219010014,360.14865369039455,360.3616042615762,360.57503461311245,360.7889554273441,361.00337733500265,361.21831089032275,361.43376654578486,361.64975462652944,361.8662853044802,362.0833685722193,362.30101421665705,362.5192317925397,362.73803059584134,362.9574196370865,363.17740761465,363.3980028880848,363.6192134515247,363.84104690721495,364.06351043921995,364.28661078736053,364.510354221433,364.7347465157622,364.9597929241421,365.18549815521595,365.4118663483501,365.6389010500524,365.86660519099024,366.0949810636585,366.3240303007498,366.55375385427715,366.78415197550044,367.0152241957036,367.2469693078731,367.4793853493215,367.71246958530327,367.946218493665,368.18062775057314,368.4156922173572,368.651405928509,368.8877620808705,369.1247530240471,369.3623702520733,369.60060439636374,369.8394452199719,370.0788816131808,370.3189015904465,370.55949228871043,370.8006399670962,371.0423300079999,371.2845469195848,371.52727433968226,371.7704950411025,372.0141909383518,372.25834309575254,372.5029317369562,372.7479362558386,372.9933352287616,373.2391064281834,373.48522683759523,373.7316726677608,373.9784193742286,374.2254416760883,374.4727135759352,374.72020838100747,374.9678987254558,375.2157565937027,375.4637533448471,375.71185973806786,375.96004595897386,376.2082816468534,376.4565359227629,376.7047774184077,376.9529743057494,377.2010943272878,377.4491048269539,377.6969727815538,377.94466483270344,378.1921473191889,378.4393863096907,378.68634763580917,378.9329969253259,379.17929963563813,379.42522108730384,379.67072649763304,379.915781014263,380.1603497486572,380.40439780946565,380.6478903356869,380.89079252957504,381.13306968923223,381.37468724083254,381.61561077042336,381.8558060552511,382.0952390945616,382.3338761398279,382.5716837243568,382.8086286922325,383.0446782265554,383.27979987693425,383.5139615861989,383.74713171629486,383.97927907333076,384.21037293174845,384.44038305758664,384.6692797308185,384.8970337667357,385.1236165363651,385.3489999858971,385.5731566551146,385.79605969480923,386.01768288317646,386.2380006411827,386.4569880469004,386.6746208488087,386.89087547805974,387.10572905971446,387.3191594229501,387.53114511024916,387.7416653855747,387.95070024154586,388.15823040562464,388.3642373453261,388.5687032724709,388.7716111464944,388.9729446768331,389.1726883244057],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[566.2689218519553,566.2689234932196,566.2689251650282,566.2689268679488,566.2689286025609,566.2689303694543,566.2689321692294,566.2689340024986,566.2689358698846,566.2689377720228,566.2689397095596,566.2689416831545,566.2689436934775,566.2689457412125,566.2689478270563,566.2689499517171,566.2689521159181,566.2689543203946,566.2689565658967,566.2689588531873,566.2689611830442,566.2689635562597,566.2689659736408,566.268968436009,566.2689709442019,566.2689734990719,566.2689761014883,566.2689787523353,566.268981452514,566.2689842029436,566.2689870045577,566.26898985831,566.2689927651704,566.268995726127,566.2689987421867,566.269001814375,566.2690049437366,566.269008131335,566.2690113782544,566.2690146855983,566.2690180544919,566.2690214860799,566.2690249815291,566.2690285420281,566.2690321687876,566.2690358630406,566.269039626043,566.2690434590742,566.2690473634373,566.26905134046,566.2690553914941,566.2690595179174,566.2690637211323,566.2690680025685,566.2690723636813,566.269076805953,566.2690813308947,566.2690859400444,566.2690906349692,566.2690954172651,566.2691002885587,566.2691052505055,566.2691103047929,566.2691154531387,566.2691206972943,566.2691260390415,566.2691314801974,566.269137022611,566.2691426681671,566.2691484187854,566.2691542764204,566.2691602430639,566.2691663207445,566.2691725115284,566.2691788175204,566.269185240864,566.2691917837435,566.2691984483837,566.2692052370497,566.2692121520495,566.2692191957345,566.2692263704989,566.2692336787823,566.2692411230694,566.2692487058908,566.2692564298246,566.2692642974966,566.2692723115817,566.2692804748046,566.2692887899403,566.2692972598156,566.2693058873107,566.2693146753576,566.269323626945,566.2693327451154,566.2693420329691,566.2693514936633,566.2693611304147,566.2693709464991,566.2693809452537,566.2693911300778,566.2694015044339,566.2694120718485,566.2694228359144,566.2694338002909,566.2694449687053,566.2694563449544,566.2694679329057,566.2694797364986,566.2694917597457,566.2695040067342,566.2695164816278,566.2695291886669,566.2695421321718,566.2695553165421,566.2695687462597,566.2695824258906,566.2695963600845,566.2696105535786,566.2696250111976,566.2696397378563,566.2696547385609,566.2696700184108,566.2696855826,566.2697014364195,566.269717585258,566.2697340346058,566.2697507900541,566.2697678572985,566.2697852421406,566.2698029504903,566.2698209883669,566.2698393619017,566.2698580773406,566.2698771410447,566.2698965594949,566.2699163392914,566.2699364871576,566.2699570099419,566.2699779146207,566.2699992082993,566.2700208982155,566.270042991742,566.2700654963883,566.270088419804,566.2701117697804,566.2701355542542,566.2701597813096,566.2701844591812,566.2702095962571,566.2702352010804,566.2702612823543,566.2702878489434,566.2703149098768,566.2703424743523,566.2703705517381,566.2703991515772,566.2704282835896,566.2704579576766,566.2704881839231,566.2705189726024,566.270550334178,566.270582279308,566.2706148188496,566.2706479638615,566.2706817256073,566.2707161155615,566.2707511454109,566.2707868270603,566.2708231726351,566.2708601944872,566.2708979051977,566.2709363175813,566.2709754446919,566.271015299825,566.2710558965244,566.2710972485856,566.27113937006,566.2711822752608,566.2712259787669,566.2712704954292,566.2713158403737,566.2713620290083,566.2714090770269,566.271457000416,566.2715058154583,566.2715555387401,566.2716061871556,566.2716577779136,566.271710328542,566.271763856895,566.271818381159,566.2718739198586,566.271930491862,566.2719881163888,566.2720468130157,566.2721066016833,566.2721675027029,566.2722295367632,566.2722927249379,566.2723570886917,566.2724226498892,566.2724894308004,566.2725574541099,566.2726267429238,566.2726973207772,566.2727692116433,566.2728424399404,566.2729170305404,566.2729930087779,566.273070400458,566.2731492318652,566.2732295297728,566.2733113214515,566.2733946346784,566.2734794977467,566.273565939476,566.2736539892204,566.27374367688,566.2738350329103,566.2739280883324,566.2740228747432,566.2741194243279,566.2742177698682,566.2743179447557,566.2744199830015,566.2745239192487,566.2746297887842,566.2747376275495,566.2748474721541,566.2749593598874,566.2750733287311,566.275189417372,566.2753076652157,566.2754281123995,566.275550799805,566.2756757690746,566.2758030626214,566.2759327236486,566.276064796159,566.2761993249735,566.2763363557451,566.2764759349733,566.2766181100212,566.2767629291312,566.2769104414405,566.2770606969983,566.2772137467826,566.2773696427172,566.2775284376892,566.2776901855674,566.2778549412195,566.2780227605315,566.2781937004261,566.2783678188807,566.2785451749494,566.2787258287816,566.2789098416414,566.2790972759293,566.279288195203,566.2794826641981,566.2796807488518,566.2798825163223,566.2800880350137,566.2802973745975,566.2805106060368,566.28072780161,566.2809490349348,566.2811743809933,566.281403916157,566.281637718212,566.2818758663861,566.2821184413745,566.2823655253668,566.2826172020754,566.2828735567632,566.2831346762712,566.2834006490494,566.2836715651844,566.2839475164316,566.2842285962444,566.2845148998057,566.2848065240607,566.2851035677483,566.285406131434,566.2857143175446,566.2860282304017,566.2863479762563,566.2866736633244,566.2870054018242,566.2873433040106,566.2876874842153,566.2880380588833,566.2883951466109,566.2887588681882,566.2891293466357,566.2895067072485,566.2898910776352,566.2902825877625,566.2906813699974,566.2910875591508,566.2915012925238,566.2919227099513,566.2923519538504,566.2927891692655,566.2932345039186,566.2936881082568,566.2941501355024,566.2946207417043,566.2951000857888,566.2955883296121,566.2960856380149,566.2965921788762,566.2971081231681,566.2976336450133,566.2981689217419,566.2987141339504,566.2992694655608,566.2998351038804,566.3004112396656,566.3009980671821,566.3015957842713,566.3022045924126,566.3028246967916,566.3034563063668,566.3040996339379,566.3047548962152,566.3054223138917,566.3061021117142,566.3067945185567,566.3074997674969,566.3082180958894,566.3089497454457,566.3096949623119,566.310453997149,566.3112271052134,566.3120145464421,566.3128165855353,566.313633492043,566.3144655404521,566.3153130102759,566.3161761861442,566.3170553578947,566.3179508206676,566.3188628750011,566.3197918269269,566.3207379880695,566.3217016757472,566.322683213073,566.3236829290587,566.3247011587204,566.325738243186,566.3267945298045,566.3278703722561,566.3289661306663,566.33008217172,566.3312188687784,566.3323766019975,566.3335557584498,566.334756732246,566.3359799246597,566.3372257442561,566.3384946070191,566.3397869364836,566.3411031638685,566.3424437282124,566.3438090765114,566.3451996638609,566.3466159535941,566.3480584174315,566.3495275356249,566.3510237971086,566.3525476996512,566.3540997500091,566.3556804640857,566.3572903670886,566.3589299936939,566.360599888209,566.3623006047425,566.3640327073716,566.3657967703175,566.3675933781193,566.3694231258129,566.3712866191122,566.3731844745937,566.3751173198825,566.377085793843,566.3790905467707,566.3811322405899,566.3832115490494,566.3853291579285,566.3874857652384,566.3896820814317,566.391918829614,566.3941967457581,566.3965165789216,566.398879091467,566.4012850592876,566.403735272034,566.4062305333442,566.4087716610801,566.4113594875629,566.4139948598151,566.4166786398055,566.4194117046966,566.4221949470963,566.425029275313,566.427915613615,566.4308549024914,566.4338480989197,566.4368961766336,566.4400001263972,566.4431609562828,566.4463796919492,566.4496573769279,566.452995072911,566.4563938600415,566.4598548372111,566.4633791223575,566.4669678527696,566.4706221853921,566.4743432971378,566.4781323852023,566.4819906673811,566.4859193823924,566.4899197902034,566.4939931723592,566.4981408323173,566.5023640957835,566.5066643110544,566.5110428493612,566.515501105218,566.5200404967749,566.5246624661734,566.5293684799051,566.534160029176,566.5390386302726,566.5440058249312,566.5490631807133,566.5542122913811,566.5594547772778,566.564792285712,566.5702264913456,566.5757590965803,566.5813918319552,566.5871264565393,566.5929647583322,566.5989085546656,566.6049596926082,566.6111200493713,566.6173915327206,566.6237760813864,566.6302756654783,566.6368922869011,566.6436279797736,566.6504848108481,566.6574648799317,566.66457032031,566.6718032991703,566.6791660180277,566.6866607131509,566.6942896559893,566.7020551536001,566.7099595490745,566.7180052219672,566.7261945887226,566.7345301030999,566.7430142566006,566.7516495788924,566.7604386382314,566.7693840418839,566.7784884365456,566.7877545087582,566.7971849853227,566.8067826337104,566.8165502624696,566.8264907216289,566.8366069030953,566.8469017410481,566.8573782123268,566.8680393368141,566.8788881778116,566.8899278424093,566.9011614818469,566.9125922918689,566.9242235130693,566.9360584312271,566.9481003776358,566.9603527294165,566.9728189098252,566.9855023885455,566.9984066819703,567.0115353534699,567.0248920136464,567.0384803205726,567.0523039800172,567.066366745652,567.0806724192432,567.095224850824,567.1100279388494,567.1250856303291,567.140401920942,567.1559808551284,567.1718265261585,567.1879430761777,567.2043346962273,567.2210056262406,567.2379601550089,567.2552026201222,567.27273740788,567.2905689531708,567.3087017393209,567.3271402979099,567.3458892085528,567.3649530986452,567.3843366430718,567.4040445638807,567.4240816299117,567.4444526563892,567.465162504471,567.4862160807522,567.5076183367263,567.5293742681977,567.5514889146474,567.5739673585489,567.5968147246322,567.6200361790951,567.6436369287629,567.6676222201879,567.6919973386947,567.7167676073661,567.7419383859672,567.7675150698079,567.793503088543,567.8199079049044,567.846735013367,567.8739899387488,567.9016782347362,567.9298054823403,567.958377288281,567.9873992832921,568.0168771203536,568.0468164728449,568.0772230326186,568.1081025079925,568.139460621661,568.1713031085219,568.2036357134175,568.236464188792,568.2697942922575,568.3036317840766,568.3379824245493,568.3728519713145,568.4082461765571,568.4441707841233,568.4806315265424,568.5176341219553,568.5551842709483,568.5932876532918,568.6319499245838,568.6711767127986,568.7109736147398,568.7513461923971,568.7922999692091,568.8338404262302,568.8759729982039,568.9187030695429,568.962035970215,569.0059769715388,569.0505312818884,569.0957040423092,569.1415003220454,569.187925113985,569.2349833300169,569.2826797963116,569.3310192485204,569.3800063268994,569.4296455713624,569.4799414164614,569.5308981863058,569.5825200894146,569.6348112135149,569.6877755202837,569.741416840044,569.7957388664144,569.8507451509205,569.9064390975759,569.962823957431,570.0199028231054,570.077678623303,570.1361541173187,570.1953318895461,570.25521434399,570.3158036987921,570.3771019807804,570.4391110200467,570.5018324445625,570.5652676748449,570.6294179186727,570.6942841658737,570.7598671831823,570.8261675091846,570.893185449355,570.9609210711999,571.0293741995138,571.0985444117624,571.1684310336003,571.2390331345366,571.3103495237567,571.3823787461141,571.4551190783014,571.5285685252112,571.602724816501,571.6775854033693,571.7531474555573,571.8294078585863,571.9063632112399,571.9840098233061,572.0623437135854,572.1413606081785,572.2210559390642,572.301424842974,572.3824621605803,572.4641624359977,572.5465199166181,572.6295285532793,572.7131820007792,572.7974736187456,572.8823964728653,572.967943336481,573.0541066925656,573.1408787360732,573.2282513766797,573.3162162419119,573.4047646806721,573.4938877671605,573.5835763051999,573.6738208329604,573.7646116280912,573.8559387132532,573.9477918620587,574.0401606054111,574.1330342382463,574.2264018266718,574.3202522154975,574.4145740361578,574.5093557150151,574.6045854820411,574.700251379867,574.7963412731954,574.8928428585626,574.9897436744458,575.0870311116979,575.1846924243047,575.2827147404458,575.3810850738527,575.4797903354417,575.5788173452149,575.6781528444072,575.7777835078655,575.8776959566449,575.9778767708001,576.0783125023551,576.1789896884356,576.2798948645392,576.3810145779264,576.4823354011131,576.5838439454404,576.6855268747039,576.7873709188183,576.8893628874979,576.9914896839304,577.0937383184195,577.1960959219786,577.2985497598498,577.4010872449282,577.5036959510668,577.6063636262444,577.7090782055697,577.8118278241035,577.914600829477,578.0173857942832,578.1201715282251,578.2229470899956,578.3257017988733,578.4284252460129,578.5311073054131,578.633738144543,578.7363082346097,578.8388083604524,578.9412296300432,579.0435634835858,579.1458017021902,579.2479364161169,579.3499601125725,579.4518656430506,579.5536462302014,579.6552954742244,579.7568073587735,579.858176256367,579.959396933296,580.0604645540241,580.1613746850736,580.2621232983969,580.3627067742252,580.463121903398,580.5633658891674,580.6634363484806,580.7633313127419,580.8630492280531,580.9625889549377,581.0619497675526,581.1611313523916,581.2601338064874,581.3589576351164,581.4576037490182,581.5560734611324,581.6543684828688,581.7524909199138,581.8504432675909,581.9482284057798,582.0458495934134,582.1433104625602,582.2406150121077,582.3377676010617,582.4347729414759,582.5316360910252,582.6283624452417,582.7249577294261,582.8214279902561,582.9177795871011,583.0140191830699,583.1101537358005,583.2061904880157,583.3021369578607,583.3980009290384,583.4937904407657,583.5895137775651,583.6851794589123,583.7807962287568,583.8763730449352,583.9719190684943,584.0674436529436,584.1629563334538,584.2584668160201,584.3539849666078,584.4495208002957,584.5450844704384,584.64068625786,584.7363365600975,584.832045880709,584.9278248186636,585.0236840578268,585.1196343565562,585.2156865374207,585.3118514770593,585.4081400961907,585.5045633497845,585.6011322174112,585.6978576937757,585.7947507794514,585.8918224718194,585.9890837562284,586.0865455973776,586.184218930938,586.2821146554144,586.3802436242588,586.4786166382398,586.577244438076,586.676137697338,586.775307015623,586.8747629120093,586.9745158187919,587.074576075504,587.1749539232275,587.2756594991941,587.3767028316802,587.4780938351953,587.579842305968,587.681957917727,587.7844502177801,587.8873286233886,587.9906024184398,588.0942807504136,588.1983726276441,588.3028869168735,588.4078323410988,588.5132174777057,588.6190507568895,588.7253404603604,588.8320947203313,588.9393215187794,589.0470286869864,589.1552239053472,589.2639147034455,589.3731084603917,589.4828124054192,589.5930336187331,589.7037790326075,589.8150554327248,589.9268694597525,590.0392276111527,590.1521362432132,590.2656015733016,590.3796296823274,590.4942265174119,590.6093978947538,590.7251495026856,590.8414869049103,590.9584155439135,591.0759407445372,591.1940677177118,591.3128015643312,591.4321472792656,591.5521097554989,591.6726937883818,591.7939040799878,591.9157452435609,592.0382218080457,592.1613382226834,592.2850988616647,592.4095080288237,592.5345699623613,592.6602888395847,592.7866687816448,592.9137138582641,593.0414280924315,593.1698154650572,593.2988799195683,593.4286253664266,593.5590556875594,593.6901747406828,593.821986363501,593.9544943777682,594.0877025931931,594.2216148111731,594.3562348283396,594.4915664398956,594.6276134427361,594.7643796383278,594.9018688353352,595.0400848519773,595.1790315180994,595.3187126769415,595.4591321865944,595.6002939211222,595.7422017713412,595.884859645241,596.0282714680329,596.1724411818153,596.3173727448427,596.463070130388,596.609537325188,596.75677832746,596.9047971444854,597.0535977897475,597.2031842796191,597.353560629595,597.5047308500612,597.6566989415996,597.8094688898246,597.9630446597487,598.1174301896791,598.2726293846431,598.428646109345,598.5854841806599,598.7431473596648,598.9016393432152,599.0609637550721,599.2211241365914,599.382123936979,599.5439665031294,599.7066550690517,599.8701927449058,600.034582505655,600.1998271793556,600.3659294351008,600.5328917706348,600.7007164996594,600.8694057388522,601.0389613946215,601.2093851496179,601.3806784490293,601.5528424866848,601.7258781909928,601.8997862107448,602.0745669008081,602.2502203077421,602.4267461553668,602.6041438303133,602.7824123675924,602.9615504362094,603.1415563248626,603.3224279277567,603.5041627305671,603.6867577965884,603.8702097531043,604.0545147780124,604.2396685867385,604.4256664194795,604.6125030288059,604.800172667662,604.9886690777973,605.1779854786643,605.3681145568164,605.5590484558392,605.7507787668499,605.9432965195921,606.1365921741644,606.3306556134064,606.5254761359736,606.721042450132,606.9173426682933,607.1143643023215,607.3120942596321,607.5105188401046,607.7096237338344,607.9093940197369,608.1098141650237,608.3108680255685,608.5125388471708,608.7148092677332,608.9176613203603,609.1210764373845,609.325035455327,609.5295186207946,609.734505597312,609.9399754730917,610.1459067697341,610.3522774518526,610.5590649376152,610.7662461101936,610.9737973301034,611.1816944484233,611.3899128208749,611.598427322742,611.8072123646105,612.016241908903,612.225489487183,612.4349282182011,612.6445308266543,612.8542696626257,613.0641167216718,613.2740436655253,613.4840218433726,613.6940223136744,613.9040158664844,614.1139730462319,614.3238641749223,614.5336593757196,614.7433285968602,614.9528416358628,615.1621681639856,615.3712777508869,615.5801398894473,615.7887240207054,615.9969995588641,616.2049359163218,616.4125025286869,616.6196688797262,616.8264045262096,617.0326791226032,617.238462445573,617.4437244182523,617.6484351342376,617.8525648812722,618.056084164575,618.2589637297841,618.4611745854735,618.6626880252131,618.8634756491368,619.063509384985,619.2627615085979,619.4612046638262,619.6588118818339,619.8555565997708,620.0514126787883,620.2463544213809,620.4403565880317,620.633394413145,620.8254436202517,621.0164804364722,621.206481606226,621.3954244041774,621.583286647409,621.7700467068174,621.9556835177275,622.1401765897164,622.3235060156562,622.5056524799668,622.6865972660847,622.8663222631523,623.0448099719312,623.2220435099465,623.3980066158704,623.5726836531553,623.7460596129249,623.9181201161374,624.0888514150325,624.2582403938778],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour les paramètres de GridSearchCV\n","FigRMSEGRidRidge = visuRMSEGrid(Ridge(), 'Ridge', alphasridge, 'alpha',\n","                                GridRidge)\n","FigRMSEGRidRidge.show()\n","if write_data is True:\n","    FigRMSEGRidRidge.write_image('./Figures/EmissionsGraphRMSERidge.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.3 Modèle Lasso"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      1.0\n","         Lasso()\n","R²      0.392858\n","RMSE  399.012393\n","MAE   104.180260\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predLasso=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[57.26759911999385,37.12584695533515,56.813908771715674,1.266960232367218,22.274724008193715,35.34048572319508,45.300992152693354,85.87272275499451,150.1433867795826,40.40466300297988,115.38764901109349,62.56669139379529,48.16767168683394,137.33264277229316,46.53512203535492,-2.7959014894783323,55.891124152476266,45.22060808579339,94.13848672443018,98.05816734431232,26.81692136813185,50.19387320714895,10.409113632503747,56.808188677082796,12.198395551691306,96.59350150821982,362.17254192788744,48.38714492512023,83.68013595892866,15.524685009077253,235.77923700637734,106.54255947861114,103.35524065227756,12.694640276292034,80.93757440750983,149.93805363263124,556.274437488444,55.45016162129126,55.928249223553934,40.850848105722505,171.59165304684564,59.608830206709925,49.15359282902615,96.95381314486986,106.85658803724775,69.28357775809805,123.2189758658869,170.9904492243214,40.49397650637893,96.77268287509895,15.509360866459872,23.507392108526204,763.4598487661405,103.29427797018437,63.32281488646219,146.96923584916695,55.59966267780998,39.344315844361844,198.9801839623266,77.27361780741302,138.72354384583284,292.67082575220167,46.076146144430034,156.99279980891131,134.7739072955618,-3.7072290186318995,130.8208558099524,112.206123150331,33.747072803528724,61.67615097722306,53.16037472997495,187.91247724596928,51.79481742431281,51.7750253822541,139.3474305229705,139.24766652924953,160.26468666705557,9.601318221897479,114.82525056531962,57.14010466873931,93.99158049561277,153.39006180301243,47.16675747666647,145.11044562998447,36.8629734970731,25.894452458879876,66.7260443475445,56.35233386768449,175.0953708688885,178.97507671248763,75.54594859838204,72.59296325893803,171.98487216985802,14.941359327341353,72.87545283526703,7.568789247422487,2544.635521999229,69.628281795395,-29.548616872354664,91.07389686292913,217.8171525388907,43.92366761144442,6.6028070919843245,50.90062382205675,6.14488548317469,83.0276791845504,93.39025951983189,238.98055533194176,35.01873821723402,205.16790575495358,-12.548108040804145,51.51229217471095,36.510090826687986,32.58325004973814,13.597623906420566,70.43753630723509,34.22225437391805,604.2594288836003,221.61344339919364,133.1101306312921,29.930258218864687,3.7975010873998016,57.2029427225925,11.214621098293541,96.48321987827754,103.41205503945432,41.44184087924727,73.60042125574151,30.008865384223903,130.55869666301842,148.5730357836996,42.415696316753085,37.19353645883892,16.720034775931403,114.61361158570116,58.051487241214886,76.25518068492593,49.528519189871126,150.1433867795826,47.565315221512904,37.96565734447317,74.97832294147474,111.99504133256198,15.362013739526773,79.79178404645525,55.18978100539648,0.03021285787432504,68.87434399822135,-32.15034801649474,45.945983775896025,91.57579530836198,113.06352099452317,29.08248912530359,89.86203639697459,69.3547782571723,38.61683437596617,29.74833236072692,35.86376524484557,455.68223907579215,92.46395740958164,24.25307405403261,33.625214268165145,46.391424379598234,63.135234986019086,76.30230037251027,50.50653332537014,544.8155602623738,55.99920520456188,129.27436702003357,6.852903330548308,81.93653084455228,16.756230371331327,73.76732238671728,86.60475616538976,50.36545329498069,97.26712808882641,51.376190611575424,247.8075117556328,8.067953567184922,25.807749102248174,115.72293769272926,131.9564247444728,29.44025324757873,101.18368292182535,39.46485952492638,252.54611570497997,25.24876481524931,61.01642173843296,6.6665268460951665,16.610936930082723,92.22447627088309,-19.419005596477795,63.026907941195965,-10.30107702251248,-35.11048259566022,143.16645378596175,41.330897306981015,75.54594859838204,11.173629492129727,2.4039500127958604,383.960981585094,38.617199564789125,19.55965004293811,20.834415671665404,32.28094445982808,34.20998166619964,35.730477724653724,45.945983775896025,87.77208186324631,65.93273859992476,302.9517308497747,46.13839587511773,38.759129168909915,52.53748423770241,102.89735946355452,435.4548373141099,72.92801806036334,28.347368447500962,68.91963449469185,34.25463793487107,73.799972275242,39.570589247312036,46.021557835152805,63.84221508934361,26.542981023456,35.35500220238176,101.18498569971214,41.11437480935493,64.82177155023135,56.35233386768449,36.58130333558763,119.01401189456435,64.43770895633813,133.44478223075112,37.187590978442174,33.81694583940357,53.9159768809955,33.53736424007558,93.48476903390122,82.2497427388609,284.4510636669777,36.151991517829345,34.9305473486591,48.51309841470301,30.465049410496405,49.943181973629116,-61.05213927675701,99.2752444487404,290.8272868428854,112.60505024276648,57.213703497716196,21.158940692409026,62.76086613548339,79.47897478362219,246.17723616342101,111.8397740968964,100.83609400684753,132.92130022321567,129.27436702003357,79.93921572901364,72.01229491791182,70.15347321552292,-8.07034324317835,64.21318985835113,23.739943946826635,182.5793026987041,32.86477460901263,233.56466880310103,46.076146144430034,76.30230037251027,109.91982476346723,103.3342985074957,24.401666060728374,52.51777359252219,48.62587645235786,51.955034006025535,80.37037130775721,1088.7462425858503,-10.371945838245843,75.00659295560759,122.23035507717884,54.82689431402444,27.54167651966307,126.14033117615381,99.88796747128512,46.60758949583667,41.336262669331205,64.69255156034775,15.46254865629058,61.0633626177264,611.5970794885907,-11.559900197466511,23.27781153691842,60.02406654603984,130.33969309729176,197.16215107094064,34.57244803980046,87.35944997201744,24.918488211495735,27.52022722540584,129.7355210254356,135.6407288893884,179.91924431518896,46.75896215850748,-38.00721325856103,35.97621336167335,255.64210366878183,41.924074377905654,42.153657348927865,79.31157893169554,44.93886991328201,69.88441859897767,46.97643945757522,189.21970499072611,41.30037944622652,48.280585331321916,36.02882147682538,120.41718754437227,36.28870448347662,-19.84494213149228,150.57549932772284,15.951269403099069,47.76838373202421,61.72248954192433,81.63448642271025,36.92422211637522,66.09780402867882,94.51344473230311,433.77059236916307,47.1055458670885,72.55281010252706,8.673521092017019,57.80196625100736,82.59317994858233,95.50732912530745,101.39078431810502,305.28631111959453,14.46608739042528,48.63975161243268,37.28439524469332,23.21361441787287,31.227276447291715,39.783491417066834,58.42098859683737,35.94354779519113,27.55883010355732,-8.705582155417211,68.46371702878835,27.54167651966307,31.903317735100586,63.44347823916563,58.116143638616215,29.875669909587465,98.58712696718322,242.296614755087,333.8090388758069,21.682065718509342,260.16726705118856,29.978278697770016,269.3065449966382,4.410100635390201,33.65322345906854,59.608830206709925,123.16515898818562,34.95759931084352,41.30037944622652,255.2929572151771,89.01946322014884,255.2929572151771,12.589964516013168,9.65070788666224,69.10254518792061,229.16048401061397,31.995665836745452,75.5165061429152,42.6352794323687,107.44292489759762,-8.07034324317835,230.64535296087692,45.5867478908309,19.42915962142171,56.7692469781237,88.21101119329086,134.93802909200994,38.350485775551235,323.7982944646334,131.7400906464521,59.42121319650949,116.05837111048965,68.36212709749839,-26.18854266869564,18.412396433025897,111.38190046045179,40.83934059605419,55.473540025944246,91.5695587111664,66.52978395288656,35.730477724653724,33.747072803528724,58.42098859683737,192.93683810188207,138.63050152760732,33.305801275261125,60.03269238512995,57.43403196510502,74.41051176541563,27.331704678596516,111.21479945385647,-8.034248143694917,65.51443569677502,42.63175756518787,14.76140093660451,66.06162122722026,297.2056501636886,58.46503951668124,-29.625542741384756,5.420757790968651,83.96868252740467,165.01548592147077,54.434442843951864,72.53280031205698,45.15384591009902,43.66552640674749,76.40951713452404,59.5508166489847,89.1485056965368,1096.4049147548783,60.878678789861254,-110.8443457313027,73.5095624698871,92.60236952269162,-58.03207024910592,135.26119188293117,1652.226340336538,653.8693149259324,7.999977802401652,336.7741540807717,61.54380507631797,76.10929995012307,67.29741287413816,33.33027819086276,15.636941097358495,19.277062225727484,1.9215190906936215,215.91781913020446,74.71887421464761,108.08544708381001,24.147414926488985,79.16601141645964,146.96923584916695,78.1473852260807,53.3608945192922,31.46377668465455,55.717168293112884,126.34538805524775,58.46503951668124,13.876078652885106,98.39790583029372,45.402284215494795,24.702881722477002,64.45349128825393,50.13229971618254,276.9189933116445,6.008243185729235,160.8337280881223,34.05560910140208,58.076840055936614,33.89563802504321,38.93808439072959,106.85658803724775,23.485804618048938,65.99917906424292,477.293993291206,128.55426836395634,2.6610331165542362,38.04207386355787,284.4510636669777,144.21533328734688,56.47800997539088,88.52326696872572,50.89110901911135,102.02479608885017,54.49435414006554,57.036225725246254,46.76684830949756,66.02862264846016,162.68684284872066,8.768983270764288,232.7964061058048,65.0222913395486,32.03905858190869,47.8024193954934,34.05560910140208,36.151991517829345,94.44776185401912,193.05253184373998,58.73238415217179,46.80943154770826,657.7115840185331,28.954973714882378,28.203203309418527,3.332724956549143,9.37889730904351,92.67784093823168,-4.066326715173396,110.51722098633644,85.41952731894148,106.31526425434498,143.6503232526561,40.23519705176626,31.967166765738625,372.60157691711817,114.15213487787992,64.36456371094233,103.50142876643383,8.056623142530228,24.25307405403261,1107.2745307175612,24.974509140838073,93.10858384396137,117.11621590544624,509.07569524436906,61.67615097722306,63.53518659875137,73.45242543941583,23.549976630541877,58.624541492257066,68.28703742315,21.91366131086467,24.186974903196223,128.93210030896017,70.48064788483134,13.397620321662721,12.325685234061226,3.1931638045458186,6.566297817242557,61.01642173843296,36.614050806768915,127.12516407271069,29.679867366833165,84.07605572135594,57.38328538537021,51.336246033481466,28.526223550542703,27.08402067844905,-53.982019310310825,125.6467336642022,25.970435733028715,40.058330929708035,-28.040177506532423,290.3504720413369,32.485599979581345,48.769156408839045,90.40488638364447,-23.737112160972636,44.8642856784755,69.33516659545654,121.88514689533969,47.545663430173114,145.62409826765986,71.02033556684536,49.11222491006089,25.08282829896833,134.93802909200994,435.05907089464586,77.01705570129278,127.12516407271069,287.3264237262463,106.78926372256694,37.846897022703004,33.281817142014056,354.6248220187632,-8.244379856658988,36.819302849651315,249.9927961539429,7.35988140308482,34.20797904518005,31.227276447291715,76.02632572002169,78.69352276201977,128.36862976113372,60.22945364899479,36.22051862559815,18.412396433025897,41.74183438781785,133.1101306312921,5.126533768026583,39.98050072846597,150.3177887923768,9.492134043765432,43.819607680807934,99.06736188164606,120.96827336668926,439.8632194528615,117.48984092433729,2.181331061727697,441.11520038210983,61.99543108831049,132.8783195360556,101.71170138605305,179.22980837349292,15.30023722419972,285.048479433552,246.44493583827904,57.775512588726286,485.146310033176,64.71429407913956,257.2965650859867,47.318576063579826,3.7975010873998016,82.70910326100983,23.96266424867774,50.23911686286929,98.72779311200644,90.74874352364839,286.2638573905174,15.364870931507625,151.70309308506222,50.14710005787176,89.45593272161685,215.354493213125,28.674727105488365,101.12032930231082,48.53066077918743,33.4387365356234,19.625319385963124,60.047832772322906,58.29871078405637,61.04277817176927,42.883017794491536,358.7116431197109,37.22628944440526,28.66758237803345,142.66041431201924,36.30483878380559,80.44090157682263,74.97832294147474,25.434549228747265,110.38269116278451,96.60016846097122,37.71682736077832,888.3142107616763,44.07889223066884,-1.69207951717069,48.58635879745134,162.52174396224925,-1.7272062830988446,85.06311663022879,100.97272382076423,50.12223162805844,23.375967020815416,229.9166510263719,124.4418228327587,46.35175560809567,34.37299262939063,154.259304309503,80.53156319245058,52.41930768244434,215.26354303654568,49.38286372799052,42.66416417113152,34.99314249674647,86.96715685781862,90.59739689913931,119.74933632375439,-22.084926891273234,25.785612458316052,102.7943706956629,23.39965574166053,54.53951761477201,47.47712435293799,50.39224279423365,87.91302638939251,71.34483231640631,58.79522220602498,362.17254192788744,629.5415614466208,65.73060540151286,131.17240452169776,39.80722470703277,33.89563802504321,108.05695357500969,38.25254965881721,79.98805241702605,198.13953018885883,39.35498536728478,11.214621098293541,35.461252237543746,54.060187990461785,140.49255716099117,35.076844017522085,213.03082170989228,-18.592423555431445,91.04193961888527,69.628281795395,215.354493213125,530.2846202207689,53.84316069812768,-15.454554359880468,42.36330704394477,68.895413293742,37.59219310756123,34.97662972773851,58.22616983090203,31.246078664900168,40.75924482082769,11.117574573883623,37.517952109106886,34.92331132927929,65.6202611123216,58.076840055936614,2043.649100336197,45.68892030029704,1026.9373222689853,122.34139354111291,31.790627799032745,38.15025740293325,38.97141619381398,57.482434866091936,213.30012314392263,229.26669097116735,130.55866203620687,39.475270349010344,58.138828536058455,78.60814839878131,52.49440263394748,25.807749102248174,73.95174420490778,34.81555858991638,804.91563357807,31.237463827698235,40.795515297404854,625.8372280391036,125.3046801064938,2.181331061727697,202.47366640402774,260.18524009222904,23.739943946826635,82.2497427388609,155.33598159390104,42.137053746985,64.4194881077197,39.644767091908165,175.0953708688885,126.74219991894239,91.79962606461675,380.62129414154776,43.91961631207607,44.83415696673612,200.07132661016806,157.4553980141309,277.97638651093894,37.07477613706875,90.74874352364839,30.37382482217999,57.00957711138236,11.033584289549701,31.36265545429925,143.4256355335577,156.00649178412428,31.36265545429925,76.48104647615563,35.222747849787176,30.58176975275829,41.950331854638904,35.12973317933662,111.85410965936296,151.57622066808824,134.5219589336999,69.66311070409671,125.5925105437479,15.11850857203489,35.619209619283566,81.29385537282033,23.383841730428912,51.99788593482411,38.625084120542134,73.29975927115179,-32.68552950309351,405.69889564949926,158.04028596304272,41.10439419941011,31.62467933882996,61.83773237268367,146.82736285259097,100.47970474385032,155.88939820339155,46.84866963468334,24.702881722477002,47.08978954952384,216.7921070870331,73.85111105277687,76.45458548810176,68.5927319551025,167.62232274947343,36.41349222071217,48.62587645235786,-125.91405466064217,1454.1675360993395,62.41860005317169,70.16443330821039,36.14895172501234,62.75885691081183,30.88415965704261,35.9380053454626,40.16555069339454,38.773066869477425,45.475674742380484,142.02565103609166,104.0541975272357,30.978984656607302,232.7964061058048,44.77827834227167,31.979104713341936,16.846714858970216,171.63277497307337,46.02042748047599,117.11621590544624,32.4507174619298,96.33839340962533,111.31572632783497,275.72069070494075,11.177967264206714,116.34694250518086,31.436525986103092,41.5421151638529,35.96990875729759,408.22590755143943,194.29565745260882,50.39224279423365,-11.39056875908458,219.00489311337046,61.33015089126719,37.187590978442174,46.80943154770826,45.61105374938802,45.90786193167095,53.649894807709316,33.148453976150606,1667.8692301778947,50.26924969849959,36.56249560359412,190.25945193978245,100.35091003244543,88.45296711194518,32.47926066802884,38.939877970801945,63.67596029322777,-192.39369429379462,1.4804466175220554,21.222681266744004,58.70084592915975,835.0119387469327,61.22937641488111,86.16934498894702,71.02033556684536,45.53545834085734,125.3046801064938,246.44493583827904,37.507115546544426,38.61683437596617,36.58213987241409,78.46435500814502,77.56464432118682,46.911105083917484,34.16831330337878,156.59221893099658,132.9945746239242,56.989402488368185,121.87480448951425,51.97642628333297,147.98632423575853,16.610936930082723,10.798315313514415,82.70910326100983,87.58211994370146,58.22883774818151,29.711904981755765,63.69065475694244,79.28985704106879,179.28841926336415,229.16048401061397,33.1405351892743,78.62303009415521,15.685828325371965,200.51261324935388,39.62647590514075,21.264007521183622,36.92496197719494,58.86157775088901,46.832203163966085,14.755618556313102,25.0579479252332,40.82535009609054,81.87523795852616,-12.013668326125448,22.306193796265916,27.55883010355732,69.71421802011102,4.987460318868827,52.14090350484714,39.47532778662719,-23.667125958803616,64.59374610464795,274.6670877959037,107.44292489759762,227.98620902905503,11.88236869096177,56.47800997539088,154.50696539779733,109.4108365265138,63.98817761583412,30.243628222522695,766.2185814209669,71.89983485274104,135.6117934687741,75.69309484097637,54.52289892953464,13.895503862807836,532.7408387598979,176.212152438141,78.69352276201977,64.07149518257458,1026.1891345967488,27.495977291403285,439.8632194528615,122.69656950070066,83.82784088918609,29.634088399869903,423.567937089118,25.24876481524931,80.0601595831549,59.971661769133696,11.089779706467468,61.12201813987478,83.50139630786902,83.0276791845504,344.9705063935093,6.690149352207989,197.45407282785817,44.21242508766883,142.66041431201924,31.535638754902628,27.759052611498554,40.38462190980999,295.1153500714182,76.72128836988767,99.11317664680217,53.178113963287714,151.03075045115582,88.85299880441765,26.9369407348429,34.863707418886676,128.41397629987145,61.30288613785224,160.11214929759882,2984.1673990870368,42.278457933663134,37.61495820645915,37.846897022703004,275.72069070494075,42.153657348927865,24.770571225980774,121.28525074932014,205.89366751296907,185.0759342081386,35.67598146093188,26.96283772655439,156.62655554055874,72.07243298088743,1.1516504602646265,29.35162214052604,399.275155758353,81.27174234979596,16.672345820478803,44.50181930487467,104.6292151751152,43.57381804716174,25.67669706088644,36.819302849651315,150.10647107090244,17.770867523720476,151.57064092363407,19.607417990301293,115.36484491756582,68.08764646772843,65.59480256677523,184.04313453593954,21.11842214931381,30.39419322385789,252.99886382209831,29.427718240350295,53.745196216603915,233.24038044487696,48.280585331321916,333.81729746224886,51.899626978125085,61.390688322135304,59.623630548399156,60.24253897923421,281.51115699238744,142.3331384360381,40.898260514931486,39.287537545528025,240.32613928202665,54.743435699014654,15.643633857796736,67.79990719229963,30.178993627720196,252.54611570497997,45.418902900732164,16.34911437185619,77.27361780741302,32.856191596124305,63.00592219121642,65.4079172388772,20.295870099354445,13.159880035167816,14.229695415491669,35.39631669854313,385.1049676588836,29.662836518077107,144.7181897782042,46.45784457862831,67.79990719229963,134.43681988852688,30.39419322385789,24.688876483444325,37.14549874667496,31.193752090091017,66.89704114244998,53.185552953609175,128.2612309720975,195.96642490531505,226.82861806050198,163.94129878447103,160.02830605720874,51.28561759725919,81.4411208115001,79.93314951680877,147.39537499092177,85.37920599557089,19.18569563717957,79.90731231717865,48.43521067893778,76.59707724260733,51.4679121889214,-40.91006880285892,135.35641773352773,31.246078664900168,141.39763932377986,105.0585721260245,58.39951846194944,54.390924510288414,160.45017574713413,34.23996192750022,83.51147468811567,110.47724989733669,131.31939854410686,96.97349233319598,78.10448603695518,139.36529537129937,112.206123150331,125.06276175130455,71.86906212241952,82.28869656030568,3.8093510994080404,54.281231253419676,0.9709959441795277,242.296614755087,27.662052470211343,66.34344501013223,70.74084342334594,88.73692018458263,69.78445624480884,61.058695701113194,27.331704678596516,-9.444178618380285,56.50299760128965,718.0639180565311,57.53483964299809,29.248181426114876,766.2185814209669,-17.93722563486083,47.16675747666647,-10.826588250070664,186.28118111191642,1074.188775654278,-2.7959014894783323,48.34131878698094,90.72274106429133,26.747152406724943,6.069144092290408,73.34628192696381,170.02169327203245,524.0683660499295,-6.030649046348621,-40.66913217830018,26.96283772655439,479.72233345592844,-103.84554621477952,42.747043700839335,25.3339225777218,51.88785674913972,47.102726790267795,676.058407826213,454.0447856375535,69.3482266003898,281.51115699238744,122.13999614207582,101.26816935675384,15.742998830166421,262.95006762609495,46.209341619066485,44.84900095187787,130.55866203620687,42.11864672050709,38.17119617183621,42.45551696421537,35.494748274131474,54.52289892953464,60.26170638566562,117.48984092433729,35.35890166984658,421.00756428914724,48.372463157850184,81.41928548778921,59.14395316735992,71.04526922464743,-69.24604088528062,56.667107466594466,65.82960072272307,30.738196561536775,35.90013028081718,51.25613269999816,55.21689703471807,288.69886117310483,35.23248691821935,47.193148205815305,69.31718031739572,12.020479851400687,47.102726790267795,0.9709959441795277,322.40825533193197,104.05208374624138,122.13999614207582,191.5505037976651,58.17728254500672,437.98963730175717,105.26345898008393,62.28551545391161,176.33027588036566,53.82246942332338,65.95720838319775,66.3864772039812,103.24842379039148,137.9456027407312,79.8476055462991,-16.39893249742584,82.58895305597724,2043.649100336197,39.37306151823785,121.69109016963816,56.47800997539088,24.098061505547477,170.62730128797025,865.8540000914927,31.113732763995067,17.770867523720476,144.2965099831692,68.65906113565667,51.28561759725919,17.937799542394117,-5.73435236851963,35.124380849812056,174.69132917528373,257.08125529095463,34.20797904518005,31.60721107986124,81.87465190953279,78.00172664844898,32.437207397180494,43.91961631207607,31.782029587626837,109.56368379249002,85.49860216382096,54.93692050631025,33.549892903402295,53.670494069245166,3.1931638045458186,24.725234655085444,37.42830754767181,36.68610737501486,46.95146804548479,45.44740801764177,99.36537285694888,374.1690295240944,31.82606488883039,36.43633511097934,36.82888655286702,56.50299760128965,76.30224999842324,116.96937283682067,16.297741315711633,89.1485056965368,44.86601760351359,142.3230818025846,61.178344645834606,99.12381919333774,41.69598020802498,32.45655002401939,314.96153895883515,46.51081960226175,-37.48215112987692,36.6498368984377,154.5955214551952,87.5839253179187,27.146090057619304,44.07061955165727,48.812613649342694,70.98640554264927,13.507061682171013,155.11386820087472,254.5003771771049,87.93628981544558,77.68714428068927,153.1661573258631,32.44525073000357,78.1473852260807,49.75477423694214,106.60278959294348,53.9159768809955,137.0154179506472,28.25656994760231,15.364870931507625,27.31001937355758,295.40194036352597,44.56514174363627,167.39675332933209,73.70520634071845,163.3163191207982,10.315082224438733,87.77208186324631,-9.611608067708012,60.62672148046048,47.545663430173114,49.09572542090897,215.91954371232873,467.7193993039542,31.193752090091017,103.29694588746386,98.05816734431232,-104.50828736751055,53.2329233530034,10.202022968071255,44.052011636430294,41.88291441820731,123.49535878262964,0.31376386630407893,60.395089906779525,122.06503934245251,1284.9118502030888,103.08243734637128,60.62672148046048,51.748113670788584,21.559325301052183,60.52506422883607,36.96644909224564,57.43403196510502,39.89893306661851,25.315290209468074,197.9038484271138,139.71559791995324,16.597525120201027,-14.7832491460153,403.6719085990786,181.00553727149747,6.990769240122809,164.601435549343,124.6183864920958,665.3451787471391,264.11881546342096,-12.622012326413845,62.428721087475516,728.963615499657,13.121457182391431,53.55256098303967,200.07132661016806,61.460830846216574,98.60439457127413,84.87167104018735,38.914159967257255,160.84020765940411,156.22448243752714,112.8656691224854,63.42320499687132,85.09320169851564,66.5380279939075,199.88864811852875,28.873428551257497,121.12633727113902,33.485048918998004,230.90657952983955,29.71408851412685,91.14692429354193,26.436606425121873,64.57018094264768,86.16934498894702,34.88932790285001],"xaxis":"x","y":[30.29,47.7,6.73,5.66,152.14,7.53,17.11,9.67,73.77,5.21,150.69,47.42,17.51,11.08,4.76,73.31,30.34,59.24,33.58,31.96,232.12,9.43,8.12,51.2,14.69,33.77,10.29,32.33,61.62,7.67,21.08,35.56,105.32,41.15,287.23,85.28,139.34,18.63,25.15,10.04,42.74,29.93,17.38,263.57,11.17,8.59,570.98,13.65,13.22,2.91,6.53,49.15,1222.69,28.59,51.64,185.23,141.7,3.27,66.39,85.15,87.31,95.26,48.51,26.78,50.14,6.02,14.87,349.47,4,14.79,4.98,20.39,6.45,19.28,93.47,52.87,372.55,5.81,12.68,5.81,8.57,47.39,128.73,99.39,37.72,3.02,12,38.97,39.27,266.18,5.9,29.84,49.18,3.82,8.8,11.85,10734.57,266.78,12.25,6.04,18.07,7.21,63.89,8.53,138.68,18.34,29.39,218.53,69.3,587.16,11.97,8.46,4.36,5.16,46.57,32.22,5.46,181.38,7.08,65.03,5.87,24.25,23.17,178.21,39.57,16.5,4.48,61.36,4.03,8.8,77.29,6.05,45.3,8.77,256.57,2.17,37.97,11.11,74.45,17.67,19.83,406.2,209.92,45.81,2.36,22.02,198.38,6.22,8.31,5.96,13.61,262.75,152.36,15.43,7.23,5.94,43.45,60.94,159.16,31.92,9.71,69.36,18.1,8.37,23.58,48.3,147.82,71.47,18.89,53.36,17.65,4.5,92.59,62.57,4.8,25.37,90.31,100.42,6.37,2.9,63.61,64.19,22.15,192.63,5.55,109.79,3.29,67.15,31.92,79.3,8.81,6.71,12.33,29.33,12.15,16.83,116.21,6.43,187.92,39.56,30.85,78.29,4.33,4.63,16.89,3.42,10.21,5.97,49.16,20.1,190.58,7.67,4.79,1000.06,50.52,122.99,134.69,5.6,27.38,7.08,2.04,44.6,3.51,41.98,6.36,5.87,8.87,3.97,127.56,37.85,4.26,147.69,5.9,10.55,5.69,32.81,34.81,56.25,653.5,65.65,97.01,3.49,4.65,9.13,41.69,14.77,39.47,2.82,142.48,68.97,6.64,7.31,82.9,8.69,256.31,93.23,11.77,38.08,17.24,43.64,18.41,12.7,27.3,6.71,3.89,71.03,1.78,152.71,50.43,22.18,12.06,29.26,20.98,77.15,3.84,91.09,14.35,839.79,226.43,27.27,57.69,5.16,6.69,20.79,176.79,59.61,9.83,8.24,16.09,20.44,170.84,35.39,30.02,19.72,15.59,66.05,3.89,7.46,14.35,7.74,141.32,46.61,20.29,3.75,235.69,243.38,151.95,100.87,56.51,6.79,4.84,247.85,24.17,168.62,47.11,12.99,104.17,30.66,3.82,120.91,132.47,106.37,44.93,6.93,61.71,4.2,24.42,8.14,111.69,6.22,9.45,23.84,4.88,507.7,553.27,9.48,122.92,103.37,50,3.86,3.4,3.66,93.43,6.85,32.8,32.73,13.43,413.1,6.19,6.16,68.14,0.68,16.45,13.88,350.87,802.89,291.28,91.73,4.72,7.21,25.02,6.75,28.93,229.55,3.01,49.59,341.23,37.18,353.49,93.87,93.21,11.79,49.25,154.4,84.93,6.02,43.63,35.36,51.47,42.8,4.35,17.11,205.07,40.78,5.6,186.96,14.51,35.56,537.03,53.38,38.9,8.16,37.51,3.82,5.51,50.76,380.17,10.75,4.3,7.12,1.6,63.45,4.42,7.51,7.14,22.78,8.45,8.59,5.37,11.59,6.93,54.95,19.7,89.41,74.04,3.91,35.22,43.13,28.54,70.4,10.06,6.3,5.11,45.38,0,10.14,765.56,14.46,28.64,27.86,8.85,256.72,196.67,525.78,93.67,8.97,165.61,3.06,17.84,19.17,50.7,47.25,2.99,3.2,54.92,96.4,16.01,4.74,10.56,132.28,15.5,7.29,63.36,47.68,297.23,67.06,3.4,98.54,0.76,3.72,50.15,2.45,265.21,188.67,156.08,83.1,94.85,20.57,29.18,12.09,13.77,34.14,1231.19,39.41,7.57,113.25,237.36,93.23,33.52,57.03,12.79,12.97,5.72,6.38,10.93,16.2,57.16,18.62,346.62,44.19,190.45,6.1,75.67,3.69,73.48,1185.15,20.12,92.3,188.13,72.71,101.31,15.8,80.06,3.18,141.15,17.74,22.65,42.2,14.32,46.86,18.42,247.85,52.43,5.71,51.26,3.07,10.14,242.19,21.3,8.45,113.31,75.12,13.15,0.89,8.57,3.48,5.33,102.04,3.71,31.58,42.52,6.89,7.69,6.16,29.16,5.88,76.85,1.3,37.58,7.29,24.1,28.78,25.39,11.28,32.02,31.46,38.54,49.7,9.47,456.22,124.92,14.19,110.62,24.36,4.13,68.89,145.48,21.24,18.72,12.4,15.62,43.16,160.3,47.78,139.27,30.86,44.97,163.99,142.69,12.63,4,109.46,26.64,64.13,285.62,9.57,74.52,4.01,226.81,10.16,13.64,92.58,7.15,8.16,75.64,60.02,223.36,8,13.87,3.92,73.32,233.43,208.26,351.53,16.76,200.3,604.45,51.91,16.06,10.92,90.73,4.73,69.42,238.61,4.93,900.81,0.7,399.76,31.85,19.95,248.29,2.43,480.9,10.32,176.76,144.3,5.15,97.97,27.69,63.6,144.53,27.56,29.24,26.55,18.5,80.06,10.77,15.34,0.75,3.91,474.59,4.25,128.41,433.86,69.88,44.32,413.35,4.43,348.22,29.78,66.32,290.4,44.51,4.02,5.2,56.82,4.55,9.65,52.88,57.79,58.32,393.2,72.69,45.02,24.62,4.19,223.34,10.89,83.27,7.56,29.73,100.71,7.53,121.88,176.47,23.82,4.48,53.21,16.3,4.9,6.39,54.95,61.64,7.12,19.68,8.07,704.76,33.35,184.81,61.25,20.88,60.04,16.47,551.43,260.13,8.61,167.31,70.14,64,21.92,9.16,111.23,7.02,236.34,318.08,149.93,364.9,295.86,34.62,3.11,156.18,118.86,4.16,3.27,6.54,3.65,48.91,8.87,5.19,148.11,88.87,1793.9,37.34,275.54,30.87,5.53,56.96,82.38,21.52,1597.56,42.41,109.72,4.68,19.81,79.81,6.74,6.02,39.05,5.79,271.32,85.55,4.52,233,9.53,157.21,15.23,35.44,3.92,64.75,40.35,20.45,15.27,29.42,39.75,119.03,28.07,95.77,23.14,60.83,224.13,98.65,443.86,3.52,221.47,5.68,7.9,11.73,4.71,38.59,32.75,4.79,101.43,5.54,5.66,5.88,130.58,60.33,25.6,69.18,253.53,84.57,3.55,42.71,5.09,12.28,19.15,93.68,42.24,58.44,1084.79,243.37,7.47,4.43,8.09,69.67,82.72,101.64,19.04,4.02,5.6,46.49,183.51,98.22,6.54,69.3,41.96,3.71,5.97,913.07,145.42,7.56,5.3,45.96,8,71.16,13.1,53.88,53.02,379.34,43.35,7.94,322.79,4.43,14.81,3.41,18.97,3.9,115.56,61.68,29.92,9.99,530.66,3.7,49.17,27.39,96.62,6.57,51.93,21.38,63.48,90.34,423.93,183.79,5.86,92.45,23.85,45.32,77.75,5.31,2451.58,9.87,39.5,67.17,162.39,220.26,8.46,23,31.79,28.83,20.02,101.99,26.72,15.5,32.77,132.56,15.74,110.66,9.65,246.83,10.83,5.92,9.2,467.18,24.35,3.67,5.35,862.64,89.53,234.03,1.67,15.2,71.38,43.57,260.57,226.73,87.14,14.44,3.99,28.82,73,188.07,45.6,5.07,103.77,3.16,395.26,27.93,10.23,23.27,60.07,112.19,7.22,20.07,15.9,72.46,3.92,176.63,32.52,32.35,6.84,96.59,44.71,3.52,48.95,116.95,36.65,142.36,8.27,3.89,315.85,9.73,1.61,5.81,1638.46,8.61,14.99,49.18,26.73,17.42,202.49,59.21,9.99,42.57,481.1,29.62,399.54,72.88,52.79,20.67,381.89,3.56,4.07,15.74,26.94,27.84,32.72,15.64,582.28,13.36,23.83,27.06,411.09,4.74,15.18,73.14,185.82,13.97,12.15,3.49,19.84,93.67,93.55,59.18,74.98,43.56,312.6,12307.16,6.47,6.19,12.59,579.99,59.63,2.81,119.34,283.04,19.72,40.76,3.18,62.01,1.58,15.77,3.34,587.09,12.07,23.74,4.61,89.6,103.42,7.52,57.33,16.82,65.75,22.76,8.42,39.54,99.91,47.12,80.75,10.17,4.54,28.3,19.36,6.35,63.87,10.05,184.64,34.08,3.15,76.89,11.99,292.92,112.01,66.1,23.22,251.43,75.58,152.14,41.06,7.07,115.34,6.12,34.87,85.3,19.76,28.03,6.67,18.73,20.07,34.98,12.56,150.23,12.35,50.23,19.16,45.22,344.2,4.74,1.34,3.77,36.71,3.92,83.35,324.92,64.95,675.34,22.17,536.43,43.84,30.26,78.19,44.61,61.72,39.57,73.52,3.99,182.11,99.62,551.53,39.45,6.99,89.94,7.27,4.76,9.66,77.56,3.78,44.26,39.31,37.14,11.29,24.91,19.73,326.74,64.79,59.06,2.35,49.79,61.19,4.39,357.86,6.61,126.3,53.25,58.98,4.95,6.87,8.93,39.1,66.44,440.31,38.45,102.08,1558.85,37.17,124.96,5.3,260.94,3232.23,74.13,31.18,225.67,6.04,49.44,8.31,67.65,320.35,46.72,143.67,3.16,94.59,96.15,5.7,25.49,3.65,22.46,1727.11,49.26,173.75,295.48,53.56,136.07,5.03,250.05,74.28,3.59,123.14,0.12,24.64,4.98,46.95,27.76,5.53,17.23,154.13,740.97,78.06,125.42,7.53,131.64,129.05,6.67,99.65,298.91,30.14,46.32,5.88,0,14.3,45.07,1.14,147.24,21.28,4.44,249.09,72.02,47.25,1623.34,8.7,226.06,10.83,47.91,434.48,5.33,120.13,38.15,180.38,37.16,8.18,7.05,148.7,0,29.26,39.35,24.18,4.02,103.39,2089.28,4.5,77.48,143.3,46.61,46.85,80.08,108.04,23.01,47.82,67.62,69.53,4.2,25.48,7.74,4.33,22.61,3.63,11.25,58.79,58.85,13.34,7.25,26.41,5.94,5.02,4.44,4.72,23.65,96.41,529.47,7.1,14.15,3.72,48.23,63.06,96.39,5.29,10.08,4.49,22.08,1084.49,8.16,65.77,20.63,375.56,112.13,58.52,26.54,68.3,464.77,5.14,57.17,8.98,91.49,3.68,44.35,92.7,4.99,0.37,305.15,57.25,0,16.27,45.59,34.26,34.92,24.31,5.07,1.08,28.19,82.87,109.71,17.19,50.31,20.25,41.22,5.74,45.98,17.88,86.5,8.55,394.6,22.57,48.85,38.13,269.23,47.26,122.83,5.23,8.81,48.65,640.71,46,328.63,4725.43,73.9,65.65,6.98,3.5,8.54,4.15,6.98,4.69,40.38,150.81,8.15,4.38,37.13,328.97,132.71,3.15,229.2,10.97,122.54,84.18,6.46,26.59,500.93,148.02,6.39,222.02,12.75,50.57,9.28,3.86,369.14,156.11,46.38,14.99,69.77,7.21,100.59,5.3,39.61,102.73,23.42,4.26,127.53,3.91,12.5,117.11,8.47],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","TotalGHGEmissions_predLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=TotalGHGEmissions_train,\n","                            y_test=TotalGHGEmissions_test,\n","                            y_test_name='TotalGHGEmissions_test',\n","                            y_pred_name='TotalGHGEmissions_predLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","print(ScoresLasso)\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[403.84724344622873,403.2617952787491,402.69294779201687,402.1375454568787,401.6025510705934]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[541.91531271792,541.0383719599771,540.1701530141472,539.3275228042492,538.525295267048]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[265.77917417453745,265.4852185975211,265.2157425698865,264.9475681095082,264.67980687413876]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[257.6678140505701,256.9531594538343,256.2985491899051,255.65611902738786,255.02687541740173],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[298.39024262350347,298.0765243783742,297.7747184144607,297.47770765356154,297.1853743560234],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[575.6823225235923,573.216906950636,570.7434175177685,568.3007357595781,565.9245892319422],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[321.0431270613959,321.0825455052978,321.13007189875606,321.18810149110976,321.2565799026424],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[566.4527109720821,566.9798401056032,567.5179819391938,568.0650633527565,568.6193364449574],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour les paramètres de GridSearchCV\n","FigRMSEGRidLasso = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso, 'alpha',\n","                                GridLasso, None, None)\n","FigRMSEGRidLasso.show()\n","if write_data is True:\n","    FigRMSEGRidLasso.write_image('./Figures/EmissionsGraphRMSELasso.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.411e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.289e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.835e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.105e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.393e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.269e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.186e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.373e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.072e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.760e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.161e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.246e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.036e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.345e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.124e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.861e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.411e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.673e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.273e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.734e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.105e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.984e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.835e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.290e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.034e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.267e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.128e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.392e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.781e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.185e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.752e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.032e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.121e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.667e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.070e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.244e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.343e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.980e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.105e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.742e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.290e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.268e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.067e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.391e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.856e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.155e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.266e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.125e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.370e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.122e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.028e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.772e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.852e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.660e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.340e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.265e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.975e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.024e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.368e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.290e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.761e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.114e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.338e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.413e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.653e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.729e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.204e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.970e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.064e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.707e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.023e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.837e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.390e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.266e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.847e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.714e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.060e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.201e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.119e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.366e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.696e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.148e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.841e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.109e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.236e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.019e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.837e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.413e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.964e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.644e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.363e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.335e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.261e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.746e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.013e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.727e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.105e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.635e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.414e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.695e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.332e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.175e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.387e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.197e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.292e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.014e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.143e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.958e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.681e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.838e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.835e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.385e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.193e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.260e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.051e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.256e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.258e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.828e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.112e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.360e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.665e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.108e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.838e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.414e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.702e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.229e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.624e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.292e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.255e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.099e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.998e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.356e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.106e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.009e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.093e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.328e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.667e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.224e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.324e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.951e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.942e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.612e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.166e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.415e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.644e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.107e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.252e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.161e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.379e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.293e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.103e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.045e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.601e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.038e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.251e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.620e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.821e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.124e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.839e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.351e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.813e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.620e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.248e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.983e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.933e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.839e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.415e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.320e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.346e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.247e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.971e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.598e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.293e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.583e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.922e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.315e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.550e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.107e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.997e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.554e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.990e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.243e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.115e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.176e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.375e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.416e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.146e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.804e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.107e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.093e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.154e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.294e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.371e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.018e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.238e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.105e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.029e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.794e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.840e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.241e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.553e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.340e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.087e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.957e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.460e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.068e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.206e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.417e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.841e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.234e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.565e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.910e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.215e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.333e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.323e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.544e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.982e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.940e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.107e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.197e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.896e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.973e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.302e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.365e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.005e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.390e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.418e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.783e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.161e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.108e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.080e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.093e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.121e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.263e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.507e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.358e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.989e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.152e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.841e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.770e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.072e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.324e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.449e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.920e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.519e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.044e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.294e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.215e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.842e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.419e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.964e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.880e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.313e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.896e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.794e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.029e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.296e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.491e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.104e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.285e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.420e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.108e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.840e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.953e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.968e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.084e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.141e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.297e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.109e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.082e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.374e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.059e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.843e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.349e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.826e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.941e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.217e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.128e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.741e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.201e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.036e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.300e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.054e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.283e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.866e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.844e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.298e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.011e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.274e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.941e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.284e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.991e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.829e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.808e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.934e+07, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.109e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.778e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.417e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.262e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.321e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.422e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.202e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.927e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.113e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.447e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.109e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.012e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.907e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.299e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.044e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.007e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.149e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.861e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.301e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.859e+07, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.845e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.095e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.264e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.704e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.159e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.970e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.032e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.978e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.116e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.423e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.783e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.370e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.846e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.127e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.965e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.300e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.247e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.911e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.935e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.312e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.229e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.110e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.958e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.424e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.894e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.699e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.074e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.643e+07, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.110e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.274e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.800e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.682e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.301e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.743e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.924e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.238e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.883e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.847e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.049e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.085e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.716e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.657e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.863e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.417e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.425e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.302e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.848e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.026e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.165e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.873e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.221e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.999e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.157e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.549e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.776e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.111e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.850e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.427e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.584e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.598e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.111e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.783e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.628e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.303e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.622e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.981e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.630e+07, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.849e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.136e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.676e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.944e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.695e+07, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.430e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.965e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.595e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.222e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.417e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.428e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.797e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.111e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.050e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.850e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.827e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.305e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.935e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.506e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.223e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.824e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.727e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.916e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.239e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.391e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.429e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.850e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.408e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.110e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.183e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.117e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.306e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.529e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.036e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.852e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.870e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.153e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.324e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.876e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.095e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.225e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.342e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.941e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.916e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.931e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.745e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.734e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.637e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.062e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.431e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.394e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.806e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.757e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.283e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.308e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.558e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.525e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.575e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.522e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.113e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.477e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.714e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.433e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.124e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.647e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.954e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.256e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.114e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.803e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.031e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.069e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.141e+07, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.299e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.885e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.273e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.988e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.615e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.170e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.921e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.370e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.227e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.849e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.351e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.856e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.180e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.341e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.435e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.240e+08, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.168e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.341e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.229e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.713e+07, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.028e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.664e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.114e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.529e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.818e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.311e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.605e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.003e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.739e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.787e+07, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.588e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.069e+08, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.437e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.806e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.115e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.313e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.427e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.960e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.487e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.849e+07, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.405e+07, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.858e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.439e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.934e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.894e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.860e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.781e+07, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.230e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.315e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.551e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.186e+07, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.265e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.232e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.322e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.518e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.940e+06, tolerance: 7.857e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.545e+07, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.501e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.441e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.461e+06, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.446e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.117e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.212e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.910e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.317e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.057e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.696e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.862e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.917e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.849e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.344e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.623e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.558e+07, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.864e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.757e+06, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.252e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.443e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.235e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.459e+07, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.319e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.340e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.905e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.118e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.209e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.509e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.445e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.347e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.775e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.119e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.321e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.535e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.456e+07, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.153e+07, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.684e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.866e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.426e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.416e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.448e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.237e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.868e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.323e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.045e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.410e+07, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.838e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.451e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.571e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.529e+07, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.876e+06, tolerance: 7.476e+04\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.121e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.326e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.429e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.871e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.941e+06, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.122e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.453e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.241e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.873e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.328e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.574e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.243e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.236e+08, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.456e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.122e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.248e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.331e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.907e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.876e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.016e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.245e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.460e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.879e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.631e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.248e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.334e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.082e+07, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.120e+07, tolerance: 1.001e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.713e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.463e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.126e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.274e+08, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.316e+08, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.882e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.466e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.207e+07, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.885e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.250e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.253e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.340e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.129e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.047e+07, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.470e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.960e+07, tolerance: 1.053e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.248e+07, tolerance: 1.100e+05\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.344e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.892e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.888e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.474e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.258e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.255e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.347e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.130e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.478e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.351e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.895e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.482e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.899e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.264e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.355e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.261e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.134e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.486e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.136e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.359e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.490e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.268e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.363e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.495e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.140e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.368e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.911e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.500e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.915e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.278e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.275e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.142e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.144e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.505e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.377e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.924e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.920e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.286e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.282e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.146e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.515e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.149e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.387e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.934e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.521e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.929e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.294e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.393e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.290e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.527e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.151e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.154e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.398e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.945e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.939e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.533e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.303e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.299e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.156e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.539e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.159e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.956e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.950e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.545e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.416e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.308e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.551e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.162e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.165e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.422e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.962e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.968e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.558e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.317e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.322e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.429e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.565e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.172e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.168e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.436e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.980e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.974e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.572e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.333e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.327e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.442e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.579e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.175e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.179e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.449e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.586e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.338e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.343e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.186e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.593e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.457e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.183e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.601e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.000e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.355e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.608e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.471e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.195e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.349e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.190e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.479e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.616e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.013e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.487e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.624e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.367e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.361e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.203e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.199e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.495e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.028e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.035e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.632e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.373e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.379e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.503e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.213e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.641e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.208e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.511e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.649e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.043e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.520e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.392e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.223e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.385e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.658e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.218e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.528e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.066e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.666e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.058e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.405e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.675e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.537e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.399e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.228e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.234e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.546e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.083e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.075e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.684e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.419e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.555e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.693e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.245e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.239e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.564e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.703e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.091e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.434e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.574e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.712e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.258e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.426e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.251e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.583e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.722e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.109e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.732e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.593e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.271e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.441e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.449e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.603e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.264e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.136e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.127e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.742e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.465e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.285e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.753e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.457e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.613e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.624e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.156e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.763e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.146e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.481e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.634e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.774e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.293e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.300e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.473e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.645e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.176e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.166e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.785e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.499e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.796e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.656e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.317e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.490e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.308e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.667e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.197e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.808e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.186e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.819e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.517e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.334e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.508e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.678e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.690e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.325e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.831e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.527e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.844e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.536e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.352e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.702e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.343e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.240e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.714e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.856e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.557e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.229e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.869e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.371e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.362e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.546e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.726e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.739e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.252e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.882e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.578e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.567e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.751e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.392e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.381e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.287e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.909e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.600e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.923e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.275e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.778e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.403e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.589e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.791e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.937e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.299e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.624e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.952e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.437e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.805e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.612e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.819e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.967e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.324e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.649e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.834e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.461e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.983e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.636e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.449e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.849e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.362e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.998e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.349e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.675e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.015e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.487e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.864e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.474e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.662e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.879e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.388e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.031e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.375e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.514e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.703e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.689e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.048e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.500e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.911e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.414e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.065e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.083e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.401e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.543e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.732e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.928e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.944e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.717e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.528e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.441e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.427e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.102e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.573e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.120e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.748e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.962e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.558e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.979e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.467e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.140e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.454e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.797e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.159e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.606e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.998e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.780e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.589e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.016e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.494e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.481e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.179e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.832e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.200e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.639e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.814e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.035e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.622e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.055e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.521e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.507e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.221e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.850e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.869e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.675e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.243e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.074e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.095e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.547e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.534e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.908e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.265e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.713e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.888e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.694e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.116e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.137e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.573e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.560e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.311e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.949e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.928e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.159e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.752e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.334e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.181e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.598e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.585e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.358e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.991e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.793e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.383e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.970e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.203e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.773e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.226e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.623e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.610e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.408e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.036e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.433e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.013e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.274e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.250e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.815e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.459e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.082e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.059e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.485e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.880e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.298e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.858e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.322e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.670e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.658e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.512e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.130e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.925e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.106e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.347e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.692e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.681e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.566e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.593e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.178e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.154e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.972e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.397e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.423e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.621e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.713e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.702e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.648e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.228e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.018e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.203e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.448e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.474e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.995e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.733e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.676e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.278e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.703e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.500e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.065e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.526e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.042e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.751e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.742e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.731e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.327e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.551e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.759e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.303e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.112e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.089e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.769e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.786e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.760e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.813e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.352e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.602e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.377e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.628e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.785e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.135e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.840e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.777e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.425e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.652e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.203e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.401e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.866e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.892e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.677e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.918e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.701e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.792e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.813e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.800e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.725e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.449e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.517e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.943e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.472e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.807e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.269e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.968e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.290e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.495e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.749e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.772e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.247e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.820e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.992e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.016e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.039e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.816e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.794e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.061e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.837e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.083e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.832e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.826e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.858e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.103e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.837e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.561e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.878e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.582e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.897e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.843e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.602e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.848e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.857e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.852e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.330e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.310e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.622e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.349e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.677e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.659e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.641e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.368e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.386e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.124e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.437e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.916e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.143e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.162e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.934e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.951e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.861e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.197e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.180e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.865e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.214e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.984e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.711e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.000e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.968e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.873e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.453e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.876e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.245e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.230e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.869e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.742e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.014e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.028e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.727e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.483e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.879e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.497e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.885e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.468e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.882e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.796e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.770e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.784e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.260e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.510e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.547e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.535e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.274e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.042e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.523e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.287e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.055e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.067e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.888e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.299e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.891e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.809e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.078e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.323e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.820e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.311e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.333e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.893e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.100e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.558e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.089e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.831e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.343e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.110e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.897e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.569e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.899e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.903e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.119e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.851e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.841e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.860e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.878e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.588e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.597e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.621e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.353e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.901e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.579e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.128e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.869e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.362e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.614e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.136e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.606e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.905e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.371e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.885e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.387e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.379e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.144e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.152e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.906e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.159e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.394e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.401e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.893e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.907e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.909e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.628e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.910e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.172e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.900e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.407e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.906e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.913e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.911e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.913e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.648e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.178e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.642e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.919e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.929e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.413e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.912e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.653e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.668e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.664e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.419e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.183e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.924e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.188e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.659e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.914e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.429e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.934e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.424e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.198e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.438e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.434e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.193e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.442e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.206e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.202e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.915e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.943e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.918e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.446e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.210e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.939e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.951e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.918e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.214e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.677e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.947e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.920e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.673e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.685e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.681e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.919e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.954e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.961e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.697e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.958e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.688e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.694e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.450e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.691e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.453e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.217e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.220e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.456e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.920e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.223e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.964e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.465e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.459e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.462e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.921e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.226e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.231e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.228e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.467e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.469e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.967e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.921e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.922e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.922e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.233e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.700e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.235e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.974e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.972e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.923e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.923e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.702e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.472e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.705e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.707e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.923e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.980e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.976e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.978e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.237e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.709e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.713e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.714e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.474e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.711e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.239e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.475e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.477e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.241e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.242e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.982e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.479e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.480e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.244e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.482e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.983e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.483e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.985e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.245e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.247e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.716e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.248e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.717e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.986e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.989e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.719e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.724e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.991e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.987e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.990e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.721e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.720e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.484e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.722e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.486e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.249e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.250e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.487e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.251e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.488e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.992e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.490e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.252e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.995e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.254e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.993e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.728e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.994e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.994e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.725e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.490e+08, tolerance: 1.100e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.996e+08, tolerance: 1.001e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.728e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.255e+08, tolerance: 1.053e+05 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+08, tolerance: 7.857e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.726e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.727e+08, tolerance: 7.476e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha     31.078662\n","1  elasticnet__l1_ratio      1.000000\n","      ElasticNet()\n","R²        0.345465\n","RMSE    414.293389\n","MAE      95.009466\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predEN=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[44.003027751210425,38.21007647802311,58.15871452863845,40.96156378402229,79.47696547082131,63.277379383012125,43.493586725724285,55.26576687698583,137.5169744737055,47.55359179587286,110.46320807051784,41.162294276685195,32.95196772189742,101.54587333354569,44.591495639375864,74.64860454720399,50.33787751134146,52.250045349385026,99.40450085656005,70.36554242463609,22.720811392882048,61.41151664427713,49.84113723164306,61.07847486584297,76.84231361236183,83.86784639098354,233.83543026470258,61.340956945456334,89.4089391915638,29.48236381856104,167.9975864889946,89.76663721240206,70.02403348234344,70.90885210555622,73.74958558008146,113.69201988855744,675.1119262158461,38.78019884449513,53.20260128346577,43.32424344855437,110.66500880914532,59.96504281845082,38.759030934848894,123.39538967039326,104.94967320466087,51.7688282034272,78.60409285895219,123.36998817881778,35.239513157667616,62.375362130169194,38.68847123602811,44.38346753803907,551.6306689776604,73.74958558008146,62.375362130169194,89.74688049673225,237.89611531047734,39.643849558061675,142.81482997985262,60.25857116554533,139.9938532209972,225.0394582097022,43.99456058735193,167.88892455281058,143.43716652345205,34.97061308907257,120.43752709582552,72.28476623256174,43.25086136178075,46.85222838959416,41.3485718815721,146.84519997649647,47.69047761158521,40.448230124618746,89.82590735941153,133.62337478637858,334.52694286993085,48.224318618598815,99.77200250519088,51.145080465851365,74.96180120582272,163.79787321518086,83.04043678121135,169.41395172373757,45.264635166126254,38.55017422633934,44.142735954875604,65.08485456488775,163.5184568078505,243.5994813875243,55.816132527788035,61.132100236946776,156.625861870137,44.99135380326134,61.4623196274281,76.95535285107306,1913.4130995033465,131.37004683111957,95.37895565660814,58.62723092880853,176.6623175041883,57.05233845112838,30.90524823878649,58.328057805808356,79.47696547082131,60.031368935342364,81.7976248275814,257.14334734873125,36.84827429078175,153.5878847958117,46.45596141204348,54.73233555390061,37.812119776673825,42.49869497235106,51.66439984917242,45.28015829986683,47.09354255956127,502.62792530880085,148.53016558433708,127.05344187844895,40.78409429100573,24.97872175514751,36.73678996664489,215.26666570420247,106.84631790896385,66.51016048106781,48.22390893467042,54.558758694801455,40.8348972741567,142.57351580988552,196.5629749596055,48.85330144815192,38.253823491291996,24.820668029788926,156.21841782784367,44.50964638874375,57.23297128010961,77.7678701487166,137.5169744737055,44.95699487926758,45.97728812421629,200.28868493527577,132.73749379426658,66.7486522630821,51.22410732853065,79.87181886651902,38.89516969609465,65.9682619941241,37.91322673569579,36.68598698349392,88.341331296222,116.18559964488438,24.343684465760347,57.8440182718977,60.45472712826714,60.847039053710766,35.43143553846018,37.39440635965471,387.75773028715423,66.75006345705853,37.11498995232436,43.16760091717221,36.97387055468277,62.24412109036252,49.08050367835489,46.857873165499825,395.22239579557913,115.42778847954902,105.68010277174392,70.15451659138468,53.97311319458885,42.53538601573788,64.89010979614235,83.74311837422127,39.54224359175973,146.57161394529317,48.07560349621767,160.03563007405603,44.76174527742875,38.11976006353249,82.5328568892941,182.61017488778353,50.685031229539774,88.58583606837476,47.55782537780211,162.9850254847653,37.75849440557001,47.72716865497202,36.26827356647482,87.0807617817888,76.96449128495428,59.467408850781155,54.94965942626867,96.34738490741611,44.69027921772498,99.36840102793593,57.08338471860952,55.816132527788035,53.151798300314795,67.2358948261334,262.3330814244455,39.17392196391518,42.363298436608815,39.339031659155836,45.637190375900055,43.55003448478092,37.848810820060635,36.68598698349392,86.42000735411008,69.01141848725644,288.10129962150927,35.12166687877831,39.26564957238221,191.47225108911272,87.94196340089628,592.7458054805381,78.27054239087748,44.19974302625761,152.75815302373934,47.918837052003354,47.463275381382246,95.32392488856581,43.95928073794154,48.252132814198745,47.48584585704586,37.0655981631498,72.38637219886368,43.43149419076198,56.10966087488254,65.08485456488775,38.42242856347667,98.35798614082213,41.91304947213846,91.44172446240775,38.970710031311285,43.29601956902606,37.11498995232436,34.135959468110364,191.87784681896792,53.484840078748945,183.62938216575367,37.580683964541606,44.01572849699817,48.60775369625556,39.738613508442185,46.493785119584516,115.81621827329154,63.9276755042267,257.5632457841598,71.2884632852121,63.166020801437035,27.683980607936817,87.96736489247178,72.80690800383562,173.3177877800826,86.49690076904639,97.22903095968941,93.66999975116848,105.68010277174392,58.65545480833685,86.71986941732011,45.10658144076767,57.36138993196346,67.09104398827158,36.78335936786662,137.88835180818467,45.325316507112134,150.71751624778176,43.99456058735193,49.08050367835489,91.35987521177563,75.02671612873786,64.43570533573643,68.98821710365416,33.83819753908661,96.53754591124562,111.04038640687195,689.6510846470425,183.3584329222818,76.57197353291328,100.43667486808278,56.874528010099965,22.854874820641562,146.31035745943484,71.54812297687263,44.403806840512544,61.4707867912866,52.92036248818258,38.61624574811272,46.73933287148088,416.70923528048786,40.292998787212994,57.859541405638275,38.56005258417425,101.55434049740418,134.41540343222513,43.78429268486596,77.66423767065919,49.33310740013333,41.15947188873236,83.61383147522868,180.11573174205654,115.98097651830406,46.530476162971325,212.76578972154547,102.4871397158151,225.45717162672133,49.32887381820408,69.67546857016872,65.79609632900137,50.47899690898305,44.57032772972963,52.91048413034767,197.03244565195897,41.91304947213846,53.757200516197216,73.64092364389742,93.66153258730998,44.89349115032887,59.186063743469234,124.16166799958711,184.74281421314583,45.088235919074265,61.33107858762142,52.526639368762545,38.79148839630646,49.70989619183638,75.29907656618613,525.323190869315,55.172628074542374,68.33060071064435,25.445826961341176,55.72071553137769,177.79434808461912,89.6184618448784,83.23986507147846,241.67179041574016,167.04534772785166,76.35888324247448,45.536995603574525,31.65704979871029,41.622343512996785,44.38064922815567,59.197353295280564,97.98501677487472,87.94816421082038,50.91223345974274,48.593641756491394,22.854874820641562,36.526522064158925,40.76998235124157,51.77588417330927,40.748814441595336,78.95689135305616,156.360880959469,469.2137037922346,107.95692756840319,380.69413720834393,38.86326097851264,195.4904675375294,53.188489343701605,50.815732456937816,59.96504281845082,135.49496682418328,36.8087608594421,41.91304947213846,197.4223920912428,92.18683488195535,197.4223920912428,80.15076145710404,30.07877678591462,52.0171983432764,147.87113799735084,120.75927932244835,159.78040470731997,50.25885064866217,69.87303572686695,57.36138993196346,167.8296544058011,81.1230741068546,31.752247790674588,39.090661519306636,158.20508159721734,94.20060868630085,42.47049758371564,216.2590092884424,104.07332174530659,46.17767766886735,133.2144773582952,51.173304345379684,48.6166705119507,37.67946754289072,95.15246267063542,49.46717082789284,50.018947672671466,58.94757196145494,59.10844807476636,37.848810820060635,43.25086136178075,59.197353295280564,100.00343831732309,132.7092699147383,42.6023869801258,64.90560652097561,58.55949361794057,55.08231166005176,58.20528392986017,71.64408416726891,24.97872175514751,71.00622448992893,50.08104020763376,58.8339627312074,50.380213330633936,190.68111846590398,90.0607514447004,19.588545152996403,40.456697288477244,54.03520572955115,170.3317013259865,114.681266866025,76.00043997246483,50.62293869457748,42.436602437388764,50.21087005346403,56.89569591974621,71.83177296613222,720.8427051077633,54.245473632037125,51.93252670469144,47.275586582518926,59.61506671229967,68.63965219147943,94.40946539481041,1723.6349925706252,403.6810924902162,38.04544157194375,341.4065135049584,39.54224359175973,56.18022057370334,43.26073971961566,47.88945596225985,36.4093929641164,31.19767482731372,28.68839049273867,271.0318527090011,69.73050513524893,91.01554388153014,44.27115460672945,52.983866217121296,89.74688049673225,50.272962588426324,63.151518817197946,37.45367650666418,101.429689151175,81.20210096953389,90.0607514447004,37.632898141669,111.01426815071872,36.33459968336636,37.40569591146604,57.52932201515695,53.84046096080576,176.94314510549506,17.781632475426356,216.97088658802872,28.24845819850318,51.75048268173379,36.326132519507865,29.635661877320018,104.94967320466087,43.84356283187543,85.1280455492641,455.4606681096352,215.62517629466808,31.462062669055,43.743827287327164,183.62938216575367,94.26693480319241,36.26827356647482,79.24966127313063,80.92619929952755,80.15358384505686,51.69544611665357,63.08134916285208,78.85669658073064,41.18346218633144,126.58327686311681,26.38991573156342,228.77488866527514,77.91260781050839,38.526183928740274,48.26342236601007,28.24845819850318,37.580683964541606,60.93735546820138,258.5077060295571,66.62305599918109,45.55392993129152,460.961455992938,101.95540677326312,75.79017206997887,79.95955597608477,59.53783970138672,56.00664371460418,107.88801853599637,125.2861458756301,69.42427604236669,225.6066313782346,99.83127265220035,40.21961670043937,42.89382928574752,255.02450782058753,87.99135519007085,63.038623299084676,126.35416309818245,38.34837348771187,37.11498995232436,664.8846303609432,32.092550590260124,81.61558080462375,161.7911553807174,354.12560481439493,46.85222838959416,40.82925249825104,68.9120126289277,36.66058549191844,73.77780945960977,58.349225715454594,42.82750316885597,45.088235919074265,119.21684430622574,70.84690994554339,53.77305058722174,37.75002724171152,46.545999296711905,32.489096097632995,47.72716865497202,45.10375905281484,98.24988309835328,51.14050036039469,57.859541405638275,72.7250587532035,47.44916344161808,62.63219943387689,42.31100617348775,52.846980401408956,109.86909540644675,69.65147827256965,40.105309988349674,176.42905058939584,194.19358027320317,26.88242242933258,92.69038857353804,65.41930753729832,29.508504458762154,43.2113479304411,61.04409743878286,82.41713898322799,44.94429413347984,126.6241972708267,45.66682544940479,74.85455046361511,100.75983828868203,94.20060868630085,273.7637526334144,81.42648081178402,98.24988309835328,361.1349926451629,83.23280910159639,53.12498561476289,38.18931514665181,310.9219012264218,32.70338345977525,45.236411286597935,281.01146478091283,24.97872175514751,49.58906591268689,41.622343512996785,56.12659520259953,69.4172200724846,82.73042404599232,55.108734132816416,48.35797236242993,37.67946754289072,41.19334054416635,127.05344187844895,46.99334778723575,55.50962096220972,140.86798271336062,36.68706829850408,80.4315890584108,140.52587335010602,99.62100474971439,436.7518686470729,104.2158523369246,59.87613759793661,290.5428490129996,61.50747783467341,119.35160091623567,87.17568507170245,160.5930516947403,42.65930792013643,198.43986294823867,347.82704830262367,51.55573791298839,401.0619164699882,41.591297245515634,220.0029069078738,44.79752995993259,24.97872175514751,52.03695505894622,36.927301153461045,47.76379151190688,92.9446460472907,371.21093028686244,206.4497999583754,31.746419822517332,96.91433470294865,39.40112419411814,88.92133202052892,153.39737360899557,39.97265775456658,65.12013441429815,56.61204593048661,34.55694298876937,41.3485718815721,71.9982938553493,59.11832643260127,40.151879389571405,41.98643155891209,231.59868781208337,38.274991400938234,32.77697966882184,135.0518519155887,37.67946754289072,77.19289888253628,200.28868493527577,62.666068089310876,71.99970504932573,77.19289888253628,207.55505302097507,1026.3664001854652,55.02586390099512,29.951769328037187,52.84133562550329,119.25212415563615,36.99786085228183,69.19142903625806,64.87458666240178,46.60950302565062,84.41323414998433,230.4838445707148,76.90924889327668,58.62158615290287,36.99631040561153,100.57356068379512,64.61841710445314,48.09407908884016,184.07569913107295,38.907206302372565,58.384505564864995,60.57467861626249,70.42199018369273,92.11873534663918,77.19289888253628,41.32034800204378,35.32479489957507,72.8308983014347,34.89610075479685,49.46434843994001,52.124449085484,41.68020246602984,77.85333766349893,53.10099531716382,52.214765499974625,233.83543026470258,1017.7488247184059,71.14593269359409,120.66472932602846,39.94302268106185,36.326132519507865,78.60409285895219,38.93825256985372,57.94562423819965,135.53871383745218,42.06969200352063,215.26666570420247,91.44878043228982,52.64094608085223,97.17117200665635,44.110278493418036,151.63620352642852,32.50603042534999,57.49545335972297,131.37004683111957,153.39737360899557,379.9335202550892,121.02943721083685,23.328056013403526,35.13931838534208,44.49271206102675,82.02227327133642,39.30657419769827,44.622541906857016,48.85894622405758,40.55830325477919,52.98952014601619,46.892034472245044,47.83441939717963,96.26095189186809,51.75048268173379,1520.8443367560149,33.98213932468103,1001.9466921244244,109.29897303997471,41.98643155891209,38.65742496854695,53.85175051261709,36.91742279562613,234.51562576133506,246.95074185451426,205.61155073638434,41.64492261661944,44.56609414780038,104.21444114294817,71.54106700699054,38.11976006353249,47.34614628133973,47.4054164283492,527.2000788579481,34.80801160373925,47.80619551765132,433.1369443599456,106.57677985946842,59.87613759793661,145.12918810117475,278.0523711277424,36.78335936786662,53.484840078748945,128.7311140952218,41.44876665389762,48.68678055893484,59.80962523635,163.5184568078505,120.3119308319245,127.6183680707916,244.29096643596807,59.64470178580441,44.735437424970286,138.2975980613453,169.6101136474684,294.5167712505868,45.4015209818386,371.21093028686244,48.324103706995956,65.50962395178894,52.49700429525781,41.709837539534576,136.85959139937748,100.37175994516764,41.709837539534576,131.45330727572812,46.52342019308925,44.86696043687725,50.2421781817718,58.59336227337455,91.49597899370617,123.64885076301158,141.37422012712477,44.71709190327688,88.16069846724075,38.43586751424965,37.236352634296125,52.30649310844166,28.012788804441726,47.82171865139189,46.4034687050939,48.22108654671759,153.22520794387282,412.4050936524193,123.58025608130376,65.33463589871336,41.879180816704476,46.95665674384894,101.8845598878855,71.93055654448135,100.06835324023822,49.01841114339258,37.40569591146604,59.09856971693145,147.10203728020417,48.448288776920556,52.218999081903874,58.54679287215282,218.48699418998376,70.20962583525531,33.83819753908661,50.71889988497376,1032.5712209161093,78.70444080837277,65.51809111564744,42.89524047972394,66.29142541472335,44.13371896396443,37.44238695485285,87.93546657502726,33.55172516187418,43.60648224383756,94.14720805912981,88.38931189142014,73.08659822712741,228.77488866527514,50.380213330633936,55.319392248089635,35.61403840192301,132.3649385844928,45.12526635547561,161.7911553807174,39.84013031970487,62.029619605947296,86.15821421470658,369.73199899957854,31.1682185357077,111.00871144667519,51.45388317738175,42.336407665063234,44.687456829772145,278.0481375458131,147.01172086571358,41.68020246602984,87.67071392821451,226.58921557774755,96.14094176024463,38.970710031311285,45.55392993129152,43.693976270375344,54.61379525988168,57.4361832127135,60.676284582564435,1358.6221372239131,40.82643011029821,37.8459884321078,141.2004240708328,122.83846139514662,161.53008449508047,38.58545407574974,24.933563547902203,40.59640549214241,243.51339855496292,50.48282330128043,35.13931838534208,37.70486903446621,422.521943269345,39.339031659155836,64.87599785637819,45.66682544940479,36.866619812475165,106.57677985946842,347.82704830262367,45.680937389168946,60.847039053710766,48.68678055893484,158.84458235796097,94.93792337598384,37.30973472106976,45.35361708594914,312.0057670426458,85.66006274103177,55.83892827675575,78.5335331601314,35.56267657826686,111.85605652524035,87.0807617817888,221.82806995797222,52.03695505894622,62.994876285815785,59.07316822535596,40.64297489336414,40.85465398982653,65.46023216261437,171.96868633862897,147.87113799735084,57.30776456085965,72.25371996508059,28.03517866847701,428.6656702375501,54.275108705541854,69.75421245955617,38.08024663219284,45.03319935399405,55.59457507349074,48.07432237317033,44.859622494894886,53.29150650397997,137.88270703227903,38.52053915283461,40.77421593317082,87.94816421082038,66.49604854130367,23.285288983448417,112.9342087232221,39.728521196646625,16.807162742395477,43.465362846195966,336.0665554982006,69.87303572686695,149.16379167974782,40.63888855195532,36.26827356647482,142.9700613172584,92.15155503254496,43.32424344855437,44.876556822611875,707.3418123353923,61.968938264961416,93.73914825601285,48.68678055893484,42.22915692285562,48.418618083968106,394.44623910855046,293.9328289224127,69.4172200724846,63.99823520304749,829.1377308579205,28.365587298545705,436.7518686470729,86.36565972923971,118.9642405844473,41.026684923369416,388.0563527833392,37.75849440557001,51.50916851176667,38.526183928740274,40.03475028952889,60.94300024410705,47.628385076622905,60.031368935342364,395.1117593282339,32.127439865824236,180.40644530353134,35.96486686154539,135.0518519155887,30.482378263169572,39.38136747844831,106.03488137252471,279.48596586458564,99.59719991714113,93.40328408962587,37.058542193267726,139.0525868387278,71.89104311314169,34.010363204209355,36.74807951845622,88.8310156060383,82.73324643394515,227.086064368524,2321.4626601719606,52.999389350861875,38.526183928740274,53.12498561476289,369.73199899957854,69.67546857016872,37.44944292473493,78.1525107864991,234.8543123156749,148.27756186255863,37.273043677682935,38.866281677056506,210.02255302472764,68.02013803583284,66.1628189919527,40.41012788725551,570.1072465938837,54.021093789786995,63.37699794815323,42.977089730356056,96.33574517261813,42.3773322903793,63.620329255364275,45.236411286597935,129.77116405584036,44.02984043676233,97.65521154056702,31.704472246745752,81.55066588170862,59.729410955552254,53.06853785570625,117.84939734307872,38.4888635856221,43.60648224383756,223.80607467431471,56.298253150497686,63.399888957047146,255.63522809227277,53.757200516197216,295.1955555532429,47.75821492245317,42.30818378553492,45.525706051763194,74.82350419613397,196.1537287064449,125.60344862587229,83.99485384886098,41.687258435911914,348.520342340071,42.371687514473635,24.97872175514751,54.61379525988168,43.730667313762154,162.9850254847653,43.569791200450744,88.91992082655251,60.25857116554533,42.67509421940306,47.711645521231446,62.63784420978256,36.08133109996,37.17002651740458,58.41837422029898,37.524236205484975,277.55421965406754,39.217752277634276,136.64367872098586,37.67946754289072,54.61379525988168,99.77200250519088,43.60648224383756,42.254558414431116,38.22277722381085,101.27071014325688,76.0060847483705,66.00238110905566,187.2166372538029,126.41816716787613,404.4737199444762,105.29823811683559,287.5837104190799,63.84202049760104,81.29947335390659,73.10043635093014,117.86774286477214,93.81255644384476,35.12379525160151,51.410384933417554,38.29474811660806,49.27101486517103,87.70859330892576,223.4970231934796,101.69545989504577,48.85894622405758,93.82664228255064,74.88983031302551,37.51012426572082,57.58576977421359,140.0706201669125,38.73375635099787,60.964168153753285,85.61631572776287,124.26327396588904,64.49215309479307,79.06696448321661,118.91343760129632,72.28476623256174,80.59387636569863,71.18403493095732,60.17389952696037,28.70403254637717,52.72138413750794,25.881885900053696,156.360880959469,53.76707887403213,89.7475876321594,45.48619262042355,138.25436275704368,44.868089658753384,60.90207561879098,58.20528392986017,23.698768818538277,65.18222694926045,909.7748958977734,36.951291451060115,78.96654565595965,707.3418123353923,43.95460864652445,83.04043678121135,43.341177776271365,187.30139110966093,2434.794237223946,74.64860454720399,38.234066775622175,57.410781721138015,53.175788597913865,23.686068072750537,61.61896215881027,109.65036034010228,338.466996452084,39.27270554226429,58.02182871292611,38.866281677056506,448.0323252332763,89.82590735941153,44.03971879459724,37.813530970650234,35.46389299991774,55.35968801671895,426.1995147718849,329.3337490367202,44.659232950243826,196.1537287064449,93.25793111005504,254.1961369564314,38.8394689915046,262.9525955800921,44.0806434199133,43.201469572606186,205.61155073638434,42.924875553228674,33.445885613642986,36.4446728135268,37.15591457764042,42.22915692285562,60.386989817399176,104.2158523369246,53.25763784854598,461.241534787693,107.65210966949734,81.28536141414243,59.005430914487995,56.31569519543927,140.2041211234832,60.51681966322944,92.83457291713026,42.22665459197174,72.70812442548652,71.04573792126857,64.35103369715148,186.35016415228353,42.00167757394988,69.59557587266164,44.56609414780038,27.060232870360977,55.35968801671895,25.881885900053696,265.2670763296402,74.49610719360547,93.25793111005504,177.1266003224291,51.81539760464892,297.28412263833843,96.69559963660419,39.090661519306636,222.0942963809222,56.225378780948645,56.84348174261882,83.81366952706858,73.71995050657672,88.91992082655251,76.48819477385476,28.438969385319325,76.23328697857346,1520.8443367560149,35.19576614439872,78.48837495288609,36.26827356647482,32.66099302171411,164.133847948239,1005.2691844114803,41.54896142622316,44.02984043676233,107.72747441045428,50.380213330633936,63.84202049760104,23.59716285223633,118.87674655790951,40.20134288512041,147.44354622249682,168.92050734957058,49.58906591268689,41.86789126489315,107.8507435191209,52.463135639823825,43.5649380831363,59.64470178580441,35.35236531880452,92.25033861089406,56.024989236297586,49.72118574364771,81.66133185927347,64.32845459352882,46.545999296711905,41.55310421613025,41.04551444476452,45.15032845403657,44.66487772614949,79.71046893646226,114.55708179610039,421.524229128019,53.332431129296026,52.21335430599821,38.01815409723055,65.18222694926045,58.14178020092146,81.42648081178402,36.50080242722968,71.83177296613222,51.54303716720065,141.52694630272543,65.31629037701995,84.20935533327619,41.16370547066161,30.905736456094345,210.5479072658872,33.81561843546395,259.54329603167184,37.90243619116444,114.12949002124638,132.02607291162963,44.56791516524192,43.218721795379054,58.56522857451145,82.44046891153761,37.39440635965471,107.2400410283839,164.24804409365754,67.80140296948838,64.4244157839251,105.9812560014209,27.915416420069022,50.272962588426324,35.2479803215261,97.8852361587228,37.11498995232436,88.31875219259933,54.417538601254826,31.746419822517332,39.090661519306636,212.355646749676,43.01801435567212,168.92050734957058,56.696717569071566,139.50275771720447,88.72168411445885,86.42000735411008,30.366277217277954,38.94954212166505,44.94429413347984,103.74169116084884,139.31365772436476,345.39595887628616,101.27071014325688,88.20021189858039,70.36554242463609,31.588754340679642,72.21420653374093,98.98878984828005,42.68638377121438,48.50897011790644,80.17898533663235,83.91101769332262,45.031788160017626,176.273200172673,857.0398581596158,72.02792892885404,38.94954212166505,54.88474450335353,40.662438567888096,67.78164625381856,38.10705931774474,58.55949361794057,40.00229282807132,39.16122121812744,204.84921050254619,97.28830110669888,24.61886729116145,18.468883941940902,267.8804849457365,222.17896801950715,29.67372214439189,149.49401107022913,78.32467645162184,455.1197849471514,162.9342225016143,22.56840244342913,41.09314577184082,1192.6985068774663,69.2140081398807,48.82648876260002,138.2975980613453,39.48861822065592,77.93800930208388,68.38987085765382,39.36584434470774,103.69794414757996,133.6406579391728,108.83327902775747,41.91304947213846,54.76197062740535,57.21885934034545,189.5535744787477,40.10107640642043,92.49870875074328,43.081518084610835,192.34632735807477,40.64438608734056,121.30964497325054,38.526183928740274,41.498158443072185,64.87599785637819,39.39830180616531],"xaxis":"x","y":[30.29,47.7,6.73,5.66,152.14,7.53,17.11,9.67,73.77,5.21,150.69,47.42,17.51,11.08,4.76,73.31,30.34,59.24,33.58,31.96,232.12,9.43,8.12,51.2,14.69,33.77,10.29,32.33,61.62,7.67,21.08,35.56,105.32,41.15,287.23,85.28,139.34,18.63,25.15,10.04,42.74,29.93,17.38,263.57,11.17,8.59,570.98,13.65,13.22,2.91,6.53,49.15,1222.69,28.59,51.64,185.23,141.7,3.27,66.39,85.15,87.31,95.26,48.51,26.78,50.14,6.02,14.87,349.47,4,14.79,4.98,20.39,6.45,19.28,93.47,52.87,372.55,5.81,12.68,5.81,8.57,47.39,128.73,99.39,37.72,3.02,12,38.97,39.27,266.18,5.9,29.84,49.18,3.82,8.8,11.85,10734.57,266.78,12.25,6.04,18.07,7.21,63.89,8.53,138.68,18.34,29.39,218.53,69.3,587.16,11.97,8.46,4.36,5.16,46.57,32.22,5.46,181.38,7.08,65.03,5.87,24.25,23.17,178.21,39.57,16.5,4.48,61.36,4.03,8.8,77.29,6.05,45.3,8.77,256.57,2.17,37.97,11.11,74.45,17.67,19.83,406.2,209.92,45.81,2.36,22.02,198.38,6.22,8.31,5.96,13.61,262.75,152.36,15.43,7.23,5.94,43.45,60.94,159.16,31.92,9.71,69.36,18.1,8.37,23.58,48.3,147.82,71.47,18.89,53.36,17.65,4.5,92.59,62.57,4.8,25.37,90.31,100.42,6.37,2.9,63.61,64.19,22.15,192.63,5.55,109.79,3.29,67.15,31.92,79.3,8.81,6.71,12.33,29.33,12.15,16.83,116.21,6.43,187.92,39.56,30.85,78.29,4.33,4.63,16.89,3.42,10.21,5.97,49.16,20.1,190.58,7.67,4.79,1000.06,50.52,122.99,134.69,5.6,27.38,7.08,2.04,44.6,3.51,41.98,6.36,5.87,8.87,3.97,127.56,37.85,4.26,147.69,5.9,10.55,5.69,32.81,34.81,56.25,653.5,65.65,97.01,3.49,4.65,9.13,41.69,14.77,39.47,2.82,142.48,68.97,6.64,7.31,82.9,8.69,256.31,93.23,11.77,38.08,17.24,43.64,18.41,12.7,27.3,6.71,3.89,71.03,1.78,152.71,50.43,22.18,12.06,29.26,20.98,77.15,3.84,91.09,14.35,839.79,226.43,27.27,57.69,5.16,6.69,20.79,176.79,59.61,9.83,8.24,16.09,20.44,170.84,35.39,30.02,19.72,15.59,66.05,3.89,7.46,14.35,7.74,141.32,46.61,20.29,3.75,235.69,243.38,151.95,100.87,56.51,6.79,4.84,247.85,24.17,168.62,47.11,12.99,104.17,30.66,3.82,120.91,132.47,106.37,44.93,6.93,61.71,4.2,24.42,8.14,111.69,6.22,9.45,23.84,4.88,507.7,553.27,9.48,122.92,103.37,50,3.86,3.4,3.66,93.43,6.85,32.8,32.73,13.43,413.1,6.19,6.16,68.14,0.68,16.45,13.88,350.87,802.89,291.28,91.73,4.72,7.21,25.02,6.75,28.93,229.55,3.01,49.59,341.23,37.18,353.49,93.87,93.21,11.79,49.25,154.4,84.93,6.02,43.63,35.36,51.47,42.8,4.35,17.11,205.07,40.78,5.6,186.96,14.51,35.56,537.03,53.38,38.9,8.16,37.51,3.82,5.51,50.76,380.17,10.75,4.3,7.12,1.6,63.45,4.42,7.51,7.14,22.78,8.45,8.59,5.37,11.59,6.93,54.95,19.7,89.41,74.04,3.91,35.22,43.13,28.54,70.4,10.06,6.3,5.11,45.38,0,10.14,765.56,14.46,28.64,27.86,8.85,256.72,196.67,525.78,93.67,8.97,165.61,3.06,17.84,19.17,50.7,47.25,2.99,3.2,54.92,96.4,16.01,4.74,10.56,132.28,15.5,7.29,63.36,47.68,297.23,67.06,3.4,98.54,0.76,3.72,50.15,2.45,265.21,188.67,156.08,83.1,94.85,20.57,29.18,12.09,13.77,34.14,1231.19,39.41,7.57,113.25,237.36,93.23,33.52,57.03,12.79,12.97,5.72,6.38,10.93,16.2,57.16,18.62,346.62,44.19,190.45,6.1,75.67,3.69,73.48,1185.15,20.12,92.3,188.13,72.71,101.31,15.8,80.06,3.18,141.15,17.74,22.65,42.2,14.32,46.86,18.42,247.85,52.43,5.71,51.26,3.07,10.14,242.19,21.3,8.45,113.31,75.12,13.15,0.89,8.57,3.48,5.33,102.04,3.71,31.58,42.52,6.89,7.69,6.16,29.16,5.88,76.85,1.3,37.58,7.29,24.1,28.78,25.39,11.28,32.02,31.46,38.54,49.7,9.47,456.22,124.92,14.19,110.62,24.36,4.13,68.89,145.48,21.24,18.72,12.4,15.62,43.16,160.3,47.78,139.27,30.86,44.97,163.99,142.69,12.63,4,109.46,26.64,64.13,285.62,9.57,74.52,4.01,226.81,10.16,13.64,92.58,7.15,8.16,75.64,60.02,223.36,8,13.87,3.92,73.32,233.43,208.26,351.53,16.76,200.3,604.45,51.91,16.06,10.92,90.73,4.73,69.42,238.61,4.93,900.81,0.7,399.76,31.85,19.95,248.29,2.43,480.9,10.32,176.76,144.3,5.15,97.97,27.69,63.6,144.53,27.56,29.24,26.55,18.5,80.06,10.77,15.34,0.75,3.91,474.59,4.25,128.41,433.86,69.88,44.32,413.35,4.43,348.22,29.78,66.32,290.4,44.51,4.02,5.2,56.82,4.55,9.65,52.88,57.79,58.32,393.2,72.69,45.02,24.62,4.19,223.34,10.89,83.27,7.56,29.73,100.71,7.53,121.88,176.47,23.82,4.48,53.21,16.3,4.9,6.39,54.95,61.64,7.12,19.68,8.07,704.76,33.35,184.81,61.25,20.88,60.04,16.47,551.43,260.13,8.61,167.31,70.14,64,21.92,9.16,111.23,7.02,236.34,318.08,149.93,364.9,295.86,34.62,3.11,156.18,118.86,4.16,3.27,6.54,3.65,48.91,8.87,5.19,148.11,88.87,1793.9,37.34,275.54,30.87,5.53,56.96,82.38,21.52,1597.56,42.41,109.72,4.68,19.81,79.81,6.74,6.02,39.05,5.79,271.32,85.55,4.52,233,9.53,157.21,15.23,35.44,3.92,64.75,40.35,20.45,15.27,29.42,39.75,119.03,28.07,95.77,23.14,60.83,224.13,98.65,443.86,3.52,221.47,5.68,7.9,11.73,4.71,38.59,32.75,4.79,101.43,5.54,5.66,5.88,130.58,60.33,25.6,69.18,253.53,84.57,3.55,42.71,5.09,12.28,19.15,93.68,42.24,58.44,1084.79,243.37,7.47,4.43,8.09,69.67,82.72,101.64,19.04,4.02,5.6,46.49,183.51,98.22,6.54,69.3,41.96,3.71,5.97,913.07,145.42,7.56,5.3,45.96,8,71.16,13.1,53.88,53.02,379.34,43.35,7.94,322.79,4.43,14.81,3.41,18.97,3.9,115.56,61.68,29.92,9.99,530.66,3.7,49.17,27.39,96.62,6.57,51.93,21.38,63.48,90.34,423.93,183.79,5.86,92.45,23.85,45.32,77.75,5.31,2451.58,9.87,39.5,67.17,162.39,220.26,8.46,23,31.79,28.83,20.02,101.99,26.72,15.5,32.77,132.56,15.74,110.66,9.65,246.83,10.83,5.92,9.2,467.18,24.35,3.67,5.35,862.64,89.53,234.03,1.67,15.2,71.38,43.57,260.57,226.73,87.14,14.44,3.99,28.82,73,188.07,45.6,5.07,103.77,3.16,395.26,27.93,10.23,23.27,60.07,112.19,7.22,20.07,15.9,72.46,3.92,176.63,32.52,32.35,6.84,96.59,44.71,3.52,48.95,116.95,36.65,142.36,8.27,3.89,315.85,9.73,1.61,5.81,1638.46,8.61,14.99,49.18,26.73,17.42,202.49,59.21,9.99,42.57,481.1,29.62,399.54,72.88,52.79,20.67,381.89,3.56,4.07,15.74,26.94,27.84,32.72,15.64,582.28,13.36,23.83,27.06,411.09,4.74,15.18,73.14,185.82,13.97,12.15,3.49,19.84,93.67,93.55,59.18,74.98,43.56,312.6,12307.16,6.47,6.19,12.59,579.99,59.63,2.81,119.34,283.04,19.72,40.76,3.18,62.01,1.58,15.77,3.34,587.09,12.07,23.74,4.61,89.6,103.42,7.52,57.33,16.82,65.75,22.76,8.42,39.54,99.91,47.12,80.75,10.17,4.54,28.3,19.36,6.35,63.87,10.05,184.64,34.08,3.15,76.89,11.99,292.92,112.01,66.1,23.22,251.43,75.58,152.14,41.06,7.07,115.34,6.12,34.87,85.3,19.76,28.03,6.67,18.73,20.07,34.98,12.56,150.23,12.35,50.23,19.16,45.22,344.2,4.74,1.34,3.77,36.71,3.92,83.35,324.92,64.95,675.34,22.17,536.43,43.84,30.26,78.19,44.61,61.72,39.57,73.52,3.99,182.11,99.62,551.53,39.45,6.99,89.94,7.27,4.76,9.66,77.56,3.78,44.26,39.31,37.14,11.29,24.91,19.73,326.74,64.79,59.06,2.35,49.79,61.19,4.39,357.86,6.61,126.3,53.25,58.98,4.95,6.87,8.93,39.1,66.44,440.31,38.45,102.08,1558.85,37.17,124.96,5.3,260.94,3232.23,74.13,31.18,225.67,6.04,49.44,8.31,67.65,320.35,46.72,143.67,3.16,94.59,96.15,5.7,25.49,3.65,22.46,1727.11,49.26,173.75,295.48,53.56,136.07,5.03,250.05,74.28,3.59,123.14,0.12,24.64,4.98,46.95,27.76,5.53,17.23,154.13,740.97,78.06,125.42,7.53,131.64,129.05,6.67,99.65,298.91,30.14,46.32,5.88,0,14.3,45.07,1.14,147.24,21.28,4.44,249.09,72.02,47.25,1623.34,8.7,226.06,10.83,47.91,434.48,5.33,120.13,38.15,180.38,37.16,8.18,7.05,148.7,0,29.26,39.35,24.18,4.02,103.39,2089.28,4.5,77.48,143.3,46.61,46.85,80.08,108.04,23.01,47.82,67.62,69.53,4.2,25.48,7.74,4.33,22.61,3.63,11.25,58.79,58.85,13.34,7.25,26.41,5.94,5.02,4.44,4.72,23.65,96.41,529.47,7.1,14.15,3.72,48.23,63.06,96.39,5.29,10.08,4.49,22.08,1084.49,8.16,65.77,20.63,375.56,112.13,58.52,26.54,68.3,464.77,5.14,57.17,8.98,91.49,3.68,44.35,92.7,4.99,0.37,305.15,57.25,0,16.27,45.59,34.26,34.92,24.31,5.07,1.08,28.19,82.87,109.71,17.19,50.31,20.25,41.22,5.74,45.98,17.88,86.5,8.55,394.6,22.57,48.85,38.13,269.23,47.26,122.83,5.23,8.81,48.65,640.71,46,328.63,4725.43,73.9,65.65,6.98,3.5,8.54,4.15,6.98,4.69,40.38,150.81,8.15,4.38,37.13,328.97,132.71,3.15,229.2,10.97,122.54,84.18,6.46,26.59,500.93,148.02,6.39,222.02,12.75,50.57,9.28,3.86,369.14,156.11,46.38,14.99,69.77,7.21,100.59,5.3,39.61,102.73,23.42,4.26,127.53,3.91,12.5,117.11,8.47],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN = np.logspace(-3, 3, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","TotalGHGEmissions_predEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train,\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","print(ScoresEN)\n","figEN.show()\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[404.10284921946163,404.10265202728806,404.1024743416511,404.1022868129807,404.10204450684256,404.1018485350034,404.10159403296984,404.10128767178514,404.10105424076926,404.1007324759307,404.1003633724102,404.1000768636765,404.0996795416738,404.09922599984236,404.09885915899025,404.09838389605807,404.0978592967825,404.09737370598975,404.09678658549035,404.09614248722875,404.0955424314792,404.0948003604905,404.0940260482923,404.0932627874515,404.0924597472505,404.09141911130143,404.0904820311697,404.08948999100505,404.088177761591,404.0870540133848,404.0858358333653,404.0843493881419,404.0829864884923,404.0812840765766,404.0795055003435,404.07782734432317,404.07573127480225,404.07375133810655,404.0714017756824,404.0691327687266,404.06644803127926,404.0635628045861,404.06076059107954,404.05746205446684,404.056047530046,404.04917086717734,404.0455860275935,404.04078357540186,404.03095022723403,404.03030250623874,404.02506467747634,404.01912866419906,404.01347575337365,404.00742501077036,403.99980587591347,403.99534416057907,403.98074116017744,403.97172613347715,403.9616095265641,403.95134992903695,403.94045094183974,403.9287019502879,403.9157016395258,403.9055727922806,403.8894027041544,403.87152131419873,403.85287077773694,403.842903229237,403.8159160694562,403.785557545208,403.76342789011153,403.73950148525967,403.71396901747823,403.68662546265324,403.65728251963367,403.6256766803611,403.59242076829923,403.55627401404115,403.5175221595111,403.47621678133544,403.4315118849245,403.3838858136112,403.33357182919127,403.27918172987177,403.2212209587734,403.1590446524717,403.0916798047964,403.01868355672286,402.94271605625767,402.8617868712652,402.7750877813954,402.6823542807623,402.5831446021033,402.477290621179,402.36431371244,402.24372057733433,402.1148257836591,401.97720496077125,401.83261005913874,401.68166575736984,401.52072533142126,401.3391268171348,401.121557332531,400.8910774713606,400.6436181523736,400.3800704340723,400.09946839412487,399.80220538974146,399.48638526351715,399.1504126165774,398.7967621302697,398.4178134624884,398.01602034004065,397.5851512061604,397.1344102786678,396.65757037953756,396.15280151043515,395.6206019273219,395.05989710920414,394.47092106459405,393.8520990370775,393.20371943274057,392.5273291989391,391.8226077785536,391.0922795264307,390.31657888335786,389.4697132866327,388.59593039771744,387.69251654728066,386.7683547926802,385.8583622773869,384.929854525123,383.96610475725737,382.97144402519064,381.94878529427586,380.90450388951894,379.8466931306425,378.78485168961026,377.73192338495903,376.7005310720456,375.8555740422694,375.30633159045976,374.80706302346596,374.3771038346554,373.9701750471787,373.3694629008894,372.8571758280058,372.3570602383811,372.0038606405393,371.874757166922,371.97419173780156,372.1403138122606,372.42220872906626,372.68551669315883,372.9238210854561,373.23624072459415,373.58506712865375,374.0050442296616,374.5237586345435,375.1616931831972,375.93822195277363,376.87515617173375,377.99690269674636,379.33059579651024,380.9061940346345,382.7565356573888,384.91734896534734,387.4272187141763,390.3275153745181,393.6623004454034,397.47822699627613,401.8244590821396,406.7526356082488,412.3169029861251,418.57403650920924,421.0138020396619,422.41660679040325,424.00685875099236,425.80933563419114,427.85181668521255,430.16535717291265,432.78456388913145,435.7478651368134,439.09776773986795,442.8810930551361,446.8765601728579,448.8926671016352,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006,449.52797111766006]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[542.3250189428588,542.3246974496398,542.3244386422873,542.3241107297209,542.3237149877295,542.3233945808846,542.3229919574047,542.3225026593752,542.3221098261467,542.321612031127,542.3210111724941,542.3205276488967,542.3199140269812,542.3191744119047,542.3185752796877,542.3178229389891,542.3169050389681,542.3161740203873,542.3152465901824,542.3141167726089,542.313215559175,542.3120689797813,542.3106815477202,542.3095657387651,542.308154025941,542.3064501618229,542.3050769375602,542.3033367467275,542.3012315688303,542.2995467746808,542.2974046433031,542.2951000752522,542.2927282574065,542.2900854016644,542.2872599284808,542.2843390445782,542.281084072136,542.2775917573495,542.2739888916177,542.2699844393616,542.2656940955052,542.2612606999188,542.2563255302409,542.2510449500224,542.2496178925956,542.2366106723262,542.2303041591929,542.2225600023577,542.2028784906986,542.2042654635165,542.1950799230945,542.185377978343,542.1752865423525,542.1642438179498,542.1521965042676,542.1403496106593,542.1258650210125,542.1114405456963,542.0954723066106,542.0788981438891,542.060886909859,542.0423161711515,542.0213206856712,542.000498489171,541.9762536272722,541.9505147033987,541.9242043512995,541.896200682899,541.8673441891722,541.8362709265117,541.8026877874189,541.7661902929447,541.7273178984826,541.685729013274,541.6409802191314,541.5927355984938,541.5423465400063,541.4872796463466,541.428097222147,541.3652173633942,541.2968644485499,541.224265726811,541.1477355742634,541.0646618384258,540.976296337398,540.8815458951273,540.7781686015377,540.6655778201737,540.549998405395,540.4269600342545,540.2950849715698,540.1540934153343,540.0033958911287,539.842585233733,539.6711205447046,539.4884619198825,539.2930895775545,539.0845397166715,538.8671370097347,538.6426542097402,538.4038565030073,538.126806814458,537.7781307486106,537.4094491158713,537.0126576501514,536.5895738804811,536.1388394968405,535.65801265491,535.1463650214444,534.6019633651952,534.0234151488821,533.4082701549728,532.7555477084858,532.0505845554549,531.3153989243131,530.5376445825607,529.7129675242937,528.8419957355918,527.9225312438145,526.9547902923944,525.9352179927355,524.8647986403622,523.7430426932842,522.5697419848442,521.3457848046634,520.0627756156383,518.7083526303154,517.308874581782,515.8596038984433,514.3595598578852,512.8080392359791,511.2092150280428,509.5740714879362,507.91328912988723,506.23663349871117,504.5593145370124,502.8991945117486,501.2757025747553,499.71396872529436,498.23256928201914,496.9586478780668,496.1485895881898,495.41154711873696,494.7706997615502,494.163728659179,493.3336004097655,492.6164803199869,492.05559439296303,491.67570321782864,491.56651961221735,491.67068226538595,491.8314126731704,492.13850921194125,492.47662133692637,492.81605429630014,493.25035169427207,493.80387597000015,494.48710282583,495.3071807333286,496.2990953857852,497.4931409059267,498.9249405500673,500.6364097767681,502.67686498143047,505.104272718768,507.9866154269397,511.40332229980453,515.4466778717442,520.2230785852175,525.8539659523615,532.4762363998284,540.2419302114429,549.3170540586764,559.879504853956,572.1162294822213,575.9755741151235,577.3171773624724,578.8060526504947,580.4599953540109,582.2990770932821,584.3459478169326,586.6261820570542,589.1686779624192,592.0061200955255,595.1755200599366,598.1028848818139,598.1822089420028,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176,598.0099675736176]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"y":[265.8806794960645,265.88060660493636,265.88051004101493,265.8804628962405,265.8803740259556,265.8803024891222,265.88019610853496,265.8800726841951,265.8799986553918,265.8798529207345,265.8797155723263,265.87962607845634,265.8794450563664,265.87927758778005,265.8791430382928,265.87894485312694,265.87881355459683,265.8785733915922,265.87832658079833,265.87816820184855,265.8778693037835,265.87753174119973,265.87737054886435,265.87695983613804,265.87676546855994,265.87638806077996,265.87588712477924,265.8756432352826,265.8751239543517,265.8745612520887,265.8742670234275,265.8735987010316,265.8732447195781,265.87248275148875,265.8717510722062,265.8713156440681,265.87037847746853,265.8699109188637,265.86881465974704,265.86828109809164,265.86720196705335,265.86586490925345,265.86519565191816,265.86387915891123,265.8624771674964,265.86173106202847,265.8608678959941,265.85900714844604,265.8590219637695,265.85633954896093,265.8550494318582,265.8528793500551,265.8516649643948,265.8506062035909,265.8474152475593,265.8503387104989,265.83561729934246,265.832011721258,265.82774674651756,265.82380171418475,265.82001497382043,265.81508772942436,265.81008259338046,265.8106470953902,265.80255178103664,265.7925279249988,265.78153720417436,265.78960577557496,265.76448794974016,265.7348441639043,265.72416799280415,265.7128126775747,265.7006201364738,265.6875219120325,265.67358482013594,265.65861776222835,265.6424949965921,265.62526838173574,265.60694709687516,265.58721619927667,265.5661593212991,265.54350590041133,265.51940808411916,265.4937016213178,265.4661455801488,265.4365434098162,265.4051910080551,265.3717892932721,265.3354337071204,265.2966137082759,265.255090591221,265.2106151461903,265.1628933130779,265.111996008625,265.0575068801753,264.99897923478613,264.93656198976373,264.86987020487095,264.79808310854276,264.7206773049995,264.6375941598352,264.55144681981164,264.4649839164514,264.3727058268498,264.27457865459587,264.1705669876635,264.0600972914092,263.946398124573,263.82640550558995,263.69886186795964,263.5701091116573,263.42735677000405,263.27649297159553,263.1197178568659,262.9534216330226,262.7774961765144,262.5926354965766,262.39920811905205,262.1972629745938,261.98705183679374,261.76898008141944,261.54264022511893,261.311615704594,261.07547357226304,260.838774248198,260.5703821510774,260.23107394295005,259.8829862136529,259.52542919611807,259.17714972747524,258.90868531879465,258.6504940222032,258.35813802657856,258.02959892049404,257.66093708984056,257.24969324202544,256.7941917495364,256.2940008044652,255.7498780446237,255.16849286207201,254.75250020647195,254.46407359272976,254.20257892819495,253.98350790776053,253.7766214351784,253.4053253920133,253.09787133602464,252.6585260837992,252.33201806324996,252.1829947216266,252.27770121021717,252.44921495135083,252.70590824619126,252.8944120493913,253.03158787461206,253.22212975491624,253.36625828730735,253.52298563349325,253.74033653575844,254.02429098060918,254.38330299962058,254.8253717934002,255.3573956167246,255.98432661159,256.708115350501,257.52645588783787,258.43137563089016,259.4077595566084,260.43195216381866,261.4706349384453,262.4802175927239,263.40698795283635,264.1882171578212,264.7543011182943,265.03184353619713,266.0520299642003,267.51603621833414,269.20766485149,271.15867591437143,273.404556277143,275.9847665288927,278.94294572120873,282.3270523112076,286.1894153842104,290.58666605033557,295.65023546390194,299.60312526126756,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253,301.04597466170253]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[257.9191870420962,257.9189774577292,257.91875002881477,257.91860670855056,257.91834799495865,257.91818470732255,257.9178903995044,257.9175710410086,257.9173695509611,257.91700625751463,257.9166120397606,257.9163634103473,257.9159149629288,257.91542834077205,257.9151215448986,257.91456798688216,257.91421844937406,257.9135887362968,257.9129054358231,257.9124741327026,257.9116968366915,257.9108533923467,257.9103212012647,257.9093617438114,257.90875541028396,257.9076639639695,257.9064796704129,257.90573152807923,257.90438433670556,257.9029225435461,257.9019994416614,257.9003366107826,257.89928492065417,257.8973933788122,257.8953410038639,257.8940434255091,257.8917088143006,257.8902304900013,257.8875748048761,257.8858905275557,257.88286958794487,257.87959205132074,257.8775141836251,257.87378599098116,257.8714186971832,257.8671778751833,257.8644808154847,257.8596568802086,257.8565840982591,257.8510968921343,257.84759603683005,257.8413544086239,257.8373658495281,257.8331677592005,257.825721979212,257.820938813841,257.800550011694,257.79190866667085,257.78142879517276,257.77121899594476,257.7612515706662,257.7486053300773,257.7356497767242,257.72210367772226,257.7075463221088,257.691208129429,257.6735416398362,257.6930715896445,257.6320728958985,257.5600166143866,257.5340997396989,257.50641739844355,257.47676898865313,257.44493511802864,257.4110325540129,257.3745378929321,257.335540994158,257.2936708586775,257.24907846740933,257.20117854540285,257.14993432153653,257.0951125248605,257.03645852161554,256.97372257747776,256.9066101521058,256.8345445507162,256.7577699604182,256.675678132307,256.58757379025536,256.4937457259121,256.39345773074194,256.28631055819363,256.1714214961224,256.0491551747368,255.9185918874098,255.7787459500965,255.630004584225,255.47133220720454,255.30212675007934,255.12175644856615,254.92893891555988,254.72416685341014,254.50619201019347,254.27436780272475,254.02786902112805,253.76597957996324,253.48793466616763,253.20132327074225,252.89894646702064,252.5778967998672,252.25314661720967,251.8938550532185,251.51443514460166,251.1143188227396,250.69303461846667,250.2502364514172,249.785738010658,249.29955369624585,248.79194724837754,248.26348937005315,247.71512585229632,247.14301738590282,246.56015131578187,245.96133489563033,245.35356475628765,244.7396247327984,244.12364917732302,243.51431912536924,242.917966804954,242.34946296276732,241.99346561247407,241.72920784099048,241.45513802800394,241.17505221963404,240.89013300306274,240.60152288997196,240.3119631888174,240.02430228668592,239.74246157782576,239.4710579914938,239.2148189215191,238.98063948242051,238.7762586617235,238.61083882437148,238.49509353227097,238.4424735923079,238.4284998433279,237.93745538288042,237.4699645605786,237.03528627687004,236.80335031503108,236.78885522381086,236.79590233754678,236.8293844725081,236.89503043837308,236.9998603116916,236.89555880331636,236.70023171437785,236.5004550639349,236.29736576956154,236.09240427504778,235.88736995349737,235.68448543079063,235.4864711269017,235.29663146409115,235.11895434633482,234.95822566721645,234.82016074455618,234.71155469588922,234.640453841004,234.61635022112443,234.65040122630893,234.75567608176334,234.94743050920607,235.24341019154338,235.66418266191687,236.23349584477688,236.9786596390158,237.93094461782198,239.12598913997758,240.60420301178368,242.41115250570186,244.59790837008254,247.22133595314193,250.34430536395007,254.0358004488756,258.37090899461265,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684,261.54742907473684],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[298.5309825101457,298.53090421013576,298.53079287050963,298.5307027192957,298.5306067410927,298.53047059304737,298.5303600904165,298.53020317119245,298.5300759451946,298.5298950843608,298.5297486030375,298.5295926530068,298.5293714940104,298.5291919455944,298.5289370486413,298.5287303295994,298.52843654585894,298.52819854308984,298.52794515467525,298.52758591548286,298.52729418812345,298.5268801533638,298.52654428458976,298.5260670937843,298.52568040295336,298.5252687140954,298.5246852155951,298.52421124558407,298.5235387611053,298.52299308681773,298.5224121192668,298.52158981747965,298.5209209800522,298.5199733078531,298.51920331114223,298.51838348989907,298.5172247025188,298.51628092093966,298.5149455220529,298.5138590422446,298.51270223196076,298.51106938027704,298.5097377199931,298.5083197793408,298.506323192013,298.50469101095723,298.5029529864654,298.5005116256055,298.4985111075373,298.49638075448917,298.4933955563345,298.49094360862506,298.488332408518,298.48555137325053,298.48167712744396,298.4910506231675,298.4692008858334,298.4646922374826,298.4597031303057,298.45448743048513,298.44897802326466,298.4430623049149,298.43655485847853,298.44619120695717,298.4317476901263,298.41338652187716,298.39347199764,298.38354758124126,298.3728927358083,298.36149569581283,298.3492841836968,298.3362048372613,298.3221809711363,298.3071771085735,298.2910747937325,298.2738865522361,298.2554383705248,298.2356875950325,298.2145468612697,298.1919147451588,298.1676896494131,298.14178704582406,298.1148203000719,298.0858765236509,298.05498263972316,298.0219217220287,297.98655313943016,297.948612732386,297.9081367133446,297.8648443089241,297.81855116381723,297.76893376200445,297.71602355342014,297.6594785192283,297.5990588194704,297.53451710095743,297.4654324405253,297.3918331887481,297.31327582762015,297.2294528836295,297.1398523800251,297.0445133203653,296.942884129838,296.8345961637144,296.7192859502948,296.5965650092517,296.4660559126502,296.32731260021905,296.1799314364232,296.02325088790417,295.85730874284116,295.6814487787798,295.49526595603743,295.302834380217,295.09470726448956,294.87511932700204,294.64377722210327,294.4004474746132,294.1449732362728,293.87729417456364,293.59747001685065,293.3082080856397,293.0054432999634,292.69176241594414,292.3677147607663,292.0343784565499,291.693680283604,291.34909968572043,291.000794857518,290.69187558186076,290.4062951440134,290.1029327570671,289.78925648732235,289.46645474448957,289.13205453440185,288.78906787705955,288.43925117166043,288.08605554206446,287.729879850595,287.3753134638816,287.0278227595393,286.69073662647054,286.3711734980776,286.0779386167021,285.8184059720962,285.60522200553066,285.44999288730935,285.36881917937205,285.3926796277519,285.5635317478938,285.9571872791259,286.47959911307714,287.0533820396024,287.3598138588353,287.4914162556001,287.6380225017079,287.80152708516056,287.98407949161196,288.1881194603306,288.41641707404733,288.67211830883355,288.95879673841034,289.2805121588298,289.6418769729401,290.0481312474673,290.5052274261026,291.0199257458381,291.59990145584777,292.2538649718528,292.9916961057797,293.82459348028465,294.76524015796275,295.82798637130287,297.02905001544286,298.38673524415805,299.9216690730732,301.65705532763485,303.6189445666752,305.83651776393094,308.34238055175285,311.17286375480575,314.3683248232151,317.9734437008244,322.0375057540642,326.6146637901885,331.76417108223666,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751,337.1115481151751],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[576.7643656442683,576.7635644358016,576.7628163400819,576.7618998144889,576.7609130290823,576.7599917155543,576.7588629060004,576.7576475615364,576.7565129236851,576.755122663239,576.7536258231233,576.7522284688705,576.7505162040696,576.7486726722123,576.7469517744172,576.744842928783,576.7425724110445,576.7404530605892,576.7378557879455,576.7350593966524,576.7324493430901,576.7292505295931,576.7258064792657,576.7225921174481,576.7186524586916,576.7144107660703,576.7104521945299,576.7056001340668,576.7003760924918,576.6955010291753,576.6895252955893,576.6830914408498,576.6770877324489,576.669728151693,576.6618043714827,576.6544107719561,576.6453469574714,576.6355883168089,576.6264831362136,576.6153205683104,576.6033023254419,576.592089458825,576.5783423532364,576.5635415206905,576.55874265927,576.526358638436,576.5086171835762,576.4893509664925,576.4689569532023,576.4470389738631,576.4234373654142,576.397975694107,576.3708429729419,576.3419020263912,576.3105719209973,576.2775111934232,576.2416501718702,576.2032177518089,576.1620037141325,576.1177584495236,576.0708257240389,576.0204030698626,575.9661697425796,575.9077971310589,575.8457227273655,575.7788536313409,575.7067613601864,575.6299561500696,575.5481259054657,575.4598203299944,575.3656434073937,575.2639219125267,575.1553196421107,575.0393388376491,574.9139487660043,574.7798931522542,574.6365059571694,574.4830407728314,574.3170908116438,574.1393712795145,573.9489448977406,573.7447935331846,573.5272455619034,573.2920929167956,573.0417992139223,572.7733669770284,572.4803178980395,572.1626810068559,571.8330743192588,571.4801334452259,571.101804491401,570.6969235614805,570.2635588559832,569.7991557650422,569.3020222216684,568.7698754573318,568.2002840770161,567.5906608892361,566.94823201105,566.2774107850129,565.5596303239511,564.7909621276492,563.969466824095,563.0979601467257,562.1582011651535,561.1533838032199,560.0792191196307,558.9311713356916,557.7044501309955,556.3940040939088,554.9945157165765,553.50039841084,551.9057961263622,550.175280704158,548.3581467757515,546.4250578007862,544.3604452073549,542.1632280138406,539.8256963633539,537.3417333554942,534.700512747024,531.8953121342605,528.9213308277879,525.7707987236672,522.4336794157342,518.909053627095,515.1904686691513,511.2839486082825,507.1716282886146,502.82959860661504,498.2595483620601,493.45095503707716,488.4056495494824,483.1285039341483,477.62750809282267,471.91598839304623,466.01602575252286,459.95260414319375,453.7674123472315,447.51344956206765,442.55689005557224,438.88454753809043,435.22512654633533,431.6324543009763,428.1614669530689,424.893247211502,421.9217626202111,419.36046477714206,417.3457959021113,416.2055005294269,415.7726249738385,415.71371705781405,416.1169987098516,416.61889268985374,417.08274035672315,417.80014800145074,418.82082874654236,420.2019464942768,422.00890363108505,424.31612200791267,427.20778586123794,430.7785083911255,435.13387807937397,440.39083795935915,446.6778526296391,454.1348254982717,462.91274375402617,473.17305107386204,485.0867767503842,498.8334816043424,514.6001109682217,532.5798676137917,552.9712276946788,575.9772175295076,601.8050484021479,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987,607.8168668871987],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[321.0288372084821,321.02885024435403,321.02886687342107,321.0288818749513,321.0289009941343,321.02891825779545,321.0289366202615,321.02896010712374,321.02898123981697,321.02900824463586,321.0290325657167,321.02906361570456,321.0290916065627,321.02912730804553,321.0291595229015,321.0292005732285,321.0292376502011,321.0292848514481,321.02932752532604,321.02938180009227,321.02943091707215,321.0294933266696,321.0295498613684,321.0296100130189,321.02968670189153,321.0297559459072,321.02984413694077,321.02992385163094,321.0300252735668,321.03011704752385,321.0302336901132,321.03033935473104,321.030473508523,321.0305951756823,321.0307494779787,321.0309155247303,321.03106707176164,321.0312580705773,321.0314325996485,321.0316523168468,321.03185333817277,321.03210611395014,321.0323376817707,321.03262851923506,321.0329414852883,321.03322998770454,321.03359013082667,321.0339225935441,321.0343370830399,321.0347831073593,321.0352630336893,321.03571084000293,321.03626333278976,321.0374354101404,321.03701361928125,321.03748466199494,321.0368798475896,321.0365215374444,321.0364924490261,321.03708539658993,321.0377225258917,321.03841066480385,321.0391524841504,321.0399559556935,321.04082281945267,321.0417580972517,321.04277343017145,321.043869792255,321.0450573314371,321.0463444208751,321.047740057533,321.04925074946436,321.05089509934606,321.052681950636,321.0546251171761,321.0567351010734,321.0590379666101,321.0615484292834,321.0642825877254,321.0672736431462,321.0704686871708,321.07373647675035,321.07731375144033,321.08124508890074,321.0855639415911,321.0903087777857,321.09553836648774,321.1013031303169,321.10766134471015,321.1146905167194,321.1224635298727,321.1310748459345,321.140622473489,321.15122234636374,321.1630024206189,321.1761099679603,321.1907187163871,321.2070054179105,321.2251908251228,321.24553461706773,321.2682864480756,321.2937989484269,321.3223883748544,321.3544827194678,321.3896517761876,321.43009697671505,321.475646008844,321.5269918399225,321.58492550374064,321.650350242467,321.72429769289494,321.8079464082745,321.9026430467652,322.00992660214484,322.13582448335654,322.2749855884754,322.4318022686273,322.61031059637844,322.81298181046265,323.0436414423637,323.305430180112,323.6042365828846,323.94362018265537,324.3321018701089,324.78189974447713,325.19180495160555,325.4303078495795,325.69649292697363,325.98858134697423,326.3111881251357,326.6682531125228,327.063713462438,327.50204479258093,327.9885289977569,328.5291969097417,329.1307674488323,329.80029896112,330.5462564030524,331.3785650363429,332.30727781366556,333.3455041550912,334.50612225664605,335.8032767870943,337.2564371300421,338.6518782388888,339.06726215148944,339.5179594389563,340.0073329753732,340.5391460601525,341.1180085111392,341.6849428756918,341.8405400385038,342.01212043651435,342.20153935348145,342.41088843616836,342.6425274271238,342.89912020899686,343.183675726616,343.49959441905196,343.85072086482495,344.24140341876,344.67656169777433,345.1617628535963,345.7033076508869,346.30832744637814,346.98489323431545,347.74213798017877,348.5903935012999,349.5413431606758,350.608191608061,351.80585271723305,353.15115671497756,354.6630772588998,356.36297887963417,358.27488474060954,360.4257640696779,362.8458378711584,365.56890063272346,368.6326547114669,372.07905294932453,375.95464388699395,380.3109128051226,385.2046108423092,390.6980637732488,396.85945185220334,403.76305261875086,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216,404.16110168396216],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0010718913192051276,0.0011489510001873086,0.0012315506032928262,0.001320088400831418,0.0014149912974345759,0.0015167168884709225,0.0016257556664437934,0.0017426333860096508,0.0018679135990207828,0.0020022003718155844,0.0021461411978584036,0.002300430119772917,0.002465811075822604,0.0026430814869741054,0.002833096101839324,0.0030367711180354575,0.0032550885998350564,0.0034891012134067737,0.0037399373024787977,0.004008806328898464,0.00429700470432084,0.004605922041145104,0.004937047852839003,0.005291978735958442,0.005672426068491977,0.006080224261649421,0.00651733960488242,0.0069858797467852495,0.007488103857590023,0.008026433522257174,0.008603464416684501,0.009221978823334321,0.009884959046625586,0.010595601792776159,0.01135733358343105,0.012173827277396614,0.013049019780144023,0.01398713102647238,0.014992684327860457,0.016070528182616384,0.017225859653987867,0.018464249428955436,0.019791668678535563,0.021214517849106298,0.022739657523579274,0.024374441501222206,0.026126752255633278,0.02800503894183631,0.03001835813575589,0.032176417502507354,0.03448962260405758,0.03696912707195026,0.03962688638701478,0.04247571552536898,0.04552935074866948,0.04880251583654431,0.052310993080562605,0.05607169938205458,0.06010276782070382,0.0644236350872137,0.06905513520162328,0.07401959996915641,0.07934096665797492,0.08504489341802678,0.09115888299750818,0.09771241535346496,0.10473708979594497,0.11226677735108136,0.12033778407775893,0.1289890261253308,0.1382622173764655,0.14820207057988585,0.15885651294280528,0.17027691722258995,0.18251834943190426,0.1956398343517063,0.2097046401323233,0.22478058335487253,0.24094035602395245,0.2582618760682675,0.2768286630392064,0.29673024081888694,0.3180625692794119,0.3409285069746811,0.36543830709572545,0.39171014908092566,0.419870708444391,0.4500557675700497,0.48241087041653685,0.5170920242896755,0.5542664520663102,0.5941133984965034,0.6368249944718586,0.6826071834272386,0.7316807143427192,0.7842822061337682,0.8406652885618325,0.9011018251665018,0.9658832241158698,1.0353218432956617,1.1097524964120722,1.1895340673703196,1.2750512407130128,1.366716356462006,1.464971398307285,1.5702901247293775,1.6831803533309566,1.8041864093920719,1.9338917504552302,2.07292177959537,2.2219468609395236,2.381685551976158,2.5529080682395167,2.7364399970746693,2.9331662783900425,3.1440354715915,3.370064329271928,3.6123426997094303,3.8720387818125532,4.150404757850472,4.448782831127585,4.768611697714469,5.111433483440165,5.478901179593939,5.872786613189477,6.294988990221888,6.747544053110693,7.2326338964835335,7.752597488629457,8.309941949353387,8.907354638610439,9.547716114208056,10.234114021054527,10.96985797892384,11.758495540521558,12.603829296797274,13.50993521198025,14.481182276745331,15.52225357427048,16.638168860761272,17.834308769319094,19.116440753857,20.49074689815846,21.96385372416547,23.542864143224154,25.23539170434766,27.049597304631316,28.99422853882875,31.07866187782014,33.3129478793467,35.707859649004625,38.27494478516307,41.0265810582719,43.97603609302721,47.13753134116719,50.526310653356795,54.15871378079465,58.05225516094896,62.22570836730231,66.69919663030115,71.49428986597577,76.63410868007446,82.14343584919422,88.04883581643465,94.37878277775371,101.1637979766207,108.43659686896086,116.23224686798518,124.58833642950081,133.54515629298973,143.14589375234786,153.436840893001,164.46761779946627,176.2914118095948,188.96523396912076,202.55019392306664,217.1117945694501,232.72024789604072,249.45081352303166,267.3841615839944,286.606761694825,307.2112998861753,329.2971255097148,352.970730273065,378.3462617131925,405.54607358408276,434.7013158125018,465.95256686646775,499.450511585514,535.3566677410719,573.8441648302393,615.0985788580505,659.3188271333541,706.7181273927491,757.5250258771905,811.9844993184009,870.3591361485165,932.9304026284676,1000],"xaxis":"x","y":[566.2708736923157,566.2709637884196,566.2711455954279,566.2713429476173,566.2714537749448,566.2716774012974,566.2719201486667,566.2720564780648,566.2723315441884,566.2726301299034,566.2727978304125,566.2731361704537,566.2735034407976,566.2737097325875,566.2741259040929,566.2745776617974,566.2748314274337,566.2753433385249,566.275899023682,566.2762111912133,566.2768408724187,566.2775244004798,566.277908414973,566.2786829691947,566.2795237624321,566.2799961664649,566.2809489383699,566.2819831956645,566.2825643440854,566.2837363598607,566.2850086201959,566.2863897168663,566.287165300783,566.2887303688426,566.29042933725,566.2913835095214,566.2933088279585,566.2953988922055,566.2965728156207,566.2989413886755,566.3015126728761,566.3029570185577,566.3058710167721,566.3090344620867,566.3108116164757,566.3143968236053,566.3182890216145,566.3204758111586,566.2963618941313,566.3222128033478,566.3256313951136,566.3296587696362,566.3345742030908,566.3390684848695,566.3440447326328,566.3497355104683,566.3554248839002,566.3622904739789,566.3684195441836,566.3761993726414,566.3834768653372,566.3930283817807,566.4009813356963,566.4118159899713,566.4211739617189,566.4324001910949,566.4478054608504,566.4640710329745,566.4814314786719,566.5001106649712,566.5203720622353,566.5417125286026,566.564680386145,566.5889942983788,566.6157313672423,566.6433307033097,566.6755805530337,566.707422414381,566.7426120695071,566.7813456934549,566.8205218687611,566.8639994874366,566.9120210109251,566.9629715425336,567.0171488465248,567.0750812347994,567.1382196596063,567.2051427817486,567.2771341137195,567.3555203595447,567.4391619911443,567.5285286761982,567.6240966315019,567.7274413005241,567.8388932130325,567.9593544103259,568.087689100142,568.2251931007571,568.3742248818213,568.534174052573,568.7069185894946,568.8421928358224,568.8668553236739,568.8939805241703,568.9230828491042,568.9543268012118,568.988486263332,569.0242279021319,569.0636727794057,569.1065610587395,569.1545418818262,569.2054186613291,569.2619614264369,569.3233955215425,569.3903382512749,569.462452730007,569.5422448434323,569.6294698555314,569.7238868875539,569.8284469804956,569.9419563891045,570.0678229750154,570.206100368507,570.3570409874172,570.5245389548878,570.7080326487406,570.9104604535057,571.1357916422415,571.3836114383423,571.6596486870222,571.9642491558644,572.3024635280425,572.6784349288974,573.0986802299242,573.5650339313506,574.0851728386846,574.6659265790918,575.3150400730549,576.0412981127998,576.8355565291191,577.132834319625,577.4696120486711,577.8594796240989,578.3078503011848,578.7240305395686,578.839109543617,578.9676643502245,579.1112288771378,579.2717170521023,579.45145876928,579.6528532453202,579.8788576280971,580.1326401218163,580.4179530911157,580.7390299404157,581.100645380997,581.5083007992525,581.9552877214253,582.4217205983151,582.9278401996396,583.4773978999888,584.0745440778611,584.7238749611408,585.4304852724636,586.2000273855969,587.0387777819193,587.9537116794774,588.9525867953158,590.0440372937885,591.2376790678297,592.5442275945171,593.9756296976568,595.545210634599,597.2678379968348,599.160103967587,601.2405275064428,603.529778021247,606.0509220293486,608.8296941905371,611.8947938978092,615.2782083237812,619.0155624244192,623.1464958836518,627.7150663316866,632.7701773821399,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273,637.0029098272273],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=1.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN = visuRMSEGrid(ElasticNet(), 'EN', alphasEN, 'alpha', GridEN,\n","                             BestParametresEN, 'elasticnet__l1_ratio')\n","FigRMSEGRidEN.show()\n","if write_data is True:\n","    FigRMSEGRidEN.write_image('./Figures/EmissionsGraphRMSEEN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                     15\n","      KNeighborsRegressor()\n","R²                 0.476201\n","RMSE             370.615649\n","MAE               87.382326\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predkNN=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[39.32066666666667,21.627333333333333,16.898,19.503333333333337,99.44133333333333,37.36866666666666,31.590000000000007,86.77466666666666,70.32933333333332,14.488,65.38733333333333,33.096,30.017333333333337,47.42933333333333,17.517999999999997,53.93666666666666,31.162666666666663,29.92733333333334,120.69266666666664,72.79066666666667,44.34,54.245999999999995,32.89,35.368,91.99066666666664,63.73866666666667,149.17000000000002,40.860666666666674,20.544666666666668,62.33133333333333,146.86266666666668,35.15933333333334,85.60666666666665,96.73800000000001,61.84400000000001,94.92800000000001,347.218,34.10333333333334,92.27466666666668,18.424666666666667,120.574,15.004666666666669,25.224666666666668,95.42533333333334,55.052666666666674,65.902,59.59666666666667,124.21266666666666,40.32733333333333,53.16066666666667,21.363999999999997,7.473333333333334,1053.252,84.57666666666667,41.11666666666667,82.11666666666669,283.4066666666667,25.80266666666667,113.62666666666668,51.716,169.21399999999997,177.21266666666665,28.45866666666667,110.55266666666665,74.57133333333334,12.664666666666667,62.02666666666668,187.08399999999997,23.580666666666666,59.10733333333334,25.214000000000002,113.73400000000001,17.156666666666663,19.454000000000004,96.83466666666666,98.22666666666667,250.9153333333333,40.518,47.143333333333324,29.03666666666667,57.946000000000005,133.72533333333334,214.63933333333333,171.22799999999998,10.686666666666667,6.022666666666667,41.94133333333333,44.00933333333333,143.33599999999998,445.82200000000006,62.846666666666664,70.09666666666666,119.66666666666667,35.51266666666667,67.41533333333334,53.668,3756.3920000000003,214.978,113.28466666666667,52.166666666666664,124.98866666666666,39.07000000000001,102.12466666666667,26.577333333333335,79.77000000000001,50.734,50.631333333333345,222.35333333333338,26.299333333333337,72.89599999999999,56.684666666666665,18.745333333333335,35.29600000000001,17.745333333333335,49.14533333333334,40.83533333333333,31.645333333333333,668.0953333333333,75.53466666666667,134.80200000000002,15.474666666666668,45.170666666666676,26.280666666666672,257.05533333333335,64.27533333333334,27.702,14.285333333333334,62.72066666666667,16.506,115.888,214.57866666666666,18.84,21.905333333333335,57.32666666666667,194.71066666666667,43.622,49.64333333333333,26.288666666666668,64.31266666666666,21.15666666666667,10.787333333333333,133.632,74.96466666666667,65.37866666666667,71.44333333333333,32.775333333333336,32.498000000000005,49.21933333333334,16.896000000000004,15.825999999999999,81.06533333333333,62.526,46.49933333333334,50.01133333333333,61.574000000000005,44.95799999999999,58.476,37.82733333333333,316.0780000000001,66.14266666666667,7.481999999999999,21.55933333333333,15.826,35.303333333333335,44.36933333333333,17.599333333333334,231.9753333333333,89.472,55.85133333333333,63.53200000000001,60.01200000000001,16,52.65733333333333,31.086000000000006,23.723333333333336,147.60199999999998,17.662,83.816,10.980666666666668,7.068666666666667,123.65333333333334,332.694,24.61533333333334,48.72866666666666,13.474000000000002,131.89600000000002,7.034666666666668,44.992,21.824666666666662,83.51066666666665,35.96533333333333,54.516666666666666,35.69866666666667,122.7,67.82533333333335,38.09666666666667,24.692666666666664,48.24666666666666,42.58866666666667,61.01933333333333,156.892,22.384666666666664,7.364666666666667,20.608666666666668,19.541333333333334,26.011333333333337,20.21466666666667,14.639999999999999,52.08933333333333,43.72066666666665,362.45933333333335,44.89066666666666,28.355333333333327,153.08333333333334,37.294000000000004,606.822,25.326,12.400666666666668,146.23933333333335,24.600666666666665,50.72066666666667,35.24333333333333,36.26466666666666,44.903999999999996,9.662,34.14533333333333,70.32933333333334,19.384666666666668,31.366,33.04933333333334,22.462666666666664,50.29733333333334,32.418,93.29866666666666,35.53999999999999,26.560666666666666,23.002666666666663,64.764,176.16666666666663,73.38666666666668,118.7,44.95933333333333,23.030000000000005,18.234,58.693999999999996,19.297333333333334,283.2653333333334,45.18866666666667,201.304,87.356,40.034,58.69466666666667,59.23133333333333,70.29333333333334,179.79466666666667,54.22466666666667,51.9,48.722666666666676,51.952666666666666,52.148,37.806000000000004,54.92266666666667,67.82400000000001,53.68333333333333,7.586,73.278,24.71266666666667,63.444666666666656,37.20733333333333,63.45533333333333,38.78866666666667,73.07133333333334,49.81133333333333,35.82533333333333,27.004,73.984,83.85466666666666,1253.705333333333,96.14666666666668,45.589333333333336,45.81866666666667,27.604000000000003,58.149333333333324,216.276,47.38999999999999,47.51533333333334,38.169333333333334,53.14666666666666,34.690666666666665,57.31733333333332,218.73066666666668,26.345999999999997,49.14533333333334,28.958000000000002,91.47466666666668,121.03733333333335,22.62133333333333,78.64000000000001,50.62533333333332,5.833333333333333,37.482,111.24933333333333,116.61200000000001,43.623999999999995,251.63866666666667,213.3826666666667,142.79666666666665,17.189333333333334,43.89666666666666,30.791333333333338,21.17,41.176,27.38666666666667,115.78933333333333,32.234666666666676,57.41466666666666,37.565333333333335,32.40066666666667,15.62333333333333,121.04666666666667,71.42200000000001,237.45666666666662,17.517999999999997,25.817333333333334,81.30199999999999,36.40466666666667,64.72533333333334,54.930000000000014,435.84999999999997,31.091333333333335,46.86933333333334,31.688000000000002,79.59333333333333,110.79400000000003,76.77533333333334,62.410666666666664,364.98133333333334,200.2906666666667,29.454,16.332,17.522,19.389999999999997,22.235999999999997,14.196666666666667,162.10399999999998,149.37333333333333,58.896,42.182,46.01266666666667,16.110666666666667,32.27866666666667,30.393999999999995,17.991333333333333,65.674,132.20333333333335,433.1986666666667,176.754,182.82399999999998,17.773333333333333,96.24933333333334,33.068000000000005,14.138,17.951333333333334,82.88666666666667,27.862000000000005,37.16066666666667,205.00733333333338,50.187999999999995,275.39599999999996,80.416,96.762,63.84400000000001,102.46066666666665,105.85733333333333,186.62533333333337,27.186666666666667,151.508,55.79733333333334,117.032,63.13,24.351999999999997,33.9,286.202,54.120000000000005,18.904666666666667,129.9466666666667,54.90200000000001,47.85933333333333,49.68733333333332,59.34066666666666,80.39999999999999,28.265333333333327,31.132000000000005,30.21266666666666,22.402666666666665,52.166666666666664,27.990000000000006,12.590666666666664,20.388,18.812666666666665,104.38266666666667,95.09866666666667,10.423333333333336,14.632666666666669,16.817333333333334,62.846666666666664,24.90266666666667,154.634,34.26333333333333,35.063333333333325,15.612666666666666,43.42133333333334,60.817999999999984,89.77199999999999,34.67866666666667,19.29866666666667,20.635333333333335,86.2606666666667,169.292,116.90266666666666,41.57066666666666,21.158,27.411999999999995,85.438,107.27266666666665,50.17666666666667,1779.5666666666666,35.87466666666667,100.70666666666666,48.27933333333333,43.71066666666666,117.53066666666669,52.84666666666667,666.8233333333334,448.94199999999995,22.922,319.5013333333334,21.418666666666663,63.74333333333333,50.678,14.452666666666666,17.733333333333334,11.240666666666668,8.177333333333333,387.58399999999995,36.71,89.45533333333334,15.155333333333333,59.071333333333335,72.77400000000002,68.75133333333333,32.54266666666667,22.787999999999997,82.84133333333334,44.37533333333333,39.95933333333333,11.450666666666667,163.2333333333333,29.839999999999996,7.586,27.003999999999998,16.298666666666666,98.85066666666667,40.52400000000001,159.6506666666667,32.77933333333334,30.393999999999995,19.622000000000003,50.56399999999999,64.24666666666667,21.95733333333333,108.28133333333332,446.8959999999999,129.20333333333332,8.574000000000002,17.086000000000002,133.906,99.60666666666664,31.546,45.37466666666668,28.502,62.967999999999996,44.71333333333333,12.089333333333332,29.34466666666667,22.758,57.92733333333334,45.42266666666667,328.7153333333333,47.79333333333334,22.959999999999997,33.99,31.729333333333333,32.714,43.76199999999999,199.28866666666667,36.78333333333333,58.55933333333333,428.8606666666667,88.65466666666664,86.58200000000001,168.65133333333335,108.88333333333335,49.07266666666668,147.832,89.74199999999999,39.657333333333334,262.4626666666667,38.09666666666667,38.751999999999995,15.277999999999999,135.51333333333335,51.37866666666666,44.88133333333334,161.87400000000005,14.285333333333332,7.586000000000001,730.2173333333334,22.42133333333333,58.25466666666667,162.59533333333334,213.25,48.67399999999999,33.374,30.79933333333333,8.472666666666665,37.13333333333333,53.602,22.07,36.163333333333334,84.61933333333333,12.311333333333334,48.284,18.391333333333332,33.73,19.528666666666666,39.02066666666667,15.656666666666663,90.62999999999998,11.914666666666665,59.550666666666665,31.338666666666665,17.434,54.441333333333326,29.113999999999997,35.24333333333333,91.70533333333333,29.008,38.04,251.32066666666665,97.70533333333331,56.148,94.25399999999999,50.584666666666664,59.333999999999996,31.788,47.05066666666665,113.99400000000001,21.942666666666664,140.442,40.53733333333334,47.82600000000001,102.67266666666664,56.38266666666667,162.45266666666666,59.73266666666665,90.09933333333335,421.03066666666666,72.23733333333332,15.989999999999998,22.163333333333334,318.48400000000004,5.71,16.564,280.8286666666666,35.58533333333333,44.96133333333333,19.513333333333335,48.24666666666666,57.63733333333334,63.244000000000014,47.86533333333333,25.781999999999996,47.458666666666666,23.275333333333332,130.04866666666666,66.90733333333334,13.344,138.19866666666667,10.859999999999998,52.382,211.2746666666667,45.351333333333336,672.3073333333334,54.906666666666666,50.836,214.98600000000002,20.028000000000002,137.49999999999997,37.294,173.108,16.191333333333333,147.2866666666667,234.90933333333334,27.988666666666667,459.2153333333334,46.73133333333333,335.6626666666667,18.707333333333334,43.164,49.411333333333324,7.586,73.35333333333332,55.042,497.7399999999999,145.218,16.49,129.63333333333333,20.580666666666666,43.14866666666665,84.25733333333334,11.455999999999998,36.708666666666666,65.89,25.244000000000003,14.184666666666669,44.00199999999999,14.196666666666667,53.314,23.149333333333335,149.17000000000002,17.947999999999997,73.16999999999999,46.328,32.789333333333325,43.45733333333333,193.13400000000001,47.40800000000001,162.85800000000003,47.82333333333333,322.378,720.2993333333333,27.31,11.450666666666669,8.146666666666667,94.92800000000001,25.168666666666663,42.44799999999999,39.88066666666667,19.235999999999997,43.55733333333332,474.294,150.17666666666665,30.112000000000002,16.26066666666667,55.501333333333335,106.84133333333332,20.793333333333333,116.04133333333333,21.16866666666667,34.74133333333332,37.83400000000001,50.43066666666666,45.28266666666668,90.92466666666668,30.35666666666667,22.136000000000003,47.99333333333333,12.808666666666669,24.384000000000004,9.004000000000001,39.132,69.674,78.59066666666666,29.450666666666663,238.48266666666666,533.098,35.063333333333325,76.96466666666666,41.03466666666666,23.841999999999995,39.712,13.790000000000001,46.41733333333333,107.85733333333334,33.815333333333335,246.26600000000002,60.11399999999999,93.55066666666666,127.23666666666668,22.077333333333332,139.85533333333333,23.63,61.42466666666666,183.47400000000002,88.03799999999998,389.41666666666674,203.19066666666666,13.560666666666664,36.818,46.039333333333325,93.06599999999999,23.280666666666665,43.62199999999999,16.764666666666667,40.59933333333333,74.44200000000001,25.499333333333333,25.621333333333332,88.31933333333333,27.988666666666667,3017.045333333333,25.462666666666667,657.6579999999999,84.35666666666667,18.572666666666667,21.62733333333333,15.828,26.280666666666672,220.9686666666667,305.24799999999993,185.5546666666667,25.074,51.69666666666667,79.268,61.18866666666667,7.027333333333334,44.23533333333334,16.333333333333332,657.1953333333335,53.53533333333334,15.048,539.836,44.742,44.77800000000001,80.44866666666667,434.4893333333333,8.472666666666665,85.802,84.04066666666668,33.650000000000006,50.55733333333333,16.318,95.968,84.28866666666667,72.08600000000001,107.734,16.926000000000002,16.971333333333334,122.50066666666667,121.14466666666665,232.63866666666664,16.557333333333332,509.3813333333333,18.876666666666665,44.00933333333334,32.89,19.39,48.586,137.314,19.52333333333333,159.05533333333332,31.369333333333334,19.090000000000007,17.496,47.965333333333334,61.39666666666667,89.534,94.334,54.35533333333333,94.89533333333334,21.363999999999997,33.13133333333334,81.30199999999999,49.884,17.15666666666667,11.570666666666666,86.73,112.37199999999999,743.3840000000001,50.62266666666666,23.419333333333334,18.57266666666667,49.44533333333333,55.656666666666666,46.61999999999999,136.746,35.45533333333333,7.421333333333332,30.19600000000001,111.26733333333335,68.11533333333333,62.234,37.28999999999999,358.31800000000004,30.735333333333333,36.774,83.57466666666667,2903.1386666666667,29.600666666666665,49.912000000000006,12.261333333333333,86.77533333333331,79.30866666666667,34.28666666666666,35.47933333333334,33.13733333333333,33.48733333333333,36.95533333333333,42.81466666666666,41.25999999999999,430.96333333333337,15.235999999999997,36.934,13.249333333333333,69.95666666666668,25.392666666666667,192.4993333333333,58.693999999999996,52.608,54.224666666666664,862.9146666666668,11.535333333333334,203.17666666666665,11.797333333333333,66.95333333333333,11.466666666666669,190.55199999999996,73.95400000000001,46.092666666666666,96.14866666666667,451.92,63.08533333333333,35.36066666666667,40.236666666666665,33.88066666666666,19.088,20.189999999999998,22.440666666666665,732.104,35.55200000000001,29.614,72.74333333333334,99.97066666666667,184.35533333333336,20.55266666666667,50.81400000000001,28.84066666666667,384.7613333333333,64.086,21.543333333333333,26.500666666666675,180.60999999999999,29.246000000000002,123.86999999999999,39.46333333333334,24.85866666666667,55.234,290.3533333333333,10.85933333333333,56.75133333333333,27.034000000000002,123.78533333333334,52.87933333333333,17.296,11.414666666666667,424.15666666666664,65.52,47.95666666666666,78.75399999999999,21.946666666666673,99.14133333333334,63.70799999999999,273.10400000000004,62.55933333333334,47.583333333333336,18.812666666666665,17.084,43.934,36.45600000000001,161.59133333333332,110.01466666666667,54.84600000000001,70.29333333333334,10.304666666666664,670.4133333333334,15.828,31.192000000000007,24.055333333333337,38.33733333333334,22.44,39.775999999999996,21.957333333333334,25.441999999999997,214.978,49.14533333333333,17.028,116.68800000000003,32.31066666666666,21.791999999999994,99.89733333333334,37.56266666666667,8.046,44.764,497.48666666666674,143.73733333333334,63.09266666666666,22.921333333333337,31.546,94.238,38.48933333333333,44.764,36.946,1656.9733333333336,60.76866666666667,57.31666666666667,47.22266666666666,20.726,16.086,418.6473333333333,245.18466666666671,51.95066666666666,62.64800000000002,409.1499999999999,36.763333333333335,579.5553333333334,104.03133333333332,292.158,23.463333333333328,301.4286666666667,7.007333333333333,68.07333333333334,28.958000000000002,21.947333333333336,18.880666666666666,73.46533333333333,61.32733333333332,736.636,26.346666666666668,120.40333333333334,29.785999999999998,80.356,63.848,11.998666666666669,156.46399999999997,304.428,59.68133333333333,40.08933333333333,21.05,46.328,50.2,59.864000000000004,27.181333333333335,99.22466666666665,58.02533333333334,133.594,3327.998,18.16,18.49933333333333,15.827999999999998,863.004,44.16,7.6433333333333335,77.99866666666667,289.04466666666667,65.352,33.13133333333334,6.0053333333333345,324.366,46.86933333333334,38.056666666666665,12.139999999999999,376.8953333333333,43.095333333333336,30.772666666666666,26.48066666666666,45.60933333333333,11.495333333333331,27.764666666666667,10.686666666666666,69.49133333333333,33.26866666666667,99.85266666666666,34.578,97.65133333333331,79.59333333333333,62.69666666666665,125.65,13.502666666666666,14.298000000000002,151.70933333333332,55.72000000000001,38.846,249.14133333333334,32.083333333333336,389.9366666666667,17.15666666666667,34.652,57.05133333333333,49.563333333333325,103.72,74.842,95.47800000000001,25.074,665.4786666666666,20.726,35.95733333333333,60.969333333333324,17.84266666666667,88.49533333333333,31.59,112.44199999999998,57.112,18.046000000000003,49.24066666666667,14.550666666666668,8.620666666666667,21.363999999999997,20.038666666666668,21.62733333333333,164.8293333333333,59.028666666666666,95.83599999999998,22.143333333333334,64.86999999999999,71.80999999999999,11.904,31.459333333333333,21.62733333333333,84.75,27.072666666666667,55.483333333333334,180.21599999999998,111.09866666666667,458.2113333333334,119.36533333333333,476.3946666666667,11.513333333333337,54.936666666666675,70.36333333333336,54.54066666666667,24.754,9.313999999999998,75.97666666666666,24.823999999999998,43.519333333333336,62.885999999999996,197.3666666666667,35.69866666666667,17.084666666666667,148.75933333333333,61.074,34.666000000000004,20.07133333333333,147.22133333333335,14.072000000000001,43.048666666666655,55.372,94.08933333333333,73.364,47.727333333333334,94.89000000000003,201.93866666666665,75.19666666666666,104.21866666666666,48.45533333333333,11.020666666666667,35.39733333333334,12.538666666666666,72.33133333333333,44.16600000000001,38.870666666666665,39.46333333333334,130.31,46.03933333333334,18.374,29.886666666666667,22.654666666666667,44.00933333333334,1807.6733333333334,26.28066666666667,50.05733333333333,1715.6933333333334,50.79933333333334,111.55799999999998,34.68200000000001,160.502,1840.5473333333332,47.63066666666667,25.995999999999995,68.28466666666667,44.166000000000004,21.791999999999994,26.953999999999997,120.57400000000001,222.62800000000001,29.614000000000008,99.23400000000001,6.484,237.87,91.906,20.375333333333334,7.068666666666668,34.537333333333336,37.538,221.76600000000005,211.5613333333333,46.039333333333325,147.0906666666667,34.742666666666665,210.50866666666667,11.450666666666665,396.602,29.836666666666662,30.36,94.00333333333334,16.800666666666668,35.20466666666667,23.086000000000002,38.10333333333333,24.697333333333333,18.607333333333333,43.81666666666667,28.586000000000002,507.4453333333333,73.518,54.936666666666675,13.053333333333333,136.852,98.14599999999999,54.263333333333335,51.274,54.535333333333334,43.82599999999999,38.402,42.27066666666666,138.02066666666667,27.125999999999998,31.780666666666665,46.039333333333325,68.04933333333332,22.831333333333337,11.490000000000002,189.00266666666664,81.96466666666667,36.54333333333333,136.96933333333334,30.393999999999995,137.29866666666663,45.45,27.096666666666675,173.54866666666666,20.689999999999998,28.722666666666665,79.41666666666669,58.45733333333333,94.93400000000001,61.63399999999999,18.783333333333335,62.407333333333334,3017.045333333333,40.395999999999994,50.48466666666668,26.280666666666665,11.554666666666668,98.23333333333333,564.4920000000001,19.40066666666667,28.968,49.782000000000004,66.25533333333334,61.63399999999999,44.946666666666665,97.46000000000001,15.418666666666669,58.494,97.22533333333332,32.992,18.57266666666667,86.80466666666666,50.97333333333332,7.802666666666666,45.056,19.622,38.48933333333333,49.14200000000001,26.16,32.751999999999995,60.957333333333345,53.332,18.082666666666665,24.351999999999993,15.656666666666665,26.52466666666667,71.07999999999998,81.18333333333334,262.0853333333333,55.605999999999995,21.476666666666667,24.055333333333337,33.04933333333334,43.99733333333333,89.7973333333333,13.065333333333333,50.54933333333334,13.446666666666665,74.05999999999999,53.582,61.08666666666667,31.383333333333326,47.72066666666667,149.04066666666665,25.054000000000006,442.5746666666667,24.372666666666667,96.87133333333334,80.74399999999999,12.596666666666668,22.645333333333333,37.605999999999995,31.98533333333334,21.363999999999997,56.312000000000005,130.18266666666668,69.06866666666666,34.162666666666674,56.312000000000005,33.251999999999995,87.02933333333334,22.80733333333333,50.15933333333333,31.876666666666665,95.68800000000002,34.128,19.082,6.144000000000001,87.25133333333332,20.47266666666667,51.68133333333332,25.485999999999997,58.059999999999995,74.13466666666666,71.59066666666666,24.892,27.755999999999997,15.472666666666667,103.462,143.73,394.97666666666663,157.082,37.294000000000004,49.058,57.39,47.16,122.96266666666666,26.525333333333336,19.458666666666666,72.31800000000001,201.60533333333336,26.606,203.0113333333333,3037.372,82.76333333333334,28.751333333333335,23.947333333333333,11.835333333333335,37.794666666666664,24.055333333333337,14.616666666666667,42.282666666666664,36.306666666666665,111.922,48.425999999999995,57.32666666666667,11.63866666666667,200.80533333333332,274.272,10.307333333333334,105.65266666666669,114.21733333333334,452.99666666666667,128.58533333333338,20.348666666666666,33.096,585.9266666666665,37.96399999999999,20.118000000000002,107.88266666666667,26.57133333333333,44.59400000000001,82.27066666666664,27.62666666666667,117.63733333333333,94.82866666666665,63.034000000000006,31.540666666666667,85.38400000000001,53.650666666666666,137.53066666666666,12.070666666666664,49.41400000000001,16.718,136.95666666666668,15.505333333333333,99.43466666666669,5.410666666666668,53.34799999999999,141.79933333333332,20.473333333333333],"xaxis":"x","y":[30.29,47.7,6.73,5.66,152.14,7.53,17.11,9.67,73.77,5.21,150.69,47.42,17.51,11.08,4.76,73.31,30.34,59.24,33.58,31.96,232.12,9.43,8.12,51.2,14.69,33.77,10.29,32.33,61.62,7.67,21.08,35.56,105.32,41.15,287.23,85.28,139.34,18.63,25.15,10.04,42.74,29.93,17.38,263.57,11.17,8.59,570.98,13.65,13.22,2.91,6.53,49.15,1222.69,28.59,51.64,185.23,141.7,3.27,66.39,85.15,87.31,95.26,48.51,26.78,50.14,6.02,14.87,349.47,4,14.79,4.98,20.39,6.45,19.28,93.47,52.87,372.55,5.81,12.68,5.81,8.57,47.39,128.73,99.39,37.72,3.02,12,38.97,39.27,266.18,5.9,29.84,49.18,3.82,8.8,11.85,10734.57,266.78,12.25,6.04,18.07,7.21,63.89,8.53,138.68,18.34,29.39,218.53,69.3,587.16,11.97,8.46,4.36,5.16,46.57,32.22,5.46,181.38,7.08,65.03,5.87,24.25,23.17,178.21,39.57,16.5,4.48,61.36,4.03,8.8,77.29,6.05,45.3,8.77,256.57,2.17,37.97,11.11,74.45,17.67,19.83,406.2,209.92,45.81,2.36,22.02,198.38,6.22,8.31,5.96,13.61,262.75,152.36,15.43,7.23,5.94,43.45,60.94,159.16,31.92,9.71,69.36,18.1,8.37,23.58,48.3,147.82,71.47,18.89,53.36,17.65,4.5,92.59,62.57,4.8,25.37,90.31,100.42,6.37,2.9,63.61,64.19,22.15,192.63,5.55,109.79,3.29,67.15,31.92,79.3,8.81,6.71,12.33,29.33,12.15,16.83,116.21,6.43,187.92,39.56,30.85,78.29,4.33,4.63,16.89,3.42,10.21,5.97,49.16,20.1,190.58,7.67,4.79,1000.06,50.52,122.99,134.69,5.6,27.38,7.08,2.04,44.6,3.51,41.98,6.36,5.87,8.87,3.97,127.56,37.85,4.26,147.69,5.9,10.55,5.69,32.81,34.81,56.25,653.5,65.65,97.01,3.49,4.65,9.13,41.69,14.77,39.47,2.82,142.48,68.97,6.64,7.31,82.9,8.69,256.31,93.23,11.77,38.08,17.24,43.64,18.41,12.7,27.3,6.71,3.89,71.03,1.78,152.71,50.43,22.18,12.06,29.26,20.98,77.15,3.84,91.09,14.35,839.79,226.43,27.27,57.69,5.16,6.69,20.79,176.79,59.61,9.83,8.24,16.09,20.44,170.84,35.39,30.02,19.72,15.59,66.05,3.89,7.46,14.35,7.74,141.32,46.61,20.29,3.75,235.69,243.38,151.95,100.87,56.51,6.79,4.84,247.85,24.17,168.62,47.11,12.99,104.17,30.66,3.82,120.91,132.47,106.37,44.93,6.93,61.71,4.2,24.42,8.14,111.69,6.22,9.45,23.84,4.88,507.7,553.27,9.48,122.92,103.37,50,3.86,3.4,3.66,93.43,6.85,32.8,32.73,13.43,413.1,6.19,6.16,68.14,0.68,16.45,13.88,350.87,802.89,291.28,91.73,4.72,7.21,25.02,6.75,28.93,229.55,3.01,49.59,341.23,37.18,353.49,93.87,93.21,11.79,49.25,154.4,84.93,6.02,43.63,35.36,51.47,42.8,4.35,17.11,205.07,40.78,5.6,186.96,14.51,35.56,537.03,53.38,38.9,8.16,37.51,3.82,5.51,50.76,380.17,10.75,4.3,7.12,1.6,63.45,4.42,7.51,7.14,22.78,8.45,8.59,5.37,11.59,6.93,54.95,19.7,89.41,74.04,3.91,35.22,43.13,28.54,70.4,10.06,6.3,5.11,45.38,0,10.14,765.56,14.46,28.64,27.86,8.85,256.72,196.67,525.78,93.67,8.97,165.61,3.06,17.84,19.17,50.7,47.25,2.99,3.2,54.92,96.4,16.01,4.74,10.56,132.28,15.5,7.29,63.36,47.68,297.23,67.06,3.4,98.54,0.76,3.72,50.15,2.45,265.21,188.67,156.08,83.1,94.85,20.57,29.18,12.09,13.77,34.14,1231.19,39.41,7.57,113.25,237.36,93.23,33.52,57.03,12.79,12.97,5.72,6.38,10.93,16.2,57.16,18.62,346.62,44.19,190.45,6.1,75.67,3.69,73.48,1185.15,20.12,92.3,188.13,72.71,101.31,15.8,80.06,3.18,141.15,17.74,22.65,42.2,14.32,46.86,18.42,247.85,52.43,5.71,51.26,3.07,10.14,242.19,21.3,8.45,113.31,75.12,13.15,0.89,8.57,3.48,5.33,102.04,3.71,31.58,42.52,6.89,7.69,6.16,29.16,5.88,76.85,1.3,37.58,7.29,24.1,28.78,25.39,11.28,32.02,31.46,38.54,49.7,9.47,456.22,124.92,14.19,110.62,24.36,4.13,68.89,145.48,21.24,18.72,12.4,15.62,43.16,160.3,47.78,139.27,30.86,44.97,163.99,142.69,12.63,4,109.46,26.64,64.13,285.62,9.57,74.52,4.01,226.81,10.16,13.64,92.58,7.15,8.16,75.64,60.02,223.36,8,13.87,3.92,73.32,233.43,208.26,351.53,16.76,200.3,604.45,51.91,16.06,10.92,90.73,4.73,69.42,238.61,4.93,900.81,0.7,399.76,31.85,19.95,248.29,2.43,480.9,10.32,176.76,144.3,5.15,97.97,27.69,63.6,144.53,27.56,29.24,26.55,18.5,80.06,10.77,15.34,0.75,3.91,474.59,4.25,128.41,433.86,69.88,44.32,413.35,4.43,348.22,29.78,66.32,290.4,44.51,4.02,5.2,56.82,4.55,9.65,52.88,57.79,58.32,393.2,72.69,45.02,24.62,4.19,223.34,10.89,83.27,7.56,29.73,100.71,7.53,121.88,176.47,23.82,4.48,53.21,16.3,4.9,6.39,54.95,61.64,7.12,19.68,8.07,704.76,33.35,184.81,61.25,20.88,60.04,16.47,551.43,260.13,8.61,167.31,70.14,64,21.92,9.16,111.23,7.02,236.34,318.08,149.93,364.9,295.86,34.62,3.11,156.18,118.86,4.16,3.27,6.54,3.65,48.91,8.87,5.19,148.11,88.87,1793.9,37.34,275.54,30.87,5.53,56.96,82.38,21.52,1597.56,42.41,109.72,4.68,19.81,79.81,6.74,6.02,39.05,5.79,271.32,85.55,4.52,233,9.53,157.21,15.23,35.44,3.92,64.75,40.35,20.45,15.27,29.42,39.75,119.03,28.07,95.77,23.14,60.83,224.13,98.65,443.86,3.52,221.47,5.68,7.9,11.73,4.71,38.59,32.75,4.79,101.43,5.54,5.66,5.88,130.58,60.33,25.6,69.18,253.53,84.57,3.55,42.71,5.09,12.28,19.15,93.68,42.24,58.44,1084.79,243.37,7.47,4.43,8.09,69.67,82.72,101.64,19.04,4.02,5.6,46.49,183.51,98.22,6.54,69.3,41.96,3.71,5.97,913.07,145.42,7.56,5.3,45.96,8,71.16,13.1,53.88,53.02,379.34,43.35,7.94,322.79,4.43,14.81,3.41,18.97,3.9,115.56,61.68,29.92,9.99,530.66,3.7,49.17,27.39,96.62,6.57,51.93,21.38,63.48,90.34,423.93,183.79,5.86,92.45,23.85,45.32,77.75,5.31,2451.58,9.87,39.5,67.17,162.39,220.26,8.46,23,31.79,28.83,20.02,101.99,26.72,15.5,32.77,132.56,15.74,110.66,9.65,246.83,10.83,5.92,9.2,467.18,24.35,3.67,5.35,862.64,89.53,234.03,1.67,15.2,71.38,43.57,260.57,226.73,87.14,14.44,3.99,28.82,73,188.07,45.6,5.07,103.77,3.16,395.26,27.93,10.23,23.27,60.07,112.19,7.22,20.07,15.9,72.46,3.92,176.63,32.52,32.35,6.84,96.59,44.71,3.52,48.95,116.95,36.65,142.36,8.27,3.89,315.85,9.73,1.61,5.81,1638.46,8.61,14.99,49.18,26.73,17.42,202.49,59.21,9.99,42.57,481.1,29.62,399.54,72.88,52.79,20.67,381.89,3.56,4.07,15.74,26.94,27.84,32.72,15.64,582.28,13.36,23.83,27.06,411.09,4.74,15.18,73.14,185.82,13.97,12.15,3.49,19.84,93.67,93.55,59.18,74.98,43.56,312.6,12307.16,6.47,6.19,12.59,579.99,59.63,2.81,119.34,283.04,19.72,40.76,3.18,62.01,1.58,15.77,3.34,587.09,12.07,23.74,4.61,89.6,103.42,7.52,57.33,16.82,65.75,22.76,8.42,39.54,99.91,47.12,80.75,10.17,4.54,28.3,19.36,6.35,63.87,10.05,184.64,34.08,3.15,76.89,11.99,292.92,112.01,66.1,23.22,251.43,75.58,152.14,41.06,7.07,115.34,6.12,34.87,85.3,19.76,28.03,6.67,18.73,20.07,34.98,12.56,150.23,12.35,50.23,19.16,45.22,344.2,4.74,1.34,3.77,36.71,3.92,83.35,324.92,64.95,675.34,22.17,536.43,43.84,30.26,78.19,44.61,61.72,39.57,73.52,3.99,182.11,99.62,551.53,39.45,6.99,89.94,7.27,4.76,9.66,77.56,3.78,44.26,39.31,37.14,11.29,24.91,19.73,326.74,64.79,59.06,2.35,49.79,61.19,4.39,357.86,6.61,126.3,53.25,58.98,4.95,6.87,8.93,39.1,66.44,440.31,38.45,102.08,1558.85,37.17,124.96,5.3,260.94,3232.23,74.13,31.18,225.67,6.04,49.44,8.31,67.65,320.35,46.72,143.67,3.16,94.59,96.15,5.7,25.49,3.65,22.46,1727.11,49.26,173.75,295.48,53.56,136.07,5.03,250.05,74.28,3.59,123.14,0.12,24.64,4.98,46.95,27.76,5.53,17.23,154.13,740.97,78.06,125.42,7.53,131.64,129.05,6.67,99.65,298.91,30.14,46.32,5.88,0,14.3,45.07,1.14,147.24,21.28,4.44,249.09,72.02,47.25,1623.34,8.7,226.06,10.83,47.91,434.48,5.33,120.13,38.15,180.38,37.16,8.18,7.05,148.7,0,29.26,39.35,24.18,4.02,103.39,2089.28,4.5,77.48,143.3,46.61,46.85,80.08,108.04,23.01,47.82,67.62,69.53,4.2,25.48,7.74,4.33,22.61,3.63,11.25,58.79,58.85,13.34,7.25,26.41,5.94,5.02,4.44,4.72,23.65,96.41,529.47,7.1,14.15,3.72,48.23,63.06,96.39,5.29,10.08,4.49,22.08,1084.49,8.16,65.77,20.63,375.56,112.13,58.52,26.54,68.3,464.77,5.14,57.17,8.98,91.49,3.68,44.35,92.7,4.99,0.37,305.15,57.25,0,16.27,45.59,34.26,34.92,24.31,5.07,1.08,28.19,82.87,109.71,17.19,50.31,20.25,41.22,5.74,45.98,17.88,86.5,8.55,394.6,22.57,48.85,38.13,269.23,47.26,122.83,5.23,8.81,48.65,640.71,46,328.63,4725.43,73.9,65.65,6.98,3.5,8.54,4.15,6.98,4.69,40.38,150.81,8.15,4.38,37.13,328.97,132.71,3.15,229.2,10.97,122.54,84.18,6.46,26.59,500.93,148.02,6.39,222.02,12.75,50.57,9.28,3.86,369.14,156.11,46.38,14.99,69.77,7.21,100.59,5.3,39.61,102.73,23.42,4.26,127.53,3.91,12.5,117.11,8.47],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors = np.linspace(1, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","TotalGHGEmissions_predkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train,\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","print(ScoreskNN)\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[429.33166141420907,384.2028287269566,377.0005705116287,374.2663683178166,375.7448138220379,367.97893015288645,369.63805022213944,366.07292899477966,366.5403119771624,368.73824105824485,372.96837663994086,375.6532313985329,375.5518513175387,376.77735693589227,377.33446578871315,376.4095699304802,378.3812599977231,379.8226128986327,380.7630329564804,382.13316767723336,382.74456506451526,384.7855810273285,386.40199573120805,387.9955043732633,389.37915747337206,390.11414167180845,390.35618937912386,390.84415949629937,391.89273973378084,391.40751861079514,392.03469021469925,392.5070995100922,392.95573981796076,393.5552604786367,394.15290300994656,394.7767564273442,395.0801120897944,395.9156261684664,396.38739455804046,396.79839995341683,397.37756152353097,397.95140875484407,398.6156487132167,399.2384663953193,399.8001145061501,399.8539457506674,400.4363041942368,401.00101862955233,401.37241670777456,401.8974926279223]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[577.3392959959842,507.5983584221158,498.8757873596974,510.6499095764365,509.68574448270124,491.9187878415061,491.65148677540697,493.39208660868695,498.3738655182718,500.5482825767927,505.06421506813797,509.6070528802139,512.8216169532208,514.5075976274046,516.3899322593488,513.6026878417362,515.0220371317039,516.5434743197432,518.6029459439807,520.2164237911899,521.2459204658691,523.3867740476596,525.5655615661735,527.1310619104447,528.5974108273604,529.8474071759309,530.6762456724184,530.7798523377401,532.1023909530846,530.5497815465509,531.6967283217716,532.252488962079,533.1240993342915,533.6093583706322,534.3185595915387,535.059344830003,535.4145553652186,536.4868510723791,537.1118914877658,537.7581838185474,538.6081006620542,539.2633408669114,540.0892113501716,540.7962964091337,541.5896068449028,542.0042941328895,542.6354153510888,543.3204673313437,543.8074486854414,544.656769190985]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[281.3240268324339,260.80729903179747,255.12535366356002,237.8828270591967,241.80388316137456,244.03907246426678,247.62461366887194,238.75377138087237,234.70675843605298,236.928199539697,240.87253821174374,241.6994099168519,238.28208568185667,239.0471162443799,238.27899931807744,239.21645201922425,241.74048286374222,243.10175147752233,242.92311996898013,244.04991156327682,244.24320966316142,246.1843880069973,247.23842989624254,248.85994683608192,250.16090411938382,250.380876167686,250.0361330858293,250.90846665485873,251.68308851447702,252.26525567503938,252.37265210762686,252.7617100581054,252.78738030163004,253.5011625866412,253.9872464283544,254.49416802468528,254.7456688143702,255.3444012645537,255.66289762831514,255.83861608828622,256.14702238500774,256.6394766427767,257.1420860762617,257.6806363815049,258.0106221673974,257.7035973684452,258.23719303738477,258.68156992776096,258.9373847301078,259.13821606485965]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[272.46830533973804,241.78463779853504,249.6093558056423,237.885715015628,233.30138606509416,231.5341163636874,241.43426515400418,235.63367658666021,233.68416201609588,234.04873189024707,231.84238510711822,231.4702630423996,230.06823397922275,229.5103969981828,228.75739829506836,229.89166381376452,230.34598749142947,232.20035581399682,229.24827641081944,229.1037633040514,229.83551635086326,231.28862091209098,231.3594977066096,232.3467760652216,233.76448374422853,233.05113866185906,232.88754616317073,232.94535334105072,232.94257043262184,232.96641181791148,232.68266495769592,232.47211402862604,232.46306352864812,232.65050949919095,232.58015341055602,232.9650768645647,232.79951378145196,233.09971376738818,232.84187099783432,232.24524147053793,232.11349071389088,232.39309424030648,232.54785062779166,232.88570849880023,232.98054689701823,232.92041850328545,233.17398738491497,233.39277833668072,233.35628514819473,233.2838435219927],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[262.93651296054526,287.6375623385943,257.2165576404688,272.5140660876978,273.1695761131853,264.35720572372287,261.07406596308107,254.88207044389853,258.38839592806045,262.11701239179627,267.6951605512542,271.53250046572884,262.7040006131894,265.75432935950613,262.114037899778,261.84228554031483,264.36731892808154,266.15215062623685,267.4327480487521,269.6309377374928,269.48444713815405,271.4818261610457,272.8903936826389,274.74314169156986,275.72989432685085,276.2294630878505,275.50914917912144,276.22245970820956,277.1382363801662,277.38974412587646,277.0465311985006,277.817702247661,277.6840789019173,278.78367075923524,279.33293067239293,279.4889979734016,279.6976477188074,280.4202812071916,280.98238272287995,281.70027733339685,282.31283440455616,282.8332689290389,283.4978860814702,284.09453162363434,284.49203098952376,283.3943231220227,284.0512192506443,284.59245044192505,284.9814104510789,285.0152253807624],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[593.4129017439026,431.0983001787857,457.27895480862026,479.1306571978581,477.0738054582887,456.97286609995115,456.56249553070666,471.5388821199321,479.42369693475564,485.17149581755456,486.86263696679043,492.3015597349677,498.30842629408426,503.0315670037348,507.45320464888886,497.5400601251607,498.68627669354555,501.6036268088199,503.9104012138094,506.24956291998734,506.96362769251715,509.5244724850893,512.842805887678,516.3140457276619,518.3045085195482,520.0005087839382,521.3081868808501,521.8772706279069,523.7443509431869,518.8420285183095,520.6869432311822,520.8283247517852,522.0528390380846,522.3710003466231,523.571522802245,524.4341673306079,524.6284744088405,526.0646636057904,526.633800732909,527.3094276713449,528.5954602242217,530.030137177145,531.1718309899468,532.0220203831934,533.1585880072793,534.1967846203248,535.011411432146,536.048876114227,536.5523401688706,537.4636937500337],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[414.98817063308195,366.50059191038247,352.4693321428457,292.2865196339643,309.2095053032843,324.9416119167027,327.445079830083,309.0957665986239,295.32107555759023,297.00137147383936,307.8875451543311,307.6622417584949,309.0720830306253,308.30067877913586,311.0097899405962,314.00424784215016,319.0210976536892,318.835795404308,321.2386676870343,322.73282576828166,322.95493488499426,325.4289473557598,327.41238309696917,329.07083997926026,330.4993838234736,331.86435604907257,331.96033624881983,333.7973917856844,335.5125812858553,337.54007580873963,338.88621378696763,339.8203630128612,340.2562486400974,341.2171584408082,342.35467069973424,343.51553870561094,344.50016094219257,345.4207419270529,346.3793373400203,347.13397510463284,347.7495695570086,348.3589148952376,349.1831273303213,349.9575426379458,350.6008009146211,350.9935965485372,351.76727407845334,352.40655108531865,352.9933159477565,353.99784454487957],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[602.8524163937774,593.9930514084854,568.4286521605666,589.5148836539347,585.9697961703371,562.0888506603684,561.6743446328222,559.2142492247835,565.8842294493098,565.3525937177873,570.5541554202105,575.2995919910735,577.6065126705715,577.2898125389017,577.3378981592344,578.769592331011,579.4856192218698,580.321135839802,581.985071421987,582.9487486563536,584.4842992560474,586.2040382226564,587.5048982821447,587.502718402603,588.597516952759,589.4252417763221,590.1157284236573,589.3783220186452,590.1259596270739,590.2993327831389,590.8710978991502,591.5969935095278,592.3224689810563,592.7539633473259,592.9252374648047,593.4800012625358,593.7747635976797,594.5727303349091,595.0995809965583,595.6030781871716,596.116452717977,596.1416285324924,596.6775485365531,597.2325288330227,597.7686057223083,597.7646059591668,598.1776288250252,598.5644371696102,598.9787318229722,599.7268559419431],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","FigRMSEGRidkNN = visuRMSEGrid(KNeighborsRegressor(), 'kNN', n_neighbors,\n","                              'n neighbors', GridkNN)\n","FigRMSEGRidkNN.show()\n","if write_data is True:\n","    FigRMSEGRidkNN.write_image('./Figures/EmissionsGraphRMSEkNN.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                      21\n","1  randomforestregressor__max_features                    auto\n","      RandomForestRegressor()\n","R²                   0.842628\n","RMSE               203.144550\n","MAE                 63.700648\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predRF=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[30.730476190476203,35.70952380952381,11.818095238095236,16.68904761904762,77.7314285714286,345.0961904761905,12.008571428571425,19.86142857142858,135.2709523809524,11.048095238095238,114.48714285714286,77.7695238095238,34.34428571428572,37.40904761904762,16.549999999999997,76.95952380952382,39.06857142857143,40.92619047619048,33.3447619047619,83.01952380952379,19.176190476190477,23.741428571428585,18.891904761904772,36.506190476190476,68.96285714285716,27.023809523809526,206.63904761904763,39.72571428571429,38.67142857142858,78.9171428571428,59.98142857142854,32.448095238095235,142.72333333333333,82.26523809523809,158.2761904761905,72.51142857142858,398.0395238095238,19.617142857142852,76.87095238095237,8.203809523809523,55.935714285714276,55.30095238095239,20.046190476190475,169.50761904761904,32.62952380952381,35.36333333333331,261.85428571428577,46.429523809523815,85.99761904761905,15.255238095238093,11.341904761904763,5.885714285714284,615.6761904761904,62.198095238095235,47.27619047619048,87.39523809523811,199.71714285714285,4.644761904761905,60.29285714285714,53.016190476190474,154.45809523809524,123.68571428571427,22.905714285714296,162.66285714285712,78.82,11.616190476190477,37.80000000000001,75.47428571428574,43.79333333333333,28.720476190476194,7.58809523809524,101.77904761904759,12.382380952380949,9.92952380952381,118.13619047619044,55.674285714285716,585.9547619047617,35.25095238095238,46.080000000000005,7.935238095238095,24.639047619047616,65.44619047619048,130.68,128.97238095238095,17.413809523809523,5.194761904761905,51.25285714285714,25.095714285714287,51.66000000000002,420.2857142857143,149.98238095238094,24.260952380952386,215.2957142857143,12.161428571428576,22.028571428571436,55.14857142857142,7830.432380952382,98.47904761904762,37.540476190476184,23.274285714285718,44.85285714285713,6.597142857142857,97.20619047619046,28.57095238095238,104.95857142857143,29.480952380952377,19.642380952380947,327.72333333333324,53.69285714285714,132.25047619047618,28.329523809523813,12.717714285714285,12.582857142857144,5.916190476190476,60.68285714285715,35.042380952380945,14.714285714285717,218.00285714285715,42.027619047619055,326.97904761904766,4.737142857142857,96.07095238095235,15.836666666666671,583.4700000000001,43.441904761904766,23.128095238095238,18.557619047619045,56.26619047619048,4.411904761904762,54.79285714285716,123.52142857142856,9.54952380952381,35.36142857142857,24.189047619047617,250.36761904761903,31.78,29.3852380952381,53.111428571428576,135.7095238095238,26.121428571428574,9.872857142857143,96.10333333333334,184.477619047619,49.52190476190477,33.885238095238094,33.4904761904762,94.13809523809525,36.279047619047624,55.56523809523809,4.5823809523809516,225.76904761904768,147.66904761904757,114.94714285714284,12.54714285714286,31.000952380952373,60.135238095238094,54.46619047619048,61.059523809523796,439.88000000000005,24.584761904761905,4.406190476190476,54.449523809523804,10.113809523809527,20.190476190476197,108.31761904761903,55.3104761904762,345.38571428571436,90.74285714285713,31.759047619047617,124.81476190476191,15.233333333333333,29.203333333333337,70.13428571428574,47.360476190476184,20.205238095238094,126.6204761904762,63.929047619047616,78.97142857142858,10.484761904761907,4.606666666666667,85.12523809523807,102.8847619047619,41.323492063492054,112.72952380952381,7.35,68.75809523809524,8.59968253968254,60.154761904761905,29.68142857142857,109.4,42.76142857142857,62.86380952380951,30.15071428571429,133.24571428571429,26.31619047619048,27.734285714285722,67.73285714285714,144.8542857142857,151.32761904761907,84.46761904761904,154.32666666666663,53.091428571428565,10.555714285714286,12.733809523809528,17.996190476190474,9.024761904761903,12.14142857142857,4.763809523809523,28.18238095238095,27.21476190476191,385.7204761904762,20.16714285714286,9.655714285714284,219.66809523809522,46.543333333333344,444.4499999999999,66.42476190476191,13.772857142857143,125.62142857142855,6.725714285714285,38.71761904761905,59.910952380952374,42.07619047619049,28.67428571428572,8.752857142857144,5.169523809523811,54.24952380952381,8.073333333333334,68.22714285714285,20.432857142857138,14.59247619047619,67.9547619047619,24.155000000000005,42.294285714285714,6.299999999999999,33.74761904761905,16.04285714285714,30.427142857142847,305.2266666666667,36.25714285714285,132.39428571428567,62.481428571428566,6.244285714285714,19.612380952380956,64.17333333333333,9.250952380952382,319.03285714285715,8.490595238095237,175.82190476190473,84.38904761904762,37.46666666666666,27.536190476190477,72.87761904761905,44.34285714285714,181.25904761904772,80.61285714285714,48.46476190476191,31.591904761904754,32.951904761904764,37.48380952380952,24.696666666666673,23.436666666666664,48.900000000000006,53.50285714285715,10.525714285714285,90.34571428571428,16.92809523809524,158.81761904761902,51.71726190476191,117.19666666666667,28.229047619047616,29.767619047619053,20.16142857142857,53.016190476190474,40.1247619047619,148.36952380952383,77.52714285714289,1094.0019047619048,302.92571428571426,63.24952380952379,37.02619047619047,5.105238095238095,23.67,109.41999999999996,108.44047619047619,105.14809523809524,15.616666666666669,32.27428571428571,26.523809523809526,55.868095238095236,260.0995238095239,22.886190476190478,75.33380952380952,54.710952380952385,30.985714285714277,60.710952380952385,26.029523809523806,29.35714285714285,14.767619047619043,6.448571428571429,161.34428571428566,99.91238095238097,91.62333333333333,43.74380952380952,275.4000000000001,137.95952380952386,441.60952380952386,57.00571428571429,21.24476190476191,24.394761904761907,7.032857142857144,197.3795238095238,53.210476190476186,206.6642857142857,24.134761904761906,74.8995238095238,85.56619047619049,52.49285714285715,6.033809523809524,112.96476190476191,42.96666666666667,235.89476190476194,35.42190476190476,24.29857142857143,79.78333333333332,14.261428571428572,34.76857142857142,10.82666666666667,420.85047619047623,32.87714285714286,11.499047619047618,22.77142857142857,73.27571428571429,221.20238095238096,237.68476190476196,50.957619047619055,138.6380952380952,219.10761904761907,38.597619047619055,4.955714285714286,13.607142857142858,19.413333333333338,56.39809523809525,28.808095238095245,92.2790476190476,61.16,22.119047619047617,230.8652380952381,18.58857142857143,20.316666666666666,46.471428571428575,11.249047619047616,3.8847619047619046,45.24714285714286,42.893809523809544,522.5400000000001,190.2657142857143,206.08142857142852,6.24047619047619,63.25095238095237,30.778571428571436,22.540952380952383,46.47000000000001,108.28904761904761,21.99809523809524,23.01047619047619,191.44714285714286,29.84428571428571,218.11952380952383,73.47999999999999,101.47190476190478,54.68619047619048,86.02714285714285,197.28999999999996,104.16761904761907,17.288571428571426,120.84666666666665,45.93571428571428,50.84666666666668,40.7552380952381,12.067619047619049,19.787619047619053,120.02761904761907,154.93761904761908,16.43,214.43571428571425,54.22714285714286,34.82047619047619,327.2419047619047,48.53380952380953,114.82904761904759,51.75190476190476,46.264761904761905,31.84761904761904,16.50047619047619,115.00571428571428,256.02142857142854,10.146190476190478,49.25238095238095,29.550952380952385,118.96000000000004,91.53809523809524,23.61142857142857,86.18095238095238,8.244761904761905,34.58619047619048,16.30619047619048,77.52809523809523,72.93238095238095,10.907142857142858,6.563333333333334,53.19333333333333,27.69,101.05285714285712,23.511904761904766,6.99904761904762,36.192380952380944,47.90904761904763,105.71333333333335,81.52809523809523,18.110952380952376,10.117619047619048,20.126190476190477,65.48190476190477,119.70761904761905,91.22904761904763,1976.4714285714283,27.270000000000003,98.61714285714284,42.36619047619048,3.891904761904762,267.30428571428564,155.27523809523814,1556.7800000000007,316.132380952381,28.536666666666665,320.022380952381,10.47,115.9880952380952,15.920000000000002,8.953809523809525,27.129523809523818,3.539523809523809,15.064920634920638,233.96666666666664,110.93714285714283,29.708571428571435,6.9504761904761905,26.645714285714288,87.85142857142857,110.24809523809525,11.951428571428572,40.3247619047619,46.786666666666655,169.87285714285716,25.40238095238096,20.576190476190472,159.50714285714278,2.766190476190477,4.132857142857143,37.62142857142857,16.882380952380952,137.18761904761905,90.60380952380952,215.62523809523805,48.545238095238105,10.737619047619043,18.073333333333338,29.854285714285716,40.16571428571428,19.598095238095237,56.00142857142859,1285.4780952380954,59.61,6.439047619047621,54.44666666666666,132.63095238095238,85.68,11.100476190476192,53.707619047619055,34.98809523809524,18.297142857142862,38.15714285714285,45.94523809523809,57.37238095238096,10.50380952380953,184.8604761904762,42.91238095238095,350.1733333333333,53.28476190476189,47.9438095238095,26.1404761904762,43.326190476190476,61.362380952380946,80.61142857142858,350.4771428571429,18.936666666666664,121.74380952380952,376.52333333333337,251.84047619047618,78.29809523809524,68.3295238095238,42.38523809523808,68.45380952380954,157.58809523809524,95.90333333333335,24.167142857142856,159.06285714285713,42.607619047619046,30.161428571428576,14.523809523809529,232.95619047619047,50.18428571428571,15.290000000000004,153.36857142857144,7.45952380952381,4.360952380952382,1434.7900000000002,17.596190476190472,21.863333333333326,304.1409523809524,329.7495238095239,40.078095238095244,9.342857142857143,32.53380952380953,5.816666666666665,54.16238095238098,67.24428571428571,14.109523809523811,26.564285714285717,66.36428571428571,63.339999999999996,61.75809523809523,4.938095238095237,26.684761904761906,24.833333333333332,58.87809523809522,13.634285714285712,222.2609523809524,12.082380952380955,75.86333333333333,40.2842857142857,7.618571428571427,50.62238095238093,25.435238095238088,39.02142857142859,51.40857142857141,53.27904761904762,45.58095238095238,202.21952380952388,124.00619047619045,39.71,92.41857142857143,19.72619047619048,5.773809523809523,29.51142857142857,110.97285714285715,125.50285714285708,26.121428571428574,21.142380952380957,54.04428571428569,48.59095238095237,83.53904761904761,159.2919047619048,144.09809523809523,74.45,198.13095238095238,230.51952380952375,54.45380952380953,9.575238095238095,13.604761904761906,165.46523809523802,18.096666666666668,7.654285714285716,302.5985714285715,14.33952380952381,148.48000000000002,19.58238095238095,164.62714285714281,21.242380952380948,26.541428571428575,45.74952380952381,21.472857142857144,48.21476190476191,56.15666666666667,314.9666666666667,32.40476190476191,7.612857142857143,313.2666666666667,4.095238095238095,64.67904761904762,331.9971428571429,149.81571428571434,527.9538095238096,80.2104761904762,48.90190476190477,677.1900000000003,26.729523809523812,89.15095238095239,32.210476190476186,94.26666666666667,5.311428571428572,67.4452380952381,196.69476190476192,5.543809523809524,575.8938095238095,10.733809523809525,299.565238095238,18.032857142857146,89.55809523809523,108.7852380952381,4.911904761904762,184.25904761904764,32.016666666666666,758.7414285714285,116.38571428571429,24.334761904761905,175.63619047619048,16.276666666666664,60.84761904761905,51.44285714285712,26.3443253968254,26.478571428571428,33.18809523809523,17.601428571428574,52.256666666666675,36.409523809523805,32.509047619047614,9.256190476190474,9.373333333333335,457.2309523809524,14.58238095238095,71.28238095238096,110.73952380952382,52.68666666666666,40.28142857142856,93.5952380952381,29.498095238095246,56.12428571428572,46.782380952380954,161.84761904761902,1057.792857142857,33.46904761904762,12.510476190476188,13.227619047619052,69.60142857142857,16.166190476190472,38.88380952380951,39.01952380952381,33.151428571428575,50.7842857142857,445.6685714285714,61.23190476190476,75.56952380952379,28.22428571428571,22.905238095238094,212.74523809523816,7.395238095238095,197.42238095238093,15.671904761904763,80.30714285714286,70.90523809523809,22.78,46.18904761904761,103.53095238095236,20.828095238095237,13.543809523809522,85.21428571428571,12.103809523809522,33.77952380952381,7.091904761904761,32.40857142857143,55.977142857142866,30.867619047619048,48.834761904761905,220.23952380952383,981.8323809523811,20.801904761904762,30.115714285714294,52.301428571428566,16.343809523809522,148.38857142857142,16.837460317460316,105.71952380952379,204.4828571428571,18.761904761904763,534.4004761904762,64.11047619047619,69.90047619047621,138.55285714285716,7.836190476190477,183.03285714285713,12.104761904761906,212.9042857142857,104.08333333333333,80.38095238095241,202.66571428571427,226.31476190476192,28.218571428571426,52.854761904761894,70.90952380952379,76.8838095238095,11.168571428571429,21.339999999999996,11.860000000000003,36.04714285714287,35.72666666666667,14.248571428571434,8.968571428571428,118.0352380952381,10.31904761904761,972.0109523809525,60.19047619047619,274.12952380952385,57.57047619047618,27.870000000000005,50.82000000000002,58.2947619047619,13.17952380952381,929.487619047619,83.22380952380954,87.76285714285714,7.800000000000001,9.901904761904763,86.99190476190476,9.840476190476192,3.3585714285714285,19.96809523809524,8.167619047619047,323.9071428571429,91.12619047619044,4.3690476190476195,318.88523809523815,60.74714285714286,52.37571428571429,58.04238095238097,296.0914285714286,10.14380952380952,31.235714285714288,87.74190476190472,16.492857142857147,27.683333333333334,42.48952380952382,48.87809523809524,62.49761904761905,87.17761904761903,129.17857142857142,23.543333333333337,43.054285714285705,94.07285714285715,181.72809523809525,462.86523809523817,9.67333333333333,852.8233333333332,8.956666666666667,19.33714285714285,12.224761904761905,32.893809523809516,74.93809523809526,40.53761904761905,34.26761904761905,137.56523809523807,45.17047619047619,55.245238095238086,19.004761904761907,111.99238095238094,52.99285714285715,128.7161904761905,90.66761904761906,172.99714285714282,58.97999999999998,7.751904761904763,53.30476190476191,21.540952380952387,17.67238095238095,15.638095238095229,46.440000000000005,88.4852380952381,62.62333333333335,1053.6685714285716,152.06095238095236,681.1838095238093,7.422380952380954,33.02952380952381,60.253333333333316,81.3704761904762,63.17666666666667,20.409047619047616,3.8633333333333333,49.29619047619047,50.43190476190477,143.3833333333333,88.98285714285716,19.319047619047616,173.8657142857143,37.260000000000005,39.11952380952381,25.517619047619053,1154.342380952381,50.62095238095237,41.716666666666676,15.109285714285715,69.95047619047621,90.8085714285714,66.3695238095238,39.997142857142855,41.45666666666668,55.51285714285714,46.97714285714284,42.149523809523814,45.81571428571429,350.75380952380954,4.795238095238097,20.95904761904763,4.114761904761904,56.32761904761905,7.12809523809524,268.27571428571423,116.43380952380951,21.562857142857144,50.42285714285715,875.9666666666668,37.987142857142864,77.6452380952381,14.16,57.876190476190494,4.998571428571427,137.04714285714286,36.16857142857144,29.825714285714287,385.97714285714284,484.41238095238094,130.3080952380952,12.535238095238096,116.12857142857142,43.95333333333333,107.54047619047617,40.46523809523809,40.488571428571426,1955.2604761904763,21.876666666666676,36.37714285714286,66.32142857142856,217.8361904761905,145.8061904761905,31.94857142857142,22.82714285714286,24.337619047619047,143.64523809523803,29.78,67.57238095238095,24.309523809523807,156.33047619047616,16.514285714285716,176.92333333333335,49.863809523809515,68.53428571428574,62.895238095238106,206.23809523809524,6.382857142857143,59.12619047619047,7.596666666666664,381.4319047619047,25.007142857142856,21.48285714285715,9.877619047619048,425.2871428571429,63.722857142857144,192.31476190476195,61.37333333333333,18.029999999999998,54.12523809523809,100.32904761904761,214.3747619047619,104.73333333333332,72.48714285714284,12.39285714285714,4.122380952380951,44.83142857142858,56.97619047619049,284.00666666666666,86.85904761904762,13.724285714285717,100.13523809523811,25.399047619047618,744.6880952380952,23.708571428571428,38.14857142857143,25.749523809523815,52.359047619047615,75.18571428571427,7.798571428571429,13.861904761904762,12.923809523809522,84.04190476190479,26.904761904761905,85.00904761904762,57.2642857142857,21.19238095238095,18.026190476190475,78.97285714285717,21.498571428571427,5.845714285714285,41.92,389.365238095238,150.53285714285715,131.35809523809522,25.53190476190476,11.100476190476192,232.40190476190477,20.684761904761906,14.435714285714282,39.12619047619047,1048.567142857143,35.103333333333325,15.092857142857145,53.225714285714275,13.84285714285714,14.999047619047618,250.64666666666662,198.10571428571424,24.706666666666663,35.734285714285704,2169.40619047619,40.48904761904762,488.21904761904756,83.37809523809524,66.36809523809524,28.23333333333334,356.64666666666665,10.211230158730157,27.910476190476185,53.698095238095235,22.569999999999997,18.250952380952384,58.91238095238096,30.05380952380952,629.0409523809524,21.127142857142864,126.8795238095238,25.98428571428571,101.04571428571428,19.661428571428573,10.692380952380953,235.14714285714285,231.2219047619048,58.084761904761926,56.43190476190476,18.730952380952385,67.15904761904761,101.59285714285716,36.17523809523811,39.3447619047619,84.27333333333334,69.4647619047619,266.99619047619046,10231.79142857143,13.495238095238097,3.217142857142857,10.062380952380952,707.3304761904762,22.01761904761905,3.0309523809523826,111.50142857142858,306.2290476190476,47.24476190476192,45.7409523809524,5.866190476190478,99.35,11.588571428571429,44.74952380952381,4.297619047619047,334.71761904761917,26.31285714285714,32.12619047619049,51.03000000000001,60.12190476190476,31.357460317460315,28.643809523809523,5.498095238095237,28.041428571428575,18.549523809523812,57.37333333333334,30.56428571428571,49.87904761904762,68.77444444444444,35.649523809523814,86.74761904761905,9.675238095238099,19.23809523809524,96.64571428571428,14.737142857142855,10.942380952380951,122.48857142857143,62.13190476190477,173.69095238095238,20.729523809523805,17.679047619047612,59.27285714285714,19.505714285714287,60.15571428571426,92.28142857142856,64.6742857142857,15.40095238095238,502.4866666666666,73.09285714285717,97.63761904761907,26.32904761904762,38.067142857142855,73.98857142857142,34.12285714285714,124.79095238095236,55.321904761904776,13.368571428571428,33.404761904761905,11.224285714285719,6.494761904761904,16.040000000000003,29.829523809523806,17.721111111111107,236.137619047619,61.12142857142858,50.460952380952385,23.84047619047619,26.23619047619048,99.07,18.08809523809524,15.769999999999998,35.70952380952381,156.23952380952383,24.715714285714284,80.69285714285714,269.9014285714286,55.0942857142857,639.1209523809525,246.2957142857143,471.48285714285703,31.941904761904766,48.490952380952386,113.66904761904763,44.33285714285715,26.1104761904762,23.89142857142858,32.832857142857144,8.930000000000001,146.38000000000002,31.55476190476191,254.31238095238092,34.739047619047625,12.260952380952382,67.03523809523809,29.570476190476192,45.68619047619048,12.057142857142859,93.13047619047617,18.021428571428572,48.28190476190475,74.14714285714285,34.50190476190477,33.81904761904762,95.04,80.94999999999999,83.66666666666669,101.31428571428573,21.650476190476194,17.269523809523815,44.84523809523811,43.519047619047626,19.747619047619047,41.05380952380955,8.96190476190476,83.91857142857145,54.54190476190475,126.3342857142857,36.41142857142857,22.162380952380957,16.799523809523812,27.126666666666676,38.20666666666666,1197.7128571428573,21.05952380952381,82.1347619047619,1412.6361904761904,21.482380952380954,121.21380952380953,28.10904761904762,103.53142857142855,7054.699047619048,71.31809523809522,23.14571428571428,213.49714285714285,16.053809523809523,34.69619047619047,14.481428571428571,17.393809523809526,285.33285714285716,36.73809523809524,166.45666666666665,3.9928571428571433,231.32142857142853,129.95809523809524,41.71452380952381,3.7673015873015876,17.522380952380953,16.55809523809524,1348.469523809524,96.60380952380953,94.54142857142854,49.764761904761905,31.287619047619053,299.01285714285723,12.824285714285715,365.9371428571428,61.00809523809525,43.22,103.12619047619046,11.224285714285717,34.879999999999995,15.01857142857143,52.50142857142858,15.78333333333333,10.401904761904763,78.79476190476191,144.8085714285714,716.3995238095237,112.56523809523812,70.96047619047619,33.429523809523815,165.82619047619048,141.80714285714282,20.86142857142857,49.09523809523809,212.85238095238094,60.81190476190477,22.47,39.670476190476194,221.67857142857147,18.689047619047617,30.163333333333338,4.5619047619047635,62.854761904761915,24.795238095238098,19.442857142857143,236.24571428571423,155.48285714285717,31.92238095238095,1146.8585714285716,6.7685714285714305,161.43000000000004,48.100476190476186,37.29571428571428,370.142380952381,10.623333333333338,83.92095238095237,39.968095238095245,112.05523809523808,102.00999999999996,15.21666666666667,17.762857142857143,86.51142857142857,1163.521904761905,53.34571428571429,69.07428571428574,12.54904761904762,13.575238095238095,139.85571428571427,2819.284285714286,8.857619047619048,17.18714285714286,63.16095238095239,32.25619047619048,31.9852380952381,65.84666666666668,88.2685714285714,13.941428571428574,48.74333333333333,161.02904761904762,137.25380952380954,7.322380952380953,85.21809523809524,17.821428571428573,6.099047619047618,28.45619047619048,29.26047619047619,21.188095238095244,41.87238095238096,50.76476190476189,32.44857142857143,19.792857142857148,28.72666666666666,8.354285714285712,9.783809523809525,11.892857142857144,12.900000000000004,40.7852380952381,82.46428571428571,287.6719047619047,47.80714285714286,10.920476190476188,10.349999999999998,35.44523809523808,51.61428571428571,119.07523809523809,11.180476190476188,79.91428571428571,39.53904761904763,117.88190476190478,970.4661904761904,37.06666666666666,53.23285714285715,29.7547619047619,272.8080952380953,72.47380952380952,410.7771428571429,26.853333333333335,51.96476190476191,323.8676190476191,10.156190476190474,19.36261904761905,30.652380952380955,64.86285714285714,10.193333333333337,48.83285714285715,79.33809523809525,31.380476190476195,19.55047619047619,169.04190476190476,78.97190476190477,104.33619047619047,20.994285714285713,39.692857142857136,18.986666666666668,97.6895238095238,15.659523809523813,24.75047619047619,2.182857142857142,69.32761904761905,41.79476190476192,33.87809523809523,24.024285714285703,48.99571428571429,106.56666666666669,26.303333333333335,9.254761904761905,8.29857142857143,20.66238095238095,73.15809523809526,100.72904761904762,352.8966666666668,161.4438095238095,44.767142857142844,78.33714285714285,193.98999999999998,53.85761904761906,194.6395238095238,12.629523809523805,15.162857142857144,53.51428571428571,307.9242857142857,31.522380952380963,335.6366666666666,2822.535714285714,94.94238095238094,7.846666666666667,17.17238095238095,13.536666666666665,13.204285714285712,19.1447619047619,7.810952380952384,51.87047619047619,37.81000000000001,168.18380952380951,25.209047619047617,31.42428571428572,31.534285714285705,194.51714285714286,155.1452380952381,24.50809523809524,172.4652380952381,59.98238095238095,497.50619047619045,118.25666666666666,11.29190476190476,85.88476190476187,653.7495238095237,83.3268253968254,20.98142857142858,84.28095238095237,10.921904761904766,42.78380952380952,83.68761904761907,4.8638095238095245,283.67714285714294,148.80380952380955,126.05476190476186,30.71809523809524,51.168571428571425,10.542380952380949,100.5685714285714,33.89714285714287,28.995238095238093,63.150952380952376,41.43047619047619,4.084761904761904,145.9995238095238,5.964285714285715,14.367142857142861,200.02761904761908,12.065714285714284],"xaxis":"x","y":[30.29,47.7,6.73,5.66,152.14,7.53,17.11,9.67,73.77,5.21,150.69,47.42,17.51,11.08,4.76,73.31,30.34,59.24,33.58,31.96,232.12,9.43,8.12,51.2,14.69,33.77,10.29,32.33,61.62,7.67,21.08,35.56,105.32,41.15,287.23,85.28,139.34,18.63,25.15,10.04,42.74,29.93,17.38,263.57,11.17,8.59,570.98,13.65,13.22,2.91,6.53,49.15,1222.69,28.59,51.64,185.23,141.7,3.27,66.39,85.15,87.31,95.26,48.51,26.78,50.14,6.02,14.87,349.47,4,14.79,4.98,20.39,6.45,19.28,93.47,52.87,372.55,5.81,12.68,5.81,8.57,47.39,128.73,99.39,37.72,3.02,12,38.97,39.27,266.18,5.9,29.84,49.18,3.82,8.8,11.85,10734.57,266.78,12.25,6.04,18.07,7.21,63.89,8.53,138.68,18.34,29.39,218.53,69.3,587.16,11.97,8.46,4.36,5.16,46.57,32.22,5.46,181.38,7.08,65.03,5.87,24.25,23.17,178.21,39.57,16.5,4.48,61.36,4.03,8.8,77.29,6.05,45.3,8.77,256.57,2.17,37.97,11.11,74.45,17.67,19.83,406.2,209.92,45.81,2.36,22.02,198.38,6.22,8.31,5.96,13.61,262.75,152.36,15.43,7.23,5.94,43.45,60.94,159.16,31.92,9.71,69.36,18.1,8.37,23.58,48.3,147.82,71.47,18.89,53.36,17.65,4.5,92.59,62.57,4.8,25.37,90.31,100.42,6.37,2.9,63.61,64.19,22.15,192.63,5.55,109.79,3.29,67.15,31.92,79.3,8.81,6.71,12.33,29.33,12.15,16.83,116.21,6.43,187.92,39.56,30.85,78.29,4.33,4.63,16.89,3.42,10.21,5.97,49.16,20.1,190.58,7.67,4.79,1000.06,50.52,122.99,134.69,5.6,27.38,7.08,2.04,44.6,3.51,41.98,6.36,5.87,8.87,3.97,127.56,37.85,4.26,147.69,5.9,10.55,5.69,32.81,34.81,56.25,653.5,65.65,97.01,3.49,4.65,9.13,41.69,14.77,39.47,2.82,142.48,68.97,6.64,7.31,82.9,8.69,256.31,93.23,11.77,38.08,17.24,43.64,18.41,12.7,27.3,6.71,3.89,71.03,1.78,152.71,50.43,22.18,12.06,29.26,20.98,77.15,3.84,91.09,14.35,839.79,226.43,27.27,57.69,5.16,6.69,20.79,176.79,59.61,9.83,8.24,16.09,20.44,170.84,35.39,30.02,19.72,15.59,66.05,3.89,7.46,14.35,7.74,141.32,46.61,20.29,3.75,235.69,243.38,151.95,100.87,56.51,6.79,4.84,247.85,24.17,168.62,47.11,12.99,104.17,30.66,3.82,120.91,132.47,106.37,44.93,6.93,61.71,4.2,24.42,8.14,111.69,6.22,9.45,23.84,4.88,507.7,553.27,9.48,122.92,103.37,50,3.86,3.4,3.66,93.43,6.85,32.8,32.73,13.43,413.1,6.19,6.16,68.14,0.68,16.45,13.88,350.87,802.89,291.28,91.73,4.72,7.21,25.02,6.75,28.93,229.55,3.01,49.59,341.23,37.18,353.49,93.87,93.21,11.79,49.25,154.4,84.93,6.02,43.63,35.36,51.47,42.8,4.35,17.11,205.07,40.78,5.6,186.96,14.51,35.56,537.03,53.38,38.9,8.16,37.51,3.82,5.51,50.76,380.17,10.75,4.3,7.12,1.6,63.45,4.42,7.51,7.14,22.78,8.45,8.59,5.37,11.59,6.93,54.95,19.7,89.41,74.04,3.91,35.22,43.13,28.54,70.4,10.06,6.3,5.11,45.38,0,10.14,765.56,14.46,28.64,27.86,8.85,256.72,196.67,525.78,93.67,8.97,165.61,3.06,17.84,19.17,50.7,47.25,2.99,3.2,54.92,96.4,16.01,4.74,10.56,132.28,15.5,7.29,63.36,47.68,297.23,67.06,3.4,98.54,0.76,3.72,50.15,2.45,265.21,188.67,156.08,83.1,94.85,20.57,29.18,12.09,13.77,34.14,1231.19,39.41,7.57,113.25,237.36,93.23,33.52,57.03,12.79,12.97,5.72,6.38,10.93,16.2,57.16,18.62,346.62,44.19,190.45,6.1,75.67,3.69,73.48,1185.15,20.12,92.3,188.13,72.71,101.31,15.8,80.06,3.18,141.15,17.74,22.65,42.2,14.32,46.86,18.42,247.85,52.43,5.71,51.26,3.07,10.14,242.19,21.3,8.45,113.31,75.12,13.15,0.89,8.57,3.48,5.33,102.04,3.71,31.58,42.52,6.89,7.69,6.16,29.16,5.88,76.85,1.3,37.58,7.29,24.1,28.78,25.39,11.28,32.02,31.46,38.54,49.7,9.47,456.22,124.92,14.19,110.62,24.36,4.13,68.89,145.48,21.24,18.72,12.4,15.62,43.16,160.3,47.78,139.27,30.86,44.97,163.99,142.69,12.63,4,109.46,26.64,64.13,285.62,9.57,74.52,4.01,226.81,10.16,13.64,92.58,7.15,8.16,75.64,60.02,223.36,8,13.87,3.92,73.32,233.43,208.26,351.53,16.76,200.3,604.45,51.91,16.06,10.92,90.73,4.73,69.42,238.61,4.93,900.81,0.7,399.76,31.85,19.95,248.29,2.43,480.9,10.32,176.76,144.3,5.15,97.97,27.69,63.6,144.53,27.56,29.24,26.55,18.5,80.06,10.77,15.34,0.75,3.91,474.59,4.25,128.41,433.86,69.88,44.32,413.35,4.43,348.22,29.78,66.32,290.4,44.51,4.02,5.2,56.82,4.55,9.65,52.88,57.79,58.32,393.2,72.69,45.02,24.62,4.19,223.34,10.89,83.27,7.56,29.73,100.71,7.53,121.88,176.47,23.82,4.48,53.21,16.3,4.9,6.39,54.95,61.64,7.12,19.68,8.07,704.76,33.35,184.81,61.25,20.88,60.04,16.47,551.43,260.13,8.61,167.31,70.14,64,21.92,9.16,111.23,7.02,236.34,318.08,149.93,364.9,295.86,34.62,3.11,156.18,118.86,4.16,3.27,6.54,3.65,48.91,8.87,5.19,148.11,88.87,1793.9,37.34,275.54,30.87,5.53,56.96,82.38,21.52,1597.56,42.41,109.72,4.68,19.81,79.81,6.74,6.02,39.05,5.79,271.32,85.55,4.52,233,9.53,157.21,15.23,35.44,3.92,64.75,40.35,20.45,15.27,29.42,39.75,119.03,28.07,95.77,23.14,60.83,224.13,98.65,443.86,3.52,221.47,5.68,7.9,11.73,4.71,38.59,32.75,4.79,101.43,5.54,5.66,5.88,130.58,60.33,25.6,69.18,253.53,84.57,3.55,42.71,5.09,12.28,19.15,93.68,42.24,58.44,1084.79,243.37,7.47,4.43,8.09,69.67,82.72,101.64,19.04,4.02,5.6,46.49,183.51,98.22,6.54,69.3,41.96,3.71,5.97,913.07,145.42,7.56,5.3,45.96,8,71.16,13.1,53.88,53.02,379.34,43.35,7.94,322.79,4.43,14.81,3.41,18.97,3.9,115.56,61.68,29.92,9.99,530.66,3.7,49.17,27.39,96.62,6.57,51.93,21.38,63.48,90.34,423.93,183.79,5.86,92.45,23.85,45.32,77.75,5.31,2451.58,9.87,39.5,67.17,162.39,220.26,8.46,23,31.79,28.83,20.02,101.99,26.72,15.5,32.77,132.56,15.74,110.66,9.65,246.83,10.83,5.92,9.2,467.18,24.35,3.67,5.35,862.64,89.53,234.03,1.67,15.2,71.38,43.57,260.57,226.73,87.14,14.44,3.99,28.82,73,188.07,45.6,5.07,103.77,3.16,395.26,27.93,10.23,23.27,60.07,112.19,7.22,20.07,15.9,72.46,3.92,176.63,32.52,32.35,6.84,96.59,44.71,3.52,48.95,116.95,36.65,142.36,8.27,3.89,315.85,9.73,1.61,5.81,1638.46,8.61,14.99,49.18,26.73,17.42,202.49,59.21,9.99,42.57,481.1,29.62,399.54,72.88,52.79,20.67,381.89,3.56,4.07,15.74,26.94,27.84,32.72,15.64,582.28,13.36,23.83,27.06,411.09,4.74,15.18,73.14,185.82,13.97,12.15,3.49,19.84,93.67,93.55,59.18,74.98,43.56,312.6,12307.16,6.47,6.19,12.59,579.99,59.63,2.81,119.34,283.04,19.72,40.76,3.18,62.01,1.58,15.77,3.34,587.09,12.07,23.74,4.61,89.6,103.42,7.52,57.33,16.82,65.75,22.76,8.42,39.54,99.91,47.12,80.75,10.17,4.54,28.3,19.36,6.35,63.87,10.05,184.64,34.08,3.15,76.89,11.99,292.92,112.01,66.1,23.22,251.43,75.58,152.14,41.06,7.07,115.34,6.12,34.87,85.3,19.76,28.03,6.67,18.73,20.07,34.98,12.56,150.23,12.35,50.23,19.16,45.22,344.2,4.74,1.34,3.77,36.71,3.92,83.35,324.92,64.95,675.34,22.17,536.43,43.84,30.26,78.19,44.61,61.72,39.57,73.52,3.99,182.11,99.62,551.53,39.45,6.99,89.94,7.27,4.76,9.66,77.56,3.78,44.26,39.31,37.14,11.29,24.91,19.73,326.74,64.79,59.06,2.35,49.79,61.19,4.39,357.86,6.61,126.3,53.25,58.98,4.95,6.87,8.93,39.1,66.44,440.31,38.45,102.08,1558.85,37.17,124.96,5.3,260.94,3232.23,74.13,31.18,225.67,6.04,49.44,8.31,67.65,320.35,46.72,143.67,3.16,94.59,96.15,5.7,25.49,3.65,22.46,1727.11,49.26,173.75,295.48,53.56,136.07,5.03,250.05,74.28,3.59,123.14,0.12,24.64,4.98,46.95,27.76,5.53,17.23,154.13,740.97,78.06,125.42,7.53,131.64,129.05,6.67,99.65,298.91,30.14,46.32,5.88,0,14.3,45.07,1.14,147.24,21.28,4.44,249.09,72.02,47.25,1623.34,8.7,226.06,10.83,47.91,434.48,5.33,120.13,38.15,180.38,37.16,8.18,7.05,148.7,0,29.26,39.35,24.18,4.02,103.39,2089.28,4.5,77.48,143.3,46.61,46.85,80.08,108.04,23.01,47.82,67.62,69.53,4.2,25.48,7.74,4.33,22.61,3.63,11.25,58.79,58.85,13.34,7.25,26.41,5.94,5.02,4.44,4.72,23.65,96.41,529.47,7.1,14.15,3.72,48.23,63.06,96.39,5.29,10.08,4.49,22.08,1084.49,8.16,65.77,20.63,375.56,112.13,58.52,26.54,68.3,464.77,5.14,57.17,8.98,91.49,3.68,44.35,92.7,4.99,0.37,305.15,57.25,0,16.27,45.59,34.26,34.92,24.31,5.07,1.08,28.19,82.87,109.71,17.19,50.31,20.25,41.22,5.74,45.98,17.88,86.5,8.55,394.6,22.57,48.85,38.13,269.23,47.26,122.83,5.23,8.81,48.65,640.71,46,328.63,4725.43,73.9,65.65,6.98,3.5,8.54,4.15,6.98,4.69,40.38,150.81,8.15,4.38,37.13,328.97,132.71,3.15,229.2,10.97,122.54,84.18,6.46,26.59,500.93,148.02,6.39,222.02,12.75,50.57,9.28,3.86,369.14,156.11,46.38,14.99,69.77,7.21,100.59,5.3,39.61,102.73,23.42,4.26,127.53,3.91,12.5,117.11,8.47],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","TotalGHGEmissions_predRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train.ravel(),\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","print(ScoresRF)\n","figRF.show()\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[492.4727392227469,359.594824075382,424.2321799472862,387.6659370726389,336.7783801546342,346.9391322025043,346.81015807687714,349.9617050081758,345.0563019335369,342.10439487027725]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[698.8981574165716,511.32894006807794,537.1854573687716,507.8394464251505,455.32205121095876,480.5517476273969,473.49782562043936,473.4420444496346,470.63100376310985,470.2202972248989]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[286.0473210289222,207.86070808268605,311.2789025258008,267.49242772012724,218.23470909830962,213.32651677761174,220.12249053331493,226.481365566717,219.4816001039639,213.98849251565557]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[263.65146620508614,246.9502365349297,411.1277081493769,377.72155597811206,229.02959624890488,254.511770166215,254.33038516709223,252.73624987545443,252.552600320227,238.1841479178054],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[334.4818953452542,158.2021799914693,261.9591640894395,197.00705841587717,202.05795478129124,180.55533431198478,201.59182667280058,209.46629148234402,196.22339233427596,195.18341510163117],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[734.9064330265344,452.86630270471676,554.9044603994134,434.97233850552266,376.9735962727131,400.8650272562497,399.21089186739687,392.43913084573074,390.8998468935685,397.20635229581484],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[382.2608484836459,349.97752971109657,348.5709251145584,359.98123625193415,341.9190832176559,328.67906891201346,315.5747065378787,333.1699479573351,326.77167800474524,322.7348004316741],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[747.063053053214,589.9778714346976,544.598641983643,568.6474962117484,533.9116702526061,570.0844603660587,563.3429801392174,561.9969048800148,558.8339921148674,557.2132586044605],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=auto<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF = visuRMSEGrid(RandomForestRegressor(), 'RF', n_estimatorsRF,\n","                             'n estimators', GridRF, BestParametresRF,\n","                             'randomforestregressor__max_features')\n","FigRMSEGRidRF.show()\n","if write_data is True:\n","    FigRMSEGRidRF.write_image('./Figures/EmissionsGraphRMSERF.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                   4\n","1          adaboostregressor__loss         exponential\n","      AdaBoostRegressor()\n","R²               0.735593\n","RMSE           263.316554\n","MAE             98.513848\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predAB=%{x}<br>TotalGHGEmissions_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,165.68011904761883,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,666.570888888889,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,66.72650022904251,65.77699938511981,66.72650022904251,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,483.4624542124539,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,8901.448,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,66.72650022904251,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,165.68011904761883,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,1161.5760810810816,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,165.68011904761883,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,165.68011904761883,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,483.4624542124539,66.72650022904251,483.4624542124539,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,2955.9725806451615,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,827.5458904109586,483.4624542124539,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,165.68011904761883,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,444.8022580645166,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,1161.5760810810816,65.77699938511981,65.77699938511981,66.72650022904251,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,165.68011904761883,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,483.4624542124539,65.77699938511981,483.4624542124539,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,165.68011904761883,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,827.5458904109586,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,165.68011904761883,827.5458904109586,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,3515.6216666666664,65.77699938511981,666.570888888889,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,444.8022580645166,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,66.72650022904251,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,165.68011904761883,65.77699938511981,65.77699938511981,66.72650022904251,66.72650022904251,483.4624542124539,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,666.570888888889,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,66.72650022904251,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,827.5458904109586,65.77699938511981,65.77699938511981,66.72650022904251,66.72650022904251,165.68011904761883,65.77699938511981,65.77699938511981,65.77699938511981,165.68011904761883,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,165.68011904761883,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,1161.5760810810816,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,444.8022580645166,65.77699938511981,65.77699938511981,1161.5760810810816,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,8901.448,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,165.68011904761883,66.72650022904251,65.77699938511981,65.77699938511981,165.68011904761883,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,483.4624542124539,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,1161.5760810810816,65.77699938511981,65.77699938511981,1161.5760810810816,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,7669.718928571443,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,483.4624542124539,65.77699938511981,66.72650022904251,65.77699938511981,483.4624542124539,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,3515.6216666666664,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,827.5458904109586,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,483.4624542124539,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,483.4624542124539,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,2955.9725806451615,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,444.8022580645166,66.72650022904251,65.77699938511981,66.72650022904251,65.77699938511981,483.4624542124539,66.72650022904251,65.77699938511981,65.77699938511981,827.5458904109586,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,66.72650022904251,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981,65.77699938511981],"xaxis":"x","y":[30.29,47.7,6.73,5.66,152.14,7.53,17.11,9.67,73.77,5.21,150.69,47.42,17.51,11.08,4.76,73.31,30.34,59.24,33.58,31.96,232.12,9.43,8.12,51.2,14.69,33.77,10.29,32.33,61.62,7.67,21.08,35.56,105.32,41.15,287.23,85.28,139.34,18.63,25.15,10.04,42.74,29.93,17.38,263.57,11.17,8.59,570.98,13.65,13.22,2.91,6.53,49.15,1222.69,28.59,51.64,185.23,141.7,3.27,66.39,85.15,87.31,95.26,48.51,26.78,50.14,6.02,14.87,349.47,4,14.79,4.98,20.39,6.45,19.28,93.47,52.87,372.55,5.81,12.68,5.81,8.57,47.39,128.73,99.39,37.72,3.02,12,38.97,39.27,266.18,5.9,29.84,49.18,3.82,8.8,11.85,10734.57,266.78,12.25,6.04,18.07,7.21,63.89,8.53,138.68,18.34,29.39,218.53,69.3,587.16,11.97,8.46,4.36,5.16,46.57,32.22,5.46,181.38,7.08,65.03,5.87,24.25,23.17,178.21,39.57,16.5,4.48,61.36,4.03,8.8,77.29,6.05,45.3,8.77,256.57,2.17,37.97,11.11,74.45,17.67,19.83,406.2,209.92,45.81,2.36,22.02,198.38,6.22,8.31,5.96,13.61,262.75,152.36,15.43,7.23,5.94,43.45,60.94,159.16,31.92,9.71,69.36,18.1,8.37,23.58,48.3,147.82,71.47,18.89,53.36,17.65,4.5,92.59,62.57,4.8,25.37,90.31,100.42,6.37,2.9,63.61,64.19,22.15,192.63,5.55,109.79,3.29,67.15,31.92,79.3,8.81,6.71,12.33,29.33,12.15,16.83,116.21,6.43,187.92,39.56,30.85,78.29,4.33,4.63,16.89,3.42,10.21,5.97,49.16,20.1,190.58,7.67,4.79,1000.06,50.52,122.99,134.69,5.6,27.38,7.08,2.04,44.6,3.51,41.98,6.36,5.87,8.87,3.97,127.56,37.85,4.26,147.69,5.9,10.55,5.69,32.81,34.81,56.25,653.5,65.65,97.01,3.49,4.65,9.13,41.69,14.77,39.47,2.82,142.48,68.97,6.64,7.31,82.9,8.69,256.31,93.23,11.77,38.08,17.24,43.64,18.41,12.7,27.3,6.71,3.89,71.03,1.78,152.71,50.43,22.18,12.06,29.26,20.98,77.15,3.84,91.09,14.35,839.79,226.43,27.27,57.69,5.16,6.69,20.79,176.79,59.61,9.83,8.24,16.09,20.44,170.84,35.39,30.02,19.72,15.59,66.05,3.89,7.46,14.35,7.74,141.32,46.61,20.29,3.75,235.69,243.38,151.95,100.87,56.51,6.79,4.84,247.85,24.17,168.62,47.11,12.99,104.17,30.66,3.82,120.91,132.47,106.37,44.93,6.93,61.71,4.2,24.42,8.14,111.69,6.22,9.45,23.84,4.88,507.7,553.27,9.48,122.92,103.37,50,3.86,3.4,3.66,93.43,6.85,32.8,32.73,13.43,413.1,6.19,6.16,68.14,0.68,16.45,13.88,350.87,802.89,291.28,91.73,4.72,7.21,25.02,6.75,28.93,229.55,3.01,49.59,341.23,37.18,353.49,93.87,93.21,11.79,49.25,154.4,84.93,6.02,43.63,35.36,51.47,42.8,4.35,17.11,205.07,40.78,5.6,186.96,14.51,35.56,537.03,53.38,38.9,8.16,37.51,3.82,5.51,50.76,380.17,10.75,4.3,7.12,1.6,63.45,4.42,7.51,7.14,22.78,8.45,8.59,5.37,11.59,6.93,54.95,19.7,89.41,74.04,3.91,35.22,43.13,28.54,70.4,10.06,6.3,5.11,45.38,0,10.14,765.56,14.46,28.64,27.86,8.85,256.72,196.67,525.78,93.67,8.97,165.61,3.06,17.84,19.17,50.7,47.25,2.99,3.2,54.92,96.4,16.01,4.74,10.56,132.28,15.5,7.29,63.36,47.68,297.23,67.06,3.4,98.54,0.76,3.72,50.15,2.45,265.21,188.67,156.08,83.1,94.85,20.57,29.18,12.09,13.77,34.14,1231.19,39.41,7.57,113.25,237.36,93.23,33.52,57.03,12.79,12.97,5.72,6.38,10.93,16.2,57.16,18.62,346.62,44.19,190.45,6.1,75.67,3.69,73.48,1185.15,20.12,92.3,188.13,72.71,101.31,15.8,80.06,3.18,141.15,17.74,22.65,42.2,14.32,46.86,18.42,247.85,52.43,5.71,51.26,3.07,10.14,242.19,21.3,8.45,113.31,75.12,13.15,0.89,8.57,3.48,5.33,102.04,3.71,31.58,42.52,6.89,7.69,6.16,29.16,5.88,76.85,1.3,37.58,7.29,24.1,28.78,25.39,11.28,32.02,31.46,38.54,49.7,9.47,456.22,124.92,14.19,110.62,24.36,4.13,68.89,145.48,21.24,18.72,12.4,15.62,43.16,160.3,47.78,139.27,30.86,44.97,163.99,142.69,12.63,4,109.46,26.64,64.13,285.62,9.57,74.52,4.01,226.81,10.16,13.64,92.58,7.15,8.16,75.64,60.02,223.36,8,13.87,3.92,73.32,233.43,208.26,351.53,16.76,200.3,604.45,51.91,16.06,10.92,90.73,4.73,69.42,238.61,4.93,900.81,0.7,399.76,31.85,19.95,248.29,2.43,480.9,10.32,176.76,144.3,5.15,97.97,27.69,63.6,144.53,27.56,29.24,26.55,18.5,80.06,10.77,15.34,0.75,3.91,474.59,4.25,128.41,433.86,69.88,44.32,413.35,4.43,348.22,29.78,66.32,290.4,44.51,4.02,5.2,56.82,4.55,9.65,52.88,57.79,58.32,393.2,72.69,45.02,24.62,4.19,223.34,10.89,83.27,7.56,29.73,100.71,7.53,121.88,176.47,23.82,4.48,53.21,16.3,4.9,6.39,54.95,61.64,7.12,19.68,8.07,704.76,33.35,184.81,61.25,20.88,60.04,16.47,551.43,260.13,8.61,167.31,70.14,64,21.92,9.16,111.23,7.02,236.34,318.08,149.93,364.9,295.86,34.62,3.11,156.18,118.86,4.16,3.27,6.54,3.65,48.91,8.87,5.19,148.11,88.87,1793.9,37.34,275.54,30.87,5.53,56.96,82.38,21.52,1597.56,42.41,109.72,4.68,19.81,79.81,6.74,6.02,39.05,5.79,271.32,85.55,4.52,233,9.53,157.21,15.23,35.44,3.92,64.75,40.35,20.45,15.27,29.42,39.75,119.03,28.07,95.77,23.14,60.83,224.13,98.65,443.86,3.52,221.47,5.68,7.9,11.73,4.71,38.59,32.75,4.79,101.43,5.54,5.66,5.88,130.58,60.33,25.6,69.18,253.53,84.57,3.55,42.71,5.09,12.28,19.15,93.68,42.24,58.44,1084.79,243.37,7.47,4.43,8.09,69.67,82.72,101.64,19.04,4.02,5.6,46.49,183.51,98.22,6.54,69.3,41.96,3.71,5.97,913.07,145.42,7.56,5.3,45.96,8,71.16,13.1,53.88,53.02,379.34,43.35,7.94,322.79,4.43,14.81,3.41,18.97,3.9,115.56,61.68,29.92,9.99,530.66,3.7,49.17,27.39,96.62,6.57,51.93,21.38,63.48,90.34,423.93,183.79,5.86,92.45,23.85,45.32,77.75,5.31,2451.58,9.87,39.5,67.17,162.39,220.26,8.46,23,31.79,28.83,20.02,101.99,26.72,15.5,32.77,132.56,15.74,110.66,9.65,246.83,10.83,5.92,9.2,467.18,24.35,3.67,5.35,862.64,89.53,234.03,1.67,15.2,71.38,43.57,260.57,226.73,87.14,14.44,3.99,28.82,73,188.07,45.6,5.07,103.77,3.16,395.26,27.93,10.23,23.27,60.07,112.19,7.22,20.07,15.9,72.46,3.92,176.63,32.52,32.35,6.84,96.59,44.71,3.52,48.95,116.95,36.65,142.36,8.27,3.89,315.85,9.73,1.61,5.81,1638.46,8.61,14.99,49.18,26.73,17.42,202.49,59.21,9.99,42.57,481.1,29.62,399.54,72.88,52.79,20.67,381.89,3.56,4.07,15.74,26.94,27.84,32.72,15.64,582.28,13.36,23.83,27.06,411.09,4.74,15.18,73.14,185.82,13.97,12.15,3.49,19.84,93.67,93.55,59.18,74.98,43.56,312.6,12307.16,6.47,6.19,12.59,579.99,59.63,2.81,119.34,283.04,19.72,40.76,3.18,62.01,1.58,15.77,3.34,587.09,12.07,23.74,4.61,89.6,103.42,7.52,57.33,16.82,65.75,22.76,8.42,39.54,99.91,47.12,80.75,10.17,4.54,28.3,19.36,6.35,63.87,10.05,184.64,34.08,3.15,76.89,11.99,292.92,112.01,66.1,23.22,251.43,75.58,152.14,41.06,7.07,115.34,6.12,34.87,85.3,19.76,28.03,6.67,18.73,20.07,34.98,12.56,150.23,12.35,50.23,19.16,45.22,344.2,4.74,1.34,3.77,36.71,3.92,83.35,324.92,64.95,675.34,22.17,536.43,43.84,30.26,78.19,44.61,61.72,39.57,73.52,3.99,182.11,99.62,551.53,39.45,6.99,89.94,7.27,4.76,9.66,77.56,3.78,44.26,39.31,37.14,11.29,24.91,19.73,326.74,64.79,59.06,2.35,49.79,61.19,4.39,357.86,6.61,126.3,53.25,58.98,4.95,6.87,8.93,39.1,66.44,440.31,38.45,102.08,1558.85,37.17,124.96,5.3,260.94,3232.23,74.13,31.18,225.67,6.04,49.44,8.31,67.65,320.35,46.72,143.67,3.16,94.59,96.15,5.7,25.49,3.65,22.46,1727.11,49.26,173.75,295.48,53.56,136.07,5.03,250.05,74.28,3.59,123.14,0.12,24.64,4.98,46.95,27.76,5.53,17.23,154.13,740.97,78.06,125.42,7.53,131.64,129.05,6.67,99.65,298.91,30.14,46.32,5.88,0,14.3,45.07,1.14,147.24,21.28,4.44,249.09,72.02,47.25,1623.34,8.7,226.06,10.83,47.91,434.48,5.33,120.13,38.15,180.38,37.16,8.18,7.05,148.7,0,29.26,39.35,24.18,4.02,103.39,2089.28,4.5,77.48,143.3,46.61,46.85,80.08,108.04,23.01,47.82,67.62,69.53,4.2,25.48,7.74,4.33,22.61,3.63,11.25,58.79,58.85,13.34,7.25,26.41,5.94,5.02,4.44,4.72,23.65,96.41,529.47,7.1,14.15,3.72,48.23,63.06,96.39,5.29,10.08,4.49,22.08,1084.49,8.16,65.77,20.63,375.56,112.13,58.52,26.54,68.3,464.77,5.14,57.17,8.98,91.49,3.68,44.35,92.7,4.99,0.37,305.15,57.25,0,16.27,45.59,34.26,34.92,24.31,5.07,1.08,28.19,82.87,109.71,17.19,50.31,20.25,41.22,5.74,45.98,17.88,86.5,8.55,394.6,22.57,48.85,38.13,269.23,47.26,122.83,5.23,8.81,48.65,640.71,46,328.63,4725.43,73.9,65.65,6.98,3.5,8.54,4.15,6.98,4.69,40.38,150.81,8.15,4.38,37.13,328.97,132.71,3.15,229.2,10.97,122.54,84.18,6.46,26.59,500.93,148.02,6.39,222.02,12.75,50.57,9.28,3.86,369.14,156.11,46.38,14.99,69.77,7.21,100.59,5.3,39.61,102.73,23.42,4.26,127.53,3.91,12.5,117.11,8.47],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predAB"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB, \\\n","BestParametresAB, \\\n","ScoresAB, \\\n","TotalGHGEmissions_predAB, \\\n","figAB = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train.ravel(),\n","                         y_test=TotalGHGEmissions_test,\n","                         y_test_name='TotalGHGEmissions_test',\n","                         y_pred_name='TotalGHGEmissions_predAB',\n","                         score=score,\n","                         param_grid=param_gridAB)\n","\n","print(BestParametresAB)\n","print(ScoresAB)\n","figAB.show()\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[492.79914819170665,432.7827630292506,413.34067088586744,413.8700869852372,414.5622635367464,399.10875562319933,410.1781131318852,439.17316957321265,379.03547483718467,437.84533932283114,355.60982846700256,412.7351007766304,432.72169705608667,438.446049117964,436.28297974751314,364.6894615688008,377.3833074181115,413.3457643442548,383.06050998467833,396.4579757988771,390.0379026836881,416.0593668168908,429.5205769717392,456.5387285252217,524.7108556453234,582.9378888430931,660.7222969065248,742.3212575844127,751.603110259941,867.3116231162385]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[623.6449863802159,573.417473601781,534.8851152109925,529.3403578830192,561.8999415758427,544.6308700443653,549.2145948956015,565.8046307789366,503.63020964858663,576.184459924336,497.7237629819972,543.6290840631771,558.3951277633655,576.5850086475095,566.5388541750075,495.37758508121186,513.2463415224822,544.8957562702983,520.0947035162935,526.9613716626798,526.9300641313423,569.6183496813381,579.0446367545183,595.4062860016192,642.9468783399863,666.6878047073287,747.8824630967033,885.875892853027,871.1331873389172,1082.4927591099377]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[361.9533100031975,292.1480524567202,291.7962265607424,298.3998160874552,267.2245854976501,253.58664120203332,271.14163136816893,312.5417083674887,254.4407400257827,299.50621872132626,213.49589395200792,281.84111749008383,307.04826634880783,300.30708958841836,306.0271053200188,234.00133805638978,241.52027331374072,281.79577241821136,246.02631645306315,265.9545799350744,253.1457412360339,262.5003839524435,279.9965171889601,317.67117104882425,406.4748329506605,499.18797297885754,573.5621307163462,598.7666223157984,632.0730331809648,652.1304871225392]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[258.84733748128656,246.23681219414442,217.71386860363623,329.6818084659154,309.89908226832557,223.83698172055875,237.18182162016114,249.19640766287762,240.09207481981574,222.70862186609173,213.4431872467496,227.54970212366817,238.52515208891623,235.13207716674196,246.36379683452165,231.03725833244727,237.47175205757117,229.18224180754555,234.05940181759965,248.56809956292233,235.0871667886669,239.1512441233534,266.57222168154306,320.0974258488525,345.38907752204193,502.5213747952427,588.09936887873,571.2676101466468,593.487984937711,604.4244671402383],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[556.5675946216785,289.21951842076294,553.8047297286265,320.21941237868526,261.44183398499973,281.85005826027617,309.172526035619,521.6279577837323,282.44599557767043,516.49621200059,243.38270150335097,519.5081511848641,532.1581408797842,510.3987159973098,509.6125184829691,276.0586910117421,243.97722223924475,513.7227453283253,261.41401145481996,322.7651052923996,297.2338288090263,315.6595000049637,331.387078658446,336.56852941682735,471.6234214443645,500.0889348931925,530.1557636076698,628.722893427565,774.7632354315289,605.8285805950379],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[521.8103303097733,482.4960760616538,389.2496358930063,406.5784139616394,514.8856615412368,542.3880393263896,502.9570643634339,474.4677602198204,496.12983812634303,479.4105002918263,374.78542655588006,460.64325474749285,462.13834345735006,530.6218951364468,475.58969217776433,339.38739528544994,475.058327230158,375.10367116913164,459.11299436363095,438.20200655370667,438.7221661504645,470.9836542350932,492.22556100702366,569.2091568488967,532.2044154742431,569.1977475882995,701.3074852903301,749.3127389868313,642.0927069422007,1002.2979504587935],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[473.9512939127622,602.1223684321236,375.04354753583027,376.7840226145408,332.5440170863396,351.7779628128712,375.2964249957523,345.9934600492656,319.38608083897105,349.3164100350222,331.54309927509706,291.58409313894106,346.29487517726153,317.2659713485439,335.5416930779145,369.34082032487595,342.82034859085536,344.69260506705706,353.5214854329184,345.0619400529013,350.1273190753591,371.0762260474946,367.4696461004133,386.3062369124951,567.9786149529949,617.194989496618,761.968495811437,775.6073090325245,918.7034907707767,1062.4278270522216],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[652.8191846330325,543.8390400375681,530.8915726682382,636.0867775054054,654.04072280283,595.6907359959008,626.2827286444598,604.580262150367,557.1233848231229,621.294952420625,614.8947277539347,564.3903026881857,584.4919736771213,598.8115859407773,614.3071981643964,607.623142889489,587.588886972728,604.0275583492146,607.1946568544229,627.6927275324557,629.0190325949238,683.4262096735492,689.9483774112699,670.512293599037,706.3587488329721,725.6863974421128,722.080370944457,986.695736328496,828.9681332174872,1061.5792903349013],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=exponential<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB,\n","                             'n estimators', GridAB, BestParametresAB,\n","                             'adaboostregressor__loss')\n","FigRMSEGRidAB.show()\n","if write_data is True:\n","    FigRMSEGRidAB.write_image('./Figures/EmissionsGraphRMSEAB.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 Émissions au log"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["TotalGHGEmissions_train_log = np.log2(1 + TotalGHGEmissions_train)\n","TotalGHGEmissions_test_log = np.log2(1 + TotalGHGEmissions_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.1 Modèle LinearRegression"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.3911637992791006\n","rmse : 399.56886073366894\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logLR=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.688835144042969,4.7408905029296875,4.7845611572265625,4.825714111328125,5.213584899902344,4.880088806152344,4.763236999511719,4.777229309082031,5.198097229003906,4.798362731933594,5.041648864746094,4.666389465332031,4.699043273925781,4.89990234375,4.7703094482421875,5.2375946044921875,4.792259216308594,4.869781494140625,5.160850524902344,4.80029296875,4.625091552734375,4.877388000488281,4.861915588378906,4.8786773681640625,5.176002502441406,4.893318176269531,5.473930358886719,4.885734558105469,5.032905578613281,4.731620788574219,5.30108642578125,4.9531097412109375,4.808174133300781,5.027374267578125,4.850486755371094,5.0804595947265625,8.811355590820312,4.732612609863281,4.865791320800781,4.783958435058594,5.011543273925781,4.850914001464844,4.725318908691406,5.253196716308594,5.0183563232421875,4.7803955078125,4.771903991699219,5.031730651855469,4.726799011230469,4.807823181152344,4.72003173828125,4.7910919189453125,7.317161560058594,4.8146209716796875,4.802467346191406,4.844657897949219,6.205299377441406,4.746971130371094,5.12469482421875,4.842498779296875,5.261146545410156,5.584846496582031,4.706703186035156,5.622955322265625,5.303443908691406,4.777122497558594,5.083915710449219,4.7904815673828125,4.7213897705078125,4.759574890136719,4.677581787109375,5.2888031005859375,4.780982971191406,4.7323760986328125,4.8647003173828125,5.1629486083984375,6.532478332519531,4.872138977050781,4.99298095703125,4.7369842529296875,4.8376617431640625,5.5521392822265625,5.251243591308594,5.639411926269531,4.788726806640625,4.7044525146484375,4.681282043457031,4.890495300292969,5.3412933349609375,6.251129150390625,4.738746643066406,4.837913513183594,5.325050354003906,4.8589324951171875,4.840492248535156,5.074943542480469,9.562271118164062,5.372062683105469,4.4258270263671875,4.7914581298828125,5.3801727294921875,4.856513977050781,4.89178466796875,4.875450134277344,5.236839294433594,4.8104705810546875,4.943267822265625,5.9443817138671875,4.676368713378906,5.2433013916015625,5.068153381347656,4.828804016113281,4.680564880371094,4.718231201171875,4.4476318359375,4.734977722167969,4.8243408203125,7.188194274902344,5.215538024902344,5.2568206787109375,4.7697906494140625,4.750465393066406,4.64013671875,6.9599609375,5.113853454589844,4.8240814208984375,4.742439270019531,4.73345947265625,4.711143493652344,5.4513702392578125,5.701698303222656,4.745109558105469,4.6824188232421875,4.721733093261719,5.5922088623046875,4.6909637451171875,4.811042785644531,4.9849700927734375,5.256935119628906,4.769493103027344,4.7917633056640625,6.444450378417969,5.28955078125,5.002418518066406,4.700538635253906,5.0667266845703125,4.8047332763671875,4.8176727294921875,4.87799072265625,4.657890319824219,5.1486968994140625,5.142509460449219,4.6332244873046875,4.7293853759765625,4.846221923828125,4.890480041503906,4.783882141113281,4.737457275390625,6.0855865478515625,4.785003662109375,4.75421142578125,4.7210845947265625,4.659034729003906,4.8018035888671875,4.6923370361328125,4.7774658203125,6.324165344238281,5.170356750488281,5.0404815673828125,5.1203460693359375,4.7816314697265625,4.819114685058594,4.818717956542969,4.945648193359375,4.72857666015625,5.633209228515625,4.788673400878906,5.221305847167969,4.7974090576171875,4.699745178222656,4.9164581298828125,5.947669982910156,4.7554931640625,4.9881744384765625,4.803192138671875,5.174263000488281,4.698188781738281,4.77349853515625,4.73583984375,5.2919921875,4.8746795654296875,5.124046325683594,4.7530059814453125,5.301300048828125,4.91436767578125,4.921905517578125,4.811378479003906,4.797492980957031,4.9173583984375,4.9805145263671875,5.649589538574219,4.745018005371094,4.7866973876953125,4.7986602783203125,4.7578582763671875,4.7814483642578125,4.6850128173828125,4.716606140136719,4.9337615966796875,4.972999572753906,6.205009460449219,4.7384033203125,4.686737060546875,6.433815002441406,4.969261169433594,8.10272216796875,4.993316650390625,4.761192321777344,5.81689453125,4.777473449707031,4.6855010986328125,4.729576110839844,4.765251159667969,4.7067718505859375,4.813743591308594,4.6774139404296875,4.808845520019531,4.78387451171875,4.757904052734375,4.831748962402344,4.688255310058594,4.9545745849609375,4.6659393310546875,4.934089660644531,4.691078186035156,4.7804107666015625,4.661376953125,4.748893737792969,5.776878356933594,4.7153778076171875,5.261749267578125,4.679557800292969,4.724678039550781,4.769844055175781,4.837615966796875,4.775970458984375,5.5319061279296875,4.8138885498046875,5.9493865966796875,4.796104431152344,4.9467010498046875,4.7337799072265625,5.057037353515625,4.846488952636719,5.312385559082031,4.945182800292969,5.0922393798828125,4.905067443847656,5.0991973876953125,4.809532165527344,5.014045715332031,4.675529479980469,4.9493408203125,4.857032775878906,4.694099426269531,5.148750305175781,4.809844970703125,5.1808319091796875,4.765419006347656,4.7511138916015625,4.9749298095703125,4.82989501953125,5.0453033447265625,4.85955810546875,4.648674011230469,5.11810302734375,5.2857666015625,7.2554473876953125,3.7437820434570312,4.895637512207031,5.022132873535156,4.779083251953125,4.6818695068359375,5.264839172363281,4.864044189453125,4.7869110107421875,4.8644561767578125,4.758964538574219,4.804473876953125,4.7415924072265625,6.3598785400390625,4.858161926269531,4.473426818847656,4.647758483886719,4.974662780761719,5.129974365234375,4.7236785888671875,4.923919677734375,4.8598480224609375,4.78662109375,4.838386535644531,5.724479675292969,5.033576965332031,4.7926788330078125,6.9852294921875,5.115470886230469,5.817863464355469,4.753395080566406,4.920684814453125,4.860191345214844,4.810768127441406,4.7291412353515625,4.7710723876953125,5.730560302734375,4.7057952880859375,4.813690185546875,5.038856506347656,4.929588317871094,4.728385925292969,4.973152160644531,5.204246520996094,5.9210662841796875,4.770042419433594,4.7979278564453125,4.7656402587890625,4.749046325683594,4.7716064453125,4.897865295410156,7.839958190917969,4.8565673828125,4.8275604248046875,4.672584533691406,4.79095458984375,5.561042785644531,5.144676208496094,4.958946228027344,5.663398742675781,5.758384704589844,5.027320861816406,4.7310791015625,4.674835205078125,4.714530944824219,4.745491027832031,4.847663879394531,5.414039611816406,5.376556396484375,4.9332275390625,4.785118103027344,4.6230926513671875,4.747138977050781,4.715934753417969,4.798316955566406,4.710838317871094,4.862648010253906,5.146453857421875,7.508125305175781,5.644378662109375,7.200340270996094,4.729194641113281,5.365440368652344,4.835227966308594,4.817237854003906,4.792167663574219,5.364067077636719,4.7350311279296875,4.7645111083984375,5.480033874511719,5.0524139404296875,5.421440124511719,5.1008148193359375,4.762664794921875,4.7255401611328125,5.169181823730469,5.649566650390625,5.878562927246094,4.858833312988281,4.844291687011719,5.008056640625,5.305244445800781,5.041908264160156,4.755241394042969,4.670860290527344,5.8113861083984375,4.901123046875,4.7292938232421875,5.417457580566406,4.919975280761719,4.70416259765625,5.4501495361328125,4.719146728515625,5.178291320800781,4.829856872558594,5.015655517578125,4.819358825683594,4.790550231933594,4.79278564453125,4.785530090332031,4.7437286376953125,4.7801666259765625,4.788917541503906,4.773590087890625,5.212486267089844,4.71673583984375,4.853660583496094,4.786231994628906,4.735649108886719,4.916007995605469,4.846527099609375,4.810737609863281,4.8568115234375,4.7589263916015625,4.934898376464844,4.721244812011719,5.204246520996094,5.092529296875,4.8221435546875,4.792900085449219,4.7720489501953125,5.396965026855469,5.227996826171875,4.979316711425781,4.811431884765625,4.7000732421875,4.764213562011719,4.83331298828125,4.824432373046875,7.630279541015625,4.7554168701171875,3.8547515869140625,4.684684753417969,4.795616149902344,5.223457336425781,4.90203857421875,15.005584716796875,6.0443115234375,4.852508544921875,6.115104675292969,4.710693359375,4.740333557128906,4.667686462402344,4.838531494140625,4.730827331542969,4.690483093261719,4.7100677490234375,6.580589294433594,4.8922271728515625,4.960273742675781,4.8025054931640625,4.7250213623046875,4.785911560058594,4.6973114013671875,4.823600769042969,4.760612487792969,5.266021728515625,4.82647705078125,5.1512451171875,4.774421691894531,5.399650573730469,4.656349182128906,4.6966400146484375,4.776985168457031,4.824989318847656,5.278038024902344,4.690940856933594,6.045013427734375,4.713958740234375,4.798248291015625,4.675865173339844,4.705879211425781,5.0771331787109375,4.741889953613281,5.1758575439453125,6.892189025878906,6.3245697021484375,4.740234375,4.746490478515625,5.320587158203125,4.854667663574219,4.69696044921875,4.937492370605469,5.060447692871094,4.9183807373046875,4.8158111572265625,4.8878631591796875,5.0672149658203125,4.649085998535156,5.074058532714844,4.7422332763671875,5.740592956542969,4.962684631347656,4.770637512207031,4.808319091796875,4.655181884765625,4.738273620605469,4.8022613525390625,6.456642150878906,4.8970184326171875,4.760833740234375,6.524940490722656,5.3049468994140625,5.0075225830078125,5.286720275878906,5.028106689453125,4.692756652832031,5.632476806640625,5.339576721191406,4.873016357421875,6.557891845703125,4.924949645996094,4.690788269042969,4.7849578857421875,5.65814208984375,4.892738342285156,4.863914489746094,5.364906311035156,4.812713623046875,4.6954345703125,6.70526123046875,4.670036315917969,4.883766174316406,5.381294250488281,6.08502197265625,4.700859069824219,4.7162017822265625,4.888763427734375,4.7523345947265625,4.8864593505859375,4.826148986816406,4.7376251220703125,4.890159606933594,5.137458801269531,4.8736724853515625,4.8791351318359375,4.724906921386719,4.975807189941406,4.7883453369140625,4.71478271484375,4.7292938232421875,5.002021789550781,4.842658996582031,4.720069885253906,5.016914367675781,4.780448913574219,5.041618347167969,4.802772521972656,4.611228942871094,5.021263122558594,5.061576843261719,4.7489166259765625,6.698036193847656,5.379447937011719,4.705467224121094,5.2199249267578125,4.779365539550781,4.83172607421875,4.7621002197265625,4.85614013671875,4.8638916015625,4.7694091796875,5.1666259765625,4.7366943359375,4.908935546875,4.3100738525390625,4.9598388671875,5.567115783691406,5.0611419677734375,5.060737609863281,7.330482482910156,4.8726348876953125,4.781120300292969,4.702781677246094,6.3477325439453125,4.771797180175781,4.729850769042969,6.2573089599609375,4.673622131347656,4.884223937988281,4.773307800292969,4.798789978027344,4.848411560058594,4.893424987792969,4.8304443359375,4.8294525146484375,4.7711029052734375,4.753593444824219,5.1982269287109375,4.9323883056640625,4.842132568359375,5.2779693603515625,4.7864837646484375,5.003440856933594,5.294136047363281,5.018707275390625,6.660011291503906,5.091888427734375,5.025337219238281,5.798469543457031,4.857429504394531,5.183135986328125,4.965995788574219,5.42132568359375,4.77117919921875,5.4189910888671875,6.9437408447265625,4.73870849609375,6.717308044433594,4.719429016113281,5.5285491943359375,4.710060119628906,4.691749572753906,4.6955413818359375,4.694732666015625,4.840629577636719,5.012054443359375,7.312171936035156,5.47052001953125,4.718475341796875,4.887565612792969,4.6693572998046875,4.951835632324219,5.1695404052734375,4.766365051269531,4.818916320800781,4.847038269042969,4.757148742675781,4.790130615234375,4.955413818359375,4.84735107421875,4.720664978027344,4.698699951171875,5.4638824462890625,4.6825714111328125,4.75726318359375,5.259429931640625,4.7387237548828125,4.9535064697265625,6.503044128417969,4.881935119628906,4.855064392089844,4.8124237060546875,6.4300079345703125,11.045631408691406,4.8701934814453125,4.760650634765625,4.820747375488281,5.08367919921875,4.7483673095703125,4.813270568847656,4.7579498291015625,4.717781066894531,5.16644287109375,5.775764465332031,4.842918395996094,4.804420471191406,4.682228088378906,4.938652038574219,4.822746276855469,4.782783508300781,5.3963775634765625,4.667243957519531,4.8203277587890625,4.905670166015625,4.8184661865234375,4.991180419921875,4.8115386962890625,4.885185241699219,4.70623779296875,4.864784240722656,4.714714050292969,4.7298736572265625,4.817771911621094,4.6956329345703125,4.8646392822265625,4.7272796630859375,4.800201416015625,5.533012390136719,11.876495361328125,4.857391357421875,5.1435699462890625,4.748252868652344,4.734580993652344,4.868904113769531,4.7441253662109375,4.780921936035156,5.099601745605469,4.717613220214844,6.9013671875,5.237876892089844,4.8685302734375,4.947868347167969,4.725120544433594,5.257591247558594,4.779914855957031,4.719184875488281,5.430900573730469,5.2283782958984375,6.3471527099609375,5.352500915527344,4.792144775390625,4.716094970703125,4.733238220214844,5.096061706542969,4.765167236328125,4.691436767578125,4.76312255859375,4.692131042480469,4.9401702880859375,4.7894287109375,4.82977294921875,5.259674072265625,4.739532470703125,12.303634643554688,4.7241973876953125,10.1424560546875,5.03118896484375,4.774810791015625,4.7411651611328125,4.842979431152344,4.6408843994140625,6.065238952636719,6.2311553955078125,5.590293884277344,4.770599365234375,4.749885559082031,5.290130615234375,4.988365173339844,4.758522033691406,4.742034912109375,4.766319274902344,6.7322998046875,4.772651672363281,4.7994537353515625,6.388336181640625,5.02215576171875,4.966621398925781,5.135215759277344,6.295684814453125,4.752876281738281,4.7741241455078125,5.098381042480469,4.695976257324219,4.767822265625,4.898170471191406,5.3998870849609375,5.200920104980469,5.386207580566406,5.4874725341796875,4.926948547363281,4.780647277832031,5.1034393310546875,5.651344299316406,6.091766357421875,4.7305755615234375,7.370765686035156,4.8000946044921875,4.892311096191406,4.890892028808594,4.71490478515625,5.31646728515625,4.907539367675781,4.773681640625,5.318840026855469,4.812324523925781,4.758918762207031,4.766578674316406,4.880943298339844,4.949150085449219,5.2064056396484375,5.282768249511719,4.673316955566406,4.934356689453125,4.71905517578125,4.7368011474609375,4.764686584472656,4.667724609375,4.781593322753906,4.793540954589844,4.755950927734375,5.6702728271484375,6.692573547363281,5.0612030029296875,4.93218994140625,4.77447509765625,4.701255798339844,4.9336395263671875,4.8656768798828125,4.90447998046875,4.82232666015625,4.7554168701171875,4.8064422607421875,5.1837005615234375,4.697135925292969,4.7295684814453125,4.826988220214844,6.2242279052734375,5.0628662109375,4.707450866699219,3.8777618408203125,9.136192321777344,4.929832458496094,4.9033660888671875,4.704887390136719,4.930778503417969,4.831428527832031,4.737648010253906,5.180931091308594,4.715095520019531,4.763771057128906,4.8727874755859375,4.910057067871094,4.984832763671875,5.681999206542969,4.751640319824219,4.857383728027344,4.775634765625,5.1571807861328125,4.722236633300781,5.440132141113281,4.829353332519531,4.747100830078125,4.943817138671875,6.800285339355469,4.748786926269531,5.2354736328125,4.837257385253906,4.807441711425781,4.78631591796875,5.697044372558594,5.160377502441406,4.7543487548828125,5.429420471191406,5.729866027832031,5.212211608886719,4.749794006347656,4.819587707519531,4.764198303222656,4.797149658203125,4.85064697265625,4.9163055419921875,11.2711181640625,4.6859893798828125,4.739387512207031,5.16925048828125,5.337791442871094,5.93414306640625,4.769195556640625,4.648773193359375,4.7126922607421875,2.8608627319335938,4.918830871582031,4.784759521484375,4.644157409667969,5.370452880859375,4.7099151611328125,4.833351135253906,4.67791748046875,4.720771789550781,5.08087158203125,7.0023345947265625,4.790489196777344,4.831733703613281,4.8316192626953125,5.425651550292969,5.15179443359375,4.719146728515625,4.7471771240234375,6.877067565917969,4.9053497314453125,4.855628967285156,4.87567138671875,4.652496337890625,5.009590148925781,5.2333984375,6.1206512451171875,4.7543182373046875,4.764396667480469,4.7883758544921875,4.769157409667969,4.656913757324219,4.856239318847656,5.480857849121094,5.110343933105469,4.816780090332031,4.844169616699219,4.669898986816406,7.2110443115234375,4.844810485839844,5.0378875732421875,4.740455627441406,4.693122863769531,4.804313659667969,4.896369934082031,4.74615478515625,4.8657684326171875,5.38848876953125,4.41888427734375,4.749755859375,5.317718505859375,4.878547668457031,4.7238006591796875,5.218536376953125,4.688621520996094,4.696403503417969,4.6839752197265625,6.709983825683594,4.7855377197265625,5.1908111572265625,4.864372253417969,4.69696044921875,5.196937561035156,4.928337097167969,4.685295104980469,4.817741394042969,9.249580383300781,4.851593017578125,4.8921051025390625,4.69061279296875,4.739997863769531,4.893424987792969,6.316215515136719,6.4259185791015625,4.907157897949219,4.916046142578125,8.133338928222656,4.7097320556640625,6.600440979003906,4.927391052246094,5.1493072509765625,4.81591796875,6.520912170410156,4.756965637207031,4.702598571777344,4.647651672363281,4.797935485839844,4.796302795410156,4.735816955566406,4.7517242431640625,6.9228057861328125,4.768096923828125,5.444671630859375,4.755668640136719,5.200592041015625,4.71484375,4.7050933837890625,5.3506011962890625,6.159332275390625,5.172859191894531,5.0547637939453125,4.664527893066406,5.265167236328125,4.8266448974609375,4.781005859375,4.734764099121094,4.8693389892578125,4.94232177734375,6.2770843505859375,13.264984130859375,4.796073913574219,4.683540344238281,4.839897155761719,6.740715026855469,4.97943115234375,4.6968841552734375,4.874053955078125,5.911918640136719,5.242271423339844,4.736915588378906,4.761604309082031,6.1139373779296875,4.826240539550781,5.0954742431640625,4.709373474121094,8.433204650878906,4.687828063964844,5.041023254394531,4.7023773193359375,4.9643096923828125,4.758583068847656,4.936637878417969,4.788627624511719,5.235252380371094,4.8122711181640625,4.9170074462890625,4.7513885498046875,4.847663879394531,4.787132263183594,4.814910888671875,4.9764404296875,4.788475036621094,4.742805480957031,5.771553039550781,4.824798583984375,4.8833770751953125,6.119361877441406,4.8724517822265625,6.166961669921875,4.781318664550781,4.686248779296875,4.695228576660156,4.9496307373046875,5.35028076171875,5.092559814453125,5.042243957519531,4.772003173828125,6.644844055175781,4.740631103515625,4.7681427001953125,4.822395324707031,4.80413818359375,5.233100891113281,4.7635955810546875,4.308830261230469,4.78375244140625,4.777748107910156,4.704475402832031,4.79498291015625,4.703849792480469,4.7136993408203125,4.9853973388671875,4.741432189941406,5.7305145263671875,4.8365020751953125,5.229240417480469,4.6671905517578125,4.763633728027344,5.06890869140625,4.801582336425781,4.8142852783203125,4.74102783203125,5.3791656494140625,4.871315002441406,4.862884521484375,5.593559265136719,5.019500732421875,7.003715515136719,4.98553466796875,6.297309875488281,4.886466979980469,4.9591217041015625,4.906501770019531,5.105133056640625,5.0793914794921875,4.756050109863281,4.76092529296875,4.664710998535156,4.6930999755859375,5.058097839355469,6.212028503417969,4.9508209228515625,4.8218994140625,4.9614410400390625,4.8781890869140625,4.7021026611328125,4.84869384765625,5.3235931396484375,4.7063751220703125,4.819297790527344,4.9414520263671875,5.127738952636719,4.832450866699219,4.94903564453125,5.11956787109375,4.8492279052734375,4.884376525878906,4.924339294433594,4.815940856933594,4.7573699951171875,4.829322814941406,4.736747741699219,5.205291748046875,4.801849365234375,5.0652313232421875,4.6771697998046875,5.4470062255859375,4.7332916259765625,4.854866027832031,4.8572540283203125,4.704620361328125,4.89093017578125,5.6988983154296875,4.640998840332031,5.110595703125,9.190986633300781,5.0054931640625,5.192405700683594,4.793601989746094,5.6619415283203125,5.0994873046875,5.1790008544921875,4.6643829345703125,4.778541564941406,4.79931640625,4.7232818603515625,4.7812347412109375,5.0072784423828125,5.9165496826171875,4.797874450683594,5.201629638671875,4.702827453613281,7.301856994628906,3.7019805908203125,4.7829742431640625,4.698326110839844,4.710517883300781,4.903045654296875,6.107872009277344,6.026496887207031,4.732933044433594,5.40911865234375,4.915885925292969,6.211647033691406,4.779510498046875,6.252532958984375,4.7070770263671875,4.70330810546875,5.531578063964844,4.713897705078125,4.697196960449219,4.731452941894531,4.677787780761719,4.681282043457031,4.852699279785156,5.033172607421875,4.83447265625,7.074302673339844,5.3099517822265625,4.959068298339844,4.841682434082031,4.865470886230469,5.678932189941406,4.8138275146484375,5.100334167480469,4.869377136230469,4.5499725341796875,4.949653625488281,4.887397766113281,5.332061767578125,4.740516662597656,4.895576477050781,4.732002258300781,4.7728118896484375,4.844268798828125,4.677970886230469,5.997474670410156,4.819793701171875,4.974601745605469,5.519508361816406,4.7984619140625,5.836952209472656,4.9654998779296875,4.642723083496094,5.693412780761719,4.8350677490234375,4.7610321044921875,5.0984039306640625,4.873237609863281,4.860832214355469,4.952491760253906,4.8172607421875,4.872169494628906,12.362228393554688,4.673271179199219,4.817283630371094,4.63818359375,4.6827850341796875,5.3854827880859375,10.82879638671875,4.7142486572265625,4.8710174560546875,5.027214050292969,4.766777038574219,4.945213317871094,4.641975402832031,5.292488098144531,4.718109130859375,5.2852630615234375,5.1058807373046875,4.942939758300781,4.7743988037109375,5.2749786376953125,4.78338623046875,4.7334136962890625,4.868202209472656,4.675163269042969,4.9287261962890625,4.7883453369140625,4.789665222167969,5.178375244140625,4.8752593994140625,4.917091369628906,4.80780029296875,4.776008605957031,4.729484558105469,4.769073486328125,4.947410583496094,5.094886779785156,6.437591552734375,4.873100280761719,4.777305603027344,4.740180969238281,4.832183837890625,4.821830749511719,4.8379364013671875,4.730690002441406,4.883186340332031,4.823982238769531,5.2987518310546875,4.8489990234375,4.884529113769531,4.753425598144531,4.6955413818359375,5.452476501464844,4.659141540527344,6.935050964355469,4.739654541015625,5.003456115722656,5.4631195068359375,4.773712158203125,4.708160400390625,4.832908630371094,5.072471618652344,4.714569091796875,4.956336975097656,5.179252624511719,4.820793151855469,4.793098449707031,4.95098876953125,4.718116760253906,4.7560882568359375,4.6600341796875,5.050941467285156,4.72015380859375,4.8582611083984375,4.949470520019531,4.777191162109375,4.703819274902344,5.495597839355469,4.7612762451171875,5.0067596435546875,4.719810485839844,5.285835266113281,5.3793487548828125,4.9925384521484375,4.850379943847656,4.649482727050781,4.710693359375,5.161643981933594,5.074012756347656,6.0529632568359375,5.4380035400390625,4.97039794921875,4.859046936035156,4.933387756347656,4.953727722167969,5.419105529785156,4.701103210449219,4.743705749511719,4.887351989746094,5.501625061035156,4.685310363769531,5.721519470214844,8.165565490722656,4.8338470458984375,4.708259582519531,4.7706756591796875,4.754737854003906,4.901908874511719,4.7404937744140625,4.844978332519531,4.748527526855469,4.812629699707031,5.825187683105469,4.972877502441406,4.719917297363281,4.668495178222656,5.6946563720703125,5.846366882324219,4.694892883300781,5.2833251953125,4.8004150390625,6.4622802734375,5.114501953125,4.7071533203125,4.666290283203125,13.731468200683594,4.5846710205078125,4.7271270751953125,5.1622772216796875,4.6517791748046875,4.9089813232421875,4.8435516357421875,4.687126159667969,4.981956481933594,5.0363616943359375,5.0347900390625,4.7299041748046875,4.775138854980469,4.821327209472656,5.627677917480469,4.708076477050781,4.97064208984375,4.7794952392578125,5.464447021484375,4.710441589355469,5.316368103027344,4.701423645019531,4.6602783203125,4.892127990722656,4.766700744628906],"xaxis":"x","y":[4.9676297534661975,5.60584986719498,2.950468414150123,2.7355221772965375,7.258707352196828,3.0925457415435655,4.178714641175443,3.4154882710497003,6.224387627929912,2.634593268445757,7.244982170376553,5.597531174224212,4.210232990095849,3.594548549550354,2.526068811667588,6.215484464192794,4.969933274697856,5.912649864897204,5.111865963867557,5.042644337408494,7.864928972289788,3.382667252745041,3.189033824390017,5.705977901682522,3.9717734471933728,5.119771161952904,3.496973580998276,5.058749412335524,5.968551603363945,3.1160319934471103,4.464668267003444,5.192194165283345,6.73226919950145,5.39746072605472,8.171076693915895,6.43095427138573,7.132782457298859,4.294988267691446,4.708739041359579,3.4646682670034443,5.450881315273369,4.95093492831454,4.20006486151431,8.047505671251574,3.605257262939004,3.261530815243406,9.159820892013935,3.8728287595348854,3.8298495598446904,1.9671686075326276,2.9126498648972037,5.648177795724818,10.257022407964342,4.887037791419197,5.718087583960517,7.540941686758214,7.156841524588401,2.0942360698457656,6.074462620704536,6.428778891337586,6.464504909131913,6.588864518190148,5.629648044187436,4.795974694206668,5.676380255466924,2.811471030529836,3.9882302230604574,8.45314714555533,2.321928094887362,3.980939266085512,2.5801454844233804,4.41886457739032,2.8972404255747994,4.341985747228616,6.561784353081841,5.751410160064371,8.545157551729474,2.7676547982373463,3.773996325111173,2.7676547982373463,3.258518924711301,5.596637034102074,7.019368330478469,6.649471757181635,5.275007047499869,2.0071955014042038,3.700439718141092,5.320845667645722,5.331633567171029,8.061668207389335,2.786596361890807,4.94673086014031,5.649040565634448,2.269033146455237,3.292781749227846,3.683696454306516,13.390111171989,8.064904402198286,3.727920454563199,2.8155754288625725,4.253232938538032,3.0373822220030804,6.01992426090841,3.2524762141352173,7.125981653854716,4.273515889702116,4.92552476974757,7.778274295338579,6.135452784185173,9.20006486151431,3.6971065744769747,3.2418401835646704,2.4222330006830477,2.6229303509201767,5.57198012018773,5.0539801681876515,2.6915341649192004,7.510803720358323,3.01435529297707,6.045049740837989,2.7803100990433753,4.658211482751795,4.595145567990858,7.485507332459785,5.342341397431548,4.129283016944966,2.454175893185802,5.96254902292306,2.3305584000308026,3.292781749227846,6.290756138396673,2.817623257511431,5.532940288372874,3.2883585621936606,8.008820757716158,1.6644828403646825,5.284292026394312,3.598126959919604,6.237448995639321,4.222650022451478,4.380590934265922,8.669593751188334,7.720552091605108,5.548744859937229,1.7484612330040354,4.524815928357506,7.639376888638463,2.851998837112446,3.218781167784069,2.7990873060740036,3.868884273028774,8.043027283594547,7.260778431893426,4.038260575175349,3.040892430646901,2.7949356628035362,5.4741115139424075,5.952799477899938,7.323370069061268,5.040892430646901,3.4208865749755315,6.136683577697236,4.255500733148386,3.228049047884462,4.6194130105979365,5.623515741490549,7.217424613813984,6.179311989210016,4.3139713710590195,5.764473550992666,4.221103725367875,2.4594316186372973,6.548282482376233,5.9902741833186015,2.5360529002402097,4.720825666089835,6.512700963425246,6.664198369291911,2.881664619320345,1.9634741239748859,6.013685569928653,6.026578770127714,4.532940288372874,7.597158683183727,2.711494906650088,6.791683858152045,2.1009776477248208,6.090641751917847,5.040892430646901,6.327328082629952,3.2942531364445142,2.9467308601403097,3.7366048753142485,4.922673592849446,3.71699089440494,4.15623479785027,6.872951851130105,2.8933622107638715,7.561631630375841,5.341985747228616,4.993221467368938,6.309067020588001,2.414135532984451,2.493134922305505,4.161081482277184,2.144046369616707,3.486714373030702,2.8011586560936985,5.648465443027314,4.399171093819823,7.581803148516525,3.1160319934471103,2.533563348214512,9.967312731471658,5.687060688339892,6.954079959320971,7.084170591577843,2.722466024471091,4.826802684285827,3.01435529297707,1.6040713236688608,5.5109619192773796,2.1731274334806563,5.425593577653059,2.8797057662822882,2.7803100990433753,3.303050084681673,2.3132458517875616,7.006298023900369,5.279842693520348,2.3950627995175777,7.216163813388774,2.786596361890807,3.5298209465286954,2.7420062108667365,5.079378111118652,5.1622906135457995,5.839203788096944,9.354249381945241,6.058532970201611,6.614857050384495,2.1667154449664223,2.4982508675278257,3.3405622690264134,5.415826257472914,3.9791107550157854,5.338780943894699,1.9335726382610239,7.164705840182799,6.12866458654003,2.933572638261024,3.0548484769956197,6.390598905552461,3.2764966656403565,8.007363716298686,6.558114538598396,3.674686619927999,5.28835856219366,4.189033824390017,5.480265122054463,4.2787282129389395,3.776103988073164,4.82273014794452,2.9467308601403097,2.2898344651775093,6.1705259991768475,1.4750848829487828,7.264067216098838,5.6845382485777325,4.534808661231747,3.7070829917717063,4.919340082442012,4.458119481174506,6.288173968112302,2.27500704749987,6.5249725982866265,3.940166750482817,9.715601700373337,7.829278760612363,4.821199978056451,5.875042803002589,2.6229303509201767,2.942983598187102,4.445594291341182,7.474030370207135,5.92148393743373,3.4369613378336026,3.2078928516413328,4.095080491901415,4.422233000683048,7.424922088210688,5.185470146066942,4.955126781261366,4.3729520979118295,4.0522419811811385,6.067165427017112,2.2898344651775093,3.0806576633452254,3.940166750482817,3.127633279725874,7.152994605492435,5.573192723781613,4.4121040446775694,2.2479275134435857,7.886854944198882,7.9329824099498465,7.25691629667084,6.670585440262214,5.845740932614582,2.9616233282869446,2.5459683691052923,7.959132576789657,4.653633311386255,7.406162478777607,5.588264894307497,3.8063240573900288,6.716579420796345,4.984589350362456,2.269033146455237,6.929672661579345,7.060371694122218,6.746447139362815,5.521364878446219,2.9873208659292536,5.970623614470605,2.37851162325373,4.667892125230234,3.192194165283345,6.816215687622865,2.851998837112446,3.3854310371935203,4.634593268445757,2.5558161550616396,8.990671283975267,9.114445113289024,3.389566811762726,6.9532652390148435,6.705563274566321,5.672425341971495,2.280956313831056,2.137503523749935,2.2203299548795554,6.561173365231147,2.9726926540042644,5.078951341394822,5.0759604132434015,3.8509993947164736,8.693835392482516,2.845991770664573,2.8399595874895316,6.111448698487674,0.7484612330040358,4.125155131322289,3.8953026213333066,8.45889870671479,9.650854294025212,8.19120730223037,6.534964249804655,2.5160151470036647,3.0373822220030804,4.701549056943071,2.9541963103868754,4.903520373723376,7.848935855524565,2.0036022366801953,5.660780334032546,8.418822422291017,5.254745197629213,8.4696011200494,6.567880042052731,6.557808298936688,3.6769443591069124,5.651051691178929,7.279842693520348,6.425089989874059,2.811471030529836,5.4799419014771305,5.184280294419382,5.713420884868084,5.452858964713811,2.4195388915137843,4.178714641175443,7.686990680088471,5.3847405872923835,2.722466024471091,7.554281862626391,3.955126781261366,5.192194165283345,9.071542807992602,5.765004246250036,5.318316841334983,3.1953475983222193,5.2671612180222604,2.269033146455237,2.702657543390911,5.6937657122177825,8.574290766120185,3.554588851677637,2.4059923596758366,3.0214797274104517,1.3785116232537298,6.010108453474289,2.438292851579147,3.089159131911238,3.0250287944915226,4.571676809970931,3.24031432933371,3.261530815243406,2.6712933724815757,3.6542063779442917,2.9873208659292536,5.8060662260890314,4.371558862611963,6.4984104488609775,6.229587922740652,2.2957230245399685,5.178714641175443,5.463687842201262,4.884597920990064,6.157852169141737,3.4672794804599825,2.867896463992655,2.611172380044005,5.535430914871279,0,3.4776773275653072,9.582254908357664,3.950468414150123,4.8894735425311096,4.850999394716474,3.300123724569014,8.009660689646706,7.626950122372515,9.04105676230812,6.564835417010355,3.3175935046234715,7.380331184220349,2.0214797274104517,4.235727059838059,4.334139178838237,5.692092375427572,5.592457037268081,1.996388746447621,2.070389327891398,5.8052924556007115,6.60584986719498,4.088311235888661,2.521050736900963,3.5310694927259543,7.058316495590823,4.044394119358453,3.0513721017210256,6.008092420948722,5.605257262939004,8.220281580441913,6.088735246190284,2.137503523749935,6.637204481902295,0.8155754288625726,2.2387868595871168,5.676662334857967,1.7865963618908067,8.05642095591347,7.5673476962842585,7.295355692891673,6.394033895367782,6.582706526780788,4.43095427138573,4.915520900751958,3.710393192170516,3.8845979209900645,5.135042286233651,10.267009017093484,5.336640446408735,3.0992952043377753,6.836050355058069,7.896998342338911,6.558114538598396,5.10936055940423,5.85872702377863,3.785550551739256,3.8042601156347384,2.7484612330040354,2.883620816285671,3.576522137920503,4.104336659814735,5.86195536414487,4.294253136444514,8.44136727845972,5.497931651896279,7.5808238499627985,2.8278190246173196,6.260590274730996,2.229587922740652,6.218781167784069,10.212070748654236,4.400537929583729,6.543805175964397,7.563234413022304,6.203788453308596,6.676803353873854,4.070389327891398,6.340918270272116,2.063502942306158,7.151270288790165,4.228049047884462,4.563768278452033,5.4329594072761065,3.9373443921502322,5.580748491763774,4.279471295644468,7.959132576789657,5.739578112048051,2.7463127664254587,5.707635219545296,2.0250287944915226,3.4776773275653072,7.925940095994448,4.478971805032942,3.24031432933371,6.836807807844575,6.2502036564992975,3.82273014794452,0.918386234446348,3.258518924711301,2.1634987322828794,2.662205499653617,6.687060688339892,2.2357270598380583,5.0259146987507926,5.443606651475615,2.980025300238734,3.1193561770396756,2.8399595874895316,4.914564523493939,2.782408564927373,6.282625134191674,1.2016338611696504,5.269781238274379,3.0513721017210256,4.649615459063409,4.896271848807375,4.721919445551544,3.6182386555954547,5.045268215138534,5.020591094773247,5.305240965954483,5.663913842115978,3.3881895371560837,8.836744701966083,6.976363635732762,3.925049964727359,6.802451741275622,4.664482840364682,2.3589588258323295,6.127014141582809,7.1945598860226125,4.475084882948782,4.301587646603187,3.7441610955704103,4.05484847699562,5.464668267003444,7.33360262826828,5.6082178530214595,7.1320626778543845,4.993674361750585,5.522620761042079,7.366234775533047,7.166815851761945,3.7687136570304847,2.321928094887362,6.7873802222963375,4.788685710613533,6.025250321561575,8.162995472417697,3.4019034716079584,6.238786859587116,2.3248106034204836,7.831687267083084,3.4802651220544627,3.871843648509318,6.548128323585586,3.0268000593437154,3.1953475983222193,6.2600256559614555,5.931210274803932,7.80967167790706,3.169925001442312,3.894332742277694,2.298658315564515,6.215678596607928,7.873013392989595,7.709152756902506,8.461602224618334,4.150559676575382,7.653203362033977,9.24186401223682,5.725468512632615,4.0925457415435655,3.575312330687437,6.519321734629434,2.5185351389821804,6.137913322088407,7.904544309292717,2.568032104771279,9.816679697610459,0.765534746362977,8.646594709581654,5.037821465434967,4.388878338811989,7.9616812016062815,1.7782085763980877,8.912589990852627,3.5008020530571575,7.473786911614368,7.182890892745479,2.6205864104518777,6.628919373511362,4.842476062993939,6.013462259806563,7.1851727751412495,4.8359240742543745,4.918386234446348,4.783980413683795,4.285402218862249,6.340918270272116,3.5570424152637194,4.030336078370959,0.8073549220576041,2.2957230245399685,8.893574570329104,2.3923174227787602,7.015805293913353,8.764407200357251,6.147306698780294,5.502075956045791,8.694706111949401,2.440952198029637,8.447992374957895,4.943921326553485,6.072963271555224,8.18685706717715,5.508111681193465,2.327687364176047,2.6322682154995127,5.853496703702325,2.4724877714627436,3.412781525338476,5.751677945687579,5.8774988727835344,5.890446692679906,8.622783966156124,6.203396948546115,5.524189078449365,4.679198570566922,2.3757345385831563,7.809543066804601,3.5716768099709313,6.39694722007283,3.097610796626422,4.9415758618182375,6.668317719915721,3.0925457415435655,6.941106310946432,7.471431358002323,4.633431210355632,2.454175893185802,5.760487101811031,4.112700132749362,2.560714954474479,2.885574364371426,5.8060662260890314,5.969012307516316,3.0214797274104517,4.37016428054021,3.1811025507537978,9.463033855317493,5.102238193930738,7.537684337147519,5.960001932068081,4.451540833017832,5.931683057059805,4.126807703142035,9.109647857806625,8.028624401408491,3.2645364309990255,7.394977085526471,6.152589069287125,6.022367813028454,4.51853513898218,3.344828496997441,6.810314561497258,3.0036022366801953,7.890811454817677,8.317774372804099,7.237735785231934,8.515305605451724,8.213638902198852,5.154615611326894,2.039138393906958,7.296273846706719,6.905206468795705,2.3673710656485296,2.0942360698457656,2.9145645234939397,2.2172307162206693,5.641256998726767,3.303050084681673,2.62993940943954,7.220233204382192,6.489767697008256,10.809687753488749,5.260778431893426,8.11134436328308,4.9941271140031285,2.7070829917717063,5.856985689782205,6.381629467033664,4.493134922305505,10.642557179595622,5.439955517478156,6.790772037862,2.5058909297299574,4.37920505996061,6.33646192818282,2.9523335663696857,2.811471030529836,5.323730337521348,2.763411574470007,8.089159131911238,6.435461914479276,2.4646682670034443,7.870364719583405,3.396433531250992,7.305696981080096,4.020591094773247,5.187451054027326,2.298658315564515,6.038918989292302,5.369815424283912,4.422905742612183,4.024142345897803,4.926948247949772,5.348728154231077,6.907251224292033,4.861459166361513,6.596487956856693,4.593353770980297,5.9502351004856875,7.814614507639093,6.638797899951696,8.79720757285578,2.176322772640463,7.7974669920591575,2.7398481026993275,3.1538053360790355,3.6701605141266334,2.513490745588118,5.307064162255372,5.076815597050831,2.533563348214512,6.678494507773734,2.7092906357233577,2.7355221772965375,2.782408564927373,7.039796407620025,5.938521045865967,4.733354340613827,6.132988042627442,7.991691898336964,6.419033185471022,2.1858665453113337,5.449891473010944,2.6064422281316078,3.7311832415722,4.332707933640605,6.56498780095989,5.434294617959925,5.893362210763872,10.084529386649065,7.932923373835633,3.0823619695574735,2.440952198029637,3.1842802944193824,6.1430260038899736,6.387500406480984,6.681449265314958,4.324810603420484,2.327687364176047,2.722466024471091,5.569551851083613,7.527555198794536,6.632559052117953,2.9145645234939397,6.135452784185173,5.424922088210688,2.2357270598380583,2.8011586560936985,9.8361608416947,7.193968819421969,3.097610796626422,2.655351828612554,5.553360503335328,3.169925001442312,6.173127433480656,3.817623257511431,5.778208576398088,5.7554217347342425,8.571145863602528,5.470862199413702,3.1602748314085933,8.3389146195993,2.440952198029637,3.982765462583646,2.140778655782796,4.3197624276692475,2.292781749227846,6.864928972289788,5.969933274697856,4.950468414150123,3.4581194811745064,9.054360117597708,2.232660756790275,5.648753032989572,4.827310943949667,6.609104846618959,2.920293300211007,5.726013748860151,4.484138131201669,6.010779838753242,6.51317488460363,8.731081391094504,7.529742876502924,2.7782085763980877,6.546122758857796,4.635173946674924,5.533563348214512,6.2992080183872785,2.657640005207824,11.260084480727762,3.442280035252584,5.339850002884624,6.091065077930535,7.352175878326325,7.78959885005233,3.2418401835646704,4.584962500721156,5.035183996618336,4.898692072560488,4.393690764187454,6.686360452903688,4.792855352362489,4.044394119358453,5.077670274232752,7.061344188288391,4.06522762277562,6.802968651005583,3.412781525338476,7.953207027098,3.5643781685650637,2.790772037862,3.350497247084133,8.870919495610579,4.663913842115978,2.2234225499349374,2.6667565918848033,9.754286253961022,6.500324049284198,7.87670110864523,1.4168397419128291,4.017921907997263,6.177519202597814,5.47800105583793,8.031053274444373,7.831180547126736,6.461724991502711,3.948600847493356,2.319039815562536,4.898208352508718,6.20945336562895,7.562776656791846,5.542258049766918,2.6016965164809576,6.711081862306696,2.0565835283663674,8.630303533275018,4.854494418154875,3.4892860226258766,4.601102203376409,5.932391940068527,6.822602695756541,3.039138393906958,4.397118409042582,4.078951341394822,6.198886990956932,2.298658315564515,7.472731449369055,5.0669502439246275,5.059614856297222,2.9708536543404835,6.608661417987563,5.51443791382931,2.176322772640463,5.642412772905056,6.8820316083521,5.234577959784565,7.163498732282879,3.212569338850806,2.2898344651775093,8.307656205306488,3.423578170981797,1.3840498067951599,2.7676547982373463,10.679004987548526,3.2645364309990255,3.9990980337056072,5.649040565634448,4.793375711264754,4.203201156316611,7.668814088419117,5.911931212309049,3.4581194811745064,5.445263208140443,8.913188619508889,4.936402377725063,8.645802514606062,6.207111961207714,5.749266083814806,4.4376272483189485,8.580786171355323,2.189033824390017,2.341985747228616,4.06522762277562,4.804260115634738,4.849999259466098,5.075532631167357,4.056583528366367,9.188044796421696,3.8439838440483265,4.634012356402116,4.8104431038451745,8.686815644595628,2.521050736900963,4.016139702655253,6.212180210044431,7.545505100691072,3.904002316283692,3.71699089440494,2.1667154449664223,4.381283372503783,6.564835417010355,6.563005553062399,5.911212201558612,6.247547806890298,5.477677327565307,8.292781749227846,13.587327482778312,2.9011082430145123,2.845991770664573,3.7644735509926663,9.182369521950886,5.921959917326167,1.9297909977185974,6.910972451655129,8.149950301681047,4.3729520979118295,5.38404980679516,2.063502942306158,5.977508904540138,1.3673710656485294,4.067810783789621,2.1176950426697547,9.199893148608071,3.7081872360207084,4.628773595201645,2.488000770834068,6.501439145158873,6.706254253573203,3.0908534304511135,5.86616616903904,4.155425431747197,6.060695931687554,4.570462931026041,3.2357270598380583,5.341274183692174,6.656925339794245,5.588564737401351,6.353146825498083,3.48155728070859,2.4698859762744636,4.872828759534886,4.34766565630097,2.877744249949002,6.019479533703628,3.465974464504069,7.536363792395467,5.132576842670009,2.0531113364595623,6.283366212949919,3.699329525673156,8.199279721336625,6.8203066288938246,6.068240861312821,4.598126959919604,7.979739567128753,6.258895755075436,7.258707352196828,5.394376944957055,3.0125686735030555,6.862203399053224,2.8318772411916733,5.164705840182799,6.431288654291593,4.375734538583156,4.8594726668519375,2.9392265777282085,4.302319050948834,4.397118409042582,5.169123281476757,3.761285273361619,7.240600549955795,3.7387678368001915,5.678916986679839,4.3334237337251915,5.530445354692906,8.431288654291592,2.521050736900963,1.2265085298086797,2.2539892662307865,5.23687524536694,2.298658315564515,6.398316163400203,8.348374075191163,6.043300754364182,9.401604869714957,4.534186139090097,9.069933046375542,5.486714373030702,4.96624587322494,6.307246355226617,5.51127826508303,5.970853654340483,5.342341397431548,6.219555769166913,2.319039815562536,7.516566771496865,6.652773284510777,9.109908988534402,5.338067797542616,2.9981955031532523,6.506843098969815,3.04788732939655,2.526068811667588,3.414135532984451,6.295723024539968,2.257010618206024,5.500164679492167,5.333065878076355,5.253232938538032,3.619413010597937,4.695437110405369,4.37364821133469,8.356407951394893,6.039796407620025,5.908332569782425,1.74416109557041,5.666472568842073,5.9586107120558465,2.430285272977781,8.487277313213783,2.927896453728821,6.9920886090139955,5.7615512324444795,5.906409617094016,2.572889668420581,2.9763636357327616,3.311793717753649,5.325530331567558,6.075532631167357,8.785648628652076,5.301953395126096,6.687620632084895,10.60719158640291,5.2543672814408335,6.976821852360685,2.655351828612554,8.033092575523664,11.658760423755194,6.231317197152663,5.008092420948722,7.824449651364088,2.8155754288625725,5.656496370553495,3.218781167784069,6.101187815259738,8.328001662149257,5.576522137920503,7.176621973161577,2.0565835283663674,6.5787877956769725,6.602142090697982,2.7441610955704103,4.727375938594897,2.2172307162206693,4.552131108253784,10.754979337466484,5.651338766105931,7.449148645375437,8.21179097625254,5.769771739249448,7.09876903815488,2.5921580021253603,7.971830914780502,6.234194722953812,2.1984941536390834,6.955824240806441,0.16349873228287956,4.680324356844016,2.5801454844233804,5.583458910130768,4.845991770664573,2.7070829917717063,4.18824265626441,7.27733390042982,9.535217045453418,6.304876050044889,6.982080909763738,3.0925457415435655,7.051372101721025,7.022922589055629,2.9392265777282085,6.653203362033977,8.22838581704935,4.960697039304312,5.564378168565064,2.782408564927373,0,3.9354597478052895,5.525755692829486,1.0976107966264221,7.211790976252541,4.477677327565307,2.4436066514756147,7.966303561412599,6.19021976366016,5.592457037268081,10.665637927637095,3.277984747299765,7.8269297659895765,3.5643781685650637,5.612057559563065,8.766462652425766,2.662205499653617,6.920412408160787,5.290940402403678,7.502871574574138,5.2539892662307865,3.1984941536390834,3.0089887832272546,7.2259304111710545,0,4.919340082442012,5.334496768390418,4.654206377944292,2.327687364176047,6.705839705883682,11.02948049374083,2.4594316186372973,6.294253136444514,7.172927489603836,5.573192723781613,5.5804470195986635,6.341274183692174,6.768713657030485,4.5855634984556914,5.609400390006429,6.10055722078293,6.140165134599453,2.37851162325373,4.726831217032493,3.127633279725874,2.414135532984451,4.561326136453918,2.211012193485512,3.6147098441152083,5.901832305785489,5.903279342056139,3.84197311892718,3.0443941193584534,4.77663042305105,2.7949356628035362,2.5897634869849773,2.4436066514756147,2.5160151470036647,4.623515741490549,6.6059979802325195,9.051127353591394,3.017921907997262,3.9212458885855885,2.2387868595871168,5.621465834460144,6.001351892999959,6.605701738949982,2.6530600171045644,3.4698859762744636,2.456806149230474,4.528571318870758,10.08413071995014,3.1953475983222193,6.061128135118254,4.434961760187254,8.556735947914529,6.821837746088096,5.895302621333307,4.783456654360239,6.1147834472498515,8.8634739090894,2.6182386555954547,5.862203399053224,3.319039815562536,6.531225485053606,2.2265085298086795,5.50303064564116,6.549977142771825,2.582556003014061,0.45417589318580215,8.258094872866513,5.864186144654281,0,4.110196177754199,5.541948425333733,5.139960569545456,5.166715444966422,4.661635602335959,2.6016965164809576,1.0565835283663676,4.867402305727543,6.390082950951663,6.790641730763425,4.185073637876762,5.681168120416691,4.409390936137702,5.399854673572431,2.752748591407134,5.553974808237472,4.238786859587116,6.451211111832329,3.2555007331483865,8.627898615871748,4.5588798534754424,5.639521599510833,5.29020320512607,8.07804403634318,5.592756010441026,6.952217064973338,2.6392321632492775,3.2942531364445142,5.633721812641012,9.325777655214823,5.554588851677638,8.364703739582406,12.206535173103617,6.2268938135713885,6.058532970201611,2.996388746447621,2.169925001442312,3.2539892662307865,2.3645724322958563,2.996388746447621,2.5084286525318573,5.370861740085285,7.2461230165968376,3.19377174339668,2.4276061727818994,5.25285462595139,8.366191054189278,7.062963556559086,2.0531113364595623,7.846744023244868,3.581351247168777,6.948834425578686,6.412442824775613,2.899175630480513,4.786073551578548,8.971342367308225,7.2193621578077165,2.885574364371426,7.801029283808189,3.7813597135246595,5.6884601404244926,3.361768359419153,2.280956313831056,8.531927241876955,7.295631200395783,5.566206293465506,3.9990980337056072,6.145066014185894,3.0373822220030804,6.666614587352882,2.655351828612554,5.343763122149601,6.696689389537669,4.609991295212678,2.3950627995175777,7.005961325845812,2.2957230245399685,3.7548875021634687,6.883987308105472,3.2433644256936605],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logLR"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBNumM_train, TotalGHGEmissions_train_log)\n","\n","TotalGHGEmissions_pred_logLR = pipeLR.predict(BEBNumM_test)\n","\n","LRr2_log = metrics.r2_score(TotalGHGEmissions_test_log,\n","                            TotalGHGEmissions_pred_logLR)\n","print(\"r2 :\", LRr2)\n","LRrmse_log = metrics.mean_squared_error(TotalGHGEmissions_test_log,\n","                                        TotalGHGEmissions_pred_logLR,\n","                                        squared=False)\n","print(\"rmse :\", LRrmse)\n","\n","fig = px.scatter(\n","    x=TotalGHGEmissions_pred_logLR.squeeze(),\n","    y=TotalGHGEmissions_test_log.squeeze(),\n","    labels={\n","        'x': f'{TotalGHGEmissions_pred_logLR=}'.partition('=')[0],\n","        'y': f'{TotalGHGEmissions_test_log=}'.partition('=')[0]\n","    },\n","    title=\n","    \"Visualisation des données d'émissions prédites par le modèle de régression linéaire<br>vs les données test\"\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.2 Modèle Ridge"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre       Ridge()\n","0  ridge__alpha  10162.650894\n","       Ridge()\n","R²    0.186454\n","RMSE  1.878833\n","MAE   1.573171\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logRidge=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.856183108316816,4.867142567195112,4.92098524304253,4.998521484917617,5.253688930692639,5.004912116719293,4.878420693036544,4.868963912500103,5.200523165380261,4.904379914215667,5.067219422605705,4.836479094005861,4.829158667511811,4.980647797233412,4.881908086645265,5.218019399924048,4.893030444727502,4.930661953709986,5.157464321079021,4.912456257046464,4.814875539815244,4.9524222639324735,4.961141214999711,4.944384966162877,5.2747863969227335,4.975863687641681,5.244104597102832,4.956450362380098,5.104993142660636,4.930390638834695,5.149517686616677,4.996655997790053,4.907611175103797,5.060281604035968,4.954265353698553,5.038064163795588,7.825740089261393,4.845871674336779,4.920189332346895,4.8883687437114896,4.98721855623865,4.93087326805525,4.8510215829278795,5.179233887292343,5.055450288614659,4.878792160420446,4.899485331459515,5.037729380650617,4.8514202005033376,4.884387805557046,4.896717041604119,4.957047931312492,6.1553080259375665,4.919679800253664,4.930024181968745,4.913280011655381,5.954876033762314,4.870203084250372,5.0843985269775915,4.909581958490724,5.170873038531473,5.317824946046503,4.873457811119643,5.3223655549638655,5.205252338553879,4.962387466328226,5.0885104999153326,4.899260235437359,4.889163095235667,4.868297237762446,4.850516934516673,5.141629404134396,4.887379332527042,4.854608266413302,4.936703372245321,5.147650099882101,6.018440759808803,5.02987387386806,5.025509401359827,4.888721236236774,4.939560159801372,5.296695234546725,5.286701276656147,5.4910471917263335,4.899493931279021,4.880555005705126,4.843954420844291,4.959094741506519,5.225348232759751,5.681529754568876,4.881399238503416,4.916071679920109,5.323474149781901,4.949966325380069,4.917338935395193,5.170061167505583,8.129982320726043,5.263223580497938,4.9257118695707875,4.8761392585883225,5.214559606310885,4.941948616429213,4.97429820382548,4.942580194529506,5.2873620878118395,4.894067328685386,4.97747686978944,5.64615995652766,4.85820342296232,5.129003896540437,5.155776375123375,4.919703585731524,4.8602608374787994,4.88755752870963,4.807916459654362,4.847610609674375,4.916864338991033,6.3584741969736065,5.089133144469817,5.207077028637134,4.889929815481331,4.870544429458712,4.823379896242464,6.528101755077768,5.088066335359597,4.8926191224472335,4.899778510691118,4.87871526144019,4.884006003504756,5.23697575120667,5.431721968972475,4.9011220053827,4.8612036936188545,4.848018771674701,5.43773025756127,4.85726453117394,4.894106519309114,5.071674993898295,5.206555420874542,4.881544468810467,4.901015152846842,6.37963248583921,5.197732339962835,5.039213707631426,4.853879257142116,5.159609454351402,4.96720877347467,4.93771261200482,5.013395618961853,4.840564229447203,5.05150741177311,5.102759412164047,4.818944020004232,4.868435165529405,4.9200169180153255,4.967341521460315,4.879012489729654,4.8654014462719415,5.803292245982006,4.904738693369714,4.882097783647003,4.888968211228953,4.841178742714204,4.929705909258061,4.849728658725106,4.885602063764636,5.698388445484645,5.181573424476207,5.13538250807133,5.228925206375736,4.870970497014699,4.922543850662767,4.929484068616448,5.029074108385771,4.852693420492515,5.5552247335643505,4.8989909090754,5.0930354038191,4.987169775700961,4.878210299947353,4.94732025677709,5.842910206544028,4.923501285545903,5.020742694934448,4.906718508224336,5.092868062381954,4.877439146043665,4.875121380482218,4.904100400681525,5.270706869987383,4.991969029405153,5.224176416147506,4.896842450784985,5.272512285121066,5.011907084552893,4.973793552027744,4.942483278075507,4.887431493997697,4.993351683391696,5.0954101707488135,5.335203928015558,4.869199981711591,4.954520732854082,4.903731628949076,4.906955111146822,4.89583396255644,4.862398013454074,4.846596484941484,5.037733510747627,5.034243889206347,6.07267195799112,4.872724895601893,4.863363527013168,6.021134230729157,4.990592510792003,6.834838622772132,5.089561213071696,4.915034104970698,5.6795061706549514,4.935786528569182,4.846276540078129,4.995454874308929,4.879414758615517,4.865253203644954,4.973304046680304,4.858667320232507,4.9167698991952165,4.888357480200005,4.899318577772607,4.953062486012238,4.870564306039398,5.0067941887474054,4.836335433599231,4.956645160164606,4.865479093022393,4.89529174496791,4.833718937137846,4.861985791908239,5.767879927299605,4.861265218816387,5.137028701773489,4.859766817009249,4.890795772641131,4.890685423953835,4.952922875960354,4.884824885221076,5.76469147733652,4.8874535322689905,5.505441838312661,4.888165037530542,5.021690906404924,4.854109798999278,5.058871995364657,4.952253123981118,5.155542077894568,4.970215118454807,5.050863570212205,4.965145109524472,5.141414763565612,4.893492281709492,5.04309989479581,4.841245965784542,5.035070312667325,4.975109024389085,4.875357632967696,5.095490970738662,4.909696602972598,5.072714220670152,4.879490066613924,4.855760914219387,4.993620036885202,4.927171819372226,5.033886733642279,4.975477049931885,4.827265733029663,5.120151776160893,5.162881065853826,6.154445455059058,4.830529203872766,4.971224060441697,5.0172635915078265,4.918244031900516,4.819920525352703,5.212909480516871,4.921012836647348,4.895029917540533,4.968546281404678,4.890905294291286,4.983671776931986,4.873326437408194,5.692375698667112,4.958471736003939,4.820907969739312,4.827271813600138,5.008136047838361,5.0552086774886895,4.890301752171581,4.9677558948679135,4.942853305896184,4.8980934079057405,4.9234431398858165,5.606221175133317,4.998314327524993,4.892881271210027,6.769237764076787,5.153950450660519,5.421681956216107,4.905158718512555,5.005074118327589,4.927253178964521,4.91060538996527,4.84475334387376,4.9140087983397045,5.55953224983516,4.872842585657256,4.9260548889469895,5.053943484063754,4.985651071899983,4.892669435641498,5.050259904507537,5.098720625056582,5.606141137194364,4.8818246145645405,4.927756938259288,4.8631169995037835,4.871094469623818,4.874397185633414,4.946312359760425,6.581399005281628,4.933168134797974,4.942698045143238,4.855285508448413,4.933525234478617,5.471112853215993,5.049420033043451,4.9765785236378575,5.379591331917579,5.55964490738622,5.0348625205402815,4.8940430535324415,4.867718482594041,4.8856868780292,4.9118958750280015,4.929234566009914,5.408341352658749,5.399552734597455,5.0043361794779235,4.87216284014254,4.813888269858422,4.869992325178192,4.838021362114634,4.896099998742572,4.883822251988643,4.951992237897828,5.078728232601055,6.754458616127954,5.3238863377525885,6.710706416714413,4.907041774124812,5.214132169961563,4.979546959116731,4.969723331333401,4.924841012560969,5.202880931914477,4.864151333498385,4.8788748411515375,5.2559187102041856,5.039759610370273,5.249886454709904,5.105243909082939,4.890639653566874,4.8746817071512085,5.066638371358676,5.33972439265914,5.806733976119791,4.92519150766838,4.902683691470753,5.041102568161606,5.165348966157966,5.0646612052971305,4.895418050889985,4.838437554528057,5.924823390070499,4.963334340770234,4.899506569340745,5.223878929018908,4.999212208604381,4.863806621345512,5.227269151947896,4.8714887059130545,5.341864048769666,4.910054195174751,5.034402971052603,4.914683126672327,4.8921628383345155,4.876823055213858,4.912937190523444,4.868430268948355,4.895195350729948,4.923202310515633,4.871308698730907,5.138030666297951,4.895156511420409,4.990043448488065,4.921840741904434,4.87983283213655,4.979420055276188,4.90392489768057,4.899669611612964,4.965702180464869,4.907887123910752,5.065140858627943,4.872437980201926,5.1270953545849345,5.162080311303455,4.965535367660724,4.932060598557221,4.866337169515667,5.2702070477167,5.187020620726527,4.994364698689249,4.910931710077076,4.870132209909989,4.86221523988899,4.927189363616017,4.932878834182703,6.331201933574041,4.897945280621063,4.701173404078325,4.845875901526604,4.878247882543718,5.19441628961027,4.963780164120804,11.795883003683866,5.535525725102451,4.967407290126928,5.683322645930714,4.83540064377007,4.882176417046976,4.837305851307887,4.923790493186304,4.893081443705639,4.8856246599981406,4.90881092705574,6.237609418897481,4.951718522014309,5.000280793892398,4.9146660347663245,4.865878668121703,4.9072477561611,4.852274069071263,4.948935607699534,4.876584781126118,5.25324311508875,4.9174543857214665,5.168112566797737,4.900496081786063,5.273508969581062,4.839814161783069,4.876686066059595,4.908662795643093,4.917799799531795,5.121862823741499,4.832551047135862,5.79515182011661,4.835942124768279,4.896045776983719,4.857864748152906,4.832168108380515,5.06148254410894,4.90772104633135,5.205982024146667,6.311212568807653,6.382476360321265,4.927867318891441,4.909956220659503,5.14306095726777,4.9408914170594045,4.8284120615179,4.977085249064882,5.145859067144326,4.956674740341229,4.904537363308704,5.015592292584295,5.05431845790361,4.828082970611487,5.067043599792943,4.866693897205222,5.436293861281239,5.00376950644975,4.881543031240379,4.9006123257532685,4.8299098692739975,4.86579907250353,4.88156417041211,6.29724918884536,4.962378170237065,4.891144740956441,5.815389325179591,5.250796821695249,5.051117307432919,5.218392374401911,5.074646707772624,4.850583441374966,5.477933313009945,5.338120674938593,4.933781603604843,6.307436948425936,4.975353497315992,4.865399855290093,4.897454795097399,5.329952389885248,4.967372909773046,4.937434100876469,5.306291133873709,4.9190737029896745,4.876065528152722,5.920742360837286,4.8546631190532015,4.971056025023378,5.296460389947244,5.56255829204156,4.862264982268165,4.838147879551958,4.949971376451266,4.8811278166275205,4.988911153542168,4.910131385005765,4.9055521759772285,4.939872906003013,5.091937098664732,5.011513637550319,5.025053165847238,4.898976442595472,4.98999165285351,4.898142103999242,4.869089124987937,4.893118271312003,5.121699952359585,4.993571998145062,4.872546274610181,5.012189940174483,4.887073924079291,5.023238656848125,4.906011241160273,4.930179968754421,5.0486584591900865,5.050800610300876,4.871188112869536,6.469650349303215,5.181102462333393,4.8316879428929544,5.281796264077425,4.901898075669802,4.964179693064472,4.877818229049288,4.949082263461771,4.93565526287484,4.88151735793104,5.160624980626961,4.848474112293526,5.008502330375994,4.8580750319642725,4.969366596264516,5.287581602911395,5.025799479598932,5.1277322078538665,7.362275826697136,4.95721536694791,4.927533074552264,4.8818758276476775,5.685978371081907,4.950091664906224,4.893401429386014,5.801377632514238,4.855742973944169,5.045124633303959,4.891719133523481,4.888094204383679,4.944264228549074,4.927589683099987,4.93558273804933,4.919460434871094,4.90402193968047,4.873510611540407,5.201044773142853,4.97766723483816,4.984850178586141,5.238053546418835,4.917389512518094,5.054677433712923,5.234948978816523,5.015522470584657,6.039624761918182,5.051394171835668,5.041037135419102,5.397038846030892,4.934165733745604,5.1631020521723645,4.988956821066603,5.22426415696348,4.939876202934672,5.2091673974002255,6.4896091559653355,4.889597821338231,5.89439352609017,4.839774532317549,5.338264048746153,4.875171821163386,4.864512173964431,4.85152905002467,4.875664889601197,5.0155093779799715,5.020365460388375,6.411782473465314,5.243558102080996,4.9115117592434085,4.948418079973015,4.846359933004606,5.013244653120914,5.106987914179756,4.8881977315179705,4.889998942615147,4.9424068698539605,4.890596769225484,4.908427520178288,4.991087387609831,4.929065876093482,4.840257304686411,4.8693828844595,5.2393300700038274,4.861248878417899,4.865896308607949,5.152257545339701,4.86600993489907,4.981985311870486,6.385664741333491,4.994346219287663,4.90808683318194,4.936175202545164,6.074436277597099,8.89397113725899,4.939746300931828,4.893166614654708,4.915667077016908,5.0401340764958045,4.918874775149436,4.9272427835819235,4.882870639102806,4.87903963996157,5.229451090446116,5.453539759798971,4.902658221594805,4.9392660607040755,4.867879865634296,4.9596822879994695,4.947957297536294,4.888240856028817,5.352114692531054,4.845305621026908,4.946934410451928,4.974642891991696,4.9298695265663595,5.09142756329248,4.909862903132554,4.98026225616706,4.8836920857439505,4.921482481551141,4.900976960875558,4.885133563192664,4.914136818489278,4.859302025459492,4.961484851026218,4.875603534946013,4.897036830242755,5.250136852597113,10.10694579749006,4.966000400138561,5.095027738919355,4.870841696076863,4.863897003647188,4.932616616916291,4.868696924282232,4.893344976013177,5.049255069864638,4.878562385791323,6.522069499583487,5.146764611529024,4.92144961771359,4.977540569832659,4.890997598076862,5.118496754080238,4.933693532066042,4.863459030321453,5.269255835992219,5.113020169674037,5.6883773649762155,5.291652108355491,4.923506756969776,4.846223132558957,4.846726588543077,5.112413184202688,4.878950063111823,4.857505516768843,4.918426831384889,4.8661228120748,5.033925390828483,4.950838127325795,4.919541956960113,5.131797596927695,4.890013521489438,8.70308635100378,4.841383144305851,8.389219430271364,5.053406444297438,4.892496312067042,4.867279649395631,4.93511667481373,4.8237654731943085,5.559301792829716,5.98577359906378,5.448053988117788,4.88177395976297,4.863417279465672,5.147097445997738,5.006963510772023,4.884242555441634,4.851239043450588,4.911225198462909,5.8876104914589265,4.9093446287176015,4.904919119484261,5.738702636273512,5.04015681435372,5.035004879924821,5.089554148977937,5.696614081719135,4.881389888461977,4.867297474310669,5.087680095020756,4.868023585954593,4.872447734462761,5.0321836212163875,5.2313804882540325,5.114060347515904,5.389450185422559,5.246416917092435,4.967590109516132,4.886950711289707,5.065088705461082,5.615065346106593,5.668882205991212,4.893753870818559,6.417814728959595,4.92761917816608,4.960001449807339,4.975388966357003,4.88587364186525,5.171342031748159,4.958373739735896,4.891905897359531,5.232433848930555,4.91101303016085,4.923543812657966,4.9318903751348016,4.962530846522076,5.014123956413205,5.225520208675725,5.224023303256673,4.840138145046707,4.956473866937237,4.896177836335525,4.8650640664390785,4.862647077593723,4.84455494725778,4.887659478281115,4.901924873467599,4.858044122692055,5.507871467365514,5.999827990151904,5.060633382968539,4.987889824274335,4.892267375751884,4.86248789394345,4.979736422823279,4.921829175350079,4.956858702686722,4.907447309974726,4.8827183215538765,4.940284224842538,5.0822894337158475,4.852005012967248,4.8680072149858,4.910553109796844,6.011091468338765,5.122172129178418,4.833297988523944,4.720832090014848,7.176385477741356,5.038223376675029,4.948117603117089,4.88113699268189,4.975135258511863,4.985951413410386,4.865503865149775,5.201792061523491,4.845625165854481,4.878661678631446,5.026218276905327,4.984371352019227,5.09020314225884,5.430261605786958,4.904381335553754,4.9508278660851,4.908997232544246,5.085417330066164,4.895070210522955,5.302492645441525,4.954441300139718,4.877369706460413,4.9694921616701,6.0559354845343725,4.878939419283638,5.2603823101783,4.988577263940237,4.8910199747367775,4.898261892425082,5.373066134151377,5.110649943190534,4.865334280953773,5.419635003356756,5.491152480095816,5.216809149220269,4.871511348516675,4.897176996450722,4.878848442467496,4.926762730176037,4.930511761697596,4.979787688168929,8.921470674494628,4.854530451804272,4.866365388651551,5.092078745419897,5.287194191810573,5.4635147483973014,4.880811692153981,4.8042171077896985,4.8364174397161435,4.786345852878659,5.022199250093464,4.888039167772847,4.825446347718753,5.197410284643,4.834966869699246,4.9193192092460825,4.842441856799245,4.848680617810296,5.046189069848002,6.495641411459617,4.900382565660223,4.961309265966033,4.92052451150404,5.3434215665255795,5.159149454495221,4.84792793035332,4.923773470101598,6.361308966527507,4.933614497548045,4.94524428314227,4.918631043609489,4.829267295553135,5.025924392754802,5.264674614493102,5.935040229149307,4.857561305518951,4.894437198970513,4.92293722636124,4.889628583487704,4.8318838936419555,4.92531046516468,5.271282163300388,5.060606115864394,4.953754367565843,4.951072294566096,4.870488996154115,6.560958138192024,4.936020370794614,5.150292760627108,4.866865433760974,4.8583821018702995,4.930316554969908,4.960388854298879,4.909889916685471,4.9374472317333815,5.271783850382833,4.792841940493498,4.902606341182077,5.393520479103174,4.944814284720354,4.857550414209157,5.182282910643082,4.8643515679522675,4.900118270982582,4.845177394237566,5.940232841008052,4.896651435976472,5.077211776177493,4.9762483019378765,4.8284120615179,5.153900989060362,4.993546005667645,4.845829336159183,4.913481678635125,7.080007406729568,4.922751580523196,4.958933136838829,4.848888221462884,4.858428877651193,5.049047868701618,5.717466443913344,6.211669708455689,4.950296484043355,4.954082900914821,6.840969903993868,4.8425615509767175,6.033592506423901,4.952933867196354,5.154536744791636,4.928095482013946,5.876662035114289,4.883471401537947,4.854912861335444,4.827199517921668,4.925821845418842,4.926928550276811,4.839632960379321,4.888035073191105,6.183040601090928,4.943869470810388,5.393102775403829,4.856801902885651,5.14622528984542,4.845198465572442,4.880905220318218,5.211112637845296,5.861754896734165,5.171970417859569,5.032366671971515,4.8351997765180075,5.155431103216221,4.933958525535273,4.8774870046877385,4.864021803741125,4.947480238011235,5.025320112580246,5.945043480411679,9.826559152277113,4.926101810349281,4.861785071366557,4.933565330046545,6.049903229040091,5.011106373821871,4.87677944797762,4.9178177172266935,5.528426605736465,5.136677026390103,4.865142386757421,4.885836072687926,5.9294706528127055,4.942035334757255,5.1677338424780235,4.883099295203936,7.368023442425715,4.856635868365669,5.113910991192808,4.871285928445585,5.019960945784209,4.876037947966947,5.061297291291007,4.899433684880295,5.122990835862021,4.933120128807784,4.948955735839112,4.871600557461682,4.93633190486142,4.9318773051120495,4.8955972271141786,4.993235477880203,4.913757341578745,4.899530192925289,5.408151417680462,4.9695740372755,4.955498031502601,5.764780427540443,4.932087144441271,5.6274737726826185,4.8875239238839825,4.846184470332632,4.859433401528062,4.997175538109233,5.19825518360917,5.136283663873096,5.068630935542451,4.882487705101629,5.996545201687125,4.8587331219647565,4.861831828769256,4.899353240923723,4.906830271532801,5.098900317876235,4.8785833583131035,4.82546059761021,4.903549702996443,4.893966324195946,4.864099485109359,4.926451511380387,4.886730877011828,4.893475785352681,5.0129386123586155,4.867323757883794,5.406597375618568,4.9622788588214854,5.146429014280299,4.845208907209916,4.893320985429442,5.013937554331184,4.90556244841957,4.9115716648976875,4.867169678074538,5.4125939832017576,4.981791059737591,5.0037979222643845,5.409916222188873,5.0148128281930635,6.475429476461535,4.974153745952043,5.844313399061887,5.015551331151387,4.993706297574377,4.958911942022145,5.058917370501465,5.142284456720054,4.882737182402123,4.860734254434186,4.843998274174562,4.850135321916504,5.131657655810204,5.853756663159692,4.996625548674262,4.924459086879171,4.961466890260765,4.928146010256459,4.831062903061827,4.929576690738378,5.3136717705623475,4.901536815594681,4.898420437125247,4.968335430814569,5.1100213340760305,4.896474486348518,4.988651042564948,5.074675332983642,4.905292490931641,4.923029030716458,4.967372605664634,4.89673353796093,4.890537537722454,4.911159578608274,4.87264608742256,5.084760488095336,4.946196456845716,5.108142880132815,4.842056279847401,5.383096571854869,4.8467691392095915,4.93287344849294,4.973387799781907,4.879276755722532,4.959302591582122,6.711087521545878,4.823837768872779,5.204186381555241,7.073975151235286,5.137776455447136,5.280669021161866,4.958526990601369,5.47750640943682,8.683787307140761,5.211987144429766,4.8438687444173025,4.869773789142367,4.944934294792415,4.857269729764087,4.911078674803845,4.985052698204465,5.467450046458382,4.942988560670346,5.139689158836217,4.879803817193645,6.619722606031221,4.712053386279002,4.887953421479913,4.87755662652118,4.834930461781945,4.994146207302091,5.5798776892230375,5.540450450937283,4.846601642642273,5.204287439103451,4.979011408123986,5.77824286684611,4.903071615331582,5.6409596867472995,4.873641562635756,4.871764887315453,5.442021732623507,4.876842044707994,4.85066942709991,4.8537544282501095,4.858860108708429,4.8523966221569115,4.931773951716198,5.0453619163413865,4.944669958243598,6.435709836258294,5.2052532871774915,4.993676174375015,4.926314230405822,4.911703531843662,5.52467111960149,4.935361840373294,5.08024586302418,4.995352872176166,4.867864210043992,4.996843704440286,4.957528335139654,5.148775299061224,4.915279330549647,5.00706262142455,4.846124502743227,4.8728979485904516,4.98811395180781,4.866613831928279,5.850988895142781,4.922251273936998,4.985043663618267,5.281977349723405,4.896184343700788,5.420159322697092,5.020538452584912,4.824858638931465,5.478433746069653,4.922890620224108,4.900884984139473,5.183695196500862,4.925648797029282,4.934769462846229,5.022800167507641,4.920377969840308,4.964962001146698,8.709118606498063,4.8478871961405,4.91278262700833,4.822379806023619,4.873513391336138,5.375896804113445,8.907404990733847,4.885530237392514,4.939152384302065,5.11527180547192,4.872076345072738,5.021583586645668,4.83171703556058,5.292692194735144,4.893107292018797,5.157282987881226,5.086309436123941,5.05115688879824,4.892243277192394,5.309994376387308,4.871682016605015,4.908932159110283,4.961557854021851,4.860044154495972,4.993756868063185,4.874397236286805,4.891714060915349,5.197811488117964,4.965392521354384,4.983959397359229,4.928135046445685,4.905369878019314,4.893217677869901,4.88131934528023,5.036160499999179,5.110543716185743,6.00292104938227,4.949619223461076,4.9255871158734275,4.866732891683777,4.953270336087841,4.899392093790591,4.931644093740286,4.912995979337723,4.938911089676985,4.917108771380917,5.358237265193154,4.952782233710554,4.9716647599244395,4.873447352821746,4.849639476616058,5.217720325731063,4.832337986183384,6.430546333417946,4.866485881449003,5.023167099632826,5.447172669242413,4.939135407557021,4.880363666021305,4.9689671930349935,5.106585533714665,4.89395474422255,4.991168176981462,5.095564088724925,4.92213261793116,4.917067166837169,4.988481187598301,4.837926793095632,4.858306324565544,4.832865767347645,5.0447600474111,4.839751192632127,4.933486214553374,5.03304332097374,4.91754401473769,4.880282776063514,5.256164661014328,4.8774055412180175,5.105777905330413,4.880940132907695,5.148312340066162,5.462417298727866,5.043765766241908,4.976978591474321,4.828103213902551,4.875485102436759,5.145367881855709,5.0423394077707915,5.585997197680187,5.418626238696039,4.991143765340342,4.918488512540745,5.06816537546098,4.998922321520478,5.243532559991371,4.8706653905387105,4.900386999318247,4.92442149431263,5.417136500539896,4.854598801802498,5.5473872447292685,6.635752971939248,4.920192720562937,4.834135469396832,4.91399666079036,4.928308581308886,4.964851284904751,4.866922667839763,4.927872997398715,4.870968213514187,4.901868642472928,5.646591843347795,4.975957552285096,4.847120954705343,4.86133603150128,5.340101847782315,5.504132541627234,4.889655166984703,5.173859199620066,4.904603827355891,5.8447595970866,5.064078615754499,4.880381099200829,4.836407744242202,10.711384836776054,4.892945452552479,4.8837719945814655,5.071120960955363,4.8292539201182105,4.951926334562963,4.933174038758503,4.8635774017286435,4.972276197843508,5.084000491050345,5.063740193079302,4.8448650047514565,4.867888514282851,4.9077185167368045,5.335310902274663,4.88243959713789,4.9830265151438065,4.894833872337595,5.265331306805147,4.883599340313358,5.18716397854417,4.879077848089002,4.833543463707473,4.925351464740364,4.879710142865507],"xaxis":"x","y":[4.9676297534661975,5.60584986719498,2.950468414150123,2.7355221772965375,7.258707352196828,3.0925457415435655,4.178714641175443,3.4154882710497003,6.224387627929912,2.634593268445757,7.244982170376553,5.597531174224212,4.210232990095849,3.594548549550354,2.526068811667588,6.215484464192794,4.969933274697856,5.912649864897204,5.111865963867557,5.042644337408494,7.864928972289788,3.382667252745041,3.189033824390017,5.705977901682522,3.9717734471933728,5.119771161952904,3.496973580998276,5.058749412335524,5.968551603363945,3.1160319934471103,4.464668267003444,5.192194165283345,6.73226919950145,5.39746072605472,8.171076693915895,6.43095427138573,7.132782457298859,4.294988267691446,4.708739041359579,3.4646682670034443,5.450881315273369,4.95093492831454,4.20006486151431,8.047505671251574,3.605257262939004,3.261530815243406,9.159820892013935,3.8728287595348854,3.8298495598446904,1.9671686075326276,2.9126498648972037,5.648177795724818,10.257022407964342,4.887037791419197,5.718087583960517,7.540941686758214,7.156841524588401,2.0942360698457656,6.074462620704536,6.428778891337586,6.464504909131913,6.588864518190148,5.629648044187436,4.795974694206668,5.676380255466924,2.811471030529836,3.9882302230604574,8.45314714555533,2.321928094887362,3.980939266085512,2.5801454844233804,4.41886457739032,2.8972404255747994,4.341985747228616,6.561784353081841,5.751410160064371,8.545157551729474,2.7676547982373463,3.773996325111173,2.7676547982373463,3.258518924711301,5.596637034102074,7.019368330478469,6.649471757181635,5.275007047499869,2.0071955014042038,3.700439718141092,5.320845667645722,5.331633567171029,8.061668207389335,2.786596361890807,4.94673086014031,5.649040565634448,2.269033146455237,3.292781749227846,3.683696454306516,13.390111171989,8.064904402198286,3.727920454563199,2.8155754288625725,4.253232938538032,3.0373822220030804,6.01992426090841,3.2524762141352173,7.125981653854716,4.273515889702116,4.92552476974757,7.778274295338579,6.135452784185173,9.20006486151431,3.6971065744769747,3.2418401835646704,2.4222330006830477,2.6229303509201767,5.57198012018773,5.0539801681876515,2.6915341649192004,7.510803720358323,3.01435529297707,6.045049740837989,2.7803100990433753,4.658211482751795,4.595145567990858,7.485507332459785,5.342341397431548,4.129283016944966,2.454175893185802,5.96254902292306,2.3305584000308026,3.292781749227846,6.290756138396673,2.817623257511431,5.532940288372874,3.2883585621936606,8.008820757716158,1.6644828403646825,5.284292026394312,3.598126959919604,6.237448995639321,4.222650022451478,4.380590934265922,8.669593751188334,7.720552091605108,5.548744859937229,1.7484612330040354,4.524815928357506,7.639376888638463,2.851998837112446,3.218781167784069,2.7990873060740036,3.868884273028774,8.043027283594547,7.260778431893426,4.038260575175349,3.040892430646901,2.7949356628035362,5.4741115139424075,5.952799477899938,7.323370069061268,5.040892430646901,3.4208865749755315,6.136683577697236,4.255500733148386,3.228049047884462,4.6194130105979365,5.623515741490549,7.217424613813984,6.179311989210016,4.3139713710590195,5.764473550992666,4.221103725367875,2.4594316186372973,6.548282482376233,5.9902741833186015,2.5360529002402097,4.720825666089835,6.512700963425246,6.664198369291911,2.881664619320345,1.9634741239748859,6.013685569928653,6.026578770127714,4.532940288372874,7.597158683183727,2.711494906650088,6.791683858152045,2.1009776477248208,6.090641751917847,5.040892430646901,6.327328082629952,3.2942531364445142,2.9467308601403097,3.7366048753142485,4.922673592849446,3.71699089440494,4.15623479785027,6.872951851130105,2.8933622107638715,7.561631630375841,5.341985747228616,4.993221467368938,6.309067020588001,2.414135532984451,2.493134922305505,4.161081482277184,2.144046369616707,3.486714373030702,2.8011586560936985,5.648465443027314,4.399171093819823,7.581803148516525,3.1160319934471103,2.533563348214512,9.967312731471658,5.687060688339892,6.954079959320971,7.084170591577843,2.722466024471091,4.826802684285827,3.01435529297707,1.6040713236688608,5.5109619192773796,2.1731274334806563,5.425593577653059,2.8797057662822882,2.7803100990433753,3.303050084681673,2.3132458517875616,7.006298023900369,5.279842693520348,2.3950627995175777,7.216163813388774,2.786596361890807,3.5298209465286954,2.7420062108667365,5.079378111118652,5.1622906135457995,5.839203788096944,9.354249381945241,6.058532970201611,6.614857050384495,2.1667154449664223,2.4982508675278257,3.3405622690264134,5.415826257472914,3.9791107550157854,5.338780943894699,1.9335726382610239,7.164705840182799,6.12866458654003,2.933572638261024,3.0548484769956197,6.390598905552461,3.2764966656403565,8.007363716298686,6.558114538598396,3.674686619927999,5.28835856219366,4.189033824390017,5.480265122054463,4.2787282129389395,3.776103988073164,4.82273014794452,2.9467308601403097,2.2898344651775093,6.1705259991768475,1.4750848829487828,7.264067216098838,5.6845382485777325,4.534808661231747,3.7070829917717063,4.919340082442012,4.458119481174506,6.288173968112302,2.27500704749987,6.5249725982866265,3.940166750482817,9.715601700373337,7.829278760612363,4.821199978056451,5.875042803002589,2.6229303509201767,2.942983598187102,4.445594291341182,7.474030370207135,5.92148393743373,3.4369613378336026,3.2078928516413328,4.095080491901415,4.422233000683048,7.424922088210688,5.185470146066942,4.955126781261366,4.3729520979118295,4.0522419811811385,6.067165427017112,2.2898344651775093,3.0806576633452254,3.940166750482817,3.127633279725874,7.152994605492435,5.573192723781613,4.4121040446775694,2.2479275134435857,7.886854944198882,7.9329824099498465,7.25691629667084,6.670585440262214,5.845740932614582,2.9616233282869446,2.5459683691052923,7.959132576789657,4.653633311386255,7.406162478777607,5.588264894307497,3.8063240573900288,6.716579420796345,4.984589350362456,2.269033146455237,6.929672661579345,7.060371694122218,6.746447139362815,5.521364878446219,2.9873208659292536,5.970623614470605,2.37851162325373,4.667892125230234,3.192194165283345,6.816215687622865,2.851998837112446,3.3854310371935203,4.634593268445757,2.5558161550616396,8.990671283975267,9.114445113289024,3.389566811762726,6.9532652390148435,6.705563274566321,5.672425341971495,2.280956313831056,2.137503523749935,2.2203299548795554,6.561173365231147,2.9726926540042644,5.078951341394822,5.0759604132434015,3.8509993947164736,8.693835392482516,2.845991770664573,2.8399595874895316,6.111448698487674,0.7484612330040358,4.125155131322289,3.8953026213333066,8.45889870671479,9.650854294025212,8.19120730223037,6.534964249804655,2.5160151470036647,3.0373822220030804,4.701549056943071,2.9541963103868754,4.903520373723376,7.848935855524565,2.0036022366801953,5.660780334032546,8.418822422291017,5.254745197629213,8.4696011200494,6.567880042052731,6.557808298936688,3.6769443591069124,5.651051691178929,7.279842693520348,6.425089989874059,2.811471030529836,5.4799419014771305,5.184280294419382,5.713420884868084,5.452858964713811,2.4195388915137843,4.178714641175443,7.686990680088471,5.3847405872923835,2.722466024471091,7.554281862626391,3.955126781261366,5.192194165283345,9.071542807992602,5.765004246250036,5.318316841334983,3.1953475983222193,5.2671612180222604,2.269033146455237,2.702657543390911,5.6937657122177825,8.574290766120185,3.554588851677637,2.4059923596758366,3.0214797274104517,1.3785116232537298,6.010108453474289,2.438292851579147,3.089159131911238,3.0250287944915226,4.571676809970931,3.24031432933371,3.261530815243406,2.6712933724815757,3.6542063779442917,2.9873208659292536,5.8060662260890314,4.371558862611963,6.4984104488609775,6.229587922740652,2.2957230245399685,5.178714641175443,5.463687842201262,4.884597920990064,6.157852169141737,3.4672794804599825,2.867896463992655,2.611172380044005,5.535430914871279,0,3.4776773275653072,9.582254908357664,3.950468414150123,4.8894735425311096,4.850999394716474,3.300123724569014,8.009660689646706,7.626950122372515,9.04105676230812,6.564835417010355,3.3175935046234715,7.380331184220349,2.0214797274104517,4.235727059838059,4.334139178838237,5.692092375427572,5.592457037268081,1.996388746447621,2.070389327891398,5.8052924556007115,6.60584986719498,4.088311235888661,2.521050736900963,3.5310694927259543,7.058316495590823,4.044394119358453,3.0513721017210256,6.008092420948722,5.605257262939004,8.220281580441913,6.088735246190284,2.137503523749935,6.637204481902295,0.8155754288625726,2.2387868595871168,5.676662334857967,1.7865963618908067,8.05642095591347,7.5673476962842585,7.295355692891673,6.394033895367782,6.582706526780788,4.43095427138573,4.915520900751958,3.710393192170516,3.8845979209900645,5.135042286233651,10.267009017093484,5.336640446408735,3.0992952043377753,6.836050355058069,7.896998342338911,6.558114538598396,5.10936055940423,5.85872702377863,3.785550551739256,3.8042601156347384,2.7484612330040354,2.883620816285671,3.576522137920503,4.104336659814735,5.86195536414487,4.294253136444514,8.44136727845972,5.497931651896279,7.5808238499627985,2.8278190246173196,6.260590274730996,2.229587922740652,6.218781167784069,10.212070748654236,4.400537929583729,6.543805175964397,7.563234413022304,6.203788453308596,6.676803353873854,4.070389327891398,6.340918270272116,2.063502942306158,7.151270288790165,4.228049047884462,4.563768278452033,5.4329594072761065,3.9373443921502322,5.580748491763774,4.279471295644468,7.959132576789657,5.739578112048051,2.7463127664254587,5.707635219545296,2.0250287944915226,3.4776773275653072,7.925940095994448,4.478971805032942,3.24031432933371,6.836807807844575,6.2502036564992975,3.82273014794452,0.918386234446348,3.258518924711301,2.1634987322828794,2.662205499653617,6.687060688339892,2.2357270598380583,5.0259146987507926,5.443606651475615,2.980025300238734,3.1193561770396756,2.8399595874895316,4.914564523493939,2.782408564927373,6.282625134191674,1.2016338611696504,5.269781238274379,3.0513721017210256,4.649615459063409,4.896271848807375,4.721919445551544,3.6182386555954547,5.045268215138534,5.020591094773247,5.305240965954483,5.663913842115978,3.3881895371560837,8.836744701966083,6.976363635732762,3.925049964727359,6.802451741275622,4.664482840364682,2.3589588258323295,6.127014141582809,7.1945598860226125,4.475084882948782,4.301587646603187,3.7441610955704103,4.05484847699562,5.464668267003444,7.33360262826828,5.6082178530214595,7.1320626778543845,4.993674361750585,5.522620761042079,7.366234775533047,7.166815851761945,3.7687136570304847,2.321928094887362,6.7873802222963375,4.788685710613533,6.025250321561575,8.162995472417697,3.4019034716079584,6.238786859587116,2.3248106034204836,7.831687267083084,3.4802651220544627,3.871843648509318,6.548128323585586,3.0268000593437154,3.1953475983222193,6.2600256559614555,5.931210274803932,7.80967167790706,3.169925001442312,3.894332742277694,2.298658315564515,6.215678596607928,7.873013392989595,7.709152756902506,8.461602224618334,4.150559676575382,7.653203362033977,9.24186401223682,5.725468512632615,4.0925457415435655,3.575312330687437,6.519321734629434,2.5185351389821804,6.137913322088407,7.904544309292717,2.568032104771279,9.816679697610459,0.765534746362977,8.646594709581654,5.037821465434967,4.388878338811989,7.9616812016062815,1.7782085763980877,8.912589990852627,3.5008020530571575,7.473786911614368,7.182890892745479,2.6205864104518777,6.628919373511362,4.842476062993939,6.013462259806563,7.1851727751412495,4.8359240742543745,4.918386234446348,4.783980413683795,4.285402218862249,6.340918270272116,3.5570424152637194,4.030336078370959,0.8073549220576041,2.2957230245399685,8.893574570329104,2.3923174227787602,7.015805293913353,8.764407200357251,6.147306698780294,5.502075956045791,8.694706111949401,2.440952198029637,8.447992374957895,4.943921326553485,6.072963271555224,8.18685706717715,5.508111681193465,2.327687364176047,2.6322682154995127,5.853496703702325,2.4724877714627436,3.412781525338476,5.751677945687579,5.8774988727835344,5.890446692679906,8.622783966156124,6.203396948546115,5.524189078449365,4.679198570566922,2.3757345385831563,7.809543066804601,3.5716768099709313,6.39694722007283,3.097610796626422,4.9415758618182375,6.668317719915721,3.0925457415435655,6.941106310946432,7.471431358002323,4.633431210355632,2.454175893185802,5.760487101811031,4.112700132749362,2.560714954474479,2.885574364371426,5.8060662260890314,5.969012307516316,3.0214797274104517,4.37016428054021,3.1811025507537978,9.463033855317493,5.102238193930738,7.537684337147519,5.960001932068081,4.451540833017832,5.931683057059805,4.126807703142035,9.109647857806625,8.028624401408491,3.2645364309990255,7.394977085526471,6.152589069287125,6.022367813028454,4.51853513898218,3.344828496997441,6.810314561497258,3.0036022366801953,7.890811454817677,8.317774372804099,7.237735785231934,8.515305605451724,8.213638902198852,5.154615611326894,2.039138393906958,7.296273846706719,6.905206468795705,2.3673710656485296,2.0942360698457656,2.9145645234939397,2.2172307162206693,5.641256998726767,3.303050084681673,2.62993940943954,7.220233204382192,6.489767697008256,10.809687753488749,5.260778431893426,8.11134436328308,4.9941271140031285,2.7070829917717063,5.856985689782205,6.381629467033664,4.493134922305505,10.642557179595622,5.439955517478156,6.790772037862,2.5058909297299574,4.37920505996061,6.33646192818282,2.9523335663696857,2.811471030529836,5.323730337521348,2.763411574470007,8.089159131911238,6.435461914479276,2.4646682670034443,7.870364719583405,3.396433531250992,7.305696981080096,4.020591094773247,5.187451054027326,2.298658315564515,6.038918989292302,5.369815424283912,4.422905742612183,4.024142345897803,4.926948247949772,5.348728154231077,6.907251224292033,4.861459166361513,6.596487956856693,4.593353770980297,5.9502351004856875,7.814614507639093,6.638797899951696,8.79720757285578,2.176322772640463,7.7974669920591575,2.7398481026993275,3.1538053360790355,3.6701605141266334,2.513490745588118,5.307064162255372,5.076815597050831,2.533563348214512,6.678494507773734,2.7092906357233577,2.7355221772965375,2.782408564927373,7.039796407620025,5.938521045865967,4.733354340613827,6.132988042627442,7.991691898336964,6.419033185471022,2.1858665453113337,5.449891473010944,2.6064422281316078,3.7311832415722,4.332707933640605,6.56498780095989,5.434294617959925,5.893362210763872,10.084529386649065,7.932923373835633,3.0823619695574735,2.440952198029637,3.1842802944193824,6.1430260038899736,6.387500406480984,6.681449265314958,4.324810603420484,2.327687364176047,2.722466024471091,5.569551851083613,7.527555198794536,6.632559052117953,2.9145645234939397,6.135452784185173,5.424922088210688,2.2357270598380583,2.8011586560936985,9.8361608416947,7.193968819421969,3.097610796626422,2.655351828612554,5.553360503335328,3.169925001442312,6.173127433480656,3.817623257511431,5.778208576398088,5.7554217347342425,8.571145863602528,5.470862199413702,3.1602748314085933,8.3389146195993,2.440952198029637,3.982765462583646,2.140778655782796,4.3197624276692475,2.292781749227846,6.864928972289788,5.969933274697856,4.950468414150123,3.4581194811745064,9.054360117597708,2.232660756790275,5.648753032989572,4.827310943949667,6.609104846618959,2.920293300211007,5.726013748860151,4.484138131201669,6.010779838753242,6.51317488460363,8.731081391094504,7.529742876502924,2.7782085763980877,6.546122758857796,4.635173946674924,5.533563348214512,6.2992080183872785,2.657640005207824,11.260084480727762,3.442280035252584,5.339850002884624,6.091065077930535,7.352175878326325,7.78959885005233,3.2418401835646704,4.584962500721156,5.035183996618336,4.898692072560488,4.393690764187454,6.686360452903688,4.792855352362489,4.044394119358453,5.077670274232752,7.061344188288391,4.06522762277562,6.802968651005583,3.412781525338476,7.953207027098,3.5643781685650637,2.790772037862,3.350497247084133,8.870919495610579,4.663913842115978,2.2234225499349374,2.6667565918848033,9.754286253961022,6.500324049284198,7.87670110864523,1.4168397419128291,4.017921907997263,6.177519202597814,5.47800105583793,8.031053274444373,7.831180547126736,6.461724991502711,3.948600847493356,2.319039815562536,4.898208352508718,6.20945336562895,7.562776656791846,5.542258049766918,2.6016965164809576,6.711081862306696,2.0565835283663674,8.630303533275018,4.854494418154875,3.4892860226258766,4.601102203376409,5.932391940068527,6.822602695756541,3.039138393906958,4.397118409042582,4.078951341394822,6.198886990956932,2.298658315564515,7.472731449369055,5.0669502439246275,5.059614856297222,2.9708536543404835,6.608661417987563,5.51443791382931,2.176322772640463,5.642412772905056,6.8820316083521,5.234577959784565,7.163498732282879,3.212569338850806,2.2898344651775093,8.307656205306488,3.423578170981797,1.3840498067951599,2.7676547982373463,10.679004987548526,3.2645364309990255,3.9990980337056072,5.649040565634448,4.793375711264754,4.203201156316611,7.668814088419117,5.911931212309049,3.4581194811745064,5.445263208140443,8.913188619508889,4.936402377725063,8.645802514606062,6.207111961207714,5.749266083814806,4.4376272483189485,8.580786171355323,2.189033824390017,2.341985747228616,4.06522762277562,4.804260115634738,4.849999259466098,5.075532631167357,4.056583528366367,9.188044796421696,3.8439838440483265,4.634012356402116,4.8104431038451745,8.686815644595628,2.521050736900963,4.016139702655253,6.212180210044431,7.545505100691072,3.904002316283692,3.71699089440494,2.1667154449664223,4.381283372503783,6.564835417010355,6.563005553062399,5.911212201558612,6.247547806890298,5.477677327565307,8.292781749227846,13.587327482778312,2.9011082430145123,2.845991770664573,3.7644735509926663,9.182369521950886,5.921959917326167,1.9297909977185974,6.910972451655129,8.149950301681047,4.3729520979118295,5.38404980679516,2.063502942306158,5.977508904540138,1.3673710656485294,4.067810783789621,2.1176950426697547,9.199893148608071,3.7081872360207084,4.628773595201645,2.488000770834068,6.501439145158873,6.706254253573203,3.0908534304511135,5.86616616903904,4.155425431747197,6.060695931687554,4.570462931026041,3.2357270598380583,5.341274183692174,6.656925339794245,5.588564737401351,6.353146825498083,3.48155728070859,2.4698859762744636,4.872828759534886,4.34766565630097,2.877744249949002,6.019479533703628,3.465974464504069,7.536363792395467,5.132576842670009,2.0531113364595623,6.283366212949919,3.699329525673156,8.199279721336625,6.8203066288938246,6.068240861312821,4.598126959919604,7.979739567128753,6.258895755075436,7.258707352196828,5.394376944957055,3.0125686735030555,6.862203399053224,2.8318772411916733,5.164705840182799,6.431288654291593,4.375734538583156,4.8594726668519375,2.9392265777282085,4.302319050948834,4.397118409042582,5.169123281476757,3.761285273361619,7.240600549955795,3.7387678368001915,5.678916986679839,4.3334237337251915,5.530445354692906,8.431288654291592,2.521050736900963,1.2265085298086797,2.2539892662307865,5.23687524536694,2.298658315564515,6.398316163400203,8.348374075191163,6.043300754364182,9.401604869714957,4.534186139090097,9.069933046375542,5.486714373030702,4.96624587322494,6.307246355226617,5.51127826508303,5.970853654340483,5.342341397431548,6.219555769166913,2.319039815562536,7.516566771496865,6.652773284510777,9.109908988534402,5.338067797542616,2.9981955031532523,6.506843098969815,3.04788732939655,2.526068811667588,3.414135532984451,6.295723024539968,2.257010618206024,5.500164679492167,5.333065878076355,5.253232938538032,3.619413010597937,4.695437110405369,4.37364821133469,8.356407951394893,6.039796407620025,5.908332569782425,1.74416109557041,5.666472568842073,5.9586107120558465,2.430285272977781,8.487277313213783,2.927896453728821,6.9920886090139955,5.7615512324444795,5.906409617094016,2.572889668420581,2.9763636357327616,3.311793717753649,5.325530331567558,6.075532631167357,8.785648628652076,5.301953395126096,6.687620632084895,10.60719158640291,5.2543672814408335,6.976821852360685,2.655351828612554,8.033092575523664,11.658760423755194,6.231317197152663,5.008092420948722,7.824449651364088,2.8155754288625725,5.656496370553495,3.218781167784069,6.101187815259738,8.328001662149257,5.576522137920503,7.176621973161577,2.0565835283663674,6.5787877956769725,6.602142090697982,2.7441610955704103,4.727375938594897,2.2172307162206693,4.552131108253784,10.754979337466484,5.651338766105931,7.449148645375437,8.21179097625254,5.769771739249448,7.09876903815488,2.5921580021253603,7.971830914780502,6.234194722953812,2.1984941536390834,6.955824240806441,0.16349873228287956,4.680324356844016,2.5801454844233804,5.583458910130768,4.845991770664573,2.7070829917717063,4.18824265626441,7.27733390042982,9.535217045453418,6.304876050044889,6.982080909763738,3.0925457415435655,7.051372101721025,7.022922589055629,2.9392265777282085,6.653203362033977,8.22838581704935,4.960697039304312,5.564378168565064,2.782408564927373,0,3.9354597478052895,5.525755692829486,1.0976107966264221,7.211790976252541,4.477677327565307,2.4436066514756147,7.966303561412599,6.19021976366016,5.592457037268081,10.665637927637095,3.277984747299765,7.8269297659895765,3.5643781685650637,5.612057559563065,8.766462652425766,2.662205499653617,6.920412408160787,5.290940402403678,7.502871574574138,5.2539892662307865,3.1984941536390834,3.0089887832272546,7.2259304111710545,0,4.919340082442012,5.334496768390418,4.654206377944292,2.327687364176047,6.705839705883682,11.02948049374083,2.4594316186372973,6.294253136444514,7.172927489603836,5.573192723781613,5.5804470195986635,6.341274183692174,6.768713657030485,4.5855634984556914,5.609400390006429,6.10055722078293,6.140165134599453,2.37851162325373,4.726831217032493,3.127633279725874,2.414135532984451,4.561326136453918,2.211012193485512,3.6147098441152083,5.901832305785489,5.903279342056139,3.84197311892718,3.0443941193584534,4.77663042305105,2.7949356628035362,2.5897634869849773,2.4436066514756147,2.5160151470036647,4.623515741490549,6.6059979802325195,9.051127353591394,3.017921907997262,3.9212458885855885,2.2387868595871168,5.621465834460144,6.001351892999959,6.605701738949982,2.6530600171045644,3.4698859762744636,2.456806149230474,4.528571318870758,10.08413071995014,3.1953475983222193,6.061128135118254,4.434961760187254,8.556735947914529,6.821837746088096,5.895302621333307,4.783456654360239,6.1147834472498515,8.8634739090894,2.6182386555954547,5.862203399053224,3.319039815562536,6.531225485053606,2.2265085298086795,5.50303064564116,6.549977142771825,2.582556003014061,0.45417589318580215,8.258094872866513,5.864186144654281,0,4.110196177754199,5.541948425333733,5.139960569545456,5.166715444966422,4.661635602335959,2.6016965164809576,1.0565835283663676,4.867402305727543,6.390082950951663,6.790641730763425,4.185073637876762,5.681168120416691,4.409390936137702,5.399854673572431,2.752748591407134,5.553974808237472,4.238786859587116,6.451211111832329,3.2555007331483865,8.627898615871748,4.5588798534754424,5.639521599510833,5.29020320512607,8.07804403634318,5.592756010441026,6.952217064973338,2.6392321632492775,3.2942531364445142,5.633721812641012,9.325777655214823,5.554588851677638,8.364703739582406,12.206535173103617,6.2268938135713885,6.058532970201611,2.996388746447621,2.169925001442312,3.2539892662307865,2.3645724322958563,2.996388746447621,2.5084286525318573,5.370861740085285,7.2461230165968376,3.19377174339668,2.4276061727818994,5.25285462595139,8.366191054189278,7.062963556559086,2.0531113364595623,7.846744023244868,3.581351247168777,6.948834425578686,6.412442824775613,2.899175630480513,4.786073551578548,8.971342367308225,7.2193621578077165,2.885574364371426,7.801029283808189,3.7813597135246595,5.6884601404244926,3.361768359419153,2.280956313831056,8.531927241876955,7.295631200395783,5.566206293465506,3.9990980337056072,6.145066014185894,3.0373822220030804,6.666614587352882,2.655351828612554,5.343763122149601,6.696689389537669,4.609991295212678,2.3950627995175777,7.005961325845812,2.2957230245399685,3.7548875021634687,6.883987308105472,3.2433644256936605],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression ridge\n","# réglage des paramètre pour la gridsearch\n","alphasridge_log = np.logspace(-3, 5, 1000)\n","param_gridRidge_log = {'ridge__alpha': alphasridge_log}\n","\n","GridRidge_log, \\\n","BestParametresRidge_log, \\\n","ScoresRidge_log, \\\n","TotalGHGEmissions_pred_logRidge_log, \\\n","figRidge_log = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=TotalGHGEmissions_train_log,\n","                            y_test=TotalGHGEmissions_test_log,\n","                            y_test_name='TotalGHGEmissions_test_log',\n","                            y_pred_name='TotalGHGEmissions_pred_logRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge_log)\n","\n","print(BestParametresRidge_log)\n","print(ScoresRidge_log)\n","figRidge_log.show()\n"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[2.093029337612897,2.093029334097565,2.0930293305168126,2.0930293268694227,2.093029323154154,2.093029319369744,2.0930293155149062,2.093029311588329,2.093029307588678,2.0930293035145935,2.0930292993646895,2.093029295137556,2.0930292908317547,2.093029286445822,2.0930292819782674,2.0930292774275716,2.0930292727921875,2.093029268070537,2.0930292632610175,2.093029258361992,2.093029253371796,2.0930292482887323,2.0930292431110717,2.0930292378370554,2.0930292324648887,2.093029226992746,2.0930292214187665,2.0930292157410553,2.093029209957682,2.0930292040666787,2.093029198066045,2.0930291919537387,2.0930291857276826,2.0930291793857587,2.0930291729258124,2.0930291663456457,2.093029159643023,2.0930291528156633,2.093029145861247,2.093029138777409,2.09302913156174,2.093029124211789,2.093029116725055,2.093029109098992,2.0930291013310085,2.0930290934184637,2.093029085358666,2.0930290771488758,2.0930290687863016,2.0930290602681,2.093029051591375,2.0930290427531766,2.0930290337504998,2.0930290245802836,2.0930290152394098,2.0930290057247034,2.0930289960329285,2.0930289861607903,2.0930289761049328,2.0930289658619357,2.093028955428318,2.093028944800531,2.0930289339749626,2.093028922947931,2.093028911715687,2.0930289002744122,2.0930288886202164,2.0930288767491376,2.0930288646571396,2.0930288523401104,2.0930288397938632,2.093028827014132,2.093028813996572,2.093028800736757,2.0930287872301787,2.093028773472245,2.093028759458279,2.093028745183515,2.0930287306430992,2.09302871583209,2.0930287007454513,2.0930286853780524,2.0930286697246694,2.0930286537799803,2.0930286375385636,2.0930286209948985,2.093028604143359,2.0930285869782166,2.0930285694936352,2.0930285516836697,2.093028533542266,2.0930285150632555,2.093028496240356,2.093028477067167,2.093028457537171,2.093028437643728,2.0930284173800735,2.0930283967393195,2.0930283757144474,2.09302835429831,2.093028332483626,2.093028310262978,2.093028287628812,2.093028264573433,2.093028241089002,2.0930282171675354,2.0930281928008996,2.0930281679808114,2.0930281426988318,2.0930281169463663,2.093028090714659,2.093028063994791,2.09302803677768,2.0930280090540716,2.093027980814541,2.0930279520494866,2.0930279227491297,2.0930278929035095,2.0930278625024785,2.093027831535702,2.0930277999926505,2.093027767862603,2.0930277351346342,2.0930277017976184,2.0930276678402215,2.0930276332509004,2.093027598017895,2.093027562129227,2.0930275255726962,2.093027488335875,2.0930274504061033,2.093027411770487,2.093027372415892,2.093027332328938,2.0930272914959978,2.093027249903191,2.0930272075363763,2.093027164381152,2.093027120422846,2.093027075646516,2.0930270300369385,2.0930269835786097,2.0930269362557365,2.093026888052231,2.0930268389517055,2.0930267889374696,2.0930267379925205,2.093026686099541,2.0930266332408887,2.093026579398596,2.093026524554359,2.0930264686895343,2.0930264117851323,2.0930263538218075,2.0930262947798566,2.0930262346392094,2.0930261733794224,2.0930261109796717,2.093026047418744,2.093025982675036,2.093025916726537,2.0930258495508314,2.093025781125083,2.0930257114260344,2.0930256404299916,2.093025568112823,2.093025494449946,2.0930254194163216,2.0930253429864454,2.0930252651343375,2.0930251858335356,2.0930251050570847,2.093025022777528,2.0930249389668996,2.093024853596711,2.093024766637945,2.093024678061045,2.0930245878359037,2.093024495931855,2.0930244023176607,2.093024306961502,2.09302420983097,2.093024110893051,2.093024010114117,2.0930239074599166,2.0930238028955594,2.0930236963855062,2.0930235878935575,2.0930234773828413,2.093023364815797,2.093023250154168,2.0930231333589857,2.093023014390555,2.0930228932084445,2.0930227697714705,2.0930226440376827,2.0930225159643507,2.093022385507949,2.093022252624144,2.0930221172677776,2.093021979392849,2.0930218389525055,2.0930216958990213,2.0930215501837828,2.093021401757274,2.093021250569056,2.0930210965677527,2.093020939701035,2.0930207799155967,2.0930206171571424,2.093020451370367,2.093020282498938,2.0930201104854738,2.093019935271527,2.0930197567975624,2.0930195750029386,2.093019389825886,2.0930192012034885,2.093019009071658,2.0930188133651155,2.093018614017369,2.0930184109606906,2.0930182041260905,2.0930179934433,2.0930177788407427,2.093017560245511,2.093017337583342,2.093017110778594,2.0930168797542166,2.0930166444317297,2.093016404731192,2.093016160571179,2.093015911868751,2.093015658539427,2.0930154004971557,2.0930151376542865,2.0930148699215403,2.093014597207979,2.093014319420974,2.0930140364661742,2.093013748247477,2.0930134546669934,2.093013155625014,2.0930128510199797,2.0930125407484415,2.0930122247050287,2.0930119027824134,2.0930115748712743,2.0930112408602577,2.0930109006359414,2.0930105540827966,2.0930102010831484,2.093009841517134,2.093009475262667,2.093009102195391,2.0930087221886406,2.093008335113398,2.0930079408382474,2.093007539229335,2.093007130150319,2.0930067134623265,2.093006289023904,2.0930058566909735,2.0930054163167804,2.093004967751846,2.0930045108439144,2.0930040454379046,2.0930035713758555,2.093003088496874,2.093002596637079,2.0930020956295485,2.0930015853042616,2.0930010654880413,2.0930005360044954,2.0929999966739605,2.092999447313436,2.092998887736527,2.0929983177533793,2.0929977371706148,2.0929971457912684,2.092996543414721,2.0929959298366287,2.09299530484886,2.092994668239421,2.0929940197923846,2.0929933592878194,2.0929926865017148,2.092992001205905,2.0929913031679943,2.092990592151277,2.092989867914656,2.0929891302125667,2.092988378794892,2.092987613406875,2.0929868337890385,2.0929860396770956,2.09298523080186,2.092984406889157,2.09298356765973,2.0929827128291505,2.092981842107716,2.0929809552003587,2.092980051806544,2.0929791316201705,2.092978194329466,2.092977239616885,2.0929762671590004,2.0929752766263965,2.092974267683557,2.0929732399887553,2.092972193193938,2.092971126944608,2.0929700408797083,2.092968934631499,2.092967807825434,2.09296666008004,2.0929654910067828,2.092964300209942,2.092963087286478,2.0929618518258954,2.0929605934101096,2.092959311613304,2.092958006001789,2.092956676133858,2.09295532155964,2.092953941820951,2.092952536451137,2.092951104974925,2.0929496469082585,2.092948161758143,2.0929466490224753,2.0929451081898827,2.092943538739549,2.092941940141043,2.092940311854144,2.092938653328659,2.092936964004244,2.092935243310215,2.0929334906653634,2.0929317054777568,2.0929298871445505,2.092928035051783,2.0929261485741746,2.092924227074921,2.0929222699054835,2.0929202764053727,2.0929182459019335,2.0929161777101215,2.0929140711322782,2.092911925457899,2.092909739963403,2.0929075139118924,2.092905246552911,2.0929029371221977,2.092900584841434,2.092898188917992,2.092895748544672,2.0928932628994383,2.09289073114515,2.092888152429289,2.0928855258836774,2.092882850624197,2.0928801257505016,2.092877350345718,2.0928745234761523,2.0928716441909843,2.0928687115219566,2.0928657244830626,2.0928626820702223,2.092859583260959,2.092856427014066,2.092853212269273,2.0928499379468968,2.0928466029474957,2.0928432061515148,2.092839746418921,2.0928362225888395,2.0928326334791767,2.0928289778862394,2.0928252545843504,2.092821462325452,2.0928175998387064,2.092813665830089,2.0928096589819725,2.0928055779527064,2.0928014213761874,2.092797187861425,2.092792875992097,2.0927884843260975,2.092784011395081,2.0927794557039916,2.0927748157305954,2.0927700899249913,2.0927652767091245,2.092760374476288,2.0927553815906137,2.0927502963865585,2.0927451171683797,2.0927398422096024,2.092734469752478,2.0927289980074324,2.0927234251525086,2.0927177493327958,2.0927119686598505,2.092706081211113,2.0927000850293034,2.0926939781218206,2.0926877584601202,2.0926814239790916,2.0926749725764173,2.092668402111927,2.09266171040694,2.092654895243594,2.0926479543641703,2.0926408854703973,2.0926336862227566,2.0926263542397634,2.09261888709725,2.092611282327625,2.092603537419127,2.092595649815072,2.0925876169130784,2.092579436064284,2.092571104572556,2.09256261969368,2.0925539786345464,2.092545178552312,2.0925362165535626,2.092527089693452,2.092517794974832,2.0925083293473734,2.0924986897066638,2.092488872893302,2.0924788756919734,2.092468694830513,2.0924583269789556,2.0924477687485696,2.0924370166908792,2.0924260672966724,2.0924149169949913,2.0924035621521133,2.0923919990705135,2.0923802239878113,2.0923682330757103,2.0923560224389117,2.092343588114022,2.092330926068441,2.092318032199235,2.092304902331996,2.0922915322196842,2.0922779175414545,2.092264053901469,2.092249936827693,2.092235561770674,2.092220924102304,2.092206019114572,2.0921908420182906,2.092175387941815,2.09215965192974,2.092143628941584,2.0921273138504555,2.092110701441704,2.092093786411551,2.09207656336571,2.092059026817985,2.092041171188856,2.092022990804046,2.0920044798930713,2.0919856325877753,2.091966442920849,2.0919469048243284,2.091927012128081,2.091906758558275,2.091886137735828,2.091865143174845,2.091843768281035,2.091822006350115,2.091799850566196,2.091777294000153,2.091754329607978,2.091730950229123,2.091707148584816,2.091682917276373,2.0916582487834874,2.0916331354625046,2.0916075695446885,2.0915815431344598,2.091555048207634,2.091528076609635,2.0915006200536967,2.091472670119055,2.091444218249117,2.0914152557496273,2.0913857737868122,2.0913557633855158,2.0913252154273216,2.0912941206486626,2.0912624696389193,2.091230252838505,2.091197460536941,2.0911640828709217,2.091130109822366,2.0910955312164625,2.091060336719704,2.0910245158379097,2.090988057914243,2.0909509521272214,2.090913187488712,2.0908747528419296,2.090835636859418,2.0907958280410344,2.090755314711923,2.090714085020483,2.090672126936337,2.09062942824829,2.090585976562287,2.090541759299374,2.0904967636936456,2.0904509767902004,2.090404385443095,2.0903569763132928,2.090308735866618,2.0902596503717117,2.0902097058979843,2.0901588883135784,2.0901071832833322,2.090054576266742,2.09000105251594,2.0899465970736673,2.0898911947712606,2.0898348302266427,2.0897774878423236,2.0897191518034064,2.089659806075604,2.0895994344032696,2.089538020307431,2.0894755470838406,2.08941199780104,2.0893473552984303,2.0892816021843634,2.08921472083424,2.089146693388629,2.0890775017513987,2.089007127587864,2.0889355523229503,2.088862757139375,2.088788722975847,2.0887134305252806,2.088636860233034,2.088558992295159,2.088479806656676,2.088399283009867,2.088317400792584,2.088234139186585,2.088149477115884,2.0880633932451222,2.0879758659779624,2.087886873455501,2.0877963935547026,2.087704403886852,2.087610881796029,2.0875158043576025,2.0874191483767426,2.0873208903869562,2.087221006648634,2.0871194731476264,2.087016265593827,2.0869113594197812,2.086804729779307,2.086696351546135,2.086586199312559,2.086474247388108,2.0863604697982225,2.0862448402829528,2.0861273322956615,2.0860079190017373,2.085886573277323,2.0857632677080424,2.0856379745877445,2.0855106659172407,2.0853813134030554,2.085249888456175,2.0851163621907967,2.0849807054230785,2.0848428886698853,2.0847028821475315,2.08456065577052,2.08441617915027,2.0842694215938424,2.0841203521026435,2.083968939371135,2.0838151517855104,2.083658957422371,2.0835003240473804,2.083339219113901,2.0831756097616134,2.083009462815112,2.0828407447824864,2.0826694218538715,2.0824954598999836,2.0823188244706254,2.0821394807931717,2.081957393771029,2.0817725279820705,2.0815848476770453,2.0813943167779674,2.0812008988764767,2.0810045572321796,2.080805254770968,2.080602954083322,2.0803976174225833,2.0801892067032295,2.079977683499119,2.0797630090417387,2.079545144218435,2.0793240495706504,2.079099685292154,2.0788720112272854,2.078640986869204,2.0784065713581623,2.078168723479796,2.0779274016634526,2.0776825639805514,2.0774341681429997,2.0771821715016516,2.076926531044844,2.076667203397001,2.0764041448173267,2.0761373111985932,2.075866658066041,2.075592140576396,2.0753137135170308,2.0750313313052677,2.0747449479878486,2.074454517240591,2.0741599923682315,2.073861326304489,2.073558471612362,2.073251380484666,2.072940004744853,2.0726242958481107,2.072304204882778,2.0719796825720858,2.0716506792762557,2.0713171449949765,2.07097902937027,2.0706362816897896,2.0702888508905644,2.069936685563211,2.069579733956645,2.069217943983319,2.0688512632250107,2.0684796389391784,2.068103018065936,2.0677213472356453,2.0673345727771784,2.0669426407268605,2.066545496838132,2.0661430865919526,2.065735355207983,2.065322247656561,2.064903708671511,2.064479682763814,2.0640501142361587,2.0636149471984124,2.0631741255840343,2.062727593167453,2.0622752935824513,2.0618171703415644,2.0613531668565384,2.060883226459855,2.06040729242737,2.0599253080020614,2.059437216418938,2.058942960931115,2.0584424848370815,2.0579357315091813,2.0574226444233297,2.056903167189976,2.056377243586339,2.0558448175899255,2.0553058334133425,2.0547602355404275,2.054207968763698,2.0536489782231317,2.053083209446291,2.052510608389791,2.0519311214821236,2.051344695667832,2.050751278453041,2.050150817952344,2.0495432629370374,2.0489285628847034,2.04830666803013,2.0476775294175598,2.047041098954253,2.046397329465357,2.0457461747500565,2.0450875896389973,2.0444215300529462,2.0437479530626796,2.0430668169500645,2.0423780812703027,2.041681706915315,2.0409776561782187,2.040265892818874,2.039546382130454,2.038819091006989,2.038083988011862,2.037341043447181,2.036590229424,2.035831519933325,2.0350648909178513,2.0342903203443807,2.0335077882768475,2.032717276949901,2.031918770842976,2.031112256754776,2.0302977238781157,2.0294751638750324,2.0286445709521046,2.027805941935895,2.0269592763484403,2.0261045764827097,2.025241847477942,2.0243710973947864,2.023492337290153,2.0226055812916903,2.02171084667179,2.020808153921047,2.0198975268210644,2.0189789925165136,2.018052581586368,2.017118328114198,2.0161762697574415,2.0152264478155493,2.0142689072969144,2.0133036969844817,2.012330869499941,2.0113504813664123,2.010362593069522,2.009367269116775,2.0083645780951285,2.0073545927266645,2.0063373899222876,2.005313050833328,2.0042816609009826,2.0032433099034903,2.0021980920009583,2.001146105777755,2.0000874542823848,1.9990222450647597,1.9979505902107935,1.9968726063742406,1.995788414805703,1.9946981413787408,1.9936019166130123,1.9924998756943815,1.9913921584919336,1.990278909571839,1.9891602782080127,1.9880364183895183,1.9869074888246687,1.9857736529417849,1.9846350788865714,1.9834919395160688,1.9823444123891687,1.98119267975364,1.980036928529671,1.9788773502898855,1.977714141235836,1.9765475021709555,1.9753776384699608,1.9742047600447123,1.9730290813065252,1.9718508211249421,1.9706702027829768,1.9694874539288403,1.9683028065241672,1.9671164967887633,1.9659287651419017,1.9647398561401839,1.9635500184120083,1.9623595045886724,1.9611685712321463,1.95997747875955,1.958786491364385,1.9575958769345565,1.9564059069672246,1.955216856480547,1.95402900392234,1.9528426310757225,1.9516580229617877,1.9504754677393443,1.9492952566017951,1.9481176836711849,1.9469430458894876,1.945771642907165,1.9446037769690663,1.943439752797703,1.942279877473963,1.9411244603152977,1.9399738127514445,1.9388282481977235,1.9376880819259519,1.9365536309330267,1.9354252138072154,1.9343031505921946,1.9331877626488758,1.9320793725150582,1.9309783037629458,1.9298848808545572,1.9287994289950667,1.927722273984109,1.9266537420650718,1.925594159772407,1.9245438537769957,1.9235031507295752,1.9224723771022767,1.9214518590282794,1.920441922139617,1.9194428914031516,1.9184550909547553,1.9174788439317023,1.9165144723033198,1.91556229669991,1.9146226362399794,1.9136958083558018,1.912782128617352,1.9118819105546485,1.9109954654785337,1.9101231022999527,1.909265127347759,1.9084218441851164,1.9075935534245452,1.9067805525416766,1.9059831356877914,1.9052015935012108,1.9044362129176284,1.9036872769794766,1.9029550646444147,1.9022398505930653,1.9015419050360969,1.9008614935207884,1.9001988767372116,1.8995543103241679,1.8989280446750478,1.898320324743763,1.897731389850939,1.8971614734905533,1.896610803137205,1.8960796000542426,1.89556807910295,1.8950764485530283,1.8946049098946183,1.8941536576520963,1.8937228791999203,1.893312754580784,1.8929234563263548,1.892555149280896,1.892207990428045,1.8918821287210683,1.8915777049168863,1.8912948514141934,1.8910336920959785,1.8907943421767777,1.8905769080549804,1.8903814871705145,1.89020816786824,1.8900570292673795,1.8899281411373068,1.8898215637800262,1.8897373479196513,1.8896755345992045,1.889636155085047,1.8896192307792332,1.8896247731400915,1.889652783611298,1.8897032535597316,1.889776164222353,1.889871486662362,1.8899891817348524,1.8901292000621854,1.890291482019272,1.8904759577289414,1.8906825470675552,1.8909111596810029,1.8911616950111934,1.891434042333144,1.8917280808027257,1.8920436795151307,1.8923806975740696,1.892738984171715,1.8931183786793597,1.8935187107487397,1.8939398004239663,1.8943814582639444,1.8948434854751857,1.895325674054846,1.895827806943839,1.8963496581898167,1.8968909931198155,1.8974515685223223,1.8980311328385056,1.8986294263623282,1.899246181449249,1.8998811227331882,1.9005339673514232,1.9012044251770621,1.9018921990587252,1.9025969850670499,1.9033184727476264,1.9040563453799515,1.9048102802419806,1.905579948879856,1.9063650173823645,1.9071651466596904,1.9079799927260068,1.9088092069854603,1.9096524365210872,1.910509324386204,1.9113795098978255,1.9122626289316362,1.9131583142180815,1.9140661956391145,1.9149859005251575,1.9159170539518442,1.9168592790361003,1.9178121972311484,1.9187754286200114,1.919748592207118,1.9207313062076097,1.9217231883339623,1.92272385607956,1.9237329269988528,1.924750018983755,1.9257747505359533,1.9268067410348066,1.9278456110005329,1.9288909823523928,1.9299424786616022,1.930999725398716,1.9320623501752288,1.9331299829791866,1.9342022564045844,1.9352788058743573,1.9363592698567942,1.937443290075203,1.9385305117106832,1.9396205835978761,1.9407131584135757,1.9418078928580982,1.9429044478293218,1.944002488589332,1.9451016849236098,1.94620171129272,1.9473022469764725,1.9484029762105362,1.949503588315507,1.950603777818434,1.9517032445668288,1.9528016938351875,1.953898836424069,1.9549943887517873,1.956088072938778,1.9571796168847189,1.9582687543384811,1.9593552249610091,1.9604387743812304,1.9615191542450972,1.9625961222578912,1.9636694422198908,1.9647388840555593,1.965804223836366,1.9668652437973946,1.9679217323478837,1.9689734840758475,1.970020299746934,1.9710619862976853,1.9720983568233528,1.9731292305604462,1.9741544328641705,1.9751737951809392,1.9761871550161207,1.9771943558971998,1.9781952473325373,1.9791896847658854,1.9801775295268549,1.9811586487775017,1.982132915455211,1.9831002082120555,1.9840604113508078,1.9850134147577752,1.9859591138326338,1.9868974094154368,1.9878282077109621,1.988751420210571,1.989666963611743,1.9905747597354497,1.9914747354415325]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[2.6400739765313586,2.6400739669511886,2.64007395719273,2.6400739472526666,2.6400739371276174,2.6400739268141398,2.6400739163087277,2.6400739056078084,2.6400738947077445,2.6400738836048294,2.640073872295288,2.640073860775276,2.6400738490408737,2.640073837088094,2.6400738249128723,2.640073812511069,2.640073799878469,2.640073787010772,2.6400737739036084,2.6400737605525184,2.6400737469529654,2.6400737331003237,2.640073718989882,2.640073704616845,2.6400736899763246,2.6400736750633427,2.6400736598728303,2.6400736443996218,2.6400736286384556,2.6400736125839726,2.6400735962307165,2.640073579573125,2.640073562605535,2.6400735453221764,2.6400735277171754,2.640073509784544,2.6400734915181863,2.6400734729118907,2.640073453959331,2.640073434654065,2.6400734149895264,2.6400733949590314,2.6400733745557687,2.6400733537727996,2.64007333260306,2.6400733110393517,2.6400732890743424,2.640073266700565,2.6400732439104106,2.6400732206961326,2.6400731970498366,2.6400731729634828,2.6400731484288826,2.640073123437695,2.6400730979814204,2.6400730720514067,2.640073045638835,2.6400730187347268,2.6400729913299346,2.6400729634151396,2.6400729349808527,2.6400729060174033,2.6400728765149477,2.640072846463452,2.6400728158526996,2.6400727846722836,2.6400727529116024,2.640072720559857,2.6400726876060485,2.6400726540389723,2.6400726198472158,2.640072585019154,2.640072549542945,2.640072513406528,2.6400724765976165,2.6400724391036947,2.6400724009120164,2.6400723620095947,2.6400723223832037,2.640072282019372,2.640072240904376,2.640072199024235,2.6400721563647105,2.6400721129112994,2.6400720686492276,2.6400720235634467,2.6400719776386268,2.6400719308591554,2.640071883209127,2.6400718346723404,2.640071785232294,2.64007173487218,2.6400716835748743,2.6400716313229373,2.640071578098605,2.6400715238837797,2.6400714686600306,2.640071412408582,2.640071355110308,2.640071296745729,2.640071237295003,2.6400711767379144,2.6400711150538783,2.640071052221919,2.6400709882206788,2.6400709230283956,2.6400708566229056,2.6400707889816326,2.640070720081579,2.640070649899321,2.6400705784109975,2.6400705055923033,2.640070431418482,2.6400703558643164,2.640070278904118,2.6400702005117243,2.6400701206604813,2.640070039323243,2.6400699564723564,2.6400698720796534,2.6400697861164426,2.6400696985535,2.6400696093610554,2.640069518508785,2.640069425965802,2.640069331700646,2.6400692356812683,2.640069137875024,2.6400690382486633,2.6400689367683166,2.640068833399482,2.6400687281070194,2.640068620855132,2.6400685116073577,2.6400684003265553,2.6400682869748953,2.6400681715138403,2.64006805390414,2.640067934105809,2.6400678120781227,2.6400676877795943,2.640067561167969,2.640067432200205,2.6400673008324587,2.640067167020068,2.640067030717545,2.640066891878553,2.640066750455893,2.640066606401488,2.640066459666367,2.6400663102006465,2.640066157953517,2.6400660028732226,2.6400658449070438,2.6400656840012804,2.640065520101233,2.640065353151187,2.6400651830943884,2.640065009873026,2.640064833428218,2.6400646536999823,2.6400644706272254,2.6400642841477118,2.6400640941980527,2.6400639007136775,2.640063703628815,2.64006350287647,2.640063298388402,2.6400630900951,2.6400628779257604,2.640062661808262,2.6400624416691416,2.6400622174335697,2.640061989025326,2.6400617563667694,2.6400615193788166,2.6400612779809123,2.6400610320910034,2.6400607816255106,2.6400605264992976,2.6400602666256456,2.6400600019162246,2.640059732281059,2.640059457628499,2.640059177865193,2.640058892896051,2.6400586026242125,2.640058306951018,2.6400580057759737,2.6400576989967117,2.640057386508964,2.6400570682065236,2.640056743981204,2.64005641372281,2.6400560773190964,2.640055734655729,2.640055385616247,2.6400550300820234,2.640054667932227,2.640054299043777,2.640053923291303,2.6400535405471053,2.6400531506811076,2.6400527535608136,2.640052349051266,2.6400519370149924,2.640051517311968,2.640051089799565,2.640050654332497,2.6400502107627815,2.640049758939681,2.6400492987096573,2.6400488299163136,2.6400483524003486,2.640047865999496,2.640047370548472,2.640046865878921,2.6400463518193558,2.6400458281951007,2.6400452948282314,2.6400447515375176,2.6400441981383573,2.640043634442714,2.6400430602590594,2.640042475392302,2.6400418796437237,2.6400412728109104,2.6400406546876876,2.640040025064043,2.640039383726065,2.6400387304558626,2.6400380650314945,2.6400373872268945,2.6400366968117925,2.6400359935516384,2.6400352772075215,2.6400345475360916,2.640033804289475,2.6400330472151885,2.640032276056058,2.6400314905501285,2.640030690430578,2.640029875425621,2.640029045258426,2.640028199647014,2.640027338304165,2.640026460937322,2.6400255672484922,2.640024656934144,2.640023729685106,2.640022785186461,2.6400218231174417,2.640020843151318,2.6400198449552916,2.6400188281903816,2.640017792511305,2.640016737566368,2.64001566299734,2.640014568439338,2.6400134535206994,2.6400123178628574,2.64001116108021,2.640009982779999,2.6400087825621674,2.640007560019227,2.6400063147361226,2.6400050462900895,2.640003754250513,2.6400024381787808,2.640001097628134,2.6399997321435196,2.639998341261435,2.6399969245097705,2.639995481407648,2.6399940114652676,2.6399925141837306,2.639990989054878,2.6399894355611186,2.6399878531752514,2.6399862413602895,2.6399845995692823,2.6399829272451205,2.639981223820363,2.639979488717034,2.639977721346431,2.6399759211089293,2.6399740873937776,2.6399722195788904,2.6399703170306434,2.6399683791036566,2.639966405140574,2.6399643944718476,2.6399623464155133,2.639960260276951,2.6399581353486625,2.6399559709100284,2.6399537662270633,2.6399515205521737,2.6399492331239025,2.6399469031666767,2.639944529890542,2.6399421124909006,2.6399396501482415,2.639937142027863,2.639934587279591,2.639931985037499,2.6399293344196106,2.6399266345276082,2.6399238844465285,2.63992108324446,2.639918229972224,2.639915323663062,2.6399123633323067,2.6399093479770563,2.639906276575836,2.639903148088256,2.6398999614546645,2.63989671559579,2.639893409412384,2.6398900417848474,2.6398866115728636,2.6398831176150113,2.6398795587283774,2.639875933708162,2.639872241327277,2.6398684803359362,2.6398646494612343,2.6398607474067277,2.6398567728519957,2.639852724452208,2.639848600837666,2.6398444006133572,2.6398401223584824,2.6398357646259845,2.63983132594207,2.639826804805714,2.6398221996881626,2.639817509032424,2.639812731252757,2.6398078647341308,2.6398029078317027,2.639797858870262,2.639792716143681,2.6397874779143446,2.6397821424125762,2.639776707836049,2.6397711723491946,2.639765534082589,2.639759791132338,2.6397539415594453,2.6397479833891717,2.6397419146103864,2.6397357331748976,2.6397294369967805,2.639723023951687,2.639716491876144,2.639709838566849,2.639703061779932,2.6396961592302275,2.6396891285905184,2.6396819674907706,2.639674673517357,2.6396672442122635,2.6396596770722836,2.6396519695481966,2.6396441190439344,2.639636122915731,2.6396279784712564,2.6396196829687373,2.639611233616062,2.639602627569868,2.6395938619346166,2.6395849337616473,2.639575840048216,2.639566577736524,2.639557143712716,2.6395475348058746,2.639537747786989,2.639527779367906,2.639517626200268,2.639507284874427,2.639496751918341,2.639486023796456,2.639475096908559,2.6394639675886227,2.639452632103618,2.6394410866523184,2.6394293273640765,2.6394173502975766,2.6394051514395755,2.6393927267036106,2.6393800719287,2.6393671828779994,2.639354055237461,2.6393406846144494,2.6393270665363446,2.639313196449118,2.6392990697158836,2.639284681615425,2.6392700273407033,2.6392551019973256,2.639239900602007,2.639224418080989,2.639208649268441,2.639192588904837,2.639176231635295,2.6391595720079,2.6391426044719903,2.6391253233764247,2.6391077229678075,2.6390897973887,2.6390715406757916,2.6390529467580413,2.639034009454798,2.6390147224738754,2.638995079409612,2.6389750737408857,2.6389546988291057,2.6389339479161604,2.638912814122348,2.638891290444264,2.638869369752653,2.638847044790231,2.6388243081694727,2.638801152370361,2.6387775697381044,2.638753552480811,2.6387290926671385,2.6387041822238935,2.638678812933604,2.6386529764320477,2.6386266642057445,2.6385998675894085,2.6385725777633633,2.6385447857509168,2.638516482415695,2.638487658458933,2.63845830441673,2.638428410657261,2.6383979673779416,2.638366964602561,2.6383353921783597,2.6383032397730712,2.6382704968719235,2.638237152774585,2.6382031965920767,2.6381686172436334,2.638133403453522,2.6380975437478127,2.6380610264511057,2.638023839683206,2.637985971355761,2.6379474091688384,2.637908140607469,2.6378681529381267,2.6378274332051754,2.6377859682272558,2.6377437445936267,2.637700748660457,2.6376569665470644,2.637612384132112,2.6375669870497447,2.637520760685676,2.637473690173233,2.6374257603893363,2.6373769559504394,2.6373272612084078,2.6372766602463535,2.637225136874407,2.63717267462545,2.6371192567507795,2.637064866215734,2.637009485695255,2.6369530975694007,2.636895683918807,2.636837226520089,2.636777706841197,2.636717106036713,2.6366554049430944,2.636592584073863,2.6365286236147494,2.6364635034187667,2.636397203001247,2.6363297015348146,2.636260977844307,2.6361910104016517,2.6361197773206717,2.636047256351859,2.6359734248770805,2.6358982599042355,2.6358217380618663,2.635743835593707,2.6356645283531925,2.635583791797906,2.6355016009839827,2.635417930560462,2.6353327547635845,2.6352460474110497,2.6351577818962184,2.6350679311822636,2.6349764677962857,2.634883363823371,2.6347885909006052,2.6346921202110467,2.6345939224776522,2.6344939679571526,2.634392226433902,2.634288667213666,2.634183259117381,2.6340759704748695,2.633966769118516,2.6338556223769043,2.6337424970684213,2.633627359494819,2.633510175434746,2.6333909101372406,2.6332695283152026,2.633145994138815,2.633020271228953,2.632892322650557,2.632762110905973,2.632629597928278,2.632494745074574,2.632357513119252,2.6322178622472485,2.632075752047273,2.631931141505012,2.631783988996323,2.63163425228041,2.6314818884929805,2.6313268541393913,2.631169105087788,2.6310085965622214,2.6308452831357645,2.630679118723622,2.630510056576227,2.6303380492723383,2.6301630487121326,2.629985006110297,2.629803871989118,2.62961959617157,2.629432127774411,2.629241415201279,2.6290474061357925,2.628850047534657,2.6286492856207793,2.628445065876388,2.628237333036166,2.6280260310803936,2.627811103228094,2.627592491930201,2.6273701388627337,2.6271439849199867,2.6269139702077307,2.626680034036431,2.626442114914472,2.626200150541413,2.6259540778012385,2.625703832755638,2.6254493506372936,2.6251905658431838,2.6249274119279047,2.6246598215969907,2.6243877267002746,2.624111058225229,2.623829746290343,2.6235437201384944,2.6232529081303406,2.622957237737708,2.622656635536999,2.622351027202589,2.622040337500244,2.621724490280517,2.621403408472167,2.6210770140755577,2.6207452281560544,2.6204079708374186,2.6200651612951846,2.619716717750031,2.6193625574611215,2.619002596719445,2.618636750841123,2.6182649341606927,2.6178870600243638,2.6175030407832436,2.617112787786529,2.6167162113746594,2.616313220872429,2.6159037245820587,2.61548762977621,2.615064842690972,2.6146352685187617,2.614198811401205,2.61375537442193,2.6133048595993205,2.612847167879192,2.612382199127407,2.6119098521224355,2.6114300245478246,2.61094261298462,2.6104475129037046,2.609944618658076,2.6094338234750483,2.608915019448388,2.608388097530381,2.6078529475238383,2.607309458074032,2.606757516660582,2.606197009589274,2.605627821983847,2.605049837777713,2.6044629397056633,2.603867009295524,2.603261926859811,2.602647571487355,2.602023821034945,2.601390552118969,2.6007476401070937,2.6000949591099767,2.5994323819730387,2.5987597802683147,2.5980770242863938,2.5973839830284735,2.596680524198563,2.5959665141958244,2.5952418181071253,2.5945062996997867,2.5937598214145834,2.5930022443590133,2.592233428300873,2.59145323166217,2.5906615115134213,2.589858123568355,2.58904292217907,2.588215760331697,2.587376489642593,2.586524960355121,2.585661021337076,2.584784520078777,2.583895302691908,2.582993213909142,2.5820780970846124,2.5811497941952837,2.5802081458432826,2.5792529912592626,2.5782841683068343,2.577301513488163,2.5763048619507782,2.575294047495661,2.5742689025866925,2.573229258361522,2.5721749446439377,2.571105789957798,2.5700216215426233,2.5689222653708885,2.5678075461671366,2.5666772874289463,2.5655313114498703,2.564369439344396,2.56319149107503,2.5619972854815627,2.560786640312615,2.559559372259542,2.558315296992756,2.557054229200578,2.5557759826306787,2.554480370134184,2.553167203712542,2.551836294567211,2.5504874531522512,2.5491204892298978,2.5477352119291896,2.5463314298077098,2.5449089509165317,2.543467582868422,2.5420071329093656,2.540527407993486,2.5390282148614145,2.5375093601221614,2.5359706503385526,2.534411892116278,2.5328328921965957,2.531233457552739,2.5296133954900744,2.52797251375002,2.5263106206177923,2.524627525033972,2.5229230367099396,2.521196966247178,2.5194491252604614,2.517679326504939,2.5158873840071116,2.5140731131996947,2.5122363310603624,2.510376856254348,2.508494509280884,2.5065891126234563,2.50466049090382,2.5027084710397602,2.500732882406516,2.4987335570018465,2.496710329614648,2.49466303799706,2.4925915230399975,2.4904956289519964,2.4883752034413043,2.486230097901103,2.4840601675977587,2.4818652718619907,2.479645274282823,2.4774000429042045,2.4751294504241406,2.472833374396217,2.4705116974333357,2.4681643074135153,2.4657910976875956,2.4633919672886555,2.4609668211429696,2.4585155702823176,2.456038132057442,2.453534430352456,2.451004395799991,2.4484479659968654,2.4458650857200546,2.44325570714273,2.4406197900501385,2.4379573020550773,2.4352682188127193,2.4325525242345423,2.429810210701108,2.42704127927343,2.424245739902663,2.421423611637862,2.418574922831531,2.4156997113426866,2.4127980247371785,2.4098699204849754,2.4069154661541505,2.403934739601281,2.4009278291579874,2.39789483381334,2.3948358633918447,2.391751038726744,2.388640491828346,2.3855043660471242,2.382342816231306,2.3791560088786916,2.3759441222824407,2.372707346670561,2.3694458843388575,2.36615994977709,2.3628497697880886,2.359515583599608,2.3561576429686752,2.352776212278222,2.349371568625784,2.3459440019040585,2.3424938148731256,2.3390213232241477,2.335526855634368,2.332010753813239,2.328473372539518,2.324915079689196,2.321336256254102,2.317737296351082,2.314118607221607,2.310480609221747,2.306823735802371,2.3031484334795427,2.2994551617950028,2.295744393266709,2.2920166133293765,2.2882723202649924,2.2845120251232833,2.2807362516321223,2.276945536097892,2.273140427295807,2.269321486350231,2.265489286605019,2.261644413483944,2.2577874643412654,2.253919048302507,2.2500397860955275,2.2461503098719824,2.242251263019273,2.238343299963087,2.2344270859606663,2.2305032968849225,2.2265726189995276,2.2226357487251422,2.2186933923969185,2.2147462660134414,2.2107950949772803,2.206840613827302,2.2028835659629435,2.19892470336061,2.194964786282393,2.191004582977288,2.1870448693751174,2.183086428773339,2.179130051516959,2.1751765346717287,2.1712266816908423,2.1672813020753408,2.1633412110284165,2.159407229103833,2.1554801818486737,2.151560899440611,2.1476502163199154,2.1437489708164135,2.1398580047715954,2.1359781631560844,2.1321102936826715,2.1282552464151343,2.1244138733730313,2.1205870281326873,2.1167755654245846,2.1129803407273453,2.1092022098585432,2.10544202856253,2.1017006520955035,2.0979789348080127,2.0942777297251394,2.090597888124551,2.0869402591126622,2.083305689199121,2.079695021869862,2.0761090971589455,2.072548751219444,2.0690148158936172,2.065508118282634,2.062029480316109,2.058579718321736,2.055159642595295,2.0517700569713555,2.0484117583949564,2.045085536494623,2.041792173157035,2.038532442103716,2.035307108470114,2.032116928387457,2.028962648567798,2.0258450058926574,2.0227647270057205,2.0197225279100373,2.0167191135702023,2.0137551775200135,2.0108314014761053,2.0079484549580955,2.0051069949157783,2.0023076653639027,1.999551097025118,1.9968379069816264,1.9941686983361224,1.9915440598825953,1.9889645657875263,1.98643077528205,1.9839432323655917,1.9815024655214726,1.979108987444954,1.9767632947840994,1.9744658678938214,1.972217170603352,1.9700176499973245,1.9678677362104973,1.9657678422360503,1.963718363747201,1.961719678931706,1.9597721483385975,1.9578761147362498,1.9560319029805815,1.954239819891873,1.9525001541382994,1.950813176123864,1.9491791378779542,1.9475982729431942,1.946070796257732,1.9445969040274182,1.9431767735827052,1.9418105632143146,1.940498411980978,1.9392404394817253,1.93803674558439,1.9368874101011573,1.9357924924011924,1.9347520309496657,1.9337660427618413,1.9328345227604604,1.9319574430243924,1.9311347519165925,1.9303663730798646,1.9296522042898345,1.9289921161560903,1.9283859506646168,1.927833519557679,1.9273346025512044,1.9268889453945295,1.9264962577832792,1.9261562111429296,1.9258684363084018,1.9256325211335203,1.925448008073233,1.9253143917906979,1.925231116850295,1.925197575565787,1.9252131060795699,1.925276990753621,1.925388454954688,1.9255466663148784,1.9257507345436715,1.9259997118581802,1.9262925940852775,1.9266283224722094,1.9270057862221635,1.9274238257488523,1.9278812366205869,1.928376774140923,1.92890915849106,1.929477080340072,1.9300792068138637,1.9307141877032916,1.9313806617866907,1.9320772631421246,1.932802627329911,1.9335553973355923,1.9343342291768504,1.9351377970938286,1.935964798259827,1.9368139569674685,1.9376840282631305,1.9385738010190139,1.9394821004470604,1.9404077900716146,1.9413497731881049,1.9423069938429658,1.9432784373757108,1.9442631305676041,1.9452601414430881,1.9462685787702327,1.9472875913053265,1.948316366824608,1.9493541309832991,1.9504001460387719,1.9514537094711546,1.9525141525309637,1.953580838739731,1.9546531623660905,1.9557305468964616,1.9568124435164205,1.9578983296160606,1.9589877073301438,1.9600801021216248,1.961175061415193,1.9622721532858076,1.9633709652057432,1.964471102852457,1.965572188978568,1.9666738623443696,1.9677757767126336,1.9688775999048675,1.9699790129177643,1.9710797090982148,1.9721793933749951,1.9732777815450397,1.9743745996120652,1.9754695831752198,1.9765624768653731,1.9776530338266385,1.9787410152407254,1.9798261898917242,1.9809083337689875,1.9819872297057812,1.983062667051482,1.9841344413751185,1.9852023541981454,1.9862662127544077,1.9873258297753174,1.9883810232983432,1.9894316164970018,1.9904774375305865,1.9915183194119732,1.9925540998918951,1.9935846213581745,1.9946097307484423,1.9956292794749768,1.996643123360358,1.9976511225826772,1.998653141629153,1.9996490492570447,2.000638718460822,2.0016220264446267,2.0025988545991273,2.0035690884819077,2.004532617800623,2.005489336398202,2.0064391422394268,2.0073819373982924,2.008317628045591,2.00924612443623,2.0101673408958396]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[1.545984698694435,1.5459847012439418,1.5459847038408951,1.5459847064861787,1.5459847091806909,1.5459847119253483,1.5459847147210848,1.5459847175688495,1.5459847204696118,1.5459847234243576,1.545984726434091,1.5459847294998363,1.5459847326226357,1.5459847358035503,1.5459847390436627,1.545984742344074,1.5459847457059062,1.5459847491303016,1.5459847526184265,1.5459847561714652,1.5459847597906267,1.545984763477141,1.5459847672322615,1.5459847710572658,1.5459847749534528,1.5459847789221492,1.5459847829647027,1.5459847870824888,1.5459847912769082,1.5459847955493848,1.5459847999013734,1.5459848043343523,1.54598480884983,1.545984813449341,1.5459848181344493,1.5459848229067474,1.5459848277678594,1.5459848327194359,1.545984837763163,1.5459848429007532,1.5459848481339542,1.5459848534645464,1.5459848588943412,1.5459848644251843,1.5459848700589571,1.5459848757975754,1.5459848816429893,1.5459848875971867,1.5459848936621925,1.5459848998400674,1.5459849061329138,1.5459849125428702,1.5459849190721167,1.5459849257228726,1.545984932497399,1.5459849393980003,1.5459849464270219,1.545984953586854,1.545984960879931,1.5459849683087317,1.5459849758757835,1.5459849835836588,1.5459849914349777,1.5459849994324095,1.545985007578674,1.545985015876541,1.5459850243288307,1.5459850329384182,1.5459850417082306,1.5459850506412485,1.5459850597405107,1.5459850690091104,1.5459850784501987,1.5459850880669856,1.5459850978627412,1.545985107840795,1.5459851180045414,1.5459851283574346,1.5459851389029946,1.545985149644808,1.5459851605865267,1.54598517173187,1.5459851830846283,1.5459851946486611,1.5459852064278996,1.5459852184263503,1.5459852306480906,1.5459852430972778,1.5459852557781435,1.545985268694999,1.5459852818522377,1.5459852952543314,1.5459853089058373,1.5459853228113967,1.5459853369757375,1.5459853514036759,1.5459853661001164,1.5459853810700572,1.545985396318587,1.545985411850891,1.5459854276722487,1.5459854437880414,1.5459854602037464,1.5459854769249464,1.5459854939573252,1.545985511306675,1.5459855289788935,1.5459855469799901,1.5459855653160848,1.5459855839934113,1.54598560301832,1.545985622397279,1.5459856421368778,1.5459856622438268,1.5459856827249634,1.5459857035872488,1.5459857248377782,1.545985746483776,1.5459857685326006,1.5459857909917503,1.5459858138688585,1.545985837171706,1.545985860908213,1.5459858850864518,1.545985909714641,1.5459859348011546,1.5459859603545216,1.5459859863834298,1.5459860128967289,1.5459860399034338,1.5459860674127242,1.5459860954339542,1.545986123976652,1.5459861530505181,1.5459861826654402,1.5459862128314863,1.5459862435589122,1.5459862748581639,1.5459863067398834,1.5459863392149091,1.5459863722942826,1.5459864059892503,1.545986440311268,1.5459864752720034,1.5459865108833426,1.5459865471573941,1.5459865841064881,1.5459866217431883,1.5459866600802892,1.5459866991308244,1.5459867389080717,1.5459867794255517,1.5459868206970417,1.5459868627365712,1.545986905558433,1.5459869491771858,1.545986993607658,1.545987038864955,1.5459870849644624,1.5459871319218537,1.5459871797530913,1.5459872284744374,1.5459872781024546,1.5459873286540162,1.545987380146306,1.545987432596831,1.5459874860234217,1.5459875404442411,1.5459875958777907,1.5459876523429146,1.5459877098588093,1.5459877684450278,1.5459878281214867,1.5459878889084733,1.5459879508266523,1.5459880138970732,1.5459880781411774,1.545988143580804,1.5459882102381992,1.545988278136024,1.5459883472973588,1.545988417745716,1.545988489505043,1.5459885625997347,1.54598863705464,1.545988712895068,1.5459887901468,1.5459888688360968,1.545988948989709,1.5459890306348827,1.545989113799372,1.5459891985114476,1.545989284799906,1.5459893726940788,1.5459894622238446,1.5459895534196364,1.5459896463124543,1.5459897409338748,1.5459898373160614,1.5459899354917783,1.545990035494395,1.5459901373579057,1.545990241116935,1.545990346806752,1.545990454463282,1.5459905641231195,1.5459906758235376,1.545990789602505,1.5459909054986962,1.5459910235515033,1.545991143801053,1.5459912662882191,1.545991391054634,1.545991518142705,1.5459916475956286,1.545991779457405,1.5459919137728515,1.5459920505876215,1.5459921899482154,1.5459923319019997,1.5459924764972208,1.545992623783024,1.5459927738094672,1.545992926627541,1.5459930822891832,1.5459932408472983,1.5459934023557733,1.5459935668695004,1.5459937344443904,1.5459939051373943,1.5459940790065219,1.5459942561108633,1.5459944365106075,1.5459946202670611,1.545994807442673,1.5459949981010515,1.545995192306989,1.5459953901264836,1.5459955916267591,1.5459957968762903,1.5459960059448252,1.545996218903409,1.5459964358244076,1.5459966567815333,1.5459968818498688,1.5459971111058926,1.545997344627505,1.5459975824940566,1.5459978247863715,1.545998071586777,1.5459983229791323,1.5459985790488553,1.5459988398829503,1.5459991055700422,1.5459993762004007,1.5459996518659762,1.5459999326604281,1.5460002186791546,1.546000510019332,1.5460008067799387,1.5460011090617956,1.5460014169675977,1.5460017306019478,1.5460020500713934,1.5460023754844645,1.5460027069517062,1.5460030445857198,1.5460033885011981,1.5460037388149677,1.5460040956460244,1.5460044591155775,1.5460048293470883,1.5460052064663121,1.546005590601343,1.5460059818826533,1.5460063804431412,1.546006786418176,1.54600719994564,1.5460076211659781,1.546008050222247,1.5460084872601598,1.546008932428137,1.5460093858773574,1.5460098477618085,1.5460103182383382,1.5460107974667094,1.546011285609652,1.5460117828329192,1.546012289305345,1.546012805198897,1.546013330688738,1.5460138659532858,1.5460144111742706,1.546014966536799,1.5460155322294145,1.5460161084441628,1.5460166953766565,1.5460172932261405,1.5460179021955578,1.5460185224916243,1.5460191543248902,1.5460197979098165,1.546020453464846,1.546021121212478,1.5460218013793408,1.5460224941962708,1.5460231998983902,1.5460239187251847,1.5460246509205855,1.5460253967330506,1.5460261564156514,1.5460269302261542,1.5460277184271098,1.5460285212859413,1.5460293390750321,1.546030172071824,1.546031020558901,1.5460318848240941,1.5460327651605725,1.5460336618669435,1.5460345752473557,1.5460355056115966,1.546036453275201,1.5460374185595538,1.5460384017920032,1.5460394033059657,1.54604042344104,1.5460414625431222,1.5460425209645212,1.5460435990640784,1.5460446972072845,1.546045815766408,1.5460469551206155,1.5460481156561015,1.5460492977662177,1.5460505018516042,1.5460517283203252,1.5460529775880056,1.5460542500779697,1.5460555462213827,1.5460568664573984,1.5460582112333037,1.546059581004668,1.5460609762354975,1.546062397398391,1.5460638449746964,1.5460653194546725,1.546066821337654,1.5460683511322184,1.5460699093563532,1.5460714965376345,1.5460731132133985,1.5460747599309241,1.5460764372476148,1.5460781457311816,1.5460798859598395,1.5460816585224948,1.5460834640189445,1.5460853030600723,1.5460871762680595,1.5460890842765842,1.5460910277310378,1.5460930072887398,1.5460950236191522,1.546097077404108,1.5460991693380342,1.5461013001281825,1.5461034704948688,1.5461056811717073,1.5461079329058558,1.5461102264582642,1.5461125626039294,1.5461149421321463,1.5461173658467753,1.5461198345665055,1.5461223491251261,1.5461249103718044,1.5461275191713644,1.546130176404573,1.5461328829684327,1.5461356397764767,1.5461384477590716,1.5461413078637216,1.546144221055386,1.5461471883167903,1.546150210648757,1.5461532890705318,1.5461564246201172,1.5461596183546185,1.546162871350586,1.5461661847043726,1.546169559532491,1.546172996971983,1.5461764981807882,1.5461800643381265,1.5461836966448828,1.5461873963239992,1.546191164620876,1.5461950028037799,1.5461989121642525,1.5462028940175392,1.5462069497030104,1.5462110805846025,1.5462152880512603,1.5462195735173894,1.5462239384233118,1.5462283842357412,1.5462329124482501,1.5462375245817586,1.546242222185027,1.5462470068351537,1.5462518801380882,1.5462568437291468,1.5462618992735426,1.5462670484669192,1.546272293035901,1.5462776347386409,1.5462830753653949,1.5462886167390897,1.5462942607159058,1.5463000091858805,1.5463058640735037,1.5463118273383372,1.5463179009756391,1.5463240870169987,1.5463303875309884,1.546336804623813,1.546343340439987,1.5463499971630101,1.5463567770160602,1.5463636822626992,1.546370715207583,1.5463778781971957,1.5463851736205836,1.546392603910109,1.546400171542216,1.546407879038206,1.5464157289650284,1.5464237239360838,1.5464318666120411,1.546440159701666,1.5464486059626674,1.5464572082025512,1.5464659692794969,1.5464748921032383,1.5464839796359673,1.5464932348932487,1.546502660944948,1.5465122609161797,1.5465220379882627,1.546531995399703,1.5465421364471772,1.5465524644865476,1.546562982933879,1.5465736952664817,1.5465846050239689,1.5465957158093255,1.5466070312900029,1.5466185551990232,1.5466302913361034,1.5466422435687988,1.5466544158336637,1.5466668121374263,1.546679436558187,1.5466922932466338,1.5467053864272728,1.5467187203996842,1.5467322995397892,1.5467461283011434,1.5467602112162484,1.5467745528978774,1.5467891580404278,1.5468040314212952,1.5468191779022558,1.5468346024308834,1.5468503100419813,1.5468663058590333,1.546882595095679,1.5468991830572119,1.5469160751420934,1.5469332768434967,1.5469507937508653,1.5469686315514992,1.5469867960321602,1.5470052930807026,1.5470241286877253,1.547043308948248,1.547062840063409,1.5470827283421893,1.547102980203158,1.5471236021762436,1.547144600904527,1.547165983146062,1.5471877557757185,1.5472099257870489,1.5472325002941814,1.5472554865337407,1.5472788918667886,1.547302723780792,1.5473269898916189,1.5473516979455577,1.5473768558213612,1.5474024715323198,1.547428553228361,1.5474551091981674,1.5474821478713336,1.5475096778205408,1.5475377077637584,1.5475662465664781,1.547595303243966,1.5476248869635527,1.5476550070469415,1.5476856729725443,1.5477168943778552,1.547748681061834,1.5477810429873333,1.5478139902835455,1.547847533248476,1.5478816823514476,1.5479164482356333,1.5479518417206128,1.5479878738049582,1.5480245556688492,1.548061898676717,1.5480999143799083,1.5481386145193914,1.5481780110284726,1.548218116035557,1.5482589418669246,1.548300501049541,1.5483428063138942,1.5483858705968592,1.5484297070445914,1.5484743290154441,1.5485197500829173,1.548565984038635,1.5486130448953432,1.5486609468899475,1.5487097044865634,1.5487593323796087,1.5488098454969101,1.5488612590028472,1.5489135883015182,1.548966849039935,1.5490210571112435,1.5490762286579707,1.5491323800753058,1.549189528014395,1.5492476893856741,1.5493068813622235,1.5493671213831512,1.5494284271570002,1.5494908166651813,1.549554308165439,1.5496189201953376,1.5496846715757724,1.5497515814145115,1.5498196691097634,1.5498889543537673,1.5499594571364104,1.5500311977488737,1.5501041967873006,1.5501784751564944,1.5502540540736376,1.5503309550720388,1.5504092000049097,1.5504888110491597,1.5505698107092218,1.550652221820906,1.5507360675552713,1.5508213714225287,1.5509081572759724,1.5509964493159283,1.5510862720937386,1.5511776505157653,1.5512706098474203,1.551365175717227,1.5514613741209002,1.551559231425458,1.55165877437336,1.5517600300866659,1.551863026071227,1.5519677902209006,1.5520743508217933,1.5521827365565268,1.5522929765085338,1.5524051001663803,1.5525191374281113,1.5526351186056262,1.5527530744290767,1.5528730360512983,1.5529950350522588,1.553119103443537,1.5532452736728304,1.5533735786284812,1.5535040516440353,1.553636726502817,1.5537716374425372,1.5539088191599184,1.5540483068153474,1.5541901360375463,1.5543343429282674,1.5544809640670099,1.554630036515753,1.5547815978237096,1.5549356860320966,1.5550923396789214,1.5552515978037773,1.5554134999526619,1.555578086182797,1.5557453970674535,1.5559154737007959,1.556088357702714,1.5562640912236665,1.556442716949515,1.5566242781063557,1.5568088184653388,1.5569963823474768,1.5571870146284317,1.557380760743286,1.557577666691277,1.5577777790405114,1.557981144932629,1.5581878120874366,1.5583978288074787,1.5586112439825626,1.5588281070942154,1.55904846822007,1.5592723780381732,1.5594998878312092,1.5597310494906222,1.5599659155206402,1.5602045390421804,1.5604469737966271,1.560693274149485,1.5609434950938699,1.5611976922538573,1.5614559218876476,1.561718240890555,1.561984706797798,1.5622553777870793,1.562530312680943,1.5628095709488878,1.5630932127092287,1.5633812987306905,1.563673890433706,1.5639710498914163,1.5642728398303505,1.5645793236307606,1.5648905653265972,1.5652066296051166,1.5655275818060839,1.5658534879205588,1.5661844145892487,1.566520429100402,1.5668615993872204,1.5672079940247747,1.5675596822263937,1.567916733839509,1.568279219340936,1.5686472098315598,1.5690207770304068,1.5693999932680858,1.5697849314795613,1.5701756651962469,1.5705722685373897,1.5709748162007224,1.5713833834523605,1.571798046115918,1.5722188805608255,1.5726459636898122,1.5730793729255503,1.573519186196413,1.573965481921344,1.574418338993808,1.5748778367647973,1.5753440550248765,1.5758170739852448,1.5762969742577901,1.5767838368341254,1.5772777430635727,1.5777787746300895,1.578287013528116,1.578802542037322,1.5793254426962435,1.5798557982747896,1.5803936917456098,1.5809392062543077,1.5814924250884859,1.5820534316456207,1.5826223093997493,1.583199141866963,1.583784012569712,1.5843770049998978,1.5849782025807717,1.585587688627622,1.5862055463072577,1.5868318585962926,1.5874667082382343,1.5881101776993765,1.5887623491235126,1.5894233042854815,1.5900931245435457,1.590771890790632,1.591459683404441,1.592156582196444,1.592862666359805,1.5935780144162197,1.5943027041617333,1.5950368126115388,1.595780415943796,1.596533589442509,1.5972964074394853,1.5980689432554258,1.5988512691401797,1.5996434562122066,1.600445574397296,1.6012576923665824,1.6020798774739136,1.6029121956926229,1.603754711551753,1.6046074880717986,1.6054705867000152,1.6063440672453648,1.607227987813154,1.608122404739434,1.609027372525229,1.6099429437706616,1.610869169109038,1.6118060971409864,1.6127537743686986,1.6137122451303643,1.6146815515348736,1.615661733396865,1.6166528281721964,1.61765487089392,1.6186678941088535,1.6196919278148127,1.6207269993986007,1.621773133574837,1.6228303523257042,1.6238986748417057,1.6249781174635127,1.6260686936249829,1.627170413797451,1.6282832854353497,1.6294073129232736,1.63054249752454,1.6316888373313558,1.6328463272166522,1.6340149587876796,1.6351947203414305,1.636385596821979,1.6375875697798061,1.6388006173331842,1.6400247141316975,1.6412598313219664,1.6425059365156374,1.6437629937597196,1.6450309635093099,1.6463098026027865,1.6475994642395184,1.6488998979601415,1.6502110496294675,1.651532861422061,1.6528652718105306,1.6542082155565907,1.6555616237049091,1.6569254235797994,1.6582995387847683,1.6596838892049635,1.6610783910125348,1.6624829566749293,1.6638974949661416,1.6653219109809283,1.6667561061519922,1.6681999782701464,1.6696534215074497,1.6711163264433155,1.6725885800935827,1.674070065942538,1.6755606639778606,1.677060250728489,1.6785686993053623,1.6800858794450195,1.6816116575560132,1.683145896768104,1.6846884569841902,1.6862391949349216,1.6877979642359517,1.6893646154477617,1.6909389961380037,1.692520950946295,1.6941103216513866,1.6957069472406467,1.6973106639817597,1.6989213054965822,1.700538702837042,1.7021626845630153,1.703793076822067,1.7054297034309667,1.7070723859588668,1.708720943812047,1.710375194320106,1.7120349528234873,1.7137000327622203,1.7153702457657571,1.7170454017437784,1.7187253089778363,1.720409774213703,1.7220986027542962,1.7237915985530303,1.725488564307462,1.7271893015530837,1.7288936107571125,1.730601291412127,1.732312142129407,1.7340259607318054,1.7357425443460104,1.7374616894940287,1.7391831921837304,1.7409068479982908,1.7426324521843712,1.7443597997388536,1.7460886854939774,1.7478189042006989,1.7495502506100968,1.751282519552658,1.7530155060152606,1.7547490052156798,1.7564828126744338,1.7582167242837963,1.7599505363737822,1.7616840457749379,1.763417049877735,1.765149346688397,1.7668807348809596,1.7686110138453863,1.770339983731541,1.7720674454888392,1.7737932009013722,1.7755170526183324,1.7772388041795362,1.7789582600358564,1.7806752255643858,1.7823895070781333,1.7841009118300821,1.7858092480114207,1.7875143247437824,1.7892159520653284,1.7909139409105075,1.7926081030833672,1.7942982512242738,1.795984198769934,1.7976657599066412,1.7993427495166663,1.8010149831177906,1.8026822767959763,1.804344447131237,1.8060013111168383,1.8076526860719908,1.8092983895483152,1.8109382392304205,1.8125720528310623,1.8141996479814597,1.815820842117505,1.8174354523627596,1.819043295409323,1.8206441873978827,1.8222379437985092,1.823824379294032,1.8254033076681795,1.8269745417010033,1.8285378930745448,1.8300931722921396,1.8316401886152722,1.833178750022451,1.8347086631951777,1.836229733536758,1.8377417652303913,1.839244561343746,1.8407379239879795,1.8422216545399808,1.8436955539373867,1.8451594230566903,1.8466130631854447,1.8480562766001645,1.8494888672619263,1.8509106416418954,1.852321409688859,1.853720985950397,1.8551091908583046,1.8564858521873397,1.8578508066941026,1.8592039019398003,1.8605449982967281,1.8618739711333594,1.8631907131670922,1.8644951369667626,1.8657871775792763,1.867066795246113,1.868333978166398,1.8695887452539468,1.8708311488267162,1.8720612771588694,1.873279256818928,1.8744852547127555,1.8756794797481584,1.8768621840392459,1.8780336635737789,1.8791942582759196,1.8803443514099754,1.8814843682876936,1.8826147742617978,1.8837360720108596,1.8848487981441422,1.8859535191784578,1.8870508269609536,1.8881413336308486,1.8892256662283107,1.8903044610691162,1.8913783580089603,1.8924479947211479,1.893514001106252,1.8945769939426367,1.8956375718734646,1.8966963108098598,1.8977537598123737,1.8988104374948283,1.8998668289768923,1.9009233833952222,1.901980511968159,1.90303858659631,1.9040979389710153,1.9051588601547398,1.9062216005917991,1.9072863705043024,1.9083533406265252,1.909422643230833,1.910494373399459,1.9115685904985964,1.9126453198141327,1.9137245543116856,1.9148062564872186,1.9158903602782051,1.9169767730089835,1.9180653773474978,1.9191560332539443,1.920248579904946,1.9213428375796917,1.9224386094970076,1.9235356835945716,1.9246338342434506,1.9257328238928562,1.9268324046414764,1.9279323197329827,1.929032304974377,1.9301320900767027,1.9312313999183803,1.9323299557320004,1.9334274762158932,1.9345236785721602,1.935618279473143,1.936710995958535,1.9378015462654907,1.938889650594218,1.9399750318115891,1.9410574160953797,1.9421365335217353,1.94321211859847,1.9442839107467949,1.9453516547340004,1.9464151010596367,1.9474740062976135,1.9485281333966438,1.9495772519413597,1.9506211383763776,1.9516595761955249,1.9526923560983687,1.953719276116119,1.9547401417089192,1.9557547658364458,1.9567629690037038,1.9577645792837992,1.9587594323194228,1.9597473713047164,1.9607282469490936,1.9617019174245567,1.9626682482979587,1.9636271124496003,1.9645783899794842,1.9655219681024882,1.9664577410336426,1.9673856098646447,1.9683054824326716,1.9692172731824973,1.9701209030228495,1.971016299177895,1.9719033950346696,1.9727821299872252]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.8351365826841115,1.8351365828607569,1.8351365830406896,1.8351365832239708,1.8351365834106632,1.8351365836008298,1.8351365837945355,1.8351365839918459,1.8351365841928284,1.8351365843975513,1.8351365846060839,1.8351365848184975,1.8351365850348644,1.8351365852552577,1.8351365854797526,1.835136585708425,1.8351365859413535,1.8351365861786166,1.8351365864202953,1.8351365866664717,1.8351365869172294,1.835136587172654,1.8351365874328318,1.8351365876978518,1.8351365879678039,1.8351365882427795,1.8351365885228725,1.8351365888081783,1.8351365890987936,1.8351365893948173,1.83513658969635,1.8351365900034944,1.8351365903163548,1.8351365906350376,1.835136590959651,1.8351365912903057,1.835136591627114,1.83513659197019,1.8351365923196512,1.8351365926756156,1.8351365930382046,1.8351365934075414,1.8351365937837518,1.8351365941669635,1.8351365945573068,1.8351365949549143,1.8351365953599217,1.8351365957724661,1.8351365961926882,1.8351365966207305,1.8351365970567388,1.8351365975008613,1.835136597953249,1.835136598414056,1.8351365988834385,1.8351365993615563,1.8351365998485718,1.835136600344651,1.835136600849962,1.8351366013646773,1.8351366018889714,1.835136602423023,1.8351366029670129,1.8351366035211267,1.8351366040855528,1.8351366046604827,1.8351366052461124,1.835136605842641,1.8351366064502708,1.8351366070692088,1.8351366076996654,1.8351366083418545,1.8351366089959953,1.8351366096623098,1.8351366103410245,1.8351366110323701,1.8351366117365817,1.835136612453899,1.8351366131845657,1.83513661392883,1.8351366146869457,1.83513661545917,1.8351366162457652,1.8351366170469994,1.8351366178631445,1.8351366186944784,1.8351366195412833,1.8351366204038477,1.8351366212824645,1.8351366221774326,1.835136623089056,1.8351366240176452,1.8351366249635157,1.8351366259269888,1.8351366269083924,1.83513662790806,1.8351366289263318,1.8351366299635539,1.835136631020079,1.8351366320962659,1.835136633192481,1.835136634309097,1.8351366354464933,1.835136636605057,1.8351366377851817,1.8351366389872688,1.8351366402117268,1.8351366414589727,1.8351366427294296,1.8351366440235304,1.8351366453417146,1.8351366466844303,1.8351366480521345,1.8351366494452916,1.8351366508643763,1.8351366523098698,1.8351366537822646,1.835136655282061,1.835136656809769,1.8351366583659077,1.8351366599510066,1.8351366615656048,1.8351366632102508,1.8351366648855043,1.8351366665919344,1.8351366683301218,1.8351366701006573,1.8351366719041426,1.8351366737411914,1.835136675612428,1.835136677518489,1.8351366794600221,1.8351366814376875,1.8351366834521579,1.835136685504118,1.8351366875942656,1.8351366897233115,1.8351366918919794,1.8351366941010065,1.8351366963511446,1.8351366986431583,1.8351367009778266,1.835136703355944,1.8351367057783186,1.8351367082457744,1.8351367107591503,1.8351367133193006,1.835136715927096,1.835136718583423,1.8351367212891856,1.8351367240453025,1.8351367268527117,1.8351367297123677,1.8351367326252428,1.835136735592327,1.8351367386146298,1.835136741693178,1.8351367448290195,1.8351367480232195,1.8351367512768648,1.8351367545910613,1.835136757966936,1.8351367614056366,1.8351367649083326,1.8351367684762148,1.8351367721104963,1.835136775812413,1.8351367795832236,1.8351367834242103,1.8351367873366786,1.8351367913219594,1.8351367953814077,1.8351367995164036,1.835136803728353,1.8351368080186883,1.835136812388868,1.8351368168403785,1.835136821374733,1.8351368259934735,1.8351368306981704,1.835136835490423,1.8351368403718615,1.835136845344145,1.8351368504089647,1.8351368555680425,1.8351368608231329,1.8351368661760221,1.8351368716285308,1.835136877182513,1.8351368828398567,1.8351368886024857,1.8351368944723594,1.8351369004514742,1.8351369065418623,1.8351369127455952,1.835136919064782,1.8351369255015717,1.8351369320581528,1.8351369387367542,1.8351369455396476,1.8351369524691457,1.8351369595276048,1.835136966717425,1.835136974041051,1.835136981500973,1.8351369890997276,1.8351369968398985,1.8351370047241173,1.8351370127550657,1.8351370209354736,1.835137029268123,1.835137037755847,1.835137046401532,1.8351370552081174,1.8351370641785978,1.8351370733160237,1.8351370826235016,1.8351370921041967,1.8351371017613327,1.8351371115981931,1.8351371216181231,1.8351371318245298,1.835137142220883,1.8351371528107183,1.835137163597637,1.8351371745853065,1.8351371857774628,1.835137197177912,1.8351372087905307,1.8351372206192673,1.835137232668144,1.8351372449412582,1.8351372574427827,1.8351372701769688,1.8351372831481467,1.8351372963607269,1.8351373098192023,1.8351373235281492,1.835137337492229,1.8351373517161904,1.83513736620487,1.8351373809631943,1.835137395996182,1.8351374113089451,1.8351374269066905,1.8351374427947218,1.835137458978442,1.8351374754633538,1.8351374922550634,1.83513750935928,1.835137526781821,1.83513754452861,1.8351375626056816,1.8351375810191832,1.8351375997753767,1.8351376188806396,1.8351376383414688,1.835137658164482,1.83513767835642,1.835137698924149,1.8351377198746637,1.8351377412150878,1.8351377629526782,1.8351377850948272,1.8351378076490643,1.8351378306230595,1.8351378540246248,1.8351378778617184,1.8351379021424463,1.8351379268750656,1.8351379520679865,1.8351379777297767,1.8351380038691625,1.835138030495033,1.835138057616443,1.8351380852426156,1.8351381133829452,1.8351381420470017,1.835138171244533,1.835138200985468,1.835138231279921,1.8351382621381944,1.8351382935707823,1.8351383255883742,1.8351383582018583,1.8351383914223258,1.8351384252610745,1.8351384597296123,1.8351384948396612,1.8351385306031616,1.835138567032276,1.8351386041393936,1.8351386419371343,1.835138680438352,1.835138719656141,1.835138759603839,1.8351388002950322,1.835138841743559,1.835138883963516,1.8351389269692624,1.8351389707754242,1.8351390153968996,1.8351390608488651,1.8351391071467789,1.835139154306387,1.8351392023437292,1.835139251275143,1.835139301117271,1.8351393518870647,1.835139403601792,1.835139456279042,1.8351395099367311,1.8351395645931097,1.8351396202667674,1.83513967697664,1.8351397347420166,1.8351397935825446,1.835139853518238,1.8351399145694824,1.8351399767570438,1.8351400401020748,1.835140104626121,1.8351401703511305,1.835140237299459,1.835140305493879,1.835140374957587,1.835140445714211,1.8351405177878195,1.8351405912029288,1.835140665984512,1.835140742158007,1.835140819749326,1.8351408987848623,1.8351409792915023,1.8351410612966321,1.835141144828148,1.8351412299144665,1.8351413165845318,1.8351414048678285,1.8351414947943896,1.835141586394808,1.8351416797002453,1.8351417747424448,1.8351418715537398,1.8351419701670664,1.835142070615974,1.8351421729346369,1.8351422771578656,1.8351423833211198,1.8351424914605186,1.8351426016128545,1.835142713815605,1.8351428281069455,1.8351429445257632,1.8351430631116694,1.8351431839050127,1.8351433069468939,1.8351434322791789,1.8351435599445143,1.8351436899863403,1.8351438224489067,1.8351439573772876,1.8351440948173972,1.835144234816004,1.8351443774207494,1.835144522680161,1.8351446706436711,1.835144821361633,1.835144974885338,1.835145131267033,1.8351452905599388,1.8351454528182676,1.8351456180972416,1.8351457864531122,1.8351459579431793,1.8351461326258092,1.8351463105604575,1.8351464918076874,1.8351466764291897,1.835146864487806,1.8351470560475487,1.835147251173623,1.8351474499324496,1.8351476523916872,1.8351478586202554,1.8351480686883583,1.8351482826675085,1.8351485006305523,1.8351487226516927,1.8351489488065171,1.8351491791720211,1.8351494138266364,1.8351496528502558,1.8351498963242625,1.8351501443315565,1.835150396956584,1.8351506542853648,1.835150916405523,1.8351511834063166,1.8351514553786676,1.835151732415194,1.8351520146102405,1.8351523020599108,1.8351525948621017,1.835152893116535,1.835153196924793,1.8351535063903515,1.8351538216186176,1.8351541427169635,1.8351544697947644,1.8351548029634361,1.835155142336472,1.8351554880294831,1.8351558401602377,1.8351561988487004,1.8351565642170744,1.8351569363898428,1.8351573154938112,1.8351577016581517,1.8351580950144464,1.8351584956967326,1.8351589038415492,1.8351593195879836,1.8351597430777182,1.83516017445508,1.8351606138670904,1.8351610614635139,1.8351615173969114,1.8351619818226914,1.8351624548991632,1.835162936787592,1.8351634276522535,1.8351639276604907,1.8351644369827704,1.8351649557927434,1.8351654842673022,1.835166022586643,1.8351665709343272,1.835167129497344,1.835167698466175,1.8351682780348593,1.8351688684010607,1.8351694697661352,1.8351700823352,1.8351707063172034,1.8351713419249989,1.835171989375415,1.8351726488893323,1.8351733206917589,1.8351740050119063,1.8351747020832696,1.8351754121437076,1.835176135435524,1.8351768722055515,1.8351776227052357,1.8351783871907217,1.835179165922944,1.835179959167713,1.8351807671958094,1.8351815902830755,1.8351824287105118,1.835183282764371,1.8351841527362598,1.8351850389232358,1.8351859416279133,1.8351868611585642,1.8351877978292266,1.8351887519598107,1.8351897238762112,1.8351907139104173,1.8351917224006284,1.835192749691371,1.8351937961336144,1.835194862084896,1.8351959479094397,1.8351970539782858,1.8351981806694146,1.8351993283678796,1.8352004974659388,1.8352016883631894,1.835202901466707,1.8352041371911836,1.835205395959072,1.8352066782007304,1.835207984354571,1.835209314867211,1.835210670193626,1.8352120507973064,1.835213457150418,1.8352148897339624,1.8352163490379456,1.8352178355615436,1.8352193498132756,1.8352208923111792,1.8352224635829875,1.8352240641663113,1.8352256946088237,1.835227355468449,1.8352290473135524,1.8352307707231386,1.835232526287047,1.8352343146061574,1.835236136292594,1.835237991969937,1.8352398822734357,1.8352418078502262,1.8352437693595542,1.8352457674730007,1.8352478028747106,1.8352498762616294,1.8352519883437393,1.835254139844304,1.835256331500116,1.8352585640617456,1.8352608382938012,1.8352631549751877,1.8352655148993728,1.8352679188746586,1.8352703677244548,1.835272862287562,1.8352754034184537,1.8352779919875701,1.8352806288816113,1.8352833150038392,1.8352860512743843,1.8352888386305568,1.8352916780271638,1.8352945704368326,1.8352975168503396,1.835300518276943,1.8353035757447254,1.8353066903009376,1.8353098630123528,1.8353130949656233,1.835316387267647,1.835319741045935,1.8353231574489923,1.8353266376466997,1.835330182830704,1.8353337942148151,1.8353374730354102,1.8353412205518433,1.8353450380468637,1.835348926827039,1.8353528882231882,1.835356923590819,1.8353610343105742,1.8353652217886858,1.8353694874574347,1.8353738327756188,1.8353782592290309,1.8353827683309396,1.8353873616225833,1.8353920406736668,1.8353968070828721,1.83540166247837,1.8354066085183467,1.835411646891533,1.835416779317746,1.835422007548437,1.835427333367248,1.8354327585905768,1.835438285068152,1.8354439146836148,1.8354496493551102,1.8354554910358878,1.8354614417149102,1.835467503417472,1.8354736782058254,1.8354799681798177,1.835486375477536,1.835492902275961,1.8354995507916312,1.8355063232813171,1.8355132220427013,1.8355202494150715,1.8355274077800219,1.8355346995621626,1.8355421272298418,1.835549693295873,1.8355574003182773,1.8355652509010292,1.8355732476948177,1.8355813933978131,1.835589690756446,1.8355981425661936,1.8356067516723786,1.8356155209709755,1.835624453409427,1.8356335519874716,1.8356428197579784,1.8356522598277951,1.835661875358602,1.8356716695677782,1.8356816457292762,1.8356918071745072,1.8357021572932355,1.8357126995344815,1.8357234374074365,1.8357343744823846,1.8357455143916364,1.83575686083047,1.8357684175580837,1.8357801883985547,1.835792177241811,1.8358043880446104,1.8358168248315272,1.835829491695952,1.8358423928010972,1.8358555323810117,1.8358689147416065,1.8358825442616875,1.8358964253939964,1.835910562666262,1.8359249606822594,1.8359396241228765,1.8359545577471903,1.8359697663935497,1.8359852549806697,1.8360010285087287,1.8360170920604792,1.8360334508023606,1.8360501099856257,1.836067074947469,1.8360843511121667,1.836101943992222,1.8361198591895176,1.8361381023964773,1.8361566793972313,1.8361755960687904,1.8361948583822276,1.8362144724038647,1.8362344442964662,1.836254780320438,1.8362754868350364,1.8362965702995784,1.8363180372746606,1.8363398944233837,1.8363621485125832,1.836384806414063,1.836407875105839,1.8364313616733852,1.8364552733108843,1.8364796173224864,1.836504401123571,1.836529632242014,1.8365553183194596,1.8365814671125993,1.8366080864944518,1.8366351844556499,1.836662769105733,1.8366908486744418,1.836719431513019,1.836748526095515,1.8367781410200958,1.8368082850103584,1.8368389669166476,1.8368701957173805,1.8369019805203717,1.8369343305641677,1.8369672552193805,1.8370007639900312,1.8370348665148934,1.8370695725688453,1.837104892064225,1.83714083505219,1.8371774117240844,1.8372146324128094,1.8372525075941992,1.8372910478884061,1.8373302640612865,1.8373701670257976,1.8374107678433993,1.8374520777254613,1.8374941080346812,1.8375368702865076,1.8375803761505716,1.8376246374521283,1.8376696661735064,1.8377154744555673,1.837762074599175,1.8378094790666761,1.837857700483393,1.8379067516391256,1.8379566454896703,1.838007395158348,1.8380590139375501,1.838111515290298,1.8381649128518178,1.838219220431134,1.8382744520126784,1.83833062175792,1.838387744007014,1.8384458332804705,1.8385049042808475,1.838564971894464,1.8386260511931403,1.8386881574359604,1.8387513060710658,1.8388155127374703,1.8388807932669116,1.8389471636857275,1.8390146402167666,1.8390832392813312,1.8391529775011568,1.839223871700423,1.8392959389078078,1.8393691963585748,1.839443661496705,1.8395193519770674,1.8395962856676342,1.8396744806517404,1.8397539552303872,1.839834727924596,1.8399168174778076,1.840000242858335,1.8400850232618642,1.84017117811401,1.8402587270729236,1.8403476900319582,1.8404380871223882,1.8405299387161873,1.840623265428865,1.8407180881223617,1.8408144279080068,1.8409123061495345,1.8410117444661653,1.8411127647357488,1.8412153890979692,1.8413196399576184,1.8414255399879296,1.8415331121339795,1.8416423796161525,1.8417533659336751,1.8418660948682108,1.8419805904875255,1.8420968771492146,1.8422149795044982,1.8423349225020815,1.8424567313920777,1.8425804317299967,1.8427060493807983,1.842833610523006,1.8429631416528864,1.8430946695886843,1.8432282214749238,1.8433638247867652,1.8435015073344188,1.8436412972676175,1.843783223080143,1.8439273136144043,1.8440735980660703,1.8442221059887483,1.8443728672987156,1.844525912279691,1.8446812715876553,1.8448389762557091,1.8449990576989723,1.8451615477195187,1.8453264785113441,1.8454938826653675,1.8456637931744566,1.8458362434384818,1.8460112672693882,1.8461888988962882,1.846369172970567,1.846552124570996,1.8467377892088557,1.8469262028330564,1.8471174018352556,1.847311423054967,1.847508303784652,1.8477080817747944,1.8479107952389413,1.8481164828587144,1.8483251837887753,1.848536937661742,1.8487517845930432,1.8489697651857064,1.8491909205350623,1.8494152922333633,1.8496429223742943,1.8498738535573709,1.8501081288922065,1.8503457920026356,1.8505868870306765,1.850831458640317,1.851079552021102,1.85133121289151,1.8515864875020895,1.8518454226383385,1.852108065623302,1.8523744643198636,1.8526446671327015,1.8529187230098856,1.8531966814440801,1.8534785924733281,1.8537645066813777,1.8540544751975199,1.8543485496959031,1.854646782394281,1.854949226052162,1.8552559339683143,1.8555669599775857,1.8558823584469952,1.8562021842710523,1.8565264928662522,1.8568553401647063,1.8571887826068554,1.8575268771332125,1.8578696811750914,1.8582172526442629,1.8585696499214883,1.8589269318438788,1.8592891576910249,1.8596563871698426,1.8600286803980837,1.8604060978864574,1.8607887005193084,1.861176549533799,1.861569706497548,1.8619682332846708,1.862372192050175,1.8627816452026649,1.8631966553753065,1.8636172853950146,1.8640435982498178,1.864475657054365,1.8649135250135402,1.8653572653841528,1.8658069414346745,1.8662626164030043,1.8667243534522346,1.8671922156244105,1.8676662657922676,1.8681465666089425,1.8686331804556602,1.869126169387398,1.8696255950765412,1.870131518754543,1.8706440011516154,1.8711631024344768,1.871688882142192,1.8722213991201457,1.8727607114521996,1.8733068763910814,1.873859950287075,1.8744199885150723,1.8749870454000641,1.8755611741411489,1.87614242673415,1.876730853892929,1.8773265049695014,1.8779294278730552,1.878539668987988,1.8791572730910777,1.8797822832679099,1.8804147408286946,1.8810546852235983,1.8817021539577379,1.8823571825059704,1.8830198042276327,1.8836900502813776,1.8843679495402585,1.8850535285072234,1.885746811231174,1.8864478192237506,1.887156571377009,1.8878730838821487,1.8885973701494607,1.8893294407296577,1.8900693032367524,1.8908169622726483,1.8915724193536037,1.8923356728387342,1.8931067178607102,1.8938855462588071,1.894672146514462,1.8954665036894909,1.8962685993671067,1.897078411595889,1.8978959148368375,1.8987210799136467,1.899553873966327,1.9003942604082982,1.9012421988870705,1.902097645248621,1.9029605515055745,1.9038308658092842,1.9047085324259,1.905593491716511,1.9064856801214356,1.907385030148732,1.908291470366981,1.9092049254024048,1.9101253159403557,1.911052558731223,1.911986566600776,1.9129272484649729,1.9138745093492429,1.914828250412248,1.9157883689741237,1.9167547585491822,1.9177273088830635,1.9187059059943068,1.9196904322203032,1.9206807662675942,1.921676783266461,1.9226783548297486,1.923685349115862,1.9246976308958588,1.9257150616245664,1.926737499515633,1.9277647996204215,1.9287968139106515,1.9298333913646835,1.9308743780573345,1.9319196172531112,1.9329689495027385,1.9340222127428597,1.9350792423987722,1.9361398714900717,1.937203930739054,1.9382712486817393,1.9393416517813666,1.940414964544206,1.9414910096375364,1.942569608009629,1.9436505790115748,1.9447337405207934,1.9458189090660591,1.946905899953875,1.9479945273960246,1.9490846046381345,1.9501759440890722,1.9512683574510108,1.9523616558499841,1.9534556499667637,1.9545501501678806,1.9556449666366238,1.9567399095038407,1.9578347889783743,1.9589294154769614,1.9600235997534308,1.9611171530270344,1.9622098871097466,1.9633016145323763,1.9643921486693279,1.965481303861864,1.9665688955397131,1.9676547403408813,1.9687386562295173,1.9698204626116995,1.9708999804490073,1.9719770323697443,1.9730514427776946,1.9741230379582884,1.9751916461820598,1.9762570978052982,1.9773192253677763,1.9783778636874687,1.9794328499521636,1.9804840238078842,1.9815312274440395,1.9825743056752332,1.9836131060196636,1.9846474787740551,1.9856772770850648,1.9867023570171218,1.9877225776166556,1.9887378009726817,1.9897478922737148,1.990752719860991,1.9917521552779824,1.9927460733162012,1.993734352057282,1.9947168729113585,1.9956935206517334,1.9966641834458703,1.997628752882718,1.9985871239964055,1.9995391952863322,2.0004848687337007,2.0014240498145304,2.0023566475091994,2.0032825743085816,2.0042017462168134,2.005114082750779,2.0060195069363624,2.006917945301542,2.007809327866406,2.0086935881301624,2.0095706630552232,2.010440493048446,2.0113030219396273,2.012158196957323,2.0130059687020987,2.013846291117294,2.0146791214573994,2.015504420254144],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.78987772130806,1.7898777212214225,1.7898777211331727,1.7898777210432806,1.7898777209517154,1.7898777208584464,1.7898777207634418,1.789877720666669,1.7898777205680951,1.789877720467687,1.7898777203654104,1.7898777202612302,1.7898777201551115,1.7898777200470177,1.7898777199369122,1.789877719824758,1.7898777197105165,1.7898777195941489,1.7898777194756157,1.7898777193548767,1.7898777192318909,1.7898777191066162,1.7898777189790103,1.7898777188490294,1.7898777187166297,1.7898777185817663,1.789877718444393,1.7898777183044632,1.7898777181619292,1.7898777180167427,1.7898777178688545,1.789877717718214,1.7898777175647702,1.789877717408471,1.7898777172492628,1.7898777170870919,1.7898777169219031,1.7898777167536402,1.7898777165822461,1.7898777164076622,1.7898777162298296,1.7898777160486874,1.7898777158641743,1.7898777156762273,1.789877715484783,1.7898777152897758,1.7898777150911396,1.7898777148888068,1.789877714682709,1.7898777144727755,1.7898777142589353,1.7898777140411157,1.7898777138192425,1.7898777135932407,1.7898777133630324,1.7898777131285406,1.7898777128896848,1.7898777126463845,1.789877712398556,1.7898777121461158,1.789877711888978,1.7898777116270546,1.789877711360257,1.7898777110884947,1.7898777108116752,1.7898777105297041,1.7898777102424857,1.7898777099499226,1.7898777096519147,1.7898777093483613,1.7898777090391589,1.7898777087242026,1.7898777084033852,1.789877708076598,1.789877707743729,1.7898777074046657,1.789877707059293,1.7898777067074931,1.7898777063491462,1.789877705984131,1.7898777056123236,1.7898777052335968,1.7898777048478225,1.7898777044548693,1.7898777040546037,1.7898777036468894,1.789877703231588,1.7898777028085584,1.7898777023776566,1.7898777019387362,1.789877701491648,1.7898777010362403,1.789877700572358,1.7898777000998431,1.7898776996185355,1.7898776991282714,1.789877698628884,1.7898776981202038,1.7898776976020578,1.7898776970742698,1.7898776965366605,1.789877695989047,1.7898776954312432,1.7898776948630597,1.789877694284303,1.7898776936947767,1.7898776930942801,1.7898776924826096,1.789877691859557,1.7898776912249101,1.7898776905784537,1.789877689919968,1.7898776892492292,1.789877688566009,1.7898776878700755,1.7898776871611919,1.7898776864391175,1.789877685703607,1.7898776849544098,1.7898776841912722,1.7898776834139338,1.789877682622131,1.7898776818155944,1.7898776809940502,1.789877680157219,1.7898776793048163,1.7898776784365524,1.7898776775521326,1.7898776766512559,1.7898776757336161,1.7898776747989014,1.7898776738467943,1.7898776728769712,1.789877671889102,1.7898776708828519,1.7898776698578782,1.789877668813833,1.7898776677503612,1.7898776666671015,1.7898776655636854,1.7898776644397387,1.7898776632948792,1.7898776621287174,1.7898776609408575,1.7898776597308954,1.7898776584984204,1.7898776572430135,1.789877655964248,1.78987765466169,1.7898776533348957,1.7898776519834154,1.7898776506067893,1.7898776492045498,1.7898776477762204,1.789877646321316,1.789877644839342,1.789877643329795,1.789877641792162,1.7898776402259207,1.789877638630539,1.7898776370054748,1.789877635350176,1.7898776336640803,1.7898776319466148,1.7898776301971961,1.7898776284152298,1.7898776266001106,1.7898776247512218,1.7898776228679356,1.789877620949612,1.7898776189955994,1.789877617005234,1.7898776149778395,1.7898776129127278,1.7898776108091972,1.7898776086665331,1.789877606484008,1.7898776042608802,1.789877601996395,1.7898775996897827,1.7898775973402608,1.789877594947031,1.7898775925092802,1.7898775900261807,1.7898775874968893,1.789877584920547,1.7898775822962796,1.7898775796231947,1.7898775769003856,1.7898775741269273,1.7898775713018782,1.789877568424279,1.7898775654931527,1.789877562507504,1.7898775594663197,1.7898775563685665,1.7898775532131936,1.789877549999129,1.7898775467252823,1.7898775433905416,1.789877539993775,1.7898775365338297,1.7898775330095311,1.789877529419683,1.7898775257630672,1.7898775220384417,1.7898775182445437,1.7898775143800847,1.7898775104437534,1.7898775064342145,1.7898775023501066,1.7898774981900443,1.7898774939526159,1.7898774896363834,1.7898774852398827,1.7898774807616222,1.7898774762000818,1.7898774715537147,1.7898774668209447,1.789877462000166,1.7898774570897433,1.7898774520880114,1.7898774469932732,1.7898774418038015,1.7898774365178356,1.7898774311335832,1.7898774256492185,1.7898774200628815,1.789877414372678,1.7898774085766787,1.7898774026729185,1.7898773966593957,1.789877390534072,1.7898773842948705,1.7898773779396764,1.7898773714663359,1.7898773648726545,1.7898773581563976,1.7898773513152892,1.789877344347011,1.7898773372492016,1.7898773300194566,1.7898773226553257,1.7898773151543141,1.7898773075138814,1.789877299731439,1.7898772918043506,1.7898772837299317,1.7898772755054477,1.7898772671281136,1.7898772585950922,1.7898772499034943,1.7898772410503776,1.7898772320327447,1.789877222847543,1.7898772134916636,1.7898772039619397,1.7898771942551461,1.7898771843679986,1.789877174297151,1.7898771640391964,1.7898771535906641,1.78987714294802,1.789877132107664,1.78987712106593,1.7898771098190838,1.7898770983633223,1.7898770866947724,1.789877074809489,1.7898770627034544,1.7898770503725774,1.7898770378126898,1.7898770250195477,1.7898770119888288,1.7898769987161298,1.789876985196968,1.7898769714267766,1.7898769574009055,1.789876943114618,1.7898769285630904,1.789876913741411,1.7898768986445759,1.7898768832674903,1.789876867604965,1.7898768516517154,1.7898768354023595,1.7898768188514163,1.7898768019933036,1.7898767848223367,1.7898767673327265,1.789876749518577,1.7898767313738841,1.789876712892533,1.7898766940682966,1.7898766748948336,1.7898766553656857,1.7898766354742763,1.7898766152139076,1.7898765945777593,1.7898765735588857,1.789876552150214,1.7898765303445405,1.7898765081345307,1.7898764855127145,1.789876462471486,1.7898764390030986,1.7898764150996644,1.7898763907531519,1.7898763659553805,1.7898763406980216,1.7898763149725938,1.7898762887704598,1.789876262082825,1.7898762349007336,1.789876207215067,1.7898761790165387,1.7898761502956937,1.7898761210429035,1.7898760912483647,1.7898760609020943,1.7898760299939278,1.7898759985135153,1.7898759664503179,1.7898759337936048,1.7898759005324503,1.789875866655729,1.7898758321521147,1.7898757970100732,1.789875761217862,1.7898757247635246,1.7898756876348878,1.7898756498195574,1.7898756113049137,1.7898755720781085,1.7898755321260613,1.7898754914354538,1.789875449992727,1.789875407784076,1.7898753647954468,1.7898753210125313,1.7898752764207622,1.7898752310053097,1.7898751847510765,1.7898751376426922,1.7898750896645106,1.789875040800602,1.7898749910347511,1.7898749403504504,1.789874888730896,1.7898748361589814,1.7898747826172934,1.789874728088106,1.7898746725533756,1.7898746159947356,1.7898745583934894,1.7898744997306069,1.7898744399867172,1.789874379142104,1.789874317176698,1.789874254070073,1.7898741898014383,1.7898741243496328,1.7898740576931191,1.7898739898099771,1.789873920677897,1.7898738502741738,1.789873778575699,1.789873705558956,1.7898736312000112,1.7898735554745082,1.789873478357661,1.7898733998242455,1.789873319848594,1.7898732384045863,1.7898731554656437,1.7898730710047195,1.7898729849942936,1.7898728974063631,1.7898728082124356,1.7898727173835198,1.789872624890119,1.7898725307022216,1.7898724347892934,1.7898723371202703,1.7898722376635465,1.78987213638697,1.7898720332578317,1.7898719282428552,1.789871821308192,1.7898717124194075,1.7898716015414755,1.7898714886387679,1.789871373675045,1.7898712566134451,1.7898711374164777,1.7898710160460107,1.7898708924632625,1.7898707666287919,1.7898706385024874,1.7898705080435577,1.7898703752105212,1.7898702399611957,1.789870102252688,1.7898699620413834,1.7898698192829359,1.7898696739322557,1.7898695259435,1.7898693752700616,1.7898692218645578,1.7898690656788192,1.7898689066638787,1.7898687447699613,1.7898685799464713,1.7898684121419808,1.78986824130422,1.7898680673800638,1.7898678903155218,1.7898677100557254,1.789867526544917,1.7898673397264375,1.7898671495427148,1.7898669559352522,1.7898667588446169,1.7898665582104263,1.7898663539713384,1.7898661460650391,1.7898659344282284,1.7898657189966114,1.7898654997048848,1.7898652764867242,1.789865049274774,1.7898648180006345,1.7898645825948498,1.7898643429868968,1.7898640991051726,1.789863850876983,1.7898635982285327,1.7898633410849105,1.7898630793700805,1.78986281300687,1.7898625419169576,1.7898622660208638,1.7898619852379396,1.7898616994863543,1.789861408683087,1.789861112743916,1.7898608115834065,1.7898605051149052,1.7898601932505258,1.7898598759011437,1.7898595529763845,1.7898592243846165,1.789858890032942,1.7898585498271902,1.7898582036719082,1.7898578514703547,1.7898574931244935,1.7898571285349871,1.7898567576011903,1.7898563802211465,1.789855996291581,1.7898556057078994,1.7898552083641817,1.7898548041531812,1.7898543929663215,1.7898539746936963,1.7898535492240666,1.7898531164448637,1.7898526762421874,1.7898522285008105,1.7898517731041796,1.78985130993442,1.7898508388723404,1.7898503597974385,1.7898498725879077,1.789849377120647,1.7898488732712676,1.789848360914106,1.789847839922235,1.7898473101674768,1.7898467715204192,1.7898462238504291,1.7898456670256746,1.7898451009131406,1.789844525378653,1.7898439402868993,1.7898433455014557,1.7898427408848137,1.7898421262984066,1.7898415016026439,1.789840866656942,1.789840221319761,1.7898395654486414,1.7898388989002447,1.789838221530397,1.7898375331941327,1.7898368337457444,1.7898361230388322,1.7898354009263584,1.7898346672607044,1.7898339218937305,1.7898331646768393,1.7898323954610418,1.7898316140970287,1.789830820435243,1.7898300143259587,1.7898291956193608,1.7898283641656316,1.78982751981504,1.7898266624180352,1.7898257918253448,1.7898249078880784,1.789824010457833,1.7898230993868076,1.7898221745279195,1.7898212357349264,1.7898202828625547,1.789819315766633,1.78981833430423,1.7898173383338005,1.7898163277153363,1.789815302310521,1.7898142619828965,1.78981320659803,1.789812136023692,1.789811050130041,1.7898099487898111,1.789808831878513,1.7898076992746372,1.7898065508598695,1.7898053865193093,1.7898042061417019,1.7898030096196742,1.7898017968499813,1.789800567733762,1.789799322176803,1.7897980600898105,1.789796781388695,1.7897954859948626,1.7897941738355179,1.7897928448439757,1.7897914989599844,1.7897901361300597,1.7897887563078294,1.7897873594543876,1.789785945538662,1.789784514537793,1.7897830664375243,1.789781601232603,1.7897801189271978,1.7897786195353225,1.7897771030812792,1.7897755696001096,1.7897740191380624,1.7897724517530733,1.7897708675152582,1.7897692665074216,1.7897676488255787,1.789766014579493,1.7897643638932257,1.7897626969057046,1.789761013771304,1.7897593146604438,1.7897575997602009,1.7897558692749382,1.7897541234269516,1.7897523624571292,1.7897505866256307,1.789748796212582,1.7897469915187876,1.789745172866459,1.7897433405999625,1.7897414950865824,1.7897396367173037,1.7897377659076132,1.7897358830983165,1.7897339887563755,1.7897320833757633,1.789730167478339,1.7897282416147409,1.7897263063652975,1.7897243623409587,1.789722410184248,1.7897204505702313,1.789718484207506,1.789716511839212,1.7897145342440592,1.7897125522373782,1.7897105666721895,1.7897085784402909,1.78970658847337,1.789704597744134,1.789702607267458,1.7897006181015584,1.7896986313491854,1.7896966481588323,1.789694669725973,1.7896926972943141,1.7896907321570699,1.7896887756582598,1.789686829194026,1.7896848942139711,1.7896829722225194,1.7896810647802968,1.7896791735055353,1.7896773000754969,1.789675446227918,1.7896736137624802,1.7896718045422977,1.789670020495429,1.789668263616412,1.7896665359678188,1.7896648396818347,1.7896631769618576,1.789661550084124,1.789659961399353,1.789658413334417,1.7896569083940352,1.7896554491624879,1.7896540383053583,1.7896526785712956,1.7896513727938055,1.7896501238930607,1.7896489348777405,1.7896478088468943,1.7896467489918315,1.789645758598035,1.789644841047105,1.7896439998187277,1.7896432384926708,1.78964256075081,1.78964197037918,1.7896414712700577,1.7896410674240748,1.7896407629523599,1.78964056207871,1.7896404691417982,1.7896404885974095,1.7896406250207104,1.7896408831085542,1.7896412676818196,1.7896417836877843,1.7896424362025343,1.7896432304334127,1.789644171721503,1.789645265544153,1.789646517517539,1.7896479333992659,1.789649519091016,1.7896512806412315,1.7896532242478471,1.789655356261062,1.7896576831861575,1.7896602116863618,1.7896629485857602,1.789665900872251,1.7896690757005518,1.7896724803952533,1.7896761224539224,1.7896800095502565,1.789684149537288,1.7896885504506395,1.7896932205118339,1.7896981681316517,1.7897034019135485,1.789708930657118,1.7897147633616157,1.7897209092295319,1.7897273776702196,1.789734178303578,1.7897413209637885,1.7897488157031052,1.7897566727956986,1.7897649027415536,1.7897735162704185,1.789782524345808,1.7897919381690581,1.7898017691834283,1.789812029078257,1.7898227297931666,1.78983388352231,1.7898455027186702,1.789857600098399,1.7898701886452026,1.7898832816147656,1.789896892539215,1.7899110352316203,1.7899257237905304,1.7899409726045399,1.7899567963568879,1.7899732100300814,1.7899902289105438,1.7900078685932845,1.7900261449865857,1.7900450743167042,1.790064673132584,1.7900849583105753,1.7901059470591587,1.7901276569236682,1.7901501057910083,1.7901733118943655,1.790197293817904,1.7902220705014467,1.7902476612451312,1.790274085714042,1.7903013639428105,1.7903295163401798,1.7903585636935262,1.7903885271733389,1.7904194283376444,1.7904512891363793,1.7904841319156986,1.7905179794222201,1.7905528548071972,1.790588781630614,1.7906257838652013,1.790663885900364,1.790703112546017,1.7907434890363254,1.790785041033342,1.790827794630538,1.7908717763562234,1.7909170131768486,1.7909635325001882,1.7910113621783967,1.7910605305109348,1.7911110662473617,1.7911629985899904,1.7912163571963968,1.7912711721817876,1.7913274741212135,1.791385294051634,1.7914446634738213,1.791505614354109,1.7915681791259754,1.7916323906914644,1.7916982824224377,1.7917658881616605,1.7918352422237145,1.7919063793957395,1.7919793349380024,1.792054144584292,1.792130844542137,1.7922094714928518,1.7922900625914058,1.792372655466117,1.7924572882181737,1.7925439994209793,1.7926328281193282,1.7927238138284085,1.7928169965326373,1.7929124166843262,1.7930101152021862,1.7931101334696649,1.793212513333129,1.7933172970998876,1.7934245275360636,1.7935342478643137,1.7936465017614047,1.7937613333556461,1.7938787872241846,1.7939989083901653,1.7941217423197642,1.7942473349190917,1.7943757325309801,1.794506981931651,1.7946411303272725,1.7947782253504092,1.7949183150563663,1.7950614479194384,1.7952076728290607,1.7953570390858693,1.7955095963976753,1.7956653948753525,1.7958244850286464,1.795986917761901,1.796152744369712,1.7963220165325022,1.7964947863120244,1.796671106146789,1.796851028847419,1.7970346075919261,1.7972218959209145,1.7974129477326994,1.7976078172783452,1.7978065591566126,1.7980092283088132,1.7982158800135601,1.7984265698814108,1.7986413538493902,1.7988602881753832,1.7990834294323863,1.799310834502602,1.799542560571365,1.799778665120879,1.8000192059237488,1.80026424103629,1.8005138287915878,1.8007680277922935,1.8010268969031191,1.8012904952430198,1.801558882177024,1.8018321173076879,1.8021102604661396,1.802393371702685,1.802681511276934,1.802974739647418,1.803273117460655,1.8035767055396297,1.8038855648716405,1.804199756595482,1.8045193419879093,1.8048443824493483,1.805174939488806,1.8055110747079297,1.805852849784178,1.8062003264530506,1.8065535664893295,1.8069126316872903,1.807277583839832,1.8076484847164784,1.808025396040206,1.8084083794630563,1.8087974965404805,1.809192808704379,1.809594377234791,1.8100022632301964,1.8104165275763853,1.8108372309138678,1.8112644336037813,1.811698195692269,1.8121385768733014,1.8125856364499116,1.813039433293826,1.8135000258034726,1.813967471860349,1.8144418287837432,1.8149231532838022,1.8154115014129433,1.8159069285156149,1.816409489176416,1.8169192371665837,1.817436225388876,1.8179605058208639,1.8184921294566723,1.8190311462472013,1.8195776050388683,1.820131553510923,1.820693038111383,1.8212621039916546,1.8218387949398995,1.822423153313224,1.8230152199687595,1.8236150341937265,1.8242226336345624,1.8248380542252127,1.8254613301146805,1.826092493593944,1.8267315750223445,1.827378602753568,1.8280336030613311,1.8286966000648999,1.829367615654569,1.8300466694172284,1.8307337785621554,1.83142895784717,1.8321322195052938,1.8328435731720532,1.8335630258135727,1.8342905816556112,1.8350262421136765,1.8357700057243802,1.836521868078174,1.837281821753619,1.838049856253339,1.8388259579418007,1.839610109985077,1.8404022922927286,1.841202481461955,1.842010650724154,1.8428267698940255,1.8436508053213596,1.844482719845637,1.8453224727535702,1.8461700197397128,1.8470253128702487,1.8478883005500855,1.8487589274933482,1.8496371346973917,1.8505228594204153,1.851416035162782,1.8523165916521211,1.8532244548322978,1.8541395468563169,1.8550617860832312,1.8559910870791103,1.8569273606221204,1.8578705137117584,1.8588204495822827,1.8597770677203587,1.8607402638869492,1.861709930143463,1.8626859548821584,1.863668222860814,1.8646566152416428,1.8656510096344445,1.8666512801439632,1.867657297421421,1.8686689287201892,1.8696860379555444,1.8707084857684622,1.8717361295933783,1.8727688237298548,1.8738064194180706,1.8748487649180556,1.875895705592578,1.8769470839935876,1.878002739952114,1.8790625106715126,1.8801262308239368,1.881193732649924,1.8822648460609672,1.8833393987449376,1.8844172162742265,1.8854981222164642,1.88658193824767,1.8876684842676845,1.8887575785177317,1.8898490376999524,1.890942677098749,1.8920383107037804,1.893135751334438,1.8942348107656375,1.8953352998547537,1.896437028669527,1.8975398066167695,1.8986434425716958,1.8997477450077,1.9008525221264079,1.9019575819878258,1.903062732640414,1.9041677822509042,1.9052725392336956,1.9063768123796472,1.9074804109841075,1.9085831449740027,1.909684825033824,1.9107852627303454,1.911884270635917,1.9129816624501716,1.9140772531199954,1.9151708589576082,1.9162622977566126,1.9173513889058695,1.9184379535010607,1.9195218144538146,1.92060279659826,1.921680726794895,1.9227554340316535,1.9238267495220582,1.9248945068003624,1.9259585418135772,1.9270186930103013,1.9280748014262628,1.9291267107664996,1.9301742674841085,1.931217320855495,1.9322557230520716,1.933289329208351,1.9343179974863927,1.9353415891365668,1.936359968554603,1.9373730033349055,1.9383805643201142,1.9393825256469064,1.9403787647880328,1.9413691625905936,1.9423536033105648,1.9433319746435898,1.9443041677520585,1.9452700772885052,1.9462296014153544,1.9471826418210583,1.9481291037326685,1.9490688959248934,1.9500019307256944,1.9509281240184828,1.9518473952409792,1.9527596673808068,1.953664866967886,1.954562924063713,1.9554537722475942,1.9563373485999265,1.9572135936826032,1.9580824515166388,1.9589438695570986,1.9597977986654294,1.9606441930792857,1.9614830103799485,1.9623142114574301],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[3.1859755845242286,3.1859755689241522,3.185975553033756,3.1859755368476392,3.185975520360297,3.1859755035661235,3.1859754864594083,3.1859754690343354,3.1859754512849805,3.1859754332053094,3.185975414789174,3.185975396030313,3.185975376922346,3.1859753574587795,3.185975337632993,3.185975317438249,3.1859752968676798,3.1859752759142888,3.1859752545709545,3.185975232830418,3.185975210685291,3.1859751881280407,3.185975165150998,3.185975141746351,3.1859751179061426,3.185975093622265,3.185975068886465,3.1859750436903296,3.185975018025292,3.1859749918826266,3.1859749652534455,3.1859749381286955,3.1859749104991506,3.185974882355418,3.185974853687931,3.185974824486941,3.1859747947425188,3.185974764444552,3.185974733582738,3.185974702146587,3.185974670125407,3.1859746375083127,3.1859746042842136,3.185974570441813,3.1859745359696046,3.1859745008558686,3.1859744650886648,3.1859744286558342,3.1859743915449874,3.185974353743509,3.185974315238544,3.1859742760170024,3.185974236065548,3.1859741953705973,3.1859741539183144,3.185974111694606,3.1859740686851143,3.1859740248752173,3.18597398025002,3.185973934794349,3.1859738884927506,3.185973841329479,3.185973793288503,3.185973744353487,3.185973694507791,3.185973643734471,3.1859735920162624,3.1859735393355817,3.1859734856745168,3.1859734310148236,3.185973375337918,3.1859733186248693,3.1859732608563953,3.185973202012857,3.1859731420742454,3.185973081020183,3.185973018829911,3.1859729554822853,3.1859728909557665,3.185972825228418,3.185972758277893,3.185972690081427,3.1859726206158343,3.185972549857497,3.185972477782358,3.1859724043659123,3.185972329583199,3.1859722534087918,3.185972175816793,3.1859720967808216,3.185972016274004,3.1859719342689723,3.1859718507378427,3.185971765652217,3.185971678983166,3.185971590701224,3.1859715007763754,3.185971409178047,3.185971315875096,3.1859712208358015,3.185971124027852,3.18597102541833,3.185970924973715,3.185970822659853,3.185970718441962,3.1859706122846077,3.1859705041516975,3.185970394006469,3.185970281811474,3.1859701675285703,3.1859700511189004,3.1859699325428896,3.1859698117602226,3.1859696887298377,3.1859695634099032,3.1859694357578174,3.1859693057301777,3.185969173282778,3.1859690383705916,3.1859689009477488,3.18596876096753,3.185968618382345,3.1859684731437183,3.1859683252022712,3.185968174507709,3.1859680210088,3.1859678646533585,3.1859677053882263,3.1859675431592587,3.185967377911302,3.1859672095881786,3.185967038132662,3.1859668634864637,3.18596668559021,3.1859665043834196,3.1859663198044914,3.1859661317906722,3.185965940278048,3.1859657452015058,3.1859655464947316,3.1859653440901696,3.1859651379190117,3.185964927911168,3.185964713995245,3.185964496098519,3.185964274146913,3.1859640480649762,3.1859638177758494,3.185963583201245,3.185963344261419,3.1859631008751412,3.1859628529596757,3.1859626004307424,3.185962343202494,3.1859620811874856,3.185961814296646,3.1859615424392493,3.185961265522878,3.1859609834533944,3.1859606961349143,3.185960403469765,3.1859601053584594,3.1859598016996578,3.185959492390135,3.1859591773247464,3.1859588563963914,3.1859585294959745,3.1859581965123738,3.185957857332398,3.1859575118407513,3.1859571599199907,3.185956801450491,3.1859564363103994,3.1859560643755986,3.185955685519659,3.185955299613802,3.18595490652685,3.18595450612519,3.18595409827272,3.185953682830805,3.1859532596582336,3.1859528286111667,3.18595238954309,3.1859519423047615,3.1859514867441656,3.1859510227064574,3.185950550033912,3.18595006856587,3.1859495781386857,3.185949078585666,3.1859485697370196,3.1859480514197966,3.185947523457828,3.1859469856716705,3.1859464378785423,3.1859458798922624,3.185945311523185,3.1859447325781365,3.185944142860354,3.1859435421694107,3.185942930301155,3.185942307047638,3.18594167219704,3.185941025533606,3.185940366837566,3.185939695885061,3.1859390124480718,3.1859383162943393,3.1859376071872787,3.185936884885913,3.185936149144778,3.185935399713848,3.185934636338445,3.185933858759159,3.185933066711751,3.185932259927071,3.1859314381309636,3.1859306010441752,3.1859297483822595,3.18592887985548,3.185927995168716,3.1859270940213555,3.1859261761071966,3.1859252411143477,3.1859242887251153,3.1859233186158993,3.1859223304570845,3.1859213239129254,3.1859202986414332,3.1859192542942663,3.185918190516601,3.1859171069470213,3.1859160032173883,3.1859148789527234,3.185913733771073,3.1859125672833857,3.1859113790933784,3.1859101687974003,3.185908935984297,3.1859076802352724,3.185906401123745,3.185905098215206,3.185903771067069,3.185902419228523,3.185901042240376,3.1858996396349033,3.185898210935687,3.1858967556574562,3.18589527330592,3.185893763377604,3.1858922253596735,3.1858906587297695,3.185889062955822,3.185887437495876,3.1858857817979107,3.185884095299639,3.1858823774283334,3.1858806276006257,3.1858788452223052,3.1858770296881267,3.185875180381598,3.1858732966747705,3.18587137792804,3.1858694234899154,3.185867432696803,3.1858654048727852,3.1858633393293903,3.185861235365359,3.185859092266407,3.185856909304986,3.185854685740035,3.1858524208167323,3.18585011376624,3.1858477638054388,3.185845370136678,3.1858429319474872,3.1858404484103167,3.1858379186822505,3.1858353419047254,3.185832717203238,3.185830043687055,3.1858273204489027,3.1858245465646737,3.1858217210931064,3.1858188430754675,3.1858159115352334,3.1858129254777596,3.1858098838899394,3.1858067857398717,3.1858036299765096,3.185800415529301,3.185797141307834,3.185793806201472,3.1857904090789675,3.185786948788096,3.1857834241552583,3.185779833985089,3.1857761770600534,3.1857724521400397,3.185768657961937,3.1857647932392146,3.185760856661489,3.1857568468940816,3.1857527625775734,3.1857486023273407,3.1857443647331016,3.1857400483584293,3.1857356517402793,3.185731173388492,3.1857266117853,3.1857219653848095,3.1857172326124865,3.1857124118646274,3.1857075015078244,3.1857024998784134,3.185697405281917,3.1856922159924808,3.1856869302522877,3.1856815462709784,3.185676062225043,3.1856704762572154,3.1856647864758543,3.1856589909543023,3.1856530877302487,3.18564707480507,3.1856409501431653,3.185634711671269,3.1856283572777677,3.185621884811986,3.185615292083476,3.1856085768612816,3.1856017368731964,3.185594769805005,3.1855876732997133,3.18558044495676,3.1855730823312207,3.185565582932991,3.1855579442259563,3.1855501636271573,3.1855422385059167,3.185534166182977,3.1855259439296004,3.1855175689666733,3.185509038463773,3.185500349538236,3.1854914992541947,3.1854824846216143,3.1854733025952897,3.185463950073848,3.185454423898712,3.185444720853061,3.1854348376607673,3.1854247709853087,3.185414517428668,3.185404073530216,3.185393435765558,3.1853826005453834,3.185371564214273,3.185360323049502,3.1853488732598088,3.185337210984147,3.1853253322904207,3.1853132331741865,3.1853009095573417,3.1852883572867814,3.1852755721330417,3.1852625497889093,3.1852492858680104,3.1852357759033727,3.1852220153459703,3.1852079995632314,3.1851937238375236,3.1851791833646192,3.1851643732521215,3.185149288517876,3.1851339240883423,3.185118274796948,3.185102335382405,3.1850861004869997,3.1850695646548592,3.185052722330172,3.1850355678553957,3.1850180954694247,3.1850002993057194,3.1849821733904196,3.184963711640407,3.1849449078613477,3.184925755745698,3.184906248870666,3.184886380696156,3.1848661445626547,3.184845533689111,3.18482454117074,3.1848031599768327,3.1847813829484943,3.1847592027963607,3.1847366120982716,3.1847136032969043,3.184690168697364,3.184666300464747,3.1846419906216354,3.184617231045584,3.1845920134665358,3.184566329464207,3.1845401704654304,3.184513527741446,3.1844863924051516,3.1844587554083064,3.184430607538691,3.1844019394172087,3.1843727414949576,3.184343004050236,3.1843127171855072,3.1842818708243192,3.1842504547081565,3.1842184583932602,3.184185871247388,3.1841526824465127,3.184118880971479,3.1840844556046055,3.1840493949262227,3.184013687311163,3.1839773209251887,3.183940283721371,3.183902563436401,3.183864147586845,3.183825023465347,3.183785178136764,3.183744598434235,3.1837032709552053,3.183661182057369,3.183618317854565,3.1835746642125895,3.1835302067449653,3.1834849308086284,3.1834388214995606,3.1833918636483385,3.183344041815636,3.1832953402876396,3.183245743071401,3.183195233890129,3.1831437961783857,3.183091413077238,3.1830380674293184,3.1829837417738234,3.1829284183414277,3.1828720790491314,3.1828147054950295,3.182756278953,3.182696780367329,3.182636190347233,3.1825744891613352,3.18251165673203,3.1824476726297966,3.182382516067404,3.182316165894062,3.1822486005894706,3.1821797982577946,3.1821097366215536,3.182038393015426,3.1819657443799803,3.1818917672553,3.1818164377745415,3.1817397316573994,3.1816616242034823,3.1815820902856062,3.181501104342995,3.181418640374402,3.181334671931127,3.181249172109963,3.1811621135460384,3.1810734684055797,3.180983208378572,3.1808913046713467,3.18079772799906,3.18070244857809,3.1806054361183436,3.1805066598154683,3.180406088342969,3.180303689844242,3.180199431924512,3.180093281642672,3.1799852055030415,3.1798751694470266,3.179763138844686,3.179649078486214,3.1795329525733176,3.179414724710521,3.1792943578963593,3.1791718145144943,3.179047056324734,3.1789200444539643,3.1787907393869883,3.1786591009572764,3.178525088337632,3.178388660030765,3.178249773859771,3.1781083869585363,3.177964455762051,3.1778179359966305,3.1776687826700623,3.1775169500616616,3.1773623917122475,3.1772050604140336,3.1770449082004455,3.176881886335846,3.176715945305195,3.1765470348036247,3.1763751037259333,3.176200100156024,3.176021971356241,3.1758406637566616,3.1756561229443023,3.175468293652255,3.175277119748768,3.1750825442262465,3.1748845091902043,3.174682955848136,3.1744778244983474,3.1742690545187138,3.1740565843553834,3.1738403515114375,3.1736202925354826,3.1733963430101952,3.1731684375408347,3.1729365097436824,3.172700492234458,3.1724603166166823,3.172215913470007,3.1719672123385,3.171714141718897,3.171456629048827,3.1711946006949874,3.170927981941315,3.1706566969771126,3.1703806688851524,3.1700998196297623,3.1698140700448896,3.169523339822147,3.1692275474988394,3.168926610445978,3.1686204448562845,3.168308965732181,3.167992086873774,3.167669720866832,3.1673417790707603,3.167008171606559,3.166668807344803,3.166323593893604,3.165972437586576,3.1656152434708065,3.1652519152948346,3.164882355496627,3.1645064651915633,3.164124144160424,3.1637352908373897,3.163339802298047,3.1629375742473957,3.1625285010078734,3.162112475507378,3.1616893892673046,3.1612591323905863,3.1608215935497332,3.1603766599748977,3.159924217441919,3.1594641502603844,3.1589963412616915,3.158520671787107,3.158037021675818,3.1575452692529913,3.157045291317817,3.156536963131543,3.156020158405497,3.1554947492890997,3.15496060635785,3.1544175986012966,3.153865593410974,3.1533044565683253,3.152734052232578,3.1521542429285785,3.151564889534607,3.1509658512701235,3.150356985683478,3.149738148639554,3.1491091943073672,3.1484699751475844,3.1478203418999846,3.1471601435708436,3.1464892274202416,3.1458074389492787,3.145114621887225,3.144410618178548,3.1436952679698846,3.1429684095968744,3.14222987957092,3.1414795125658252,3.140717141404334,3.1399425970445574,3.1391557085662827,3.1383563031571753,3.1375442060988687,3.1367192407529343,3.135881228546743,3.1350299889592153,3.1341653395064633,3.13328709572733,3.1323950711688244,3.131489077371473,3.1305689238545695,3.1296344181013644,3.1286853655441753,3.1277215695494447,3.126742831402751,3.125748950293799,3.124739723301387,3.123714945378391,3.1226744093367644,3.1216179058325957,3.120545223351233,3.119456148192504,3.118350464456078,3.1172279540269683,3.116088396561241,3.1149315694719495,3.113757247915327,3.1125652047773023,3.1113552106603555,3.110127033870783,3.1088804404064048,3.1076151939447794,3.1063310558319657,3.10502778507192,3.1037051383165553,3.1023628698565466,3.1010007316129546,3.099618473129728,3.0982158415671517,3.096792581696356,3.09534843589491,3.093883144143637,3.092396444024705,3.0908880707210957,3.089357757017546,3.087805233303054,3.086230227575048,3.0846324654453263,3.0830116701478674,3.0813675625486274,3.0796998611574185,3.0780082821419996,3.0762925393444864,3.0745523443001956,3.072787406259048,3.070997432209663,3.0691821269062314,3.067341192898354,3.0654743305639025,3.063581238145089,3.061661611787843,3.0597151455846374,3.0577415316208834,3.055740460025048,3.0537116190226046,3.0516546949939527,3.0495693725364537,3.0474553345306954,3.0453122622111213,3.043139835241163,3.0409377317929867,3.0387056286319947,3.0364432012061986,3.034150123740592,3.0318260693366264,3.0294707100769314,3.027083717135369,3.0246647608925397,3.022213511056854,3.019729636791258,3.0172128068457136,3.0146626896955246,3.0120789536856027,3.0094612671807366,3.0068092987219552,3.0041227171890483,3.001401191969295,2.9986443931324773,2.995851991612198,2.99302365939357,2.9901590697072846,2.987257897230096,2.984319818291731,2.9813445110882375,2.9783316559017443,2.975280935326656,2.9721920345022186,2.969064641351454,2.9658984468264062,2.962693145159639,2.9594484341219327,2.95616401528609,2.9528395942967767,2.9494748811462794,2.946069590456083,2.942623441764144,2.9391361598177097,2.9356074748715555,2.9320371229914586,2.9284248463627525,2.9247703936037714,2.9210735200839775,2.9173339882465767,2.913551567935382,2.9097260367257083,2.905857180259029,2.9019447925811486,2.8979886764836134,2.893988643848072,2.889944515993291,2.8858561240245204,2.88172330918487,2.8775459232083933,2.873323828674502,2.869056899363385,2.8647450206120486,2.8603880896706078,2.855986016058446,2.8515387219198467,2.8470461423786935,2.842508225891827,2.8379249346006423,2.833296244680496,2.828622146687492,2.8239026459022107,2.81913776266993,2.81432753273689,2.8094720075821655,2.8045712547446544,2.799625358144764,2.7946344184003062,2.7895985531361496,2.784517897287173,2.779392603394049,2.7742228418914063,2.769008801387913,2.763750688937819,2.7584487303035274,2.753103170208723,2.7477142725816535,2.7422823207880938,2.736807617853602,2.7312904866746344,2.725731270218112,2.720130331709056,2.7144880548058845,2.7088048437630206,2.7030811235804353,2.6973173401397803,2.6915139603267786,2.6856714721395463,2.6797903847825553,2.6738712287459347,2.6679145558698387,2.6619209393936414,2.655890973989695,2.6498252757814607,2.643724482345782,2.637589252699143,2.631420267267725,2.6252182278411342,2.618983857509665,2.6127179005849936,2.6064211225042198,2.6000943097171874,2.5937382695570363,2.587353830093955,2.5809418399721364,2.574503168229933,2.5680387041032593,2.5615493568122703,2.5550360553314,2.548499748142842,2.5419414029735705,2.5353620065160243,2.5287625641325944,2.522144099544069,2.515507654502194,2.508854288446552,2.5021850781459456,2.495501117324492,2.4888035162726743,2.4820934014435627,2.4753719150344757,2.468640214554326,2.4618994723769303,2.4551508752805513,2.448395623973974,2.4416349326093996,2.434870028282462,2.428102150519676,2.4213325507536276,2.41456249178623,2.4077932472403605,2.4010261010002054,2.3942623466406414,2.387503286845981,2.3807502328184103,2.3740045036764608,2.367267425843823,2.360540332428857,2.35382456259512,2.347121460923229,2.340432376764406,2.333758663586009,2.3271016783093885,2.3204627806403826,2.3138433323927687,2.307244696804991,2.300668237850473,2.294115319541836,2.287587305229324,2.281085556893755,2.274611434434292,2.268166294951353,2.2617514920249677,2.2553683749888807,2.249018288200716,2.24270257030852,2.2364225535139814,2.230179562832667,2.2239749153515853,2.217809919484411,2.2116858742247087,2.2056040683975,2.1995657799095296,2.19357227499861,2.1876248074823983,2.181724618007027,2.175872933295973,2.1700709653996104,2.164319910945873,2.158620950392498,2.1529752472813377,2.1473839474952214,2.141848178517928,2.136369048697792,2.130947646515535,2.1255850398569223,2.120282275290872,2.115040377353674,2.1098603478400126,2.1047431651014925,2.09968978335343,2.09470113199066,2.089778114913179,2.084921609862445,2.08013246776919,2.075411512113629,2.0707595382989834,2.0661773130392422,2.0616655737621277,2.0572250280282405,2.052856352967383,2.0485601947330725,2.044337167976285,2.0401878553394552,2.0361128069717953,2.0321125400669757,2.028187538424236,2.024338252033968,2.02056509668883,2.016868453621421,2.013248669169542,2.009706054470053,2.0062408851822964,2.0028534012420556,1.9995438066469517,1.9963122692741793,1.9931589207314058,1.9900838562416459,1.9870871345628425,1.9841687779428472,1.9813287721104247,1.9785670663028487,1.9758835733305822,1.9732781696794583,1.9707506956507201,1.9683009555391733,1.9659287178496443,1.9636337155518402,1.9614156463736228,1.9592741731326202,1.957208924106001,1.9552194934381613,1.953305441585959,1.9514662958010534,1.9497015506488105,1.9480106685631313,1.94639308043649,1.9448481862443532,1.9433753557030837,1.941973928960331,1.940643217316836,1.9393825039784942,1.9381910448374386,1.9370680692808437,1.936012781026062,1.9350243589806626,1.934101958125864,1.9332447104218016,1.9324517257330325,1.9317220927726133,1.9310548800630702,1.930449136912534,1.9299038944042866,1.9294181663979526,1.928990950540543,1.9286212292855536,1.9283079709183222,1.92805013058584,1.92784665132923,1.927696465117115,1.927598493878118,1.9275516505307595,1.9275548400090528,1.9276069602821244,1.927706903366232,1.927853556327596,1.9280458022745026,1.9282825213371975,1.9285625916341351,1.928884890223213,1.929248294036683,1.9296516807984867,1.9300939299228417,1.930573923392961,1.9310905466188684,1.9316426892733352,1.9322292461050437,1.9328491177281486,1.9335012113874892,1.9341844416987677,1.934897731363097,1.9356400118553785,1.9364102240860548,1.937207319035847,1.9380302583631612,1.9388780149839109,1.9397495736235755,1.9406439313413784,1.9415600980265315,1.9424970968665516,1.943453964787722,1.944429752867817,1.9454235267212727,1.9464343668570343,1.9474613690093583,1.9485036444418995,1.9495603202254501,1.9506305394897474,1.951713461649797,1.9528082626072023,1.953914134927016,1.9550302879906656,1.9561559481255346,1.9572903587117914,1.958432780267106,1.9595824905098855,1.9607387844017008,1.9619009741695754,1.963068389308829,1.9642403765671774,1.9654162999107905,1.9665955404730309,1.967777496486583,1.9689615831996972,1.9701472327772669,1.9713338941874592,1.9725210330746088,1.9737081316190914,1.9748946883848761,1.976080218155455,1.977264251758837,1.9784463358822801,1.9796260328774324,1.980802920556534,1.9819765919803178,1.983146655238239,1.9843127332216453,1.9854744633904822,1.9866314975341175,1.987783501526848,1.9889301550786356,1.9900711514816074,1.9912061973528263,1.9923350123738377,1.9934573290274606,1.9945728923322914,1.9956814595753556,1.99678280004334,1.9978766947528022],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.796351310570382,1.796351310659258,1.7963513107497873,1.7963513108420015,1.796351310935932,1.7963513110316105,1.7963513111290697,1.7963513112283425,1.796351311329463,1.796351311432465,1.7963513115373841,1.7963513116442558,1.796351311753116,1.7963513118640027,1.7963513119769527,1.796351312092005,1.7963513122091979,1.7963513123285721,1.796351312450168,1.7963513125740267,1.7963513127001904,1.796351312828702,1.7963513129596056,1.796351313092945,1.7963513132287658,1.7963513133671143,1.7963513135080376,1.7963513136515834,1.7963513137978009,1.796351313946739,1.7963513140984493,1.7963513142529828,1.7963513144103924,1.7963513145707313,1.796351314734054,1.7963513149004162,1.7963513150698744,1.7963513152424864,1.7963513154183108,1.7963513155974071,1.7963513157798365,1.796351315965661,1.7963513161549438,1.7963513163477491,1.7963513165441425,1.796351316744191,1.7963513169479621,1.7963513171555257,1.796351317366952,1.7963513175823131,1.796351317801682,1.7963513180251334,1.7963513182527433,1.796351318484589,1.7963513187207494,1.796351318961305,1.7963513192063372,1.7963513194559295,1.7963513197101668,1.7963513199691357,1.7963513202329238,1.796351320501621,1.796351320775319,1.7963513210541104,1.7963513213380902,1.7963513216273548,1.7963513219220029,1.7963513222221343,1.7963513225278511,1.7963513228392578,1.7963513231564594,1.7963513234795643,1.7963513238086826,1.7963513241439253,1.7963513244854072,1.7963513248332441,1.7963513251875545,1.7963513255484587,1.796351325916079,1.796351326290541,1.7963513266719722,1.7963513270605014,1.7963513274562615,1.7963513278593868,1.796351328270014,1.7963513286882835,1.796351329114337,1.7963513295483193,1.7963513299903782,1.7963513304406638,1.7963513308993295,1.7963513313665311,1.7963513318424273,1.79635133232718,1.7963513328209542,1.7963513333239174,1.7963513338362411,1.7963513343580995,1.7963513348896694,1.796351335431132,1.7963513359826715,1.7963513365444752,1.7963513371167343,1.796351337699643,1.7963513382934,1.796351338898207,1.7963513395142696,1.7963513401417974,1.7963513407810034,1.7963513414321053,1.7963513420953243,1.7963513427708862,1.7963513434590204,1.7963513441599608,1.7963513448739459,1.7963513456012186,1.796351346342026,1.7963513470966201,1.796351347865257,1.796351348648199,1.7963513494457115,1.7963513502580661,1.7963513510855387,1.7963513519284107,1.796351352786969,1.7963513536615052,1.7963513545523169,1.7963513554597066,1.7963513563839832,1.796351357325461,1.79635135828446,1.796351359261306,1.796351360256332,1.796351361269875,1.7963513623022807,1.7963513633538999,1.79635136442509,1.796351365516215,1.7963513666276465,1.796351367759762,1.7963513689129467,1.7963513700875922,1.7963513712840984,1.7963513725028721,1.7963513737443277,1.7963513750088869,1.79635137629698,1.7963513776090452,1.7963513789455283,1.7963513803068838,1.7963513816935748,1.7963513831060722,1.7963513845448573,1.796351386010418,1.7963513875032537,1.7963513890238718,1.796351390572789,1.7963513921505323,1.7963513937576379,1.7963513953946524,1.7963513970621323,1.7963513987606448,1.7963514004907672,1.796351402253088,1.7963514040482063,1.7963514058767325,1.7963514077392881,1.796351409636507,1.7963514115690338,1.7963514135375258,1.7963514155426523,1.7963514175850952,1.7963514196655488,1.7963514217847207,1.7963514239433314,1.7963514261421145,1.7963514283818185,1.7963514306632042,1.7963514329870478,1.7963514353541392,1.7963514377652832,1.7963514402212999,1.7963514427230243,1.7963514452713072,1.7963514478670148,1.7963514505110303,1.796351453204252,1.7963514559475962,1.7963514587419953,1.7963514615884,1.7963514644877778,1.7963514674411145,1.7963514704494148,1.7963514735137014,1.796351476635016,1.79635147981442,1.7963514830529952,1.7963514863518417,1.7963514897120825,1.796351493134859,1.7963514966213356,1.796351500172698,1.7963515037901534,1.7963515074749323,1.7963515112282875,1.7963515150514953,1.7963515189458552,1.7963515229126925,1.7963515269533552,1.7963515310692177,1.7963515352616797,1.796351539532166,1.7963515438821298,1.7963515483130497,1.7963515528264327,1.7963515574238131,1.7963515621067547,1.7963515668768495,1.79635157173572,1.7963515766850178,1.7963515817264266,1.7963515868616604,1.7963515920924653,1.7963515974206203,1.796351602847937,1.7963516083762614,1.796351614007473,1.7963516197434868,1.7963516255862533,1.7963516315377597,1.7963516376000292,1.7963516437751241,1.7963516500651437,1.7963516564722273,1.7963516629985536,1.796351669646342,1.7963516764178524,1.7963516833153887,1.796351690341296,1.7963516974979634,1.7963517047878248,1.796351712213359,1.7963517197770913,1.796351727481594,1.7963517353294876,1.7963517433234402,1.7963517514661704,1.7963517597604475,1.796351768209092,1.7963517768149773,1.7963517855810296,1.796351794510231,1.796351803605617,1.7963518128702816,1.796351822307375,1.7963518319201066,1.796351841711746,1.7963518516856225,1.7963518618451286,1.7963518721937195,1.7963518827349143,1.7963518934722977,1.7963519044095222,1.796351915550307,1.7963519268984405,1.7963519384577833,1.7963519502322658,1.7963519622258928,1.7963519744427434,1.7963519868869724,1.7963519995628126,1.7963520124745744,1.7963520256266499,1.7963520390235117,1.7963520526697163,1.7963520665699055,1.7963520807288063,1.7963520951512346,1.796352109842096,1.796352124806387,1.796352140049197,1.796352155575711,1.7963521713912096,1.7963521875010724,1.7963522039107787,1.7963522206259102,1.7963522376521517,1.7963522549952953,1.7963522726612395,1.7963522906559937,1.7963523089856785,1.7963523276565287,1.7963523466748956,1.796352366047248,1.796352385780175,1.7963524058803901,1.7963524263547301,1.7963524472101593,1.7963524684537722,1.7963524900927956,1.79635251213459,1.7963525345866542,1.7963525574566253,1.7963525807522835,1.7963526044815536,1.796352628652508,1.7963526532733696,1.7963526783525143,1.796352703898474,1.7963527299199393,1.796352756425763,1.7963527834249629,1.7963528109267242,1.7963528389404033,1.7963528674755311,1.7963528965418165,1.7963529261491484,1.7963529563076002,1.7963529870274324,1.7963530183190979,1.796353050193243,1.7963530826607133,1.7963531157325558,1.7963531494200236,1.7963531837345792,1.7963532186878988,1.7963532542918763,1.7963532905586266,1.7963533275004906,1.7963533651300394,1.796353403460078,1.7963534425036503,1.7963534822740426,1.7963535227847893,1.7963535640496766,1.7963536060827483,1.7963536488983087,1.7963536925109298,1.7963537369354543,1.796353782187002,1.7963538282809743,1.7963538752330594,1.7963539230592382,1.7963539717757893,1.7963540213992943,1.7963540719466444,1.7963541234350457,1.7963541758820243,1.7963542293054335,1.7963542837234592,1.7963543391546264,1.796354395617805,1.7963544531322175,1.7963545117174438,1.796354571393429,1.7963546321804909,1.7963546940993247,1.7963547571710132,1.7963548214170302,1.7963548868592516,1.79635495351996,1.7963550214218547,1.7963550905880572,1.7963551610421205,1.796355232808037,1.7963553059102464,1.7963553803736443,1.7963554562235906,1.796355533485918,1.7963556121869413,1.796355692353466,1.7963557740127973,1.7963558571927507,1.79635594192166,1.796356028228387,1.7963561161423331,1.796356205693448,1.796356296912239,1.796356389829785,1.7963564844777424,1.7963565808883601,1.7963566790944876,1.7963567791295887,1.796356881027751,1.7963569848236984,1.796357090552803,1.7963571982510973,1.7963573079552868,1.7963574197027616,1.7963575335316104,1.7963576494806328,1.7963577675893532,1.7963578878980337,1.7963580104476884,1.796358135280098,1.7963582624378225,1.7963583919642176,1.7963585239034487,1.7963586583005067,1.7963587952012219,1.7963589346522821,1.796359076701247,1.796359221396565,1.7963593687875896,1.7963595189245976,1.7963596718588044,1.7963598276423838,1.7963599863284843,1.7963601479712477,1.7963603126258292,1.7963604803484143,1.79636065119624,1.7963608252276129,1.7963610025019305,1.796361183079702,1.796361367022567,1.79636155439332,1.7963617452559295,1.796361939675561,1.7963621377185994,1.796362339452673,1.7963625449466751,1.7963627542707885,1.796362967496511,1.796363184696678,1.7963634059454894,1.7963636313185345,1.7963638608928187,1.7963640947467896,1.7963643329603647,1.796364575614959,1.7963648227935125,1.7963650745805206,1.7963653310620618,1.7963655923258282,1.7963658584611564,1.796366129559058,1.7963664057122513,1.796366687015194,1.7963669735641152,1.7963672654570502,1.7963675627938729,1.7963678656763324,1.7963681742080868,1.796368488494741,1.7963688086438823,1.796369134765118,1.7963694669701151,1.7963698053726367,1.7963701500885847,1.7963705012360378,1.7963708589352947,1.7963712233089149,1.7963715944817624,1.7963719725810492,1.7963723577363797,1.7963727500797961,1.7963731497458255,1.7963735568715262,1.7963739715965361,1.7963743940631223,1.7963748244162305,1.7963752628035359,1.796375709375496,1.796376164285403,1.796376627689438,1.7963770997467257,1.7963775806193913,1.796378070472617,1.7963785694747012,1.796379077797117,1.7963795956145736,1.796380123105078,1.7963806604499981,1.7963812078341272,1.7963817654457486,1.7963823334767035,1.7963829121224593,1.7963835015821774,1.7963841020587867,1.796384713759054,1.7963853368936573,1.796385971677263,1.7963866183286004,1.7963872770705414,1.7963879481301788,1.7963886317389088,1.7963893281325132,1.7963900375512436,1.7963907602399087,1.7963914964479613,1.7963922464295874,1.7963930104437993,1.7963937887545272,1.796394581630715,1.7963953893464173,1.7963962121808985,1.796397050418733,1.7963979043499088,1.7963987742699323,1.7963996604799346,1.7964005632867825,1.7964014830031878,1.7964024199478226,1.7964033744454337,1.796404346826963,1.7964053374296662,1.7964063465972362,1.7964073746799292,1.7964084220346932,1.7964094890252977,1.796410576022467,1.7964116834040171,1.7964128115549944,1.7964139608678162,1.7964151317424164,1.7964163245863924,1.7964175398151556,1.7964187778520848,1.7964200391286829,1.7964213240847373,1.7964226331684816,1.7964239668367636,1.7964253255552138,1.7964267097984201,1.796428120050103,1.7964295568032982,1.7964310205605385,1.7964325118340436,1.7964340311459115,1.7964355790283142,1.796437156023697,1.7964387626849845,1.7964403995757863,1.7964420672706112,1.796443766355084,1.796445497426167,1.796447261092385,1.7964490579740573,1.796450888703533,1.7964527539254314,1.796454654296887,1.7964565904878007,1.7964585631810959,1.7964605730729784,1.7964626208732044,1.7964647073053523,1.7964668331071005,1.7964689990305107,1.7964712058423182,1.7964734543242282,1.796475745273216,1.7964780795018376,1.7964804578385418,1.796482881127995,1.7964853502314062,1.7964878660268635,1.7964904294096775,1.7964930412927278,1.7964957026068225,1.7964984143010607,1.7965011773432058,1.7965039927200637,1.7965068614378725,1.796509784522697,1.7965127630208337,1.7965157979992243,1.7965188905458764,1.796522041770295,1.796525252803921,1.7965285248005822,1.7965318589369488,1.796535256413005,1.7965387184525246,1.7965422463035603,1.7965458412389415,1.796549504556784,1.79655323758101,1.7965570416618777,1.7965609181765236,1.7965648685295175,1.796568894153425,1.796572996509386,1.7965771770877037,1.7965814374084457,1.7965857790220578,1.796590203509992,1.7965947124853456,1.7965993075935156,1.7966039905128643,1.7966087629554035,1.7966136266674864,1.7966185834305197,1.796623635061688,1.7966287834146921,1.7966340303805057,1.7966393778881458,1.7966448279054579,1.79665038243992,1.7966560435394625,1.7966618132933028,1.7966676938327997,1.7966736873323235,1.7966797960101455,1.7966860221293433,1.7966923679987266,1.7966988359737808,1.7967054284576298,1.7967121479020176,1.7967189968083108,1.79672597772852,1.7967330932663417,1.796740346078222,1.7967477388744402,1.796755274420213,1.7967629555368234,1.7967707851027683,1.7967787660549301,1.7967869013897713,1.796795194164551,1.7968036474985662,1.7968122645744151,1.7968210486392866,1.7968300030062725,1.7968391310557044,1.7968484362365176,1.7968579220676382,1.796867592139398,1.7968774501149718,1.7968874997318471,1.7968977448033145,1.7969081892199894,1.7969188369513573,1.7969296920473523,1.796940758639957,1.7969520409448352,1.7969635432629927,1.796975269982465,1.796987225580036,1.796999414622986,1.7970118417708685,1.7970245117773185,1.7970374294918896,1.797050599861923,1.7970640279344465,1.7970777188581049,1.797091677885123,1.7971059103732967,1.797120421788021,1.7971352177043456,1.797150303809066,1.797165685902844,1.7971813699023653,1.7971973618425257,1.797213667878653,1.797230294288762,1.7972472474758427,1.797264533970182,1.797282160431721,1.7973001336524443,1.7973184605588046,1.7973371482141833,1.7973562038213833,1.7973756347251588,1.7973954484147787,1.7974156525266274,1.797436254846839,1.7974572633139674,1.7974786860216951,1.797500531221574,1.7975228073258056,1.7975455229100572,1.7975686867163139,1.7975923076557687,1.79761639481175,1.7976409574426864,1.7976660049851085,1.7976915470566917,1.7977175934593344,1.7977441541822776,1.7977712394052632,1.7977988595017307,1.7978270250420563,1.7978557467968321,1.7978850357401852,1.7979149030531405,1.7979453601270245,1.7979764185669127,1.7980080901951212,1.798040387054741,1.7980733214132198,1.7981069057659873,1.7981411528401294,1.7981760755981084,1.798211687241534,1.7982480012149806,1.7982850312098593,1.7983227911683386,1.7983612952873198,1.798400558022467,1.7984405940922898,1.7984814184822873,1.798523046449146,1.7985654935250015,1.7986087755217584,1.798652908535474,1.7986979089508077,1.798743793445534,1.7987905789951268,1.79883828287741,1.7988869226772821,1.7989365162915125,1.7989870819336136,1.799038638138791,1.7990912037689706,1.79914479801791,1.7991994404163891,1.7992551508374892,1.7993119495019572,1.7993698569836596,1.7994288942151269,1.799489082493192,1.7995504434847236,1.799612999232453,1.7996767721609064,1.7997417850824298,1.7998080612033205,1.7998756241300629,1.7999444978756658,1.8000147068661099,1.8000862759469,1.8001592303897285,1.8002335958992475,1.8003093986199525,1.8003866651431752,1.8004654225141912,1.800545698239436,1.8006275202938373,1.8007109171282532,1.8007959176770252,1.8008825513656412,1.8009708481185058,1.8010608383668187,1.8011525530565586,1.8012460236565717,1.8013412821667587,1.8014383611263627,1.8015372936223493,1.8016381132978785,1.8017408543608628,1.8018455515926086,1.8019522403565316,1.8020609566069445,1.8021717368979118,1.8022846183921553,1.8023996388700192,1.80251683673847,1.8026362510401344,1.8027579214623606,1.8028818883462936,1.803008192695955,1.8031368761873157,1.80326798117735,1.8034015507130547,1.8035376285404268,1.8036762591133761,1.8038174876025677,1.8039613599041688,1.8041079226484906,1.8042572232085015,1.8044093097081957,1.8045642310308017,1.8047220368267967,1.8048827775217249,1.8050465043237764,1.8052132692311202,1.805383125038954,1.8055561253462533,1.8057323245621877,1.8059117779121783,1.8060945414435676,1.8062806720308686,1.8064702273805653,1.806663266035428,1.8068598473783126,1.8070600316354046,1.807263879878877,1.8074714540289196,1.8076828168551005,1.8078980319770246,1.8081171638642426,1.8083402778353703,1.8085674400563754,1.8087987175379836,1.809034178132163,1.8092738905276353,1.8095179242443693,1.809766349627005,1.8100192378371638,1.810276660844586,1.8105386914170536,1.8108054031090413,1.8110768702490438,1.811353167925531,1.8116343719714723,1.8119205589473832,1.812211806122837,1.812508191456394,1.8128097935738896,1.8131166917450363,1.813428965858285,1.8137466963938973,1.8140699643951812,1.81439885143784,1.8147334395973933,1.815073811414623,1.8154200498590032,1.8157722382900783,1.8161304604167485,1.8164948002544314,1.8168653420800691,1.8172421703849522,1.8176253698253404,1.8180150251708571,1.818411221250645,1.818814042897268,1.8192235748883612,1.8196399018860179,1.8200631083739245,1.8204932785922516,1.8209304964703126,1.8213748455570165,1.8218264089491365,1.8222852692174314,1.8227515083306565,1.823225207577511,1.8237064474865758,1.824195307744299,1.824691867111095,1.8251962033356337,1.8257083930673934,1.826228511767571,1.8267566336184364,1.827292831431238,1.8278371765527588,1.8283897387706431,1.8289505862176123,1.8295197852746894,1.8300974004735775,1.8306834943983177,1.8312781275863768,1.8318813584293123,1.8324932430731715,1.8331138353187757,1.8337431865220633,1.8343813454946487,1.835028358404776,1.8356842686788333,1.8363491169036112,1.8370229407294791,1.8377057747746612,1.8383976505307928,1.8390985962699429,1.8398086369532798,1.840527794141567,1.8412560859076677,1.8419935267512413,1.8427401275158077,1.8434958953083564,1.8442608334216746,1.8450349412595641,1.845818214265113,1.8466106438521852,1.8474122173402847,1.848222917892945,1.849042724459795,1.8498716117224363,1.8507095500442736,1.851556505424417,1.8524124394557835,1.8532773092875088,1.8541510675917747,1.8550336625351487,1.8559250377545276,1.8568251323377645,1.8577338808090509,1.8586512131191208,1.8595770546403267,1.8605113261666404,1.8614539439186066,1.8624048195532874,1.8633638601792077,1.8643309683763176,1.8653060422209684,1.8662889753158969,1.8672796568252001,1.868277971514274,1.8692837997946836,1.870297017773918,1.8713174973099844,1.8723451060707772,1.873379707598157,1.8744211613766633,1.87546932290678,1.8765240437826616,1.8775851717742236,1.8786525509134955,1.879726021585123,1.880805420620903,1.881890581398232,1.8829813339423351,1.8840775050321439,1.885178918309686,1.886285394392837,1.8873967509912926,1.8885128030256026,1.889633362749115,1.890758239872665,1.891887241691847,1.8930201732167053,1.8941568373036664,1.8952970347895493,1.896440564627472,1.897587224024482,1.8987368085807286,1.899889112430002,1.9010439283814555,1.9022010480623288,1.9033602620614922,1.9045213600736282,1.9056841310438692,1.9068483633127093,1.9080138447610067,1.9091803629549002,1.9103477052904578,1.911515659137881,1.9126840119850863,1.9138525515804963,1.9150210660748583,1.9161893441619327,1.9173571752178744,1.9185243494391526,1.9196906579788418,1.9208558930811315,1.9220198482139013,1.9231823181992114,1.924343099341565,1.9255019895538024,1.9266587884804893,1.9278132976186757,1.9289653204358903,1.9301146624852574,1.9312611315176202,1.9324045375905605,1.9335446931742113,1.9346814132537675,1.9358145154286004,1.9369438200078954,1.9380691501027278,1.939190331714511,1.9403071938197467,1.9414195684510223,1.942527290774196,1.9436301991617313,1.9447281352621362,1.9458209440654732,1.9469084739649236,1.9479905768143695,1.9490671079819986,1.9501379263999112,1.9512028946097384,1.9522618788042698,1.9533147488651106,1.9543613783963805,1.9554016447544806,1.9564354290739623,1.957462616289525,1.9584830951541936,1.9594967582537173,1.960503502017238,1.961503226724298,1.9624958365082277,1.9634812393560006,1.964459347104611,1.9654300754340532,1.9663933438569823,1.9673490757051337,1.9682971981125903,1.9692376419959818,1.9701703420317092,1.9710952366302863,1.9720122679078946,1.9729213816552493,1.9738225273038752,1.9747156578898932,1.9756007300154224,1.9764777038076986,1.9773465428760126,1.978207214266579,1.9790596884154321],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[1.8578054889777014,1.8578054868222371,1.8578054846266587,1.857805482390221,1.857805480112163,1.8578054777917108,1.8578054754280748,1.8578054730204516,1.8578054705680223,1.8578054680699536,1.8578054655253955,1.8578054629334841,1.857805460293336,1.857805457604056,1.8578054548647283,1.8578054520744218,1.8578054492321876,1.8578054463370597,1.8578054433880535,1.8578054403841666,1.8578054373243778,1.8578054342076462,1.8578054310329126,1.8578054277990976,1.8578054245051012,1.8578054211498043,1.8578054177320646,1.857805414250722,1.857805410704592,1.8578054070924683,1.8578054034131237,1.8578053996653068,1.8578053958477436,1.8578053919591364,1.8578053879981624,1.8578053839634752,1.8578053798537029,1.8578053756674482,1.8578053714032885,1.8578053670597732,1.8578053626354256,1.8578053581287421,1.8578053535381895,1.8578053488622082,1.857805344099207,1.8578053392475682,1.8578053343056407,1.8578053292717451,1.8578053241441703,1.8578053189211718,1.857805313600975,1.8578053081817705,1.8578053026617158,1.857805297038934,1.8578052913115144,1.8578052854775091,1.8578052795349345,1.85780527348177,1.8578052673159582,1.8578052610354021,1.8578052546379673,1.8578052481214782,1.8578052414837194,1.857805234722434,1.8578052278353239,1.8578052208200468,1.8578052136742185,1.8578052063954087,1.857805198981143,1.8578051914289009,1.8578051837361147,1.8578051759001697,1.8578051679184,1.857805159788094,1.8578051515064866,1.8578051430707623,1.8578051344780533,1.857805125725438,1.8578051168099408,1.8578051077285318,1.8578050984781218,1.857805089055567,1.8578050794576637,1.8578050696811488,1.8578050597226987,1.857805049578928,1.857805039246388,1.857805028721566,1.8578050180008838,1.8578050070806964,1.8578049959572922,1.8578049846268891,1.857804973085635,1.8578049613296066,1.8578049493548074,1.857804937157166,1.8578049247325361,1.8578049120766937,1.857804899185336,1.857804886054081,1.8578048726784648,1.8578048590539398,1.8578048451758744,1.8578048310395507,1.8578048166401628,1.8578048019728168,1.8578047870325252,1.8578047718142092,1.8578047563126963,1.8578047405227158,1.8578047244389007,1.8578047080557827,1.8578046913677932,1.8578046743692591,1.8578046570544013,1.8578046394173346,1.8578046214520634,1.8578046031524802,1.8578045845123645,1.8578045655253803,1.8578045461850727,1.8578045264848677,1.857804506418069,1.8578044859778546,1.8578044651572767,1.8578044439492583,1.8578044223465893,1.857804400341928,1.8578043779277928,1.8578043550965666,1.8578043318404873,1.85780430815165,1.8578042840220037,1.857804259443345,1.8578042344073202,1.8578042089054196,1.8578041829289744,1.8578041564691559,1.8578041295169698,1.8578041020632547,1.8578040740986803,1.8578040456137408,1.8578040165987546,1.8578039870438599,1.8578039569390115,1.8578039262739765,1.8578038950383329,1.8578038632214648,1.857803830812557,1.8578037978005961,1.8578037641743614,1.857803729922424,1.8578036950331436,1.8578036594946616,1.8578036232949007,1.8578035864215576,1.8578035488621005,1.8578035106037651,1.8578034716335488,1.857803431938208,1.8578033915042524,1.8578033503179403,1.8578033083652759,1.8578032656320016,1.8578032221035952,1.8578031777652646,1.8578031326019424,1.8578030865982815,1.8578030397386485,1.8578029920071197,1.8578029433874763,1.8578028938631959,1.8578028434174503,1.8578027920330977,1.8578027396926788,1.8578026863784076,1.8578026320721694,1.8578025767555117,1.8578025204096384,1.857802463015405,1.8578024045533108,1.8578023450034915,1.8578022843457147,1.857802222559372,1.8578021596234708,1.8578020955166288,1.8578020302170664,1.857801963702598,1.857801895950627,1.857801826938136,1.8578017566416802,1.8578016850373782,1.8578016121009058,1.8578015378074852,1.8578014621318795,1.8578013850483819,1.8578013065308074,1.8578012265524857,1.8578011450862495,1.8578010621044276,1.857800977578833,1.8578008914807569,1.8578008037809552,1.857800714449642,1.857800623456476,1.8578005307705538,1.8578004363603964,1.8578003401939405,1.8578002422385282,1.8578001424608923,1.8578000408271496,1.8577999373027858,1.8577998318526472,1.8577997244409252,1.857799615031147,1.8577995035861627,1.8577993900681304,1.8577992744385075,1.8577991566580336,1.857799036686721,1.857798914483837,1.857798790007894,1.857798663216634,1.8577985340670131,1.8577984025151884,1.8577982685165033,1.8577981320254717,1.8577979929957638,1.857797851380187,1.8577977071306748,1.8577975601982675,1.8577974105330965,1.8577972580843678,1.8577971028003442,1.8577969446283271,1.8577967835146414,1.8577966194046143,1.85779645224256,1.857796281971757,1.8577961085344334,1.857795931871743,1.8577957519237491,1.857795568629403,1.857795381926522,1.857795191751771,1.8577949980406385,1.8577948007274179,1.8577945997451828,1.8577943950257654,1.8577941864997334,1.8577939740963676,1.8577937577436359,1.857793537368172,1.8577933128952477,1.8577930842487502,1.8577928513511552,1.8577926141235013,1.8577923724853624,1.8577921263548236,1.8577918756484495,1.8577916202812597,1.8577913601666982,1.8577910952166061,1.857790825341189,1.8577905504489909,1.8577902704468598,1.8577899852399193,1.8577896947315355,1.8577893988232839,1.8577890974149167,1.8577887904043315,1.8577884776875344,1.8577881591586056,1.8577878347096644,1.8577875042308343,1.8577871676102038,1.8577868247337914,1.8577864754855038,1.8577861197471024,1.85778575739816,1.857785388316021,1.8577850123757609,1.8577846294501459,1.8577842394095883,1.8577838421221033,1.8577834374532678,1.8577830252661722,1.8577826054213762,1.8577821777768633,1.8577817421879914,1.8577812985074464,1.8577808465851935,1.8577803862684246,1.8577799174015106,1.857779439825947,1.8577789533803049,1.8577784579001722,1.8577779532181047,1.8577774391635653,1.857776915562872,1.857776382239137,1.8577758390122099,1.8577752856986172,1.857774722111502,1.857774148060562,1.8577735633519876,1.857772967788395,1.8577723611687647,1.8577717432883725,1.8577711139387227,1.8577704729074807,1.8577698199784007,1.857769154931255,1.8577684775417642,1.8577677875815193,1.8577670848179098,1.8577663690140458,1.8577656399286808,1.8577648973161318,1.8577641409262005,1.8577633705040886,1.8577625857903166,1.8577617865206384,1.857760972425954,1.857760143232223,1.8577592986603744,1.857758438426215,1.8577575622403375,1.8577566698080266,1.857755760829162,1.857754834998122,1.8577538920036833,1.8577529315289205,1.8577519532511022,1.8577509568415886,1.8577499419657235,1.857748908282726,1.8577478554455813,1.857746783100928,1.8577456908889445,1.857744578443232,1.8577434453906994,1.8577422913514396,1.8577411159386106,1.8577399187583095,1.8577386994094467,1.8577374574836187,1.8577361925649762,1.8577349042300917,1.8577335920478233,1.85773225557918,1.8577308943771798,1.8577295079867087,1.8577280959443758,1.857726657778366,1.8577251930082923,1.8577237011450414,1.8577221816906222,1.8577206341380068,1.8577190579709715,1.8577174526639346,1.8577158176817916,1.8577141524797474,1.8577124565031455,1.8577107291872963,1.857708969957298,1.8577071782278614,1.8577053534031254,1.8577034948764717,1.8577016020303403,1.8576996742360348,1.8576977108535309,1.8576957112312777,1.857693674705999,1.8576916006024877,1.8576894882334016,1.8576873368990516,1.857685145887188,1.8576829144727867,1.857680641917825,1.8576783274710602,1.8576759703678036,1.857673569829688,1.857671125064433,1.8576686352656102,1.8576660996123981,1.85766351726934,1.857660887386093,1.8576582090971756,1.8576554815217132,1.8576527037631745,1.85764987490911,1.8576469940308824,1.8576440601833943,1.8576410724048134,1.8576380297162904,1.857634931121676,1.8576317756072318,1.857628562141338,1.8576252896741965,1.85762195713753,1.857618563444276,1.857615107488278,1.8576115881439712,1.8576080042660648,1.8576043546892185,1.8576006382277157,1.8575968536751317,1.8575929998039993,1.8575890753654654,1.8575850790889494,1.8575810096817904,1.8575768658288958,1.8575726461923823,1.857568349411209,1.8575639741008163,1.8575595188527467,1.8575549822342732,1.8575503627880137,1.857545659031546,1.8575408694570188,1.8575359925307524,1.857531026692842,1.8575259703567515,1.8575208219089034,1.8575155797082663,1.8575102420859344,1.8575048073447065,1.8574992737586566,1.8574936395727035,1.857487903002172,1.8574820622323538,1.8574761154180597,1.8574700606831733,1.8574638961201932,1.8574576197897763,1.8574512297202754,1.8574447239072713,1.8574381003131042,1.8574313568663952,1.8574244914615718,1.8574175019583832,1.8574103861814129,1.8574031419195924,1.8573957669257042,1.8573882589158865,1.8573806155691346,1.8573728345267944,1.8573649133920587,1.8573568497294577,1.8573486410643474,1.8573402848823928,1.8573317786290549,1.8573231197090687,1.8573143054859242,1.8573053332813416,1.8572962003747493,1.8572869040027542,1.85727744135862,1.8572678095917348,1.8572580058070833,1.85724802706472,1.8572378703792367,1.8572275327192378,1.8572170110068054,1.8572063021169773,1.857195402877217,1.8571843100668888,1.8571730204167352,1.8571615306083535,1.8571498372736777,1.8571379369944627,1.8571258263017711,1.857113501675463,1.8571009595436925,1.8570881962824057,1.857075208214847,1.857061991611068,1.8570485426874435,1.857034857606196,1.8570209324749236,1.8570067633461393,1.856992346216815,1.8569776770279378,1.8569627516640732,1.8569475659529386,1.85693211566499,1.8569163965130162,1.8569004041517467,1.8568841341774753,1.856867582127691,1.8568507434807289,1.8568336136554329,1.8568161880108338,1.856798461845846,1.8567804303989806,1.8567620888480758,1.856743432310046,1.8567244558406544,1.8567051544343,1.856685523023834,1.8566655564803884,1.8566452496132424,1.8566245971696969,1.856603593834989,1.8565822342322222,1.8565605129223313,1.8565384244040704,1.856515963114034,1.856493123426708,1.8564698996545492,1.8564462860481026,1.8564222767961476,1.8563978660258826,1.8563730478031435,1.856347816132659,1.8563221649583463,1.856296088163644,1.8562695795718862,1.856242632946718,1.8562152419925542,1.8561874003550813,1.8561591016218044,1.8561303393226407,1.856101106930561,1.856071397862277,1.8560412054789812,1.8560105230871342,1.855979343939308,1.8559476612350772,1.8559154681219667,1.855882757696456,1.8558495230050336,1.855815757045316,1.8557814527672165,1.8557466030741798,1.8557112008244718,1.8556752388325313,1.8556387098703824,1.8556016066691121,1.8555639219204052,1.8555256482781468,1.8554867783600901,1.8554473047495819,1.8554072199973644,1.8553665166234303,1.855325187118956,1.8552832239482901,1.8552406195510174,1.8551973663440835,1.855153456723993,1.855108883069064,1.8550636377417626,1.8550177130910919,1.8549711014550565,1.854923795163187,1.8548757865391328,1.85482706790332,1.8547776315756723,1.8547274698783955,1.8546765751388261,1.85462493969234,1.8545725558853219,1.854519416078195,1.8544655126485077,1.8544108379940751,1.8543553845361798,1.8542991447228208,1.8542421110320166,1.8541842759751572,1.854125632100402,1.8540661719961244,1.8540058882943975,1.8539447736745198,1.8538828208665814,1.8538200226550612,1.8537563718824577,1.853691861452951,1.8536264843360866,1.8535602335704868,1.8534931022675785,1.8534250836153385,1.8533561708820547,1.8532863574200904,1.853215636669661,1.8531440021626102,1.8530714475261856,1.8529979664868088,1.8529235528738377,1.8528482006233142,1.8527719037817,1.8526946565095876,1.85261645308539,1.8525372879090019,1.852457155505432,1.8523760505283948,1.8522939677638715,1.8522109021336204,1.85212684869865,1.8520418026626362,1.8519557593752929,1.8518687143356838,1.8517806631954765,1.85169160176214,1.8516015260020742,1.8515104320436737,1.8514183161803317,1.8513251748733612,1.8512310047548606,1.8511358026304934,1.8510395654822038,1.8509422904708546,1.8508439749387915,1.8507446164123342,1.8506442126041918,1.8505427614158072,1.85044026093963,1.8503367094613152,1.85023210546186,1.8501264476196693,1.8500197348125618,1.8499119661197123,1.8498031408235447,1.8496932584115677,1.8495823185781652,1.8494703212263464,1.8493572664694586,1.8492431546328698,1.8491279862556307,1.849011762092116,1.8488944831136618,1.8487761505102,1.8486567656919008,1.8485363302908324,1.8484148461626477,1.8482923153883069,1.8481687402758442,1.8480441233621951,1.8479184674150897,1.847791775435024,1.847664050657322,1.8475352965543062,1.847405516837573,1.8472747154604041,1.8471428966203143,1.8470100647617507,1.8468762245789594,1.8467413810190332,1.8466055392851461,1.8464687048399955,1.8463308834094643,1.8461920809865102,1.846052303835302,1.845911558495614,1.8457698517874854,1.8456271908161666,1.8454835829773582,1.8453390359627535,1.845193557765905,1.8450471566884112,1.8448998413464492,1.8447516206776533,1.8446025039483542,1.8444525007611834,1.844301621063057,1.844149875153541,1.843997273693607,1.8438438277147862,1.8436895486287228,1.8435344482371374,1.8433785387421966,1.8432218327572976,1.8430643433182674,1.8429060838949738,1.8427470684033564,1.8425873112178652,1.8424268271843145,1.8422656316331412,1.8421037403930671,1.8419411698051593,1.841777936737276,1.841614058598899,1.8414495533563355,1.8412844395482828,1.8411187363017425,1.8409524633482726,1.8407856410405603,1.8406182903693065,1.8404504329803963,1.8402820911923476,1.8401132880140096,1.8399440471624984,1.8397743930813446,1.8396043509588302,1.839433946746495,1.8392632071777824,1.8390921597868026,1.8389208329271867,1.8387492557910008,1.8385774584276953,1.8384054717630547,1.8382333276181237,1.8380610587280715,1.837888698760966,1.8377162823364257,1.837543845044109,1.837371423462016,1.8371990551745587,1.8370267787903685,1.8368546339598057,1.8366826613921285,1.8365109028722906,1.8363394012773273,1.83616820059229,1.835997345925695,1.8358268835244458,1.8356568607881891,1.8354873262830718,1.8353183297548512,1.8351499221413317,1.8349821555840764,1.8348150834393695,1.8346487602883799,1.8344832419464956,1.8343185854717907,1.8341548491725865,1.833992092614073,1.8338303766239552,1.8336697632970895,1.8335103159990773,1.833352099368781,1.8331951793197332,1.8330396230404087,1.8328854989933263,1.8327328769129554,1.8325818278024,1.8324324239288308,1.832284738817649,1.83213884724535,1.8319948252310694,1.831852750026797,1.8317127001062325,1.8315747551522723,1.8314389960431126,1.8313055048369562,1.8311743647553136,1.8310456601648895,1.8309194765580508,1.8307959005318724,1.8306750197657577,1.8305569229976393,1.8304416999987578,1.8303294415470324,1.8302202393990255,1.8301141862605186,1.830011375755707,1.8299119023950388,1.8298158615417088,1.8297233493768374,1.8296344628633552,1.8295492997086222,1.8294679583258138,1.8293905377941053,1.8293171378176953,1.8292478586837002,1.82918280121897,1.829122066745865,1.829065757037043,1.829013974269307,1.8289668209765657,1.8289244000019633,1.828886814449235,1.828854167633351,1.8288265630305076,1.8288041042275336,1.8287868948707777,1.8287750386145432,1.828768639069143,1.8287677997486467,1.8287726240183886,1.828783215042318,1.8287996757302596,1.8288221086851657,1.8288506161504343,1.8288852999573695,1.8289262614728614,1.8289736015473665,1.8290274204632544,1.829087817883611,1.8291548928015575,1.8292287434901728,1.8293094674530797,1.82939716137577,1.8294919210777407,1.8295938414654958,1.8297030164864874,1.8298195390840515,1.8299435011533924,1.8300749934986773,1.83021410579128,1.8303609265292298,1.8305155429978994,1.8306780412319723,1.8308485059787214,1.8310270206626267,1.8312136673513533,1.8314085267231093,1.831611678035395,1.8318231990951468,1.8320431662302827,1.8322716542626425,1.8325087364823098,1.8327544846233064,1.8330089688406321,1.8332722576886273,1.8335444181006242,1.83382551536985,1.8341156131315377,1.8344147733462008,1.8347230562840116,1.835040520510233,1.835367222871638,1.8357032184838513,1.8360485607195405,1.836403301197388,1.8367674897717607,1.837141174523002,1.8375244017482582,1.8379172159527608,1.8383196598414708,1.8387317743110019,1.8391535984417322,1.8395851694900123,1.8400265228803845,1.8404776921977188,1.8409387091791827,1.8414096037059515,1.8418904037945816,1.8423811355879538,1.842881823345718,1.8433924894341502,1.8439131543153577,1.8444438365357578,1.844984552713766,1.8455353175266345,1.8460961436963794,1.8466670419747617,1.8472480211272577,1.8478390879159974,1.8484402470816321,1.8490515013241067,1.849672851282319,1.8503042955126567,1.850945830466406,1.8515974504660324,1.8522591476803527,1.8529309120986075,1.8536127315034636,1.854304591442978,1.8550064752015674,1.8557183637700227,1.8564402358146326,1.8571720676454702,1.8579138331839162,1.858665503929492,1.8594270489260818,1.8601984347276403,1.860979625363466,1.8617705823031536,1.8625712644213193,1.863381627962215,1.8642016265043448,1.8650312109251967,1.8658703293662184,1.8667189271981601,1.86757694698691,1.8684443284599588,1.8693210084736207,1.8702069209811487,1.8711019970018787,1.8720061645915398,1.8729193488138653,1.873841471713646,1.874772452291353,1.875712206479473,1.8766606471206855,1.877617683948013,1.8785832235670736,1.8795571694405626,1.8805394218750842,1.8815298780104528,1.8825284318115822,1.8835349740630676,1.8845493923665722,1.8855715711411138,1.8866013916263544,1.8876387318889738,1.888683466832219,1.889735468208703,1.8907946046365252,1.8918607416187778,1.8929337415664986,1.8940134638251154,1.895099764704432,1.896192497512186,1.8972915125912118,1.8983966573602253,1.8995077763582475,1.9006247112926693,1.9017473010909558,1.9028753819559812,1.904008787424974,1.9051473484320494,1.9062908933742875,1.9074392481813276,1.908592236388416,1.909749679212861,1.9109113956338217,1.9120772024753643,1.913246914492704,1.9144203444615433,1.9155973032704205,1.916777600015957,1.917961042100907,1.919147435334888,1.9203365840376774,1.9215282911449478,1.92272235831631,1.9239185860455255,1.9251167737727521,1.9263167199986702,1.9275182224003429,1.928721077948655,1.9299250830271704,1.931130033552246,1.9323357250942446,1.933541952999664,1.9347485125140298,1.9359551989053683,1.937161807588091,1.9383681342471162,1.9395739749620524,1.9407791263312673,1.9419833855956705,1.943186550762032,1.9443884207256645,1.9455887953922957,1.9467874757989643,1.9479842642337664,1.9491789643542878,1.9503713813045611,1.9515613218303867,1.9527485943928584,1.9539330092799487,1.955114378715997,1.9562925169689644,1.9574672404553135,1.9586383678423795,1.959805720148107,1.9609691208380273,1.9621283959193654,1.96328337403216,1.9644338865372988,1.965579767601368,1.9667208542782306,1.967856986587244,1.9689880075880468,1.9701137634518395,1.9712341035291037,1.9723488804136986,1.9734579500032903,1.974561171556075,1.9756584077437582,1.9767495247007696,1.9778343920696886,1.9789128830428762,1.979984874400302,1.9810502465435702,1.982108883526158,1.9831606730798743,1.9842055066375661,1.9852432793521013,1.9862738901116568,1.9872972415513597,1.9883132400613195,1.9893217957911111,1.9903228226507583,1.9913162383082808,1.992301964183876,1.9932799254407998,1.9942500509730245,1.9952122733897528,1.9961665289968697,1.9971127577754177,1.9980509033571865,1.9989809129975051,1.999902737545337,2.000816331410767,2.001721652529983,2.002618662327855],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","FigRMSEGRidRidge_log = visuRMSEGrid(Ridge(), 'Ridge', alphasridge_log, 'alpha',\n","                                    GridRidge_log)\n","FigRMSEGRidRidge_log.show()\n","if write_data is True:\n","    FigRMSEGRidRidge_log.write_image(\n","        './Figures/EmissionsGraphRMSERidge_log.pdf')"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.3 Modèle Lasso"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha     0.55\n","       Lasso()\n","R²    0.121578\n","RMSE  1.952310\n","MAE   1.629347\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logLasso=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.990612746230603,4.982549420789273,5.01031633149296,5.03312893270772,5.158888683184256,5.035964554832657,4.989903645016396,5.006289579445524,5.1530264568964865,4.995554847767846,5.0831200085106065,4.988925798531015,4.980087836189453,5.050255139736665,4.992141802595297,5.14043852245671,4.999430351079843,5.022916130767835,5.136647543364604,5.027307260863849,4.974281161940147,5.015569310661398,4.998738928289258,5.021291827000005,5.17101500174781,5.046101389444273,5.254844287044783,5.018064803373169,5.110376023117573,5.021131946766899,5.166512846069394,5.070135772626356,5.035010601623641,5.0402191357224195,5.032017578624704,5.093696213650497,6.403900028530703,4.99085195428514,5.027213178693199,4.996067003413859,5.083400899019281,5.012830596185993,4.98331352154364,5.142779611234548,5.07544560838898,5.0014221201413545,5.019808188572732,5.097880947053361,4.983325163416127,5.016339391950963,4.983215308079068,5.022710059641801,5.852446617602542,5.032017578624704,5.016209233338099,5.0253727576149645,5.407261142367334,4.984545118389368,5.128150882016692,5.021101927675706,5.124224307703119,5.242600996551285,4.990600960614854,5.252324261569837,5.160271757503179,5.013520963067167,5.09700346386245,5.029978667100197,4.989565790698269,4.994578605930005,4.986917955693419,5.160199599166025,4.995745381889115,4.985652919284315,5.054394534392701,5.12090009728441,5.39499882952706,5.050880559862656,5.077981668161635,5.000553913114541,5.033704885946045,5.225383226234124,5.201114559724935,5.297617085606755,4.992368802977143,4.983907888255983,4.993765359182816,5.01995696517531,5.174090450683479,5.451871149161603,5.007055644469182,5.014455046890008,5.214694086932139,5.01083664996676,5.015263747638702,5.095542967589826,7.366098577806479,5.163245200826046,5.062123934054487,5.010968468897715,5.175263880971696,5.008776344368477,5.05616794030724,5.019987359980446,5.183533624899971,5.011456857557797,5.043219806393743,5.340695058996878,4.980653900923041,5.142081179682236,5.119343073741626,5.005547085653363,4.981995496849089,4.988518835165936,4.993218762817398,4.99236674473701,5.003273045444934,5.80074843492845,5.14919539606063,5.160719718173217,4.986132247976846,4.983774549854079,4.980498723649018,5.847987493504715,5.081339551638262,5.021725523818373,4.996487875681277,5.005305480530517,4.986202961671338,5.197627338867568,5.236253846649583,4.997363939785256,4.982610313137307,4.980515939713126,5.262929501363379,4.991317918906227,5.011293713961729,5.0744484912707915,5.1530264568964865,4.991940592271611,4.993360758969317,5.745768257184925,5.154887346035303,5.037597240444454,5.000400045188783,5.1332530688954465,5.0191232618597015,5.021222095555255,5.027959744208854,4.980428009954527,5.121809157165081,5.091085120487365,4.9769151650823655,5.009878299440971,5.016858457240839,5.014058264493139,5.00046441004727,4.981414073138826,5.54528597906854,5.0222748029392,4.981025147819122,4.989439249469022,4.980828720889979,5.016002891091657,4.997680187141176,4.99458646300717,5.479482051751563,5.090030307877865,5.132058543826396,5.146962194330549,5.007448459066933,4.995759394638067,5.026775990633822,5.071713812135127,4.984403691000385,5.344042913931929,5.0008868107487805,5.1523882769664695,5.017362763224911,4.982423707554622,5.046018080869995,5.498257228447367,4.989910280296888,5.064953636801454,4.9970066844386265,5.156226182999133,4.981920854616015,4.998872929194747,4.987635987594106,5.167272160437033,5.057248885297931,5.144787426763591,5.0058495831242436,5.119286018107822,4.99770336582177,5.067321945306283,5.012854475291669,5.007055644469182,5.044164845041068,5.056022193530912,5.277353469420275,4.983891016715321,5.021066210431526,4.9946021543383115,5.000769065709047,4.989982215788053,4.983324488776966,4.980428009954527,5.065059593330317,5.074471804455881,5.588706458017937,5.000798089841605,4.984018694219265,5.598269355221765,5.051772234888639,5.876097047809778,5.107234108562852,5.003936947229511,5.4038130853884425,5.013740033171748,4.995429134533195,5.074631666878224,4.990551853882568,4.996527161067106,5.03167367166839,4.980956398393921,5.03012009448918,4.996067197105273,5.007464212481801,5.01995696517531,4.9867034727729305,5.0662705065287,4.988886923526936,5.052402818472262,4.985312056505755,4.9896286473155955,4.986941448403843,4.989809241902875,5.349265287188736,5.005135922930714,5.185019458209227,4.981673356685294,4.990630424654226,4.987125369310425,5.033997676362997,4.994079681529981,5.452632514889813,5.018346264356335,5.350499102380551,5.008547466599411,5.066105555716941,4.984488426244155,5.0619540472386815,5.030705446738027,5.170608562751001,5.049760823134212,5.089804320996761,5.061572157563124,5.132058543826396,5.011007754283544,5.077292076672561,4.992148804816503,5.038140778982939,5.039653598529496,4.980563544535635,5.134705871345865,4.998705613014608,5.139150790048713,4.990600960614854,4.997680187141176,5.05388037571065,5.036753392625811,5.066336475574343,5.023397523998478,4.982716460982349,5.093574869511736,5.138791166273507,5.839690354734058,5.169993558257502,5.01752157392663,5.06916387519498,5.008528846437756,4.973677349764714,5.13301637705157,5.028953318530069,5.025485142100753,5.029325563883046,5.012762011385497,5.041794904567051,5.012170366140854,5.509390015982911,4.996442352009956,5.001697547272519,4.983036559573549,5.078051056158381,5.116459551194087,4.990308284490431,5.0369091461240565,5.00882075709064,4.991224494180005,5.045747820971816,5.351604221212431,5.0906441111046705,4.99908269612097,5.9124536720025445,5.072017958475431,5.317826018218744,4.999901365821825,5.041814309429034,5.021707793153387,4.999614945407816,4.990545702161091,5.005634501677389,5.354252606020919,4.990079649724816,5.0239017058611575,5.083146456837394,5.063565997845596,4.991852200153497,5.011746319537123,5.132046739573628,5.257854772425474,4.992123269315714,5.0147320088601,5.002476932750854,4.985041295623638,4.998556251245155,5.034174346306696,5.864541161966022,5.013933956640614,5.024474784545604,4.978078898982371,5.023817351869024,5.1980012749843425,5.120599089078542,5.053492361526442,5.285797484061259,5.227240451979135,5.057007357655406,4.99274790695039,4.9833594563498345,4.9872990239359565,5.0092755082679075,5.011762033691454,5.249682825943235,5.249243898546526,5.014888035010161,5.0078281610455555,4.973677349764714,4.9842054667198665,4.986112605283932,5.001431941487812,4.98608314124456,5.041689049029591,5.147005902945151,5.8359343931002865,5.285198063105631,5.875562231397622,5.004934086290398,5.201471161857985,5.0033981750485355,5.031154620234649,5.012830596185993,5.165257124076295,4.980598901382881,4.990079649724816,5.213059545857494,5.071840830736739,5.213059545857494,5.058012596959199,5.011066936229086,5.002631611422016,5.135188858887894,5.249207977039348,5.457592478526558,5.019387260322779,5.028197833356909,5.038140778982939,5.190805713727734,5.079566595217468,4.993841789533758,4.990002622822605,5.490753372306675,5.060483769196141,5.002608788124471,5.230379313019994,5.051213754404669,4.995490304031601,5.201709062957686,5.00059319850037,5.199761329334452,5.009148897277575,5.073938983832182,5.002078191428501,4.998870466728518,5.011414358026871,5.016118107528845,4.983324488776966,4.989565790698269,5.011762033691454,5.010742022168147,5.114084749620745,4.991032455876944,5.044329222133559,5.010874183971727,5.006034224437638,5.021378371906457,5.029086888841888,5.001852397920753,5.02819903912216,5.001645261563039,5.0639748576914645,5.00112927768067,5.157859216464123,5.121462499689573,5.017772936362933,5.00527132503783,5.004576736623395,5.180923440160559,5.089617154024562,5.053313630939633,4.999827133476712,4.988432407317113,5.001762078291535,5.0355496570047436,5.029348136657648,5.962225286602063,5.006486929327681,4.998573499075106,4.995167886717434,5.012343457401719,5.105600932975792,5.060774481051273,8.466720152517738,5.447106080400984,5.024006839203702,5.5023213940633235,4.984403691000385,5.0075624259463725,4.98957954058331,5.007625392565963,4.991539568469636,4.991681871480535,4.993758924976972,5.70118799663817,5.026423339682705,5.072469330788096,4.990985957395975,5.007965905740867,5.0253727576149645,4.999339994692437,5.017265916246048,4.988359481127342,5.159028582538229,5.041869067041185,5.121462499689573,4.981746034649078,5.234880040301562,4.979938906900959,4.981429787293157,5.013359224896301,5.004305667461177,5.171413966301563,4.97044364814033,5.463294940856801,4.983983197173142,5.001396584640566,4.980408708152807,4.9817336158934555,5.07544560838898,4.990390783800671,5.162949405125306,5.684021223273184,5.724256776287238,5.004665178918695,5.008644747179756,5.185019458209227,5.046558007247399,4.979846586244262,5.0421069827053255,5.119194720862183,5.040931432669224,5.006663580826321,5.066058695166472,5.069250888507681,4.983715786772557,5.105557856626637,4.981479082724512,5.262716594399906,5.037812173034429,4.991508889952553,5.003997413958186,4.983983197173142,4.981673356685294,5.0144904420980785,5.706655179770403,5.022098018702971,5.0124171459035125,5.570985572423645,5.164477968156927,5.034857912020115,5.1739359246059164,5.069699217381339,4.998674739149014,5.304587134160514,5.235798739401084,5.026003009547049,5.710514562703665,5.0683212036689556,4.985346540260273,4.990944297852893,5.2843377904556394,5.051840984313839,5.017108774702733,5.201252490120317,4.993324997665783,4.981025147819122,5.650038780632767,4.978137859496477,5.0429664156551475,5.153712463894382,5.413886920697103,4.994578605930005,4.986195104594172,5.025284063493674,4.98039265310728,5.032056864010533,5.010581507847304,4.98897650991084,5.028153539876667,5.095304370925361,5.055057724450706,5.042124965349052,4.9845548386217535,5.048626105497527,4.990673877631781,4.998872929194747,4.99214487627792,5.130027101547919,5.042917518824057,5.001697547272519,5.064158242116599,4.995539650453145,5.070339986363671,4.996216194896743,5.015518625824881,5.0822930511389135,5.066450707029251,4.985187434447667,5.789998648907277,5.198605797314377,4.98125113589694,5.18902350929793,5.020422496997379,5.002528637269342,4.989510791158109,5.02850290599816,5.0532476534785316,4.991922913847988,5.150696914679912,4.992928619725202,5.033555601479896,5.067180910062281,5.060483769196141,5.259293835983525,5.083922069743849,5.130027101547919,6.21457529644008,5.045217468263129,5.003309782930422,4.99255486329539,5.487598834248404,5.008256879132616,4.9923295175913145,5.4550542388555705,4.978331553316135,5.083295739108345,4.9872990239359565,5.007487783713298,5.036253236674615,5.044518188395379,5.019887095583487,5.004969134683742,5.009148897277575,4.986701886071361,5.160719718173217,5.030272836367476,5.0401508492187785,5.185307060180617,4.9892112236385575,5.062589860841943,5.135575130694628,5.0680285275445325,5.495796901224092,5.090601720676298,5.049002168047782,5.364302228748627,5.014977542521529,5.161446041508815,5.050705636663391,5.1909444756104985,5.005466296594778,5.205576484677079,5.777768144928271,5.001125515478348,5.6267438389304,4.987255810011545,5.226493615671097,4.991718629841679,4.983774549854079,4.998995737870037,4.980763900003361,5.057348535628672,5.059853750337279,5.656079214434512,5.216725677175249,5.00450864490401,5.062140656914001,4.984207264071242,5.058908563887464,5.142880937433143,4.985002793134273,5.020006071907596,5.034242545327002,5.00107161739592,4.986917955693419,5.0402777751513295,5.011652034611134,4.987459035452385,4.987937147286127,5.251730920217863,4.982639777176679,4.9921454389730675,5.133805768131334,4.981810855535695,5.045709695035336,5.745768257184925,5.023952452057422,5.031694004012073,5.021019789435178,5.559112379538909,7.034947772007287,5.018007157957058,4.976682102011998,5.0029149648028435,5.095353477657647,4.98612170468775,5.025672988813378,5.019309311015803,4.994240751611878,5.145915586244041,5.2735355189259945,5.0286533912010665,5.01096061182055,4.984852023267506,5.054899703727658,5.042269383716568,4.996307162906465,5.2280528475038155,4.983519769819241,5.015704433961046,5.018571941103457,5.027385831635507,5.092014222383522,5.036888490863517,4.999302901535505,4.993392290911672,5.029330759776818,5.000571039031014,4.998214468388446,5.0019171160027955,4.992393032092398,5.036773599549698,5.003276390352466,5.002042829237447,5.254844287044783,7.469303426165776,5.028393501782012,5.097319711218371,4.9849615434791525,4.980408708152807,5.025894878614293,4.983562983743653,5.025346495124733,5.102105894456933,4.993640393446532,5.847987493504715,5.116741759336511,5.027957805066067,5.0802340398179195,4.990762030696752,5.156895777144832,4.995018714810358,5.0067662874663705,5.163245200826046,5.142880937433143,5.529655870190215,5.15179086787849,4.999345013900787,4.980092658350743,4.991765285201273,5.1035332488545695,4.989951727769461,4.991475060449542,4.997371796862422,4.985817964890217,5.0433599463855066,5.028963883969934,5.0049846613901785,5.139974794494474,5.001396584640566,7.434367199188749,4.987744517727208,6.735740797229348,5.0852019072510934,4.987805805413146,4.982664475564488,5.004321381615509,4.980750150118321,5.382385698234885,5.591242062810427,5.161128935836702,4.9918611552323995,4.991396489677884,5.150004147318076,5.0501013713346925,4.982423707554622,4.99475729833172,5.003537942761617,5.6631844444248935,5.009739357754347,4.995906451971013,5.532256074804474,5.087529103332406,5.049002168047782,5.131505992047855,5.4742755006773365,4.980563544535635,5.005135922930714,5.107777172152067,4.9870574188131105,4.997277697003251,5.064534012445088,5.174090450683479,5.1198430531700785,5.243361163038555,5.231768703246061,5.035599087291168,4.995281376193512,5.126596296482594,5.378113900294886,5.372297158636778,4.992559337098412,5.656079214434512,5.013775144415725,5.020548210232031,5.007760353621983,4.9874208086320255,5.156506048168494,5.068551701096002,4.9874208086320255,5.118928492062867,4.999602975306904,5.006729965166413,5.01116481503534,5.010921326434722,5.07598228795118,5.177341908249979,5.164113488680447,4.99143509377511,5.052076696628811,4.9828637038759025,4.981194074978185,5.002170506741391,4.98232399656991,4.9959280589332185,4.99395396829533,4.999039788995294,5.162547281526941,5.517465590914276,5.10137789157447,5.027113119552791,4.987656520946997,4.994723961857571,5.07117921548799,5.029485635508048,5.067590999845135,5.008290430749529,4.981429787293157,5.011624534841054,5.134118332124063,4.999050755241043,5.009235843995641,5.010856505548104,5.5787326299026505,5.100450055863854,4.982716460982349,4.999960703788529,6.364429308258879,5.055586244923381,5.034639607979299,4.984560207271037,5.065938857457994,5.055013002462809,4.981480858294734,5.1285933404499575,4.9796151482090885,4.99006078655971,5.079169059661449,5.051684952183856,5.072148915186471,5.262716594399906,4.999489279158586,5.003438019409395,4.993184350550486,5.113605467913636,5.0003695924226825,5.153712463894382,5.033483193677912,5.015704322159359,5.049289398504269,5.538698945959587,4.978666384129982,5.192264933826286,5.040477258260767,5.008379967942667,4.991565416836948,5.316384843945357,5.133992618889412,4.992393032092398,5.255990944401406,5.290292328884908,5.147148520654071,4.985312056505755,5.0124171459035125,4.9901825712557795,5.013664907851492,5.012436798844713,5.0217720958947405,7.119122136598333,4.989374145770169,4.982042639312084,5.118332076458967,5.190666193611594,5.350242374274874,4.9910589222101684,4.964069575295478,4.985105430865422,5.268315245845426,5.027784939757469,4.995314116495114,4.981846212382941,5.2240240493713275,4.984120836222419,5.0419328422008345,4.992928619725202,4.981733721188027,5.087529103332406,5.777768144928271,4.992948262418116,5.014058264493139,5.005651628850865,5.150463016898072,5.127251605587517,4.981296216981339,5.0053498099538,5.70355981518316,5.048454020230359,5.026602347973638,5.03867645152266,4.984074345893452,5.089782316688091,5.167272160437033,5.36266800661455,4.998995737870037,5.015629153474483,5.011589177993808,4.985935821047702,4.986052972423876,5.020479460806831,5.199423305481808,5.135188858887894,5.009131877110226,5.029935453175786,4.983381950194174,5.62415049811621,5.0049106624029385,5.108414720761786,4.982368708014461,4.992046662813348,5.015936452258617,5.019620652203227,4.991805057690502,5.016463950443958,5.168994808758328,4.982981560033388,4.997744028740783,5.249243898546526,5.021921234466742,4.975595786928994,5.086559444039905,4.984662974546854,4.987836577525805,4.99447907408665,5.550495908260339,5.028197833356909,5.141838312778201,5.028915462094862,4.979846586244262,5.12836695163875,5.057631650184981,4.994874277215978,5.00102492655089,6.433089823884801,5.018657287300023,5.057721051008278,4.997132156008867,4.988143659731272,5.0688956859607,5.495015858943289,5.539152135775497,5.036253236674615,5.038240419577384,5.963884369892771,4.97736615105424,5.495796901224092,5.049759184888002,5.094952766722194,5.014070795754125,5.518264908668475,4.981920854616015,5.0010606945917315,4.982989417110554,5.01210423271305,5.014191834804956,4.998305906921897,5.011456857557797,5.635531416724425,5.022414005671584,5.248466629656156,4.9974476864479,5.133805768131334,4.979144677255965,4.9841809475612795,5.146925851969357,5.515678550892657,5.126483961586179,5.078067100373967,4.987856816130419,5.136043594343347,5.03002226602636,4.9994223496233,4.9805144378033495,5.050283493232802,5.044522116933962,5.545902987174706,8.310497963185833,5.013146526812949,4.982989417110554,5.003309782930422,5.538698945959587,5.041814309429034,4.981490679641191,5.038146098813973,5.309576640562355,5.135754568443827,4.981245145979762,4.983462806009789,5.544452527208034,5.024042645301489,5.121704391890116,4.985611716614616,6.2082190883757,4.991564897846433,5.085650388358751,4.9891847224557315,5.0635740346457725,4.988349908006873,5.063277416842218,4.9923295175913145,5.136753385240561,5.016901876301665,5.048046284217104,4.984880651461125,5.0428760592677415,5.0231018758534125,5.011940006619474,5.091361053540352,4.996788304035318,4.996024417549109,5.298583538273126,5.015684981810054,5.017611627641339,5.438853173773987,5.0239017058611575,5.443242457424262,4.995839666815105,4.995026639720981,4.992732192796058,5.033512387555484,5.202394368424958,5.115548133703987,5.046278173680503,4.992307009369383,5.4789476345527275,4.988342050929707,4.999833756161246,5.014374863921659,4.996819668068268,5.156226182999133,4.990009715558133,5.053133473507601,5.021101927675706,4.988764368827365,4.995774845928487,5.01400927949277,4.985793053143303,4.981101754321488,5.042197440037745,4.98261593939456,5.315697349693355,5.037127269918781,5.119561132405258,4.983377491930529,5.014374863921659,5.099255501804175,4.996024417549109,4.999663739273579,4.982567099212896,5.248255080546548,5.027786734693142,5.042053919191093,5.21510366152785,5.10532803711954,5.633818054440389,5.074932111365263,5.392813803103047,5.057493166450801,5.042526419333867,5.031114014750646,5.096185892075466,5.129957955933039,4.9812886260488725,5.000923195741331,4.982667276946759,4.9979453634955195,5.097601506142169,5.285342206012148,5.070916003402938,4.997371796862422,5.066288946419095,5.033604708212182,4.981575143220723,5.011866426232656,5.216718456934547,5.000056771027892,5.014221298844328,5.048535119096358,5.110611418730133,5.023983338552379,5.039239089777046,5.095303293629335,5.029978667100197,5.04154428468815,5.036237121662841,5.013121308041126,4.983655738230737,5.010842684373087,4.974406006350436,5.147005902945151,5.004203525458023,5.080671692229064,4.992677193255898,5.232954743517212,4.991816843306251,5.0141348709955045,5.021378371906457,4.982119566439263,5.0200924997564185,6.4546828479417275,4.9807972925813155,5.1377425460854065,6.433089823884801,5.114362220182281,5.201114559724935,4.989691503932921,5.323926472752493,8.886790417537107,5.14043852245671,4.982582813367228,5.006935963517302,5.003380496624913,4.975448416378062,5.015132719795552,5.081988589398741,5.40048306938874,5.00124140048691,5.124588464784294,4.983462806009789,5.928765212283473,5.054394534392701,4.995857145885449,4.981997461118381,4.983838636453346,5.050008687458327,5.429465323163111,5.3877703185345895,4.991698887428193,5.202394368424958,5.059417755413788,5.3371295931130565,4.983425484893252,5.48165369554544,4.990720781041632,4.9894970412730695,5.161128935836702,4.992629876819624,4.997216729766395,4.984854741876392,4.981082111628574,4.988143659731272,5.013417912704131,5.090601720676298,5.024688979458522,5.6917985746573905,5.157875049696996,5.042506776640953,5.009936539493804,5.032888173722713,5.186580160221385,5.019397783312088,5.075546756818664,5.053358236666412,5.021104233472307,5.033053341696646,5.018935545143765,5.188748589657378,5.009622089739662,5.039292726732297,4.991396489677884,4.996108346491572,5.050008687458327,4.974406006350436,5.496898116488686,5.033663689519864,5.059417755413788,5.217140824343499,5.001486941027972,5.34315979865687,5.0639565973033935,4.9815742610096105,5.238501566540067,5.007625282563698,5.008485632513345,5.154661337348172,5.031976328969584,5.053133473507601,5.060012756847439,4.993470702878949,5.038823318708578,7.434367199188749,4.984873504827137,5.038787534142525,4.979846586244262,4.986517784057779,5.233298664123093,6.965329255112102,4.987196881932802,5.016901876301665,5.131840114649449,4.997160623248439,5.057493166450801,4.9740597623476885,5.094830982026126,4.998700225397037,5.148488708845866,5.120352297276877,5.083295739108345,4.987640806792665,5.215913460341228,5.007788939806473,4.998154984731581,5.035599087291168,4.980999014716667,5.057769149035382,5.009712876558203,4.998571965399487,5.134720568164299,5.038293017111351,5.048626105497527,5.0046453948578105,5.000887370607997,4.9922096971645376,4.991781289892726,5.040314652111714,5.088818353725052,5.394938099431356,5.013288247613918,5.002040864968156,4.982282280165639,5.0200924997564185,5.014635324890649,5.039958040098785,4.99860999859102,5.029348136657648,5.003722841913173,5.2222728027488765,5.0305320542524266,5.043518015210499,4.986660636416241,4.996407276351256,5.2224299151975675,4.985863269347798,5.645375507927159,4.982121210083741,5.088223180129748,5.274150032284184,5.014139666720043,4.993175621338627,5.033600274292918,5.105708125963682,4.981414073138826,5.0786336174489755,5.157984204014966,5.033141551710673,5.01903768714692,5.076881489241018,4.985192759425723,4.999339994692437,4.9862868165034175,5.100537581927798,4.986941448403843,5.052296694789451,5.0612544073857055,5.00450864490401,4.983775124827127,5.224946144159892,4.989241686265183,5.164487899638898,4.996096135822745,5.153122244913306,5.273235166041649,5.065059593330317,5.026441931276124,4.9835786978979835,4.991922913847988,5.0737641938755145,5.123277529904648,5.410127631609671,5.248255080546548,5.052131696168971,5.027307260863849,5.003288482394434,5.034421805964458,5.1665766992686795,4.988780082981696,4.996884658078146,5.042380785356218,5.279629320430244,4.989698293732157,5.303490537397197,6.117448436741651,5.042954145086885,4.9835786978979835,5.005759226736838,5.007502782384171,5.023710683791237,4.982406029130999,5.010874183971727,4.985044042789393,5.004167432557409,5.406492663699492,5.0647815904057945,4.9799451504758,4.970633494729478,5.3022322837005875,5.28291476517536,4.985096519595717,5.137447768573041,5.033700029604429,5.581230670401725,5.138353320845216,4.982729301652587,4.988876879740412,7.997466045336077,5.025704417122041,4.997326618668718,5.126596296482594,4.984329048767311,5.037835697280506,5.040034326185475,4.984158157338956,5.073659520903154,5.073621551305894,5.0808512774790024,4.990436994280133,5.005588335308483,5.009008128144866,5.251077183488322,4.985181541639792,5.058114860430673,4.989330078383298,5.197094769876674,4.985937785316994,5.143817860671353,4.982989417110554,4.98712616823831,5.0419328422008345,4.99042965026802],"xaxis":"x","y":[4.9676297534661975,5.60584986719498,2.950468414150123,2.7355221772965375,7.258707352196828,3.0925457415435655,4.178714641175443,3.4154882710497003,6.224387627929912,2.634593268445757,7.244982170376553,5.597531174224212,4.210232990095849,3.594548549550354,2.526068811667588,6.215484464192794,4.969933274697856,5.912649864897204,5.111865963867557,5.042644337408494,7.864928972289788,3.382667252745041,3.189033824390017,5.705977901682522,3.9717734471933728,5.119771161952904,3.496973580998276,5.058749412335524,5.968551603363945,3.1160319934471103,4.464668267003444,5.192194165283345,6.73226919950145,5.39746072605472,8.171076693915895,6.43095427138573,7.132782457298859,4.294988267691446,4.708739041359579,3.4646682670034443,5.450881315273369,4.95093492831454,4.20006486151431,8.047505671251574,3.605257262939004,3.261530815243406,9.159820892013935,3.8728287595348854,3.8298495598446904,1.9671686075326276,2.9126498648972037,5.648177795724818,10.257022407964342,4.887037791419197,5.718087583960517,7.540941686758214,7.156841524588401,2.0942360698457656,6.074462620704536,6.428778891337586,6.464504909131913,6.588864518190148,5.629648044187436,4.795974694206668,5.676380255466924,2.811471030529836,3.9882302230604574,8.45314714555533,2.321928094887362,3.980939266085512,2.5801454844233804,4.41886457739032,2.8972404255747994,4.341985747228616,6.561784353081841,5.751410160064371,8.545157551729474,2.7676547982373463,3.773996325111173,2.7676547982373463,3.258518924711301,5.596637034102074,7.019368330478469,6.649471757181635,5.275007047499869,2.0071955014042038,3.700439718141092,5.320845667645722,5.331633567171029,8.061668207389335,2.786596361890807,4.94673086014031,5.649040565634448,2.269033146455237,3.292781749227846,3.683696454306516,13.390111171989,8.064904402198286,3.727920454563199,2.8155754288625725,4.253232938538032,3.0373822220030804,6.01992426090841,3.2524762141352173,7.125981653854716,4.273515889702116,4.92552476974757,7.778274295338579,6.135452784185173,9.20006486151431,3.6971065744769747,3.2418401835646704,2.4222330006830477,2.6229303509201767,5.57198012018773,5.0539801681876515,2.6915341649192004,7.510803720358323,3.01435529297707,6.045049740837989,2.7803100990433753,4.658211482751795,4.595145567990858,7.485507332459785,5.342341397431548,4.129283016944966,2.454175893185802,5.96254902292306,2.3305584000308026,3.292781749227846,6.290756138396673,2.817623257511431,5.532940288372874,3.2883585621936606,8.008820757716158,1.6644828403646825,5.284292026394312,3.598126959919604,6.237448995639321,4.222650022451478,4.380590934265922,8.669593751188334,7.720552091605108,5.548744859937229,1.7484612330040354,4.524815928357506,7.639376888638463,2.851998837112446,3.218781167784069,2.7990873060740036,3.868884273028774,8.043027283594547,7.260778431893426,4.038260575175349,3.040892430646901,2.7949356628035362,5.4741115139424075,5.952799477899938,7.323370069061268,5.040892430646901,3.4208865749755315,6.136683577697236,4.255500733148386,3.228049047884462,4.6194130105979365,5.623515741490549,7.217424613813984,6.179311989210016,4.3139713710590195,5.764473550992666,4.221103725367875,2.4594316186372973,6.548282482376233,5.9902741833186015,2.5360529002402097,4.720825666089835,6.512700963425246,6.664198369291911,2.881664619320345,1.9634741239748859,6.013685569928653,6.026578770127714,4.532940288372874,7.597158683183727,2.711494906650088,6.791683858152045,2.1009776477248208,6.090641751917847,5.040892430646901,6.327328082629952,3.2942531364445142,2.9467308601403097,3.7366048753142485,4.922673592849446,3.71699089440494,4.15623479785027,6.872951851130105,2.8933622107638715,7.561631630375841,5.341985747228616,4.993221467368938,6.309067020588001,2.414135532984451,2.493134922305505,4.161081482277184,2.144046369616707,3.486714373030702,2.8011586560936985,5.648465443027314,4.399171093819823,7.581803148516525,3.1160319934471103,2.533563348214512,9.967312731471658,5.687060688339892,6.954079959320971,7.084170591577843,2.722466024471091,4.826802684285827,3.01435529297707,1.6040713236688608,5.5109619192773796,2.1731274334806563,5.425593577653059,2.8797057662822882,2.7803100990433753,3.303050084681673,2.3132458517875616,7.006298023900369,5.279842693520348,2.3950627995175777,7.216163813388774,2.786596361890807,3.5298209465286954,2.7420062108667365,5.079378111118652,5.1622906135457995,5.839203788096944,9.354249381945241,6.058532970201611,6.614857050384495,2.1667154449664223,2.4982508675278257,3.3405622690264134,5.415826257472914,3.9791107550157854,5.338780943894699,1.9335726382610239,7.164705840182799,6.12866458654003,2.933572638261024,3.0548484769956197,6.390598905552461,3.2764966656403565,8.007363716298686,6.558114538598396,3.674686619927999,5.28835856219366,4.189033824390017,5.480265122054463,4.2787282129389395,3.776103988073164,4.82273014794452,2.9467308601403097,2.2898344651775093,6.1705259991768475,1.4750848829487828,7.264067216098838,5.6845382485777325,4.534808661231747,3.7070829917717063,4.919340082442012,4.458119481174506,6.288173968112302,2.27500704749987,6.5249725982866265,3.940166750482817,9.715601700373337,7.829278760612363,4.821199978056451,5.875042803002589,2.6229303509201767,2.942983598187102,4.445594291341182,7.474030370207135,5.92148393743373,3.4369613378336026,3.2078928516413328,4.095080491901415,4.422233000683048,7.424922088210688,5.185470146066942,4.955126781261366,4.3729520979118295,4.0522419811811385,6.067165427017112,2.2898344651775093,3.0806576633452254,3.940166750482817,3.127633279725874,7.152994605492435,5.573192723781613,4.4121040446775694,2.2479275134435857,7.886854944198882,7.9329824099498465,7.25691629667084,6.670585440262214,5.845740932614582,2.9616233282869446,2.5459683691052923,7.959132576789657,4.653633311386255,7.406162478777607,5.588264894307497,3.8063240573900288,6.716579420796345,4.984589350362456,2.269033146455237,6.929672661579345,7.060371694122218,6.746447139362815,5.521364878446219,2.9873208659292536,5.970623614470605,2.37851162325373,4.667892125230234,3.192194165283345,6.816215687622865,2.851998837112446,3.3854310371935203,4.634593268445757,2.5558161550616396,8.990671283975267,9.114445113289024,3.389566811762726,6.9532652390148435,6.705563274566321,5.672425341971495,2.280956313831056,2.137503523749935,2.2203299548795554,6.561173365231147,2.9726926540042644,5.078951341394822,5.0759604132434015,3.8509993947164736,8.693835392482516,2.845991770664573,2.8399595874895316,6.111448698487674,0.7484612330040358,4.125155131322289,3.8953026213333066,8.45889870671479,9.650854294025212,8.19120730223037,6.534964249804655,2.5160151470036647,3.0373822220030804,4.701549056943071,2.9541963103868754,4.903520373723376,7.848935855524565,2.0036022366801953,5.660780334032546,8.418822422291017,5.254745197629213,8.4696011200494,6.567880042052731,6.557808298936688,3.6769443591069124,5.651051691178929,7.279842693520348,6.425089989874059,2.811471030529836,5.4799419014771305,5.184280294419382,5.713420884868084,5.452858964713811,2.4195388915137843,4.178714641175443,7.686990680088471,5.3847405872923835,2.722466024471091,7.554281862626391,3.955126781261366,5.192194165283345,9.071542807992602,5.765004246250036,5.318316841334983,3.1953475983222193,5.2671612180222604,2.269033146455237,2.702657543390911,5.6937657122177825,8.574290766120185,3.554588851677637,2.4059923596758366,3.0214797274104517,1.3785116232537298,6.010108453474289,2.438292851579147,3.089159131911238,3.0250287944915226,4.571676809970931,3.24031432933371,3.261530815243406,2.6712933724815757,3.6542063779442917,2.9873208659292536,5.8060662260890314,4.371558862611963,6.4984104488609775,6.229587922740652,2.2957230245399685,5.178714641175443,5.463687842201262,4.884597920990064,6.157852169141737,3.4672794804599825,2.867896463992655,2.611172380044005,5.535430914871279,0,3.4776773275653072,9.582254908357664,3.950468414150123,4.8894735425311096,4.850999394716474,3.300123724569014,8.009660689646706,7.626950122372515,9.04105676230812,6.564835417010355,3.3175935046234715,7.380331184220349,2.0214797274104517,4.235727059838059,4.334139178838237,5.692092375427572,5.592457037268081,1.996388746447621,2.070389327891398,5.8052924556007115,6.60584986719498,4.088311235888661,2.521050736900963,3.5310694927259543,7.058316495590823,4.044394119358453,3.0513721017210256,6.008092420948722,5.605257262939004,8.220281580441913,6.088735246190284,2.137503523749935,6.637204481902295,0.8155754288625726,2.2387868595871168,5.676662334857967,1.7865963618908067,8.05642095591347,7.5673476962842585,7.295355692891673,6.394033895367782,6.582706526780788,4.43095427138573,4.915520900751958,3.710393192170516,3.8845979209900645,5.135042286233651,10.267009017093484,5.336640446408735,3.0992952043377753,6.836050355058069,7.896998342338911,6.558114538598396,5.10936055940423,5.85872702377863,3.785550551739256,3.8042601156347384,2.7484612330040354,2.883620816285671,3.576522137920503,4.104336659814735,5.86195536414487,4.294253136444514,8.44136727845972,5.497931651896279,7.5808238499627985,2.8278190246173196,6.260590274730996,2.229587922740652,6.218781167784069,10.212070748654236,4.400537929583729,6.543805175964397,7.563234413022304,6.203788453308596,6.676803353873854,4.070389327891398,6.340918270272116,2.063502942306158,7.151270288790165,4.228049047884462,4.563768278452033,5.4329594072761065,3.9373443921502322,5.580748491763774,4.279471295644468,7.959132576789657,5.739578112048051,2.7463127664254587,5.707635219545296,2.0250287944915226,3.4776773275653072,7.925940095994448,4.478971805032942,3.24031432933371,6.836807807844575,6.2502036564992975,3.82273014794452,0.918386234446348,3.258518924711301,2.1634987322828794,2.662205499653617,6.687060688339892,2.2357270598380583,5.0259146987507926,5.443606651475615,2.980025300238734,3.1193561770396756,2.8399595874895316,4.914564523493939,2.782408564927373,6.282625134191674,1.2016338611696504,5.269781238274379,3.0513721017210256,4.649615459063409,4.896271848807375,4.721919445551544,3.6182386555954547,5.045268215138534,5.020591094773247,5.305240965954483,5.663913842115978,3.3881895371560837,8.836744701966083,6.976363635732762,3.925049964727359,6.802451741275622,4.664482840364682,2.3589588258323295,6.127014141582809,7.1945598860226125,4.475084882948782,4.301587646603187,3.7441610955704103,4.05484847699562,5.464668267003444,7.33360262826828,5.6082178530214595,7.1320626778543845,4.993674361750585,5.522620761042079,7.366234775533047,7.166815851761945,3.7687136570304847,2.321928094887362,6.7873802222963375,4.788685710613533,6.025250321561575,8.162995472417697,3.4019034716079584,6.238786859587116,2.3248106034204836,7.831687267083084,3.4802651220544627,3.871843648509318,6.548128323585586,3.0268000593437154,3.1953475983222193,6.2600256559614555,5.931210274803932,7.80967167790706,3.169925001442312,3.894332742277694,2.298658315564515,6.215678596607928,7.873013392989595,7.709152756902506,8.461602224618334,4.150559676575382,7.653203362033977,9.24186401223682,5.725468512632615,4.0925457415435655,3.575312330687437,6.519321734629434,2.5185351389821804,6.137913322088407,7.904544309292717,2.568032104771279,9.816679697610459,0.765534746362977,8.646594709581654,5.037821465434967,4.388878338811989,7.9616812016062815,1.7782085763980877,8.912589990852627,3.5008020530571575,7.473786911614368,7.182890892745479,2.6205864104518777,6.628919373511362,4.842476062993939,6.013462259806563,7.1851727751412495,4.8359240742543745,4.918386234446348,4.783980413683795,4.285402218862249,6.340918270272116,3.5570424152637194,4.030336078370959,0.8073549220576041,2.2957230245399685,8.893574570329104,2.3923174227787602,7.015805293913353,8.764407200357251,6.147306698780294,5.502075956045791,8.694706111949401,2.440952198029637,8.447992374957895,4.943921326553485,6.072963271555224,8.18685706717715,5.508111681193465,2.327687364176047,2.6322682154995127,5.853496703702325,2.4724877714627436,3.412781525338476,5.751677945687579,5.8774988727835344,5.890446692679906,8.622783966156124,6.203396948546115,5.524189078449365,4.679198570566922,2.3757345385831563,7.809543066804601,3.5716768099709313,6.39694722007283,3.097610796626422,4.9415758618182375,6.668317719915721,3.0925457415435655,6.941106310946432,7.471431358002323,4.633431210355632,2.454175893185802,5.760487101811031,4.112700132749362,2.560714954474479,2.885574364371426,5.8060662260890314,5.969012307516316,3.0214797274104517,4.37016428054021,3.1811025507537978,9.463033855317493,5.102238193930738,7.537684337147519,5.960001932068081,4.451540833017832,5.931683057059805,4.126807703142035,9.109647857806625,8.028624401408491,3.2645364309990255,7.394977085526471,6.152589069287125,6.022367813028454,4.51853513898218,3.344828496997441,6.810314561497258,3.0036022366801953,7.890811454817677,8.317774372804099,7.237735785231934,8.515305605451724,8.213638902198852,5.154615611326894,2.039138393906958,7.296273846706719,6.905206468795705,2.3673710656485296,2.0942360698457656,2.9145645234939397,2.2172307162206693,5.641256998726767,3.303050084681673,2.62993940943954,7.220233204382192,6.489767697008256,10.809687753488749,5.260778431893426,8.11134436328308,4.9941271140031285,2.7070829917717063,5.856985689782205,6.381629467033664,4.493134922305505,10.642557179595622,5.439955517478156,6.790772037862,2.5058909297299574,4.37920505996061,6.33646192818282,2.9523335663696857,2.811471030529836,5.323730337521348,2.763411574470007,8.089159131911238,6.435461914479276,2.4646682670034443,7.870364719583405,3.396433531250992,7.305696981080096,4.020591094773247,5.187451054027326,2.298658315564515,6.038918989292302,5.369815424283912,4.422905742612183,4.024142345897803,4.926948247949772,5.348728154231077,6.907251224292033,4.861459166361513,6.596487956856693,4.593353770980297,5.9502351004856875,7.814614507639093,6.638797899951696,8.79720757285578,2.176322772640463,7.7974669920591575,2.7398481026993275,3.1538053360790355,3.6701605141266334,2.513490745588118,5.307064162255372,5.076815597050831,2.533563348214512,6.678494507773734,2.7092906357233577,2.7355221772965375,2.782408564927373,7.039796407620025,5.938521045865967,4.733354340613827,6.132988042627442,7.991691898336964,6.419033185471022,2.1858665453113337,5.449891473010944,2.6064422281316078,3.7311832415722,4.332707933640605,6.56498780095989,5.434294617959925,5.893362210763872,10.084529386649065,7.932923373835633,3.0823619695574735,2.440952198029637,3.1842802944193824,6.1430260038899736,6.387500406480984,6.681449265314958,4.324810603420484,2.327687364176047,2.722466024471091,5.569551851083613,7.527555198794536,6.632559052117953,2.9145645234939397,6.135452784185173,5.424922088210688,2.2357270598380583,2.8011586560936985,9.8361608416947,7.193968819421969,3.097610796626422,2.655351828612554,5.553360503335328,3.169925001442312,6.173127433480656,3.817623257511431,5.778208576398088,5.7554217347342425,8.571145863602528,5.470862199413702,3.1602748314085933,8.3389146195993,2.440952198029637,3.982765462583646,2.140778655782796,4.3197624276692475,2.292781749227846,6.864928972289788,5.969933274697856,4.950468414150123,3.4581194811745064,9.054360117597708,2.232660756790275,5.648753032989572,4.827310943949667,6.609104846618959,2.920293300211007,5.726013748860151,4.484138131201669,6.010779838753242,6.51317488460363,8.731081391094504,7.529742876502924,2.7782085763980877,6.546122758857796,4.635173946674924,5.533563348214512,6.2992080183872785,2.657640005207824,11.260084480727762,3.442280035252584,5.339850002884624,6.091065077930535,7.352175878326325,7.78959885005233,3.2418401835646704,4.584962500721156,5.035183996618336,4.898692072560488,4.393690764187454,6.686360452903688,4.792855352362489,4.044394119358453,5.077670274232752,7.061344188288391,4.06522762277562,6.802968651005583,3.412781525338476,7.953207027098,3.5643781685650637,2.790772037862,3.350497247084133,8.870919495610579,4.663913842115978,2.2234225499349374,2.6667565918848033,9.754286253961022,6.500324049284198,7.87670110864523,1.4168397419128291,4.017921907997263,6.177519202597814,5.47800105583793,8.031053274444373,7.831180547126736,6.461724991502711,3.948600847493356,2.319039815562536,4.898208352508718,6.20945336562895,7.562776656791846,5.542258049766918,2.6016965164809576,6.711081862306696,2.0565835283663674,8.630303533275018,4.854494418154875,3.4892860226258766,4.601102203376409,5.932391940068527,6.822602695756541,3.039138393906958,4.397118409042582,4.078951341394822,6.198886990956932,2.298658315564515,7.472731449369055,5.0669502439246275,5.059614856297222,2.9708536543404835,6.608661417987563,5.51443791382931,2.176322772640463,5.642412772905056,6.8820316083521,5.234577959784565,7.163498732282879,3.212569338850806,2.2898344651775093,8.307656205306488,3.423578170981797,1.3840498067951599,2.7676547982373463,10.679004987548526,3.2645364309990255,3.9990980337056072,5.649040565634448,4.793375711264754,4.203201156316611,7.668814088419117,5.911931212309049,3.4581194811745064,5.445263208140443,8.913188619508889,4.936402377725063,8.645802514606062,6.207111961207714,5.749266083814806,4.4376272483189485,8.580786171355323,2.189033824390017,2.341985747228616,4.06522762277562,4.804260115634738,4.849999259466098,5.075532631167357,4.056583528366367,9.188044796421696,3.8439838440483265,4.634012356402116,4.8104431038451745,8.686815644595628,2.521050736900963,4.016139702655253,6.212180210044431,7.545505100691072,3.904002316283692,3.71699089440494,2.1667154449664223,4.381283372503783,6.564835417010355,6.563005553062399,5.911212201558612,6.247547806890298,5.477677327565307,8.292781749227846,13.587327482778312,2.9011082430145123,2.845991770664573,3.7644735509926663,9.182369521950886,5.921959917326167,1.9297909977185974,6.910972451655129,8.149950301681047,4.3729520979118295,5.38404980679516,2.063502942306158,5.977508904540138,1.3673710656485294,4.067810783789621,2.1176950426697547,9.199893148608071,3.7081872360207084,4.628773595201645,2.488000770834068,6.501439145158873,6.706254253573203,3.0908534304511135,5.86616616903904,4.155425431747197,6.060695931687554,4.570462931026041,3.2357270598380583,5.341274183692174,6.656925339794245,5.588564737401351,6.353146825498083,3.48155728070859,2.4698859762744636,4.872828759534886,4.34766565630097,2.877744249949002,6.019479533703628,3.465974464504069,7.536363792395467,5.132576842670009,2.0531113364595623,6.283366212949919,3.699329525673156,8.199279721336625,6.8203066288938246,6.068240861312821,4.598126959919604,7.979739567128753,6.258895755075436,7.258707352196828,5.394376944957055,3.0125686735030555,6.862203399053224,2.8318772411916733,5.164705840182799,6.431288654291593,4.375734538583156,4.8594726668519375,2.9392265777282085,4.302319050948834,4.397118409042582,5.169123281476757,3.761285273361619,7.240600549955795,3.7387678368001915,5.678916986679839,4.3334237337251915,5.530445354692906,8.431288654291592,2.521050736900963,1.2265085298086797,2.2539892662307865,5.23687524536694,2.298658315564515,6.398316163400203,8.348374075191163,6.043300754364182,9.401604869714957,4.534186139090097,9.069933046375542,5.486714373030702,4.96624587322494,6.307246355226617,5.51127826508303,5.970853654340483,5.342341397431548,6.219555769166913,2.319039815562536,7.516566771496865,6.652773284510777,9.109908988534402,5.338067797542616,2.9981955031532523,6.506843098969815,3.04788732939655,2.526068811667588,3.414135532984451,6.295723024539968,2.257010618206024,5.500164679492167,5.333065878076355,5.253232938538032,3.619413010597937,4.695437110405369,4.37364821133469,8.356407951394893,6.039796407620025,5.908332569782425,1.74416109557041,5.666472568842073,5.9586107120558465,2.430285272977781,8.487277313213783,2.927896453728821,6.9920886090139955,5.7615512324444795,5.906409617094016,2.572889668420581,2.9763636357327616,3.311793717753649,5.325530331567558,6.075532631167357,8.785648628652076,5.301953395126096,6.687620632084895,10.60719158640291,5.2543672814408335,6.976821852360685,2.655351828612554,8.033092575523664,11.658760423755194,6.231317197152663,5.008092420948722,7.824449651364088,2.8155754288625725,5.656496370553495,3.218781167784069,6.101187815259738,8.328001662149257,5.576522137920503,7.176621973161577,2.0565835283663674,6.5787877956769725,6.602142090697982,2.7441610955704103,4.727375938594897,2.2172307162206693,4.552131108253784,10.754979337466484,5.651338766105931,7.449148645375437,8.21179097625254,5.769771739249448,7.09876903815488,2.5921580021253603,7.971830914780502,6.234194722953812,2.1984941536390834,6.955824240806441,0.16349873228287956,4.680324356844016,2.5801454844233804,5.583458910130768,4.845991770664573,2.7070829917717063,4.18824265626441,7.27733390042982,9.535217045453418,6.304876050044889,6.982080909763738,3.0925457415435655,7.051372101721025,7.022922589055629,2.9392265777282085,6.653203362033977,8.22838581704935,4.960697039304312,5.564378168565064,2.782408564927373,0,3.9354597478052895,5.525755692829486,1.0976107966264221,7.211790976252541,4.477677327565307,2.4436066514756147,7.966303561412599,6.19021976366016,5.592457037268081,10.665637927637095,3.277984747299765,7.8269297659895765,3.5643781685650637,5.612057559563065,8.766462652425766,2.662205499653617,6.920412408160787,5.290940402403678,7.502871574574138,5.2539892662307865,3.1984941536390834,3.0089887832272546,7.2259304111710545,0,4.919340082442012,5.334496768390418,4.654206377944292,2.327687364176047,6.705839705883682,11.02948049374083,2.4594316186372973,6.294253136444514,7.172927489603836,5.573192723781613,5.5804470195986635,6.341274183692174,6.768713657030485,4.5855634984556914,5.609400390006429,6.10055722078293,6.140165134599453,2.37851162325373,4.726831217032493,3.127633279725874,2.414135532984451,4.561326136453918,2.211012193485512,3.6147098441152083,5.901832305785489,5.903279342056139,3.84197311892718,3.0443941193584534,4.77663042305105,2.7949356628035362,2.5897634869849773,2.4436066514756147,2.5160151470036647,4.623515741490549,6.6059979802325195,9.051127353591394,3.017921907997262,3.9212458885855885,2.2387868595871168,5.621465834460144,6.001351892999959,6.605701738949982,2.6530600171045644,3.4698859762744636,2.456806149230474,4.528571318870758,10.08413071995014,3.1953475983222193,6.061128135118254,4.434961760187254,8.556735947914529,6.821837746088096,5.895302621333307,4.783456654360239,6.1147834472498515,8.8634739090894,2.6182386555954547,5.862203399053224,3.319039815562536,6.531225485053606,2.2265085298086795,5.50303064564116,6.549977142771825,2.582556003014061,0.45417589318580215,8.258094872866513,5.864186144654281,0,4.110196177754199,5.541948425333733,5.139960569545456,5.166715444966422,4.661635602335959,2.6016965164809576,1.0565835283663676,4.867402305727543,6.390082950951663,6.790641730763425,4.185073637876762,5.681168120416691,4.409390936137702,5.399854673572431,2.752748591407134,5.553974808237472,4.238786859587116,6.451211111832329,3.2555007331483865,8.627898615871748,4.5588798534754424,5.639521599510833,5.29020320512607,8.07804403634318,5.592756010441026,6.952217064973338,2.6392321632492775,3.2942531364445142,5.633721812641012,9.325777655214823,5.554588851677638,8.364703739582406,12.206535173103617,6.2268938135713885,6.058532970201611,2.996388746447621,2.169925001442312,3.2539892662307865,2.3645724322958563,2.996388746447621,2.5084286525318573,5.370861740085285,7.2461230165968376,3.19377174339668,2.4276061727818994,5.25285462595139,8.366191054189278,7.062963556559086,2.0531113364595623,7.846744023244868,3.581351247168777,6.948834425578686,6.412442824775613,2.899175630480513,4.786073551578548,8.971342367308225,7.2193621578077165,2.885574364371426,7.801029283808189,3.7813597135246595,5.6884601404244926,3.361768359419153,2.280956313831056,8.531927241876955,7.295631200395783,5.566206293465506,3.9990980337056072,6.145066014185894,3.0373822220030804,6.666614587352882,2.655351828612554,5.343763122149601,6.696689389537669,4.609991295212678,2.3950627995175777,7.005961325845812,2.2957230245399685,3.7548875021634687,6.883987308105472,3.2433644256936605],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression lasso\n","# réglage des paramètre pour la gridsearch\n","alphaslasso_log = np.linspace(0.1, 1, 5)\n","param_gridLasso_log = {'lasso__alpha': alphaslasso_log}\n","\n","GridLasso_log, \\\n","BestParametresLasso_log, \\\n","ScoresLasso_log, \\\n","TotalGHGEmissions_pred_logLasso_log, \\\n","figLasso_log = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBNumM_train,\n","                            X_test=BEBNumM_test,\n","                            y_train=TotalGHGEmissions_train_log,\n","                            y_test=TotalGHGEmissions_test_log,\n","                            y_test_name='TotalGHGEmissions_test_log',\n","                            y_pred_name='TotalGHGEmissions_pred_logLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso_log)\n","\n","print(BestParametresLasso_log)\n","print(ScoresLasso_log)\n","figLasso_log.show()\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.9724490599359714,1.9408681257942408,1.9361564003504834,1.983783874824295,2.0408733003742032]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[2.271642178639365,2.0743643421552442,1.9686298670661477,2.002252784315961,2.0671053732822733]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[1.6732559412325778,1.8073719094332374,1.903682933634819,1.965314965332629,2.014641227466133]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.8494777347911668,1.9085697424581585,1.9541025447337186,2.008594380230596,2.0501490564859943],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.7983110478365405,1.8496262426326397,1.8964457228011036,1.9555453927883142,2.000717805867012],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[2.569465371970112,2.204842951653322,1.987883675003429,1.9847955645663056,2.0788961939545008],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.8033116765232493,1.860794612409126,1.9100163040685554,1.9729797889673635,2.0256644532245804],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[1.8416794685587885,1.8805070798179586,1.9323337551456117,1.9970042475688978,2.0489389923389285],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","FigRMSEGRidLasso_log = visuRMSEGrid(Lasso(), 'Lasso', alphaslasso_log, 'alpha',\n","                                    GridLasso_log, None, None)\n","FigRMSEGRidLasso_log.show()\n","if write_data is True:\n","    FigRMSEGRidLasso_log.write_image(\n","        './Figures/EmissionsGraphRMSELasso_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.4 Modèle ElasticNet"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.508e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.855e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.944e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.929e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.841e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.625e+02, tolerance: 1.765e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.566e+02, tolerance: 1.765e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.856e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.463e+01, tolerance: 1.765e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.618e+02, tolerance: 1.765e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.508e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.842e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.944e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.929e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.836e+00, tolerance: 1.765e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.856e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.508e+01, tolerance: 1.765e+00\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.509e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.842e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.945e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.930e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.857e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.843e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.510e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.945e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.930e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.857e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.511e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.844e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.946e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.931e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.858e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.513e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.846e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.947e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.932e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.859e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.515e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.848e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.948e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.933e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.861e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.518e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.851e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.950e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.935e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.864e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.523e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.854e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.953e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.938e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.867e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.859e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.528e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.957e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.942e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.871e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.536e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.866e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.962e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.947e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.546e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.877e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.875e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.953e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.968e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.886e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.886e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.560e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.977e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.963e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.577e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.897e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.975e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.911e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.921e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.990e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.902e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.006e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.600e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.931e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.956e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.991e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.027e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.054e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.718e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.630e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.669e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.077e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.068e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.013e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.041e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.989e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.019e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.032e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.946e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.979e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.089e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.134e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.085e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.150e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.780e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.189e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.257e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.122e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.061e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.857e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.950e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.378e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.332e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.179e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.228e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.319e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.249e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.283e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.128e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.337e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.431e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.200e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.425e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.190e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.542e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.537e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.429e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.656e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.335e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.659e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.493e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.855e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.669e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.539e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.790e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.802e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.660e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.784e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.484e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.600e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.918e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.725e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.938e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.827e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.071e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.055e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.274e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.925e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.188e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.989e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.314e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.140e+03, tolerance: 1.760e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.197e+03, tolerance: 1.761e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.238e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.060e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.192e+03, tolerance: 1.777e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.314e+03, tolerance: 1.784e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.987e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.116e+03, tolerance: 1.765e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre  ElasticNet()\n","0     elasticnet__alpha      2.807216\n","1  elasticnet__l1_ratio      0.000000\n","      ElasticNet()\n","R²        0.171240\n","RMSE      1.896320\n","MAE       1.585481\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning:\n","\n","Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.674e+03, tolerance: 2.212e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logEN=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.889881223462303,4.898179117360157,4.945572941191267,5.0162356142831,5.241653716720657,5.019303655777304,4.907938215234238,4.9001374978366,5.191252289109838,4.930209867153281,5.071728786278678,4.873013563291908,4.865674871308649,4.997278278631819,4.910953170053256,5.206314919563447,4.9205802090080875,4.953063166012628,5.154655142597015,4.938575055563184,4.8542773744924785,4.971587527755871,4.978752580363055,4.964800154713198,5.262174412528196,4.99305991904116,5.225745525471465,4.975063120568468,5.1093473434923515,4.958173965005644,5.142508982121226,5.011182108916524,4.9345422864326975,5.064110880829791,4.974370621029099,5.046056253005024,7.505775104370096,4.8801266449841,4.944180835082642,4.916521787534372,5.002464662698457,4.953134884728747,4.8844206101143905,5.168304953778925,5.061544813149929,4.908450824671322,4.927424057487711,5.046796537146706,4.884746664526405,4.913483386682888,4.924382200494812,4.979317907594217,6.013815234380507,4.94482567434044,4.953394352748682,4.9384047025956965,5.84819416334642,4.90082741956103,5.0871675945478545,4.935063385681669,5.1600448743771965,5.287586475066069,4.904638057226705,5.291363564964007,5.1908590229408995,4.98406635651871,5.09015221143199,4.9273475536688425,4.918036854134164,4.899369441927385,4.884978215155372,5.1355942568519675,4.91569023368443,4.887524255932678,4.959747550083058,5.141982790140962,5.88867605679473,5.043980015127149,5.035981503784049,4.917845682474559,4.961837214210901,5.269041217609373,5.273791090101087,5.447940998406979,4.925981967379841,4.9105847688803115,4.879479397444647,4.977364090735246,5.208302563006555,5.602211496102889,4.911701044056301,4.940518066817607,5.302182402160983,4.9700113870635,4.941613296557168,5.1648968383844185,7.8462900959118125,5.24037272492641,4.979491687633539,4.906346411067386,5.198229735156517,4.96252734257839,4.9950326808430665,4.963228763254598,5.271691062182125,4.921674246368636,4.9934615415689985,5.576415294250535,4.891438251768814,5.1245837926388935,5.156707436116973,4.943469624334224,4.89321855728377,4.916647538117369,4.8627548863473695,4.88166031915599,4.940980662979034,6.201923453305507,5.0906022712016705,5.196816244508892,4.917706023002116,4.901047293000858,4.861687383944457,6.37507496593807,5.088569905901297,4.920607463116901,4.927222500744277,4.909378566437229,4.913574360661828,5.217549537514603,5.384676765327761,4.92838504285214,4.894034422036598,4.881758424009535,5.398735374402482,4.890816991661232,4.921693814118708,5.07815611971993,5.195477788929684,4.910641255964853,4.927298298690315,6.255008604577872,5.1843153300897775,5.046059571517334,4.888079861790744,5.159515760763942,4.987549077667464,4.960047198207821,5.0265942670216734,4.876366019809229,5.057827524151737,5.101296508876452,4.8577964375969405,4.900674250104347,4.943919067048017,4.984308954326406,4.908557754280692,4.896672504493913,5.732940685006664,4.931896954934607,4.910928871700671,4.917868259592226,4.876897765526726,4.95311903850576,4.884487284880511,4.914152341658332,5.616701471947999,5.169533633374838,5.138480612558727,5.221120374836685,4.901862449559056,4.945708531878927,4.953090999805739,5.041177909090536,4.885867271257584,5.50956560385313,4.926333954825381,5.094028225173621,5.005786498706391,4.90855926869876,4.967742188292016,5.769298453968023,4.947406203255739,5.032488979619991,4.932227923016501,5.094878733840562,4.907891979955232,4.905262629881318,4.930741170432382,5.253836113595857,5.009783749235243,5.216525504764847,4.924873067054827,5.250179335802222,5.022083798060631,4.9916523186935065,4.9639687664527,4.915926543876147,5.007844430213813,5.098209485888218,5.303073011467543,4.8999594228751135,4.977108965282131,4.929608612002555,4.933402205217474,4.922814952444743,4.895062969516192,4.880591519629074,5.048952514935344,5.046652584839029,5.97234802640857,4.905205768239104,4.895903351837804,5.917693861630753,5.004810663402265,6.593882561526398,5.097179539654344,4.940886848013344,5.618789303642916,4.959802691939792,4.881500125114567,5.026058459456289,4.9087983921301905,4.897729685988707,4.993758217610362,4.891839667653592,4.942307701972288,4.916512614129509,4.927015689504746,4.9731385909154,4.902626588468846,5.019824453488559,4.872893418175959,4.975834261288416,4.897727427429578,4.922345765046951,4.8706111403636125,4.893858371494648,5.695142569802122,4.8944649152664335,5.133091243145262,4.892791075432449,4.919449629520851,4.918397899504931,4.976114046620437,4.9134798397214965,5.719934397652799,4.91613678810132,5.449501737056989,4.916638330903563,5.035844948232971,4.887029113206581,5.063281200479586,4.972629414463959,5.147734681052597,4.987369035083828,5.056867440932724,4.984160330927649,5.142706112378573,4.921171016344803,5.050944416585118,4.877147108701716,5.043416214414096,4.993766766793404,4.906090821667041,5.0967144679926,4.934786430259452,5.076445088943692,4.90886355704655,4.888712784700357,5.0074406010818615,4.951297238570562,5.041900762892296,4.992350667113692,4.865025813639788,5.117034752942159,5.153300782425473,6.013473932332028,4.944948992577198,4.98773937877058,5.027889470180339,4.94320093823576,4.857651615808785,5.197031447871112,4.944984883379419,4.92355511695932,4.9864815606931705,4.919889322980731,5.004728085667171,4.9048385065781845,5.612072092574935,4.976400392699385,4.873997151060409,4.865055106821945,5.021148575175199,5.06110615773637,4.919022147669528,4.9850520072100695,4.96326871265618,4.9247525133217716,4.9482733116103805,5.551550248460653,5.012066567318772,4.920432094022215,6.597155446534709,5.145631142053973,5.377359467759573,4.931870850684175,5.017894256032141,4.950190628003535,4.9355968916408255,4.879191094495805,4.939526056821441,5.508963278603765,4.904096562869397,4.950075175978718,5.0602350304103725,5.0017055035989,4.921070932639888,5.055103356785821,5.0984516262289326,5.53392811103859,4.910883669453712,4.951432570470362,4.895078093730406,4.901592278795658,4.904647800152549,4.966685690631149,6.376938358452049,4.955090780200126,4.964361289876195,4.8888622394715995,4.9579742107966895,5.422228175385653,5.056032773923504,4.992843598362006,5.341950907144154,5.497225194012246,5.04265359529354,4.922259540714296,4.900470678793435,4.915028841594984,4.939358029597002,4.951716896148752,5.380456434503208,5.374471167832559,5.01588218763476,4.902863958800753,4.853426115988939,4.90062967498812,4.873362537002584,4.923236330998923,4.91341535826591,4.972585526555575,5.0826433691448,6.548607261900786,5.293800332374565,6.527627216109301,4.935426272674937,5.199236501211044,4.995482401151779,4.990245165888165,4.948909384908901,5.187942254549411,4.895590766882337,4.908322062689243,5.234175233760524,5.0471099456766915,5.229949733940678,5.102807182735142,4.919543773587736,4.9058849633859545,5.0711875834917635,5.305953035831783,5.736242049566226,4.948332434003247,4.929309570026917,5.047641714233942,5.157299165466176,5.069371818301126,4.924277762880856,4.874693020595705,5.8500374825824855,4.982600473056033,4.928314993126715,5.208052837485218,5.013160950032257,4.89647082787447,5.209275715308198,4.903125341063318,5.327772098946039,4.935205988111797,5.044290439094697,4.939110525204343,4.9198299044692835,4.906938108507935,4.938782820139495,4.899288469336038,4.92226235395401,4.947491396328906,4.902388702647483,5.131817149102845,4.923862440515867,5.007965216394804,4.946313214641116,4.910345613796012,4.994718385802488,4.93038965860759,4.926180179029661,4.9840758704772705,4.934229093908129,5.0739521422788965,4.903940456000117,5.124255570130075,5.159182080775786,4.986962787710587,4.954890090467026,4.897864545553962,5.246747273887598,5.173250468509474,5.008004971256145,4.935879214876606,4.901760374520245,4.8942881485512215,4.951220045543588,4.956055782831437,6.166310510861406,4.9258211418644935,4.801575061615906,4.881153447759531,4.908171028725468,5.180527729233581,4.982986249360884,10.984981616753947,5.478085425628542,4.987531823490702,5.618891839701572,4.871094797913255,4.912373545993137,4.873737680277758,4.946961476874168,4.921383259877478,4.916740903977087,4.9370062946238855,6.11583449865361,4.9711725335772075,5.0143164009203565,4.938919396679472,4.898443430847226,4.934179202775851,4.886689859053481,4.9695675534988695,4.906323200231697,5.238353227950203,4.943093176300916,5.163407580595632,4.926657966017165,5.257315923283551,4.875716977242282,4.907240330791632,4.935086290611923,4.941822255248642,5.11899026604099,4.868375071150245,5.717163342711434,4.871504542389871,4.923189412259144,4.891143341351869,4.868254908695901,5.065770312969775,4.933904098073342,5.200639773673647,6.161307266761306,6.261425281127943,4.95379401232112,4.937497924043614,5.137316742965108,4.963234221429512,4.865047493675042,4.993318450994892,5.1459777435489675,4.975652383122291,4.93051674525707,5.031561754876051,5.059455355714507,4.865768435292153,5.0719589931732525,4.897731753923902,5.389085848417889,5.015830527668088,4.91060726647346,4.927112249455994,4.867279042570025,4.8970165752522945,4.911039474555759,6.175117992092392,4.980205281088544,4.920058434170973,5.7191291973324825,5.2329360056266365,5.056227626332856,5.205654723055737,5.080131860816968,4.885260170763362,5.436027913274452,5.31714779741491,4.955842653947389,6.181726011528871,4.993000793707097,4.89766541117618,4.924210269759947,5.298654020745814,4.985903921120994,4.958812052242034,5.285329996836961,4.942692727737966,4.906703371880825,5.811079584844323,4.888359011798021,4.988899790780735,5.268758806056617,5.499580610099072,4.89514394210754,4.873472014062069,4.969660707517654,4.910089547577953,5.003967699650845,4.935377858215127,4.9320273484821735,4.9609750675147835,5.092123005145778,5.027413341268793,5.0383313573736945,4.92632711802389,5.004081357882187,4.924750380479637,4.901037130061472,4.921459315541394,5.127907799324778,5.011736657463422,4.904071994653604,5.023370161240951,4.915425459803874,5.03285299157951,4.931590884173599,4.967515714173975,5.055858935742846,5.056372834781942,4.901679776667019,6.330118198170564,5.170047638234808,4.867826540360794,5.267900386907957,4.929438934289506,4.983641034100458,4.907416895903358,4.970916627330832,4.958805379696087,4.910617796594964,5.156562744156149,4.882407427393359,5.020729006242483,4.922733049961924,4.986825972875879,5.262186583603992,5.035308421693967,5.132133299144624,7.145591529122655,4.977114477202351,4.951047684269344,4.91272948679952,5.605863028430049,4.972695336161055,4.921704335626908,5.712279328711481,4.8892546197465,5.060374597569897,4.91925434141483,4.916499995140115,4.9658682430683285,4.950867081924571,4.959189473059522,4.943227341545407,4.930980488291951,4.903689462687563,5.1925907446890465,4.993298652720465,5.003532722971402,5.224391588949251,4.942110067081289,5.060603119010298,5.215870534613239,5.026382857314095,5.91018267555263,5.057360891498363,5.047747975679084,5.358155214753617,4.955983894872007,5.1591207709655045,5.003395281418924,5.2066721330534635,4.96307370609757,5.19413731469426,6.3309712543218035,4.91860420210099,5.787148880720895,4.874879576255446,5.305728401957795,4.90612121072306,4.8968217931810125,4.886055945665818,4.9063566945257895,5.0349292291498635,5.03037808251593,6.229417595670128,5.223704830648975,4.940056879860045,4.969892577514065,4.881381111772297,5.0251927539585965,5.106714462859214,4.916207229925835,4.918339362274289,4.96420026889125,4.920489378500746,4.933521135008205,5.0050065936193855,4.951570926736105,4.87528884298434,4.901111469372541,5.221614069774239,4.894073520986415,4.897225938629832,5.144255745693412,4.897199037018102,4.997328545251114,6.259234104397718,5.0084512555274445,4.9339829132646305,4.958777688717236,5.962182166704672,8.419149440179769,4.960766505444973,4.920294059743461,4.9399767848173255,5.047870750029151,4.9433441636376925,4.9511788404910515,4.913166803325631,4.909468080827312,5.218998022375955,5.403976490902975,4.929323423392466,4.96120037823824,4.900329517037037,4.978501582785416,4.971168925602944,4.916435720327589,5.325556156619336,4.880468802943257,4.967816391671429,4.990608112074653,4.95345179277369,5.097323694744267,4.93652186932126,4.995059318991233,4.91431133555426,4.945396684871516,4.930271547120725,4.914741225859167,4.938652633716889,4.892560790117424,4.980621413635951,4.906685952093231,4.924046982558442,5.229971025291311,9.520247929338716,4.984333923546057,5.094797373313194,4.901380018051763,4.895368841171715,4.954884104596915,4.899524121233829,4.9221700984324315,5.055824434834761,4.909033126609449,6.370849466118224,5.13914378076306,4.94526550825126,4.995024448237757,4.919624271496695,5.1158068013871425,4.956108784200225,4.896378437029706,5.244598224746256,5.11093996267906,5.609714274070843,5.267863080378131,4.948511734293915,4.8802614676078955,4.880893463982752,5.112287219701687,4.908373700194495,4.891025519393585,4.943167942583087,4.8982909943732365,5.045144850314556,4.974357862582816,4.943295022901909,5.126702612185472,4.918963912439298,8.216396407642517,4.8762289401472545,7.9800343199748545,5.0599531936676385,4.919926843351666,4.89829968732495,4.956615581366207,4.862021028316221,5.495861538729859,5.89290668945821,5.397616824744724,4.910822422982352,4.8951467553472545,5.139945757750905,5.01870418959925,4.912784768518605,4.88480191119974,4.93709598062826,5.782387737719484,4.937556332070221,4.930676447954419,5.652962457753311,5.048655795264577,5.0435224758592385,5.09162830232153,5.615553529317199,4.910316321486886,4.898690415086279,5.089627787080827,4.899935756862164,4.902960357215596,5.045970950296894,5.212528062826401,5.11136923413163,5.362041350716845,5.2265144025943995,4.984817181986396,4.915305316566642,5.070631556125405,5.566033959028082,5.590507459595319,4.922009307435473,6.233643095489973,4.951247455500862,4.978148676328221,4.991060864930836,4.915190450587557,5.16088337580379,4.978501185254312,4.919415950407403,5.212524149846349,4.935928511274286,4.948986723894919,4.9565453466370455,4.980146219469326,5.02778412674351,5.217789695887752,5.20932705134182,4.8761891575581435,4.975669739195038,4.923915619693674,4.89638056566862,4.89467146465232,4.879765546857019,4.915932647173289,4.928085490879944,4.890678648776641,5.448518367398313,5.8748102278477115,5.066412155492685,5.002063478631358,4.9197287420059315,4.895336830259966,4.996793391839252,4.9456912710727625,4.97719227481868,4.9330141819727436,4.911465830611477,4.962081407907427,5.0845394616594435,4.88644839129074,4.900276314848865,4.935742781746743,5.916999208341789,5.124378390256565,4.869251313459633,4.8183829598656525,6.894300275332835,5.048748106314537,4.96819367284933,4.911109108052829,4.992641667665755,5.006665160773337,4.896761128780163,5.1931712992376236,4.879737381569113,4.90814674296659,5.042912215436928,5.000424432099972,5.094789167146211,5.384860348598044,4.931205380432203,4.970030736991212,4.93545244297255,5.086863719486183,4.924400198025527,5.272984305876462,4.977860019280542,4.908405415781304,4.986743451886771,5.922859629165675,4.908173151990202,5.249553544063388,5.007238209548627,4.918954492682059,4.924915869348191,5.336955145342536,5.109692086342375,4.89678628993727,5.38890479647185,5.440388046903146,5.206930824045668,4.9019529272494236,4.924283933990819,4.908308351959163,4.950540434317494,4.952810053135794,4.995048209496425,8.439168507864647,4.8884389258147305,4.897506615423322,5.092657024876904,5.268386125970387,5.413282454163731,4.909976476566524,4.844111309346886,4.871977586220152,4.968688921118488,5.034088976115799,4.916195705965611,4.863475509249377,5.185282851859386,4.870719447995021,4.944619240731203,4.878181927573513,4.882390892594349,5.052881295084423,6.3351967541416485,4.92675091339289,4.98008345450656,4.944147235729482,5.308818079421485,5.155760962340832,4.88174363535032,4.9496497623077325,6.2110665391770405,4.956080972054731,4.967331365089227,4.943115063474379,4.866761796534987,5.03655106765458,5.249610613776011,5.830249674976144,4.890281445485664,4.92298840103116,4.947262015823319,4.917445363336675,4.869046677326529,4.948512498756658,5.247003301010444,5.066962083671918,4.97354611009732,4.971607628575433,4.90327540192692,6.367781003888357,4.957397560362527,5.150206336581506,4.897939310467953,4.891784039020016,4.95361211787292,4.978394188395521,4.935780847664512,4.958773745906236,5.247792772842235,4.849679751866756,4.929624772439188,5.370245668012713,4.965198214045318,4.889825791516054,5.169153276906356,4.896758315540448,4.9293646716461295,4.880531283834951,5.824044681644431,4.925084070207072,5.080318235572428,4.995397941380791,4.865047493675042,5.146544213557155,5.0083606414025,4.881093143021088,4.938050386328802,6.811575729463877,4.946286590417667,4.97880020838599,4.8837600444139335,4.8908302312875644,5.062053919506579,5.634140318358232,6.083953875259344,4.970093742888174,4.973333545396482,6.602256638229581,4.877067297217437,5.905957175732785,4.972605839019222,5.146520817902111,4.952723885556805,5.77043241132361,4.912117479775078,4.8889732377227375,4.864992548502239,4.9496542761595155,4.950715756390402,4.874938072740606,4.91744874654879,6.041207277886311,4.968636503738385,5.362328676639182,4.8895442896888035,5.140030245873566,4.879353539986026,4.910891211145498,5.1945957469863355,5.776258100155556,5.1661941501561985,5.040886498089243,4.871888708168087,5.14701466607426,4.9569877787425,4.907234136272905,4.895478683226197,4.968892205917965,5.035281635364009,5.85111014152443,9.280559168009098,4.949961887829168,4.894537495190898,4.955273184089189,5.918634129345829,5.022119755851987,4.907321135287918,4.942411282377691,5.468470658654467,5.131028176705548,4.896448337181634,4.914163658148784,5.84357766662885,4.963787838612227,5.1644343318225046,4.912789775068854,7.091908990832637,4.890322939131259,5.116613261450145,4.902758701038882,5.031026177597446,4.905876397280606,5.070253223660264,4.925929835446754,5.11927367745454,4.955972501969592,4.96923049696237,4.901992904600825,4.959234940645974,4.956561037689016,4.922958977685247,5.008673301445932,4.939438905533864,4.926984706514975,5.365525283109071,4.9881430592562275,4.97425181432989,5.684809120335601,4.954300675798563,5.556127757968072,4.91581535032384,4.881394423348896,4.892693741252402,5.010124687591603,5.185689128294284,5.134808431756109,5.071382379281548,4.911438548210916,5.871202333414923,4.89109349754966,4.89382879801942,4.9262080100705745,4.9323048766722675,5.099104233660408,4.908078971453576,4.893698928024696,4.930837885861823,4.921198862519014,4.8967313594700705,4.9503127379859615,4.916431526304448,4.921577502494676,5.023643515820824,4.898331978923941,5.3655877832021535,4.984970583573742,5.139084340575317,4.880379093772905,4.921982510250729,5.0254657164359315,4.93121020633482,4.936388832662825,4.898202576730047,5.38416958790147,4.99783493087749,5.0197353389469415,5.3666423898124345,5.0273366013317,6.300194011034873,4.991163363800304,5.7412634263108115,5.030743418722729,5.007313886294324,4.97739708638792,5.063922379979269,5.142696859807934,4.911470485529902,4.893016275776775,4.879337539995246,4.884839175428856,5.131009685011148,5.746000395526585,5.011216581231891,4.947393442402933,4.9801571229792,4.951157304257043,4.867341298730915,4.952003917572484,5.295487843585918,4.931106833408497,4.9254354084714045,4.985742518771481,5.10873395888416,4.923924081346016,5.002940204656181,5.077375746943076,4.9315730534886875,4.9469206945898065,4.984688256368814,4.92397571434494,4.918776729696767,4.936236453684656,4.902715549629673,5.086868868964645,4.967006159091426,5.109370119783794,4.877848283201749,5.352064114627337,4.880932093686967,4.9548656649072695,4.9904928859826425,4.909406102394037,4.9775439459044,6.680272775953724,4.862083586635927,5.198168888961388,6.807350229644031,5.141364567170799,5.269565590281241,4.977293569697363,5.435158241150012,8.792670550956055,5.202089419743601,4.879225456339108,4.900847285212154,4.965913995093231,4.889585622494823,4.93719184284353,5.000590519703942,5.4190090278154415,4.96416416832186,5.133997770219151,4.909938158328938,6.446168262687454,4.82677167625074,4.916167038618639,4.907993637224754,4.870668296459662,5.011583387641267,5.514893010804784,5.480227000809643,4.880786492886776,5.1899146281141295,4.995973907677478,5.682343157627431,4.928886606156679,5.568050666538096,4.904797059622624,4.903173149906931,5.393391324924878,4.9075529540541485,4.885219479203183,4.88676706835637,4.892006489839473,4.8866047314677195,4.953914257128413,5.053135391678517,4.965986146349442,6.267406357255944,5.190677605743998,5.00728782032778,4.949195884348698,4.937023986713301,5.464078730191979,4.9579908939839346,5.081750059586198,5.014864008696649,4.914442624210792,5.009819101271765,4.976008660474957,5.14226165446734,4.942739515933009,5.020724777290163,4.880374282002926,4.903238725404433,5.0073578878214215,4.898490049809827,5.774797763408602,4.947048468312338,5.000199407497324,5.256408644222253,4.9233093157052465,5.376711164158534,5.0315263559740036,4.8629754170361315,5.4256457414717785,4.946227403594581,4.928371119765035,5.181819460634695,4.948996435630543,4.958074115030932,5.035810437652794,4.9437677501739214,4.983613687840775,8.220621907462363,4.882677611486565,4.939047973103957,4.860821993855196,4.905626741194486,5.348408582281403,8.437933597716214,4.9148932985689555,4.960198001789438,5.120872628023103,4.902648499363581,5.034968918542574,4.868664888436748,5.2645389956641715,4.922717280832821,5.148805355830946,5.088799593074281,5.064600097389743,4.919707889232696,5.293162498849302,4.902468748958158,4.936198400600079,4.98059168216655,4.89322632449476,5.008543103168308,4.904829922242979,4.919441126270113,5.190364265755901,4.98392456956821,4.999855858062341,4.95223967315292,4.932839658890659,4.9215453332309895,4.9104455033373515,5.044470778674609,5.108835469861756,5.876706617099409,4.969127567548523,4.9493638228306,4.897824620215158,4.973318446084554,4.9262595035331795,4.955189072669067,4.940143132294249,4.960281282651283,4.941214249219248,5.335964495414455,4.973047851120231,4.98943829296887,4.90363472415782,4.884317772093329,5.2017294406447,4.869402679363992,6.267413974290431,4.8976108792894975,5.034183304353826,5.413459948137315,4.963107881630922,4.911086796577488,4.988919871426458,5.110135749950749,4.921991951362726,5.006685426142707,5.097211637846252,4.94691198406911,4.942373756992482,5.00436034192698,4.873215468891693,4.890915358873326,4.869865409936122,5.05273370311133,4.874836640183458,4.956963704856157,5.045618587849319,4.9442823796798905,4.910352607196987,5.2346134376476465,4.9070597921617045,5.120084655808769,4.911159435445886,5.141173827943024,5.431957946413923,5.05317801475519,4.996641348187439,4.86577452749856,4.906392296775118,5.137401432955253,5.051155681559624,5.52044225745453,5.388395087721315,5.00528767059002,4.94280055538303,5.0710939739742225,5.011618745082322,5.222535868548654,4.9022217421280745,4.927749033268466,4.9481201752286825,5.387095940852861,4.888519322658005,5.495357271811203,6.429209525950363,4.945409437916892,4.870000027318405,4.939525636953054,4.953642701811665,4.982345296941808,4.897988835804386,4.950538714460962,4.901489495111248,4.928150005152037,5.588160063635119,4.9925292063557105,4.880982646450682,4.893897946158107,5.307627642923086,5.44729220347551,4.919541418196949,5.162820009710301,4.931989438045188,5.7500422757917224,5.070035304117036,4.9103533130361825,4.872951641563968,10.02411293534902,4.935918175788656,4.913563044171378,5.07485705594525,4.866770247420542,4.971543575997849,4.95644291340429,4.896088420200267,4.989535028620658,5.086027805799485,5.068718167142843,4.8792739173869375,4.899206942830979,4.933289974294952,5.302494854542682,4.912218930401539,4.998454890654999,4.921949562355482,5.24197192371787,4.913222470113484,5.174157524332775,4.909309968535227,4.87048204105641,4.948844740551049,4.909030060137166],"xaxis":"x","y":[4.9676297534661975,5.60584986719498,2.950468414150123,2.7355221772965375,7.258707352196828,3.0925457415435655,4.178714641175443,3.4154882710497003,6.224387627929912,2.634593268445757,7.244982170376553,5.597531174224212,4.210232990095849,3.594548549550354,2.526068811667588,6.215484464192794,4.969933274697856,5.912649864897204,5.111865963867557,5.042644337408494,7.864928972289788,3.382667252745041,3.189033824390017,5.705977901682522,3.9717734471933728,5.119771161952904,3.496973580998276,5.058749412335524,5.968551603363945,3.1160319934471103,4.464668267003444,5.192194165283345,6.73226919950145,5.39746072605472,8.171076693915895,6.43095427138573,7.132782457298859,4.294988267691446,4.708739041359579,3.4646682670034443,5.450881315273369,4.95093492831454,4.20006486151431,8.047505671251574,3.605257262939004,3.261530815243406,9.159820892013935,3.8728287595348854,3.8298495598446904,1.9671686075326276,2.9126498648972037,5.648177795724818,10.257022407964342,4.887037791419197,5.718087583960517,7.540941686758214,7.156841524588401,2.0942360698457656,6.074462620704536,6.428778891337586,6.464504909131913,6.588864518190148,5.629648044187436,4.795974694206668,5.676380255466924,2.811471030529836,3.9882302230604574,8.45314714555533,2.321928094887362,3.980939266085512,2.5801454844233804,4.41886457739032,2.8972404255747994,4.341985747228616,6.561784353081841,5.751410160064371,8.545157551729474,2.7676547982373463,3.773996325111173,2.7676547982373463,3.258518924711301,5.596637034102074,7.019368330478469,6.649471757181635,5.275007047499869,2.0071955014042038,3.700439718141092,5.320845667645722,5.331633567171029,8.061668207389335,2.786596361890807,4.94673086014031,5.649040565634448,2.269033146455237,3.292781749227846,3.683696454306516,13.390111171989,8.064904402198286,3.727920454563199,2.8155754288625725,4.253232938538032,3.0373822220030804,6.01992426090841,3.2524762141352173,7.125981653854716,4.273515889702116,4.92552476974757,7.778274295338579,6.135452784185173,9.20006486151431,3.6971065744769747,3.2418401835646704,2.4222330006830477,2.6229303509201767,5.57198012018773,5.0539801681876515,2.6915341649192004,7.510803720358323,3.01435529297707,6.045049740837989,2.7803100990433753,4.658211482751795,4.595145567990858,7.485507332459785,5.342341397431548,4.129283016944966,2.454175893185802,5.96254902292306,2.3305584000308026,3.292781749227846,6.290756138396673,2.817623257511431,5.532940288372874,3.2883585621936606,8.008820757716158,1.6644828403646825,5.284292026394312,3.598126959919604,6.237448995639321,4.222650022451478,4.380590934265922,8.669593751188334,7.720552091605108,5.548744859937229,1.7484612330040354,4.524815928357506,7.639376888638463,2.851998837112446,3.218781167784069,2.7990873060740036,3.868884273028774,8.043027283594547,7.260778431893426,4.038260575175349,3.040892430646901,2.7949356628035362,5.4741115139424075,5.952799477899938,7.323370069061268,5.040892430646901,3.4208865749755315,6.136683577697236,4.255500733148386,3.228049047884462,4.6194130105979365,5.623515741490549,7.217424613813984,6.179311989210016,4.3139713710590195,5.764473550992666,4.221103725367875,2.4594316186372973,6.548282482376233,5.9902741833186015,2.5360529002402097,4.720825666089835,6.512700963425246,6.664198369291911,2.881664619320345,1.9634741239748859,6.013685569928653,6.026578770127714,4.532940288372874,7.597158683183727,2.711494906650088,6.791683858152045,2.1009776477248208,6.090641751917847,5.040892430646901,6.327328082629952,3.2942531364445142,2.9467308601403097,3.7366048753142485,4.922673592849446,3.71699089440494,4.15623479785027,6.872951851130105,2.8933622107638715,7.561631630375841,5.341985747228616,4.993221467368938,6.309067020588001,2.414135532984451,2.493134922305505,4.161081482277184,2.144046369616707,3.486714373030702,2.8011586560936985,5.648465443027314,4.399171093819823,7.581803148516525,3.1160319934471103,2.533563348214512,9.967312731471658,5.687060688339892,6.954079959320971,7.084170591577843,2.722466024471091,4.826802684285827,3.01435529297707,1.6040713236688608,5.5109619192773796,2.1731274334806563,5.425593577653059,2.8797057662822882,2.7803100990433753,3.303050084681673,2.3132458517875616,7.006298023900369,5.279842693520348,2.3950627995175777,7.216163813388774,2.786596361890807,3.5298209465286954,2.7420062108667365,5.079378111118652,5.1622906135457995,5.839203788096944,9.354249381945241,6.058532970201611,6.614857050384495,2.1667154449664223,2.4982508675278257,3.3405622690264134,5.415826257472914,3.9791107550157854,5.338780943894699,1.9335726382610239,7.164705840182799,6.12866458654003,2.933572638261024,3.0548484769956197,6.390598905552461,3.2764966656403565,8.007363716298686,6.558114538598396,3.674686619927999,5.28835856219366,4.189033824390017,5.480265122054463,4.2787282129389395,3.776103988073164,4.82273014794452,2.9467308601403097,2.2898344651775093,6.1705259991768475,1.4750848829487828,7.264067216098838,5.6845382485777325,4.534808661231747,3.7070829917717063,4.919340082442012,4.458119481174506,6.288173968112302,2.27500704749987,6.5249725982866265,3.940166750482817,9.715601700373337,7.829278760612363,4.821199978056451,5.875042803002589,2.6229303509201767,2.942983598187102,4.445594291341182,7.474030370207135,5.92148393743373,3.4369613378336026,3.2078928516413328,4.095080491901415,4.422233000683048,7.424922088210688,5.185470146066942,4.955126781261366,4.3729520979118295,4.0522419811811385,6.067165427017112,2.2898344651775093,3.0806576633452254,3.940166750482817,3.127633279725874,7.152994605492435,5.573192723781613,4.4121040446775694,2.2479275134435857,7.886854944198882,7.9329824099498465,7.25691629667084,6.670585440262214,5.845740932614582,2.9616233282869446,2.5459683691052923,7.959132576789657,4.653633311386255,7.406162478777607,5.588264894307497,3.8063240573900288,6.716579420796345,4.984589350362456,2.269033146455237,6.929672661579345,7.060371694122218,6.746447139362815,5.521364878446219,2.9873208659292536,5.970623614470605,2.37851162325373,4.667892125230234,3.192194165283345,6.816215687622865,2.851998837112446,3.3854310371935203,4.634593268445757,2.5558161550616396,8.990671283975267,9.114445113289024,3.389566811762726,6.9532652390148435,6.705563274566321,5.672425341971495,2.280956313831056,2.137503523749935,2.2203299548795554,6.561173365231147,2.9726926540042644,5.078951341394822,5.0759604132434015,3.8509993947164736,8.693835392482516,2.845991770664573,2.8399595874895316,6.111448698487674,0.7484612330040358,4.125155131322289,3.8953026213333066,8.45889870671479,9.650854294025212,8.19120730223037,6.534964249804655,2.5160151470036647,3.0373822220030804,4.701549056943071,2.9541963103868754,4.903520373723376,7.848935855524565,2.0036022366801953,5.660780334032546,8.418822422291017,5.254745197629213,8.4696011200494,6.567880042052731,6.557808298936688,3.6769443591069124,5.651051691178929,7.279842693520348,6.425089989874059,2.811471030529836,5.4799419014771305,5.184280294419382,5.713420884868084,5.452858964713811,2.4195388915137843,4.178714641175443,7.686990680088471,5.3847405872923835,2.722466024471091,7.554281862626391,3.955126781261366,5.192194165283345,9.071542807992602,5.765004246250036,5.318316841334983,3.1953475983222193,5.2671612180222604,2.269033146455237,2.702657543390911,5.6937657122177825,8.574290766120185,3.554588851677637,2.4059923596758366,3.0214797274104517,1.3785116232537298,6.010108453474289,2.438292851579147,3.089159131911238,3.0250287944915226,4.571676809970931,3.24031432933371,3.261530815243406,2.6712933724815757,3.6542063779442917,2.9873208659292536,5.8060662260890314,4.371558862611963,6.4984104488609775,6.229587922740652,2.2957230245399685,5.178714641175443,5.463687842201262,4.884597920990064,6.157852169141737,3.4672794804599825,2.867896463992655,2.611172380044005,5.535430914871279,0,3.4776773275653072,9.582254908357664,3.950468414150123,4.8894735425311096,4.850999394716474,3.300123724569014,8.009660689646706,7.626950122372515,9.04105676230812,6.564835417010355,3.3175935046234715,7.380331184220349,2.0214797274104517,4.235727059838059,4.334139178838237,5.692092375427572,5.592457037268081,1.996388746447621,2.070389327891398,5.8052924556007115,6.60584986719498,4.088311235888661,2.521050736900963,3.5310694927259543,7.058316495590823,4.044394119358453,3.0513721017210256,6.008092420948722,5.605257262939004,8.220281580441913,6.088735246190284,2.137503523749935,6.637204481902295,0.8155754288625726,2.2387868595871168,5.676662334857967,1.7865963618908067,8.05642095591347,7.5673476962842585,7.295355692891673,6.394033895367782,6.582706526780788,4.43095427138573,4.915520900751958,3.710393192170516,3.8845979209900645,5.135042286233651,10.267009017093484,5.336640446408735,3.0992952043377753,6.836050355058069,7.896998342338911,6.558114538598396,5.10936055940423,5.85872702377863,3.785550551739256,3.8042601156347384,2.7484612330040354,2.883620816285671,3.576522137920503,4.104336659814735,5.86195536414487,4.294253136444514,8.44136727845972,5.497931651896279,7.5808238499627985,2.8278190246173196,6.260590274730996,2.229587922740652,6.218781167784069,10.212070748654236,4.400537929583729,6.543805175964397,7.563234413022304,6.203788453308596,6.676803353873854,4.070389327891398,6.340918270272116,2.063502942306158,7.151270288790165,4.228049047884462,4.563768278452033,5.4329594072761065,3.9373443921502322,5.580748491763774,4.279471295644468,7.959132576789657,5.739578112048051,2.7463127664254587,5.707635219545296,2.0250287944915226,3.4776773275653072,7.925940095994448,4.478971805032942,3.24031432933371,6.836807807844575,6.2502036564992975,3.82273014794452,0.918386234446348,3.258518924711301,2.1634987322828794,2.662205499653617,6.687060688339892,2.2357270598380583,5.0259146987507926,5.443606651475615,2.980025300238734,3.1193561770396756,2.8399595874895316,4.914564523493939,2.782408564927373,6.282625134191674,1.2016338611696504,5.269781238274379,3.0513721017210256,4.649615459063409,4.896271848807375,4.721919445551544,3.6182386555954547,5.045268215138534,5.020591094773247,5.305240965954483,5.663913842115978,3.3881895371560837,8.836744701966083,6.976363635732762,3.925049964727359,6.802451741275622,4.664482840364682,2.3589588258323295,6.127014141582809,7.1945598860226125,4.475084882948782,4.301587646603187,3.7441610955704103,4.05484847699562,5.464668267003444,7.33360262826828,5.6082178530214595,7.1320626778543845,4.993674361750585,5.522620761042079,7.366234775533047,7.166815851761945,3.7687136570304847,2.321928094887362,6.7873802222963375,4.788685710613533,6.025250321561575,8.162995472417697,3.4019034716079584,6.238786859587116,2.3248106034204836,7.831687267083084,3.4802651220544627,3.871843648509318,6.548128323585586,3.0268000593437154,3.1953475983222193,6.2600256559614555,5.931210274803932,7.80967167790706,3.169925001442312,3.894332742277694,2.298658315564515,6.215678596607928,7.873013392989595,7.709152756902506,8.461602224618334,4.150559676575382,7.653203362033977,9.24186401223682,5.725468512632615,4.0925457415435655,3.575312330687437,6.519321734629434,2.5185351389821804,6.137913322088407,7.904544309292717,2.568032104771279,9.816679697610459,0.765534746362977,8.646594709581654,5.037821465434967,4.388878338811989,7.9616812016062815,1.7782085763980877,8.912589990852627,3.5008020530571575,7.473786911614368,7.182890892745479,2.6205864104518777,6.628919373511362,4.842476062993939,6.013462259806563,7.1851727751412495,4.8359240742543745,4.918386234446348,4.783980413683795,4.285402218862249,6.340918270272116,3.5570424152637194,4.030336078370959,0.8073549220576041,2.2957230245399685,8.893574570329104,2.3923174227787602,7.015805293913353,8.764407200357251,6.147306698780294,5.502075956045791,8.694706111949401,2.440952198029637,8.447992374957895,4.943921326553485,6.072963271555224,8.18685706717715,5.508111681193465,2.327687364176047,2.6322682154995127,5.853496703702325,2.4724877714627436,3.412781525338476,5.751677945687579,5.8774988727835344,5.890446692679906,8.622783966156124,6.203396948546115,5.524189078449365,4.679198570566922,2.3757345385831563,7.809543066804601,3.5716768099709313,6.39694722007283,3.097610796626422,4.9415758618182375,6.668317719915721,3.0925457415435655,6.941106310946432,7.471431358002323,4.633431210355632,2.454175893185802,5.760487101811031,4.112700132749362,2.560714954474479,2.885574364371426,5.8060662260890314,5.969012307516316,3.0214797274104517,4.37016428054021,3.1811025507537978,9.463033855317493,5.102238193930738,7.537684337147519,5.960001932068081,4.451540833017832,5.931683057059805,4.126807703142035,9.109647857806625,8.028624401408491,3.2645364309990255,7.394977085526471,6.152589069287125,6.022367813028454,4.51853513898218,3.344828496997441,6.810314561497258,3.0036022366801953,7.890811454817677,8.317774372804099,7.237735785231934,8.515305605451724,8.213638902198852,5.154615611326894,2.039138393906958,7.296273846706719,6.905206468795705,2.3673710656485296,2.0942360698457656,2.9145645234939397,2.2172307162206693,5.641256998726767,3.303050084681673,2.62993940943954,7.220233204382192,6.489767697008256,10.809687753488749,5.260778431893426,8.11134436328308,4.9941271140031285,2.7070829917717063,5.856985689782205,6.381629467033664,4.493134922305505,10.642557179595622,5.439955517478156,6.790772037862,2.5058909297299574,4.37920505996061,6.33646192818282,2.9523335663696857,2.811471030529836,5.323730337521348,2.763411574470007,8.089159131911238,6.435461914479276,2.4646682670034443,7.870364719583405,3.396433531250992,7.305696981080096,4.020591094773247,5.187451054027326,2.298658315564515,6.038918989292302,5.369815424283912,4.422905742612183,4.024142345897803,4.926948247949772,5.348728154231077,6.907251224292033,4.861459166361513,6.596487956856693,4.593353770980297,5.9502351004856875,7.814614507639093,6.638797899951696,8.79720757285578,2.176322772640463,7.7974669920591575,2.7398481026993275,3.1538053360790355,3.6701605141266334,2.513490745588118,5.307064162255372,5.076815597050831,2.533563348214512,6.678494507773734,2.7092906357233577,2.7355221772965375,2.782408564927373,7.039796407620025,5.938521045865967,4.733354340613827,6.132988042627442,7.991691898336964,6.419033185471022,2.1858665453113337,5.449891473010944,2.6064422281316078,3.7311832415722,4.332707933640605,6.56498780095989,5.434294617959925,5.893362210763872,10.084529386649065,7.932923373835633,3.0823619695574735,2.440952198029637,3.1842802944193824,6.1430260038899736,6.387500406480984,6.681449265314958,4.324810603420484,2.327687364176047,2.722466024471091,5.569551851083613,7.527555198794536,6.632559052117953,2.9145645234939397,6.135452784185173,5.424922088210688,2.2357270598380583,2.8011586560936985,9.8361608416947,7.193968819421969,3.097610796626422,2.655351828612554,5.553360503335328,3.169925001442312,6.173127433480656,3.817623257511431,5.778208576398088,5.7554217347342425,8.571145863602528,5.470862199413702,3.1602748314085933,8.3389146195993,2.440952198029637,3.982765462583646,2.140778655782796,4.3197624276692475,2.292781749227846,6.864928972289788,5.969933274697856,4.950468414150123,3.4581194811745064,9.054360117597708,2.232660756790275,5.648753032989572,4.827310943949667,6.609104846618959,2.920293300211007,5.726013748860151,4.484138131201669,6.010779838753242,6.51317488460363,8.731081391094504,7.529742876502924,2.7782085763980877,6.546122758857796,4.635173946674924,5.533563348214512,6.2992080183872785,2.657640005207824,11.260084480727762,3.442280035252584,5.339850002884624,6.091065077930535,7.352175878326325,7.78959885005233,3.2418401835646704,4.584962500721156,5.035183996618336,4.898692072560488,4.393690764187454,6.686360452903688,4.792855352362489,4.044394119358453,5.077670274232752,7.061344188288391,4.06522762277562,6.802968651005583,3.412781525338476,7.953207027098,3.5643781685650637,2.790772037862,3.350497247084133,8.870919495610579,4.663913842115978,2.2234225499349374,2.6667565918848033,9.754286253961022,6.500324049284198,7.87670110864523,1.4168397419128291,4.017921907997263,6.177519202597814,5.47800105583793,8.031053274444373,7.831180547126736,6.461724991502711,3.948600847493356,2.319039815562536,4.898208352508718,6.20945336562895,7.562776656791846,5.542258049766918,2.6016965164809576,6.711081862306696,2.0565835283663674,8.630303533275018,4.854494418154875,3.4892860226258766,4.601102203376409,5.932391940068527,6.822602695756541,3.039138393906958,4.397118409042582,4.078951341394822,6.198886990956932,2.298658315564515,7.472731449369055,5.0669502439246275,5.059614856297222,2.9708536543404835,6.608661417987563,5.51443791382931,2.176322772640463,5.642412772905056,6.8820316083521,5.234577959784565,7.163498732282879,3.212569338850806,2.2898344651775093,8.307656205306488,3.423578170981797,1.3840498067951599,2.7676547982373463,10.679004987548526,3.2645364309990255,3.9990980337056072,5.649040565634448,4.793375711264754,4.203201156316611,7.668814088419117,5.911931212309049,3.4581194811745064,5.445263208140443,8.913188619508889,4.936402377725063,8.645802514606062,6.207111961207714,5.749266083814806,4.4376272483189485,8.580786171355323,2.189033824390017,2.341985747228616,4.06522762277562,4.804260115634738,4.849999259466098,5.075532631167357,4.056583528366367,9.188044796421696,3.8439838440483265,4.634012356402116,4.8104431038451745,8.686815644595628,2.521050736900963,4.016139702655253,6.212180210044431,7.545505100691072,3.904002316283692,3.71699089440494,2.1667154449664223,4.381283372503783,6.564835417010355,6.563005553062399,5.911212201558612,6.247547806890298,5.477677327565307,8.292781749227846,13.587327482778312,2.9011082430145123,2.845991770664573,3.7644735509926663,9.182369521950886,5.921959917326167,1.9297909977185974,6.910972451655129,8.149950301681047,4.3729520979118295,5.38404980679516,2.063502942306158,5.977508904540138,1.3673710656485294,4.067810783789621,2.1176950426697547,9.199893148608071,3.7081872360207084,4.628773595201645,2.488000770834068,6.501439145158873,6.706254253573203,3.0908534304511135,5.86616616903904,4.155425431747197,6.060695931687554,4.570462931026041,3.2357270598380583,5.341274183692174,6.656925339794245,5.588564737401351,6.353146825498083,3.48155728070859,2.4698859762744636,4.872828759534886,4.34766565630097,2.877744249949002,6.019479533703628,3.465974464504069,7.536363792395467,5.132576842670009,2.0531113364595623,6.283366212949919,3.699329525673156,8.199279721336625,6.8203066288938246,6.068240861312821,4.598126959919604,7.979739567128753,6.258895755075436,7.258707352196828,5.394376944957055,3.0125686735030555,6.862203399053224,2.8318772411916733,5.164705840182799,6.431288654291593,4.375734538583156,4.8594726668519375,2.9392265777282085,4.302319050948834,4.397118409042582,5.169123281476757,3.761285273361619,7.240600549955795,3.7387678368001915,5.678916986679839,4.3334237337251915,5.530445354692906,8.431288654291592,2.521050736900963,1.2265085298086797,2.2539892662307865,5.23687524536694,2.298658315564515,6.398316163400203,8.348374075191163,6.043300754364182,9.401604869714957,4.534186139090097,9.069933046375542,5.486714373030702,4.96624587322494,6.307246355226617,5.51127826508303,5.970853654340483,5.342341397431548,6.219555769166913,2.319039815562536,7.516566771496865,6.652773284510777,9.109908988534402,5.338067797542616,2.9981955031532523,6.506843098969815,3.04788732939655,2.526068811667588,3.414135532984451,6.295723024539968,2.257010618206024,5.500164679492167,5.333065878076355,5.253232938538032,3.619413010597937,4.695437110405369,4.37364821133469,8.356407951394893,6.039796407620025,5.908332569782425,1.74416109557041,5.666472568842073,5.9586107120558465,2.430285272977781,8.487277313213783,2.927896453728821,6.9920886090139955,5.7615512324444795,5.906409617094016,2.572889668420581,2.9763636357327616,3.311793717753649,5.325530331567558,6.075532631167357,8.785648628652076,5.301953395126096,6.687620632084895,10.60719158640291,5.2543672814408335,6.976821852360685,2.655351828612554,8.033092575523664,11.658760423755194,6.231317197152663,5.008092420948722,7.824449651364088,2.8155754288625725,5.656496370553495,3.218781167784069,6.101187815259738,8.328001662149257,5.576522137920503,7.176621973161577,2.0565835283663674,6.5787877956769725,6.602142090697982,2.7441610955704103,4.727375938594897,2.2172307162206693,4.552131108253784,10.754979337466484,5.651338766105931,7.449148645375437,8.21179097625254,5.769771739249448,7.09876903815488,2.5921580021253603,7.971830914780502,6.234194722953812,2.1984941536390834,6.955824240806441,0.16349873228287956,4.680324356844016,2.5801454844233804,5.583458910130768,4.845991770664573,2.7070829917717063,4.18824265626441,7.27733390042982,9.535217045453418,6.304876050044889,6.982080909763738,3.0925457415435655,7.051372101721025,7.022922589055629,2.9392265777282085,6.653203362033977,8.22838581704935,4.960697039304312,5.564378168565064,2.782408564927373,0,3.9354597478052895,5.525755692829486,1.0976107966264221,7.211790976252541,4.477677327565307,2.4436066514756147,7.966303561412599,6.19021976366016,5.592457037268081,10.665637927637095,3.277984747299765,7.8269297659895765,3.5643781685650637,5.612057559563065,8.766462652425766,2.662205499653617,6.920412408160787,5.290940402403678,7.502871574574138,5.2539892662307865,3.1984941536390834,3.0089887832272546,7.2259304111710545,0,4.919340082442012,5.334496768390418,4.654206377944292,2.327687364176047,6.705839705883682,11.02948049374083,2.4594316186372973,6.294253136444514,7.172927489603836,5.573192723781613,5.5804470195986635,6.341274183692174,6.768713657030485,4.5855634984556914,5.609400390006429,6.10055722078293,6.140165134599453,2.37851162325373,4.726831217032493,3.127633279725874,2.414135532984451,4.561326136453918,2.211012193485512,3.6147098441152083,5.901832305785489,5.903279342056139,3.84197311892718,3.0443941193584534,4.77663042305105,2.7949356628035362,2.5897634869849773,2.4436066514756147,2.5160151470036647,4.623515741490549,6.6059979802325195,9.051127353591394,3.017921907997262,3.9212458885855885,2.2387868595871168,5.621465834460144,6.001351892999959,6.605701738949982,2.6530600171045644,3.4698859762744636,2.456806149230474,4.528571318870758,10.08413071995014,3.1953475983222193,6.061128135118254,4.434961760187254,8.556735947914529,6.821837746088096,5.895302621333307,4.783456654360239,6.1147834472498515,8.8634739090894,2.6182386555954547,5.862203399053224,3.319039815562536,6.531225485053606,2.2265085298086795,5.50303064564116,6.549977142771825,2.582556003014061,0.45417589318580215,8.258094872866513,5.864186144654281,0,4.110196177754199,5.541948425333733,5.139960569545456,5.166715444966422,4.661635602335959,2.6016965164809576,1.0565835283663676,4.867402305727543,6.390082950951663,6.790641730763425,4.185073637876762,5.681168120416691,4.409390936137702,5.399854673572431,2.752748591407134,5.553974808237472,4.238786859587116,6.451211111832329,3.2555007331483865,8.627898615871748,4.5588798534754424,5.639521599510833,5.29020320512607,8.07804403634318,5.592756010441026,6.952217064973338,2.6392321632492775,3.2942531364445142,5.633721812641012,9.325777655214823,5.554588851677638,8.364703739582406,12.206535173103617,6.2268938135713885,6.058532970201611,2.996388746447621,2.169925001442312,3.2539892662307865,2.3645724322958563,2.996388746447621,2.5084286525318573,5.370861740085285,7.2461230165968376,3.19377174339668,2.4276061727818994,5.25285462595139,8.366191054189278,7.062963556559086,2.0531113364595623,7.846744023244868,3.581351247168777,6.948834425578686,6.412442824775613,2.899175630480513,4.786073551578548,8.971342367308225,7.2193621578077165,2.885574364371426,7.801029283808189,3.7813597135246595,5.6884601404244926,3.361768359419153,2.280956313831056,8.531927241876955,7.295631200395783,5.566206293465506,3.9990980337056072,6.145066014185894,3.0373822220030804,6.666614587352882,2.655351828612554,5.343763122149601,6.696689389537669,4.609991295212678,2.3950627995175777,7.005961325845812,2.2957230245399685,3.7548875021634687,6.883987308105472,3.2433644256936605],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# régression elasticnet\n","# réglage des paramètre pour la gridsearch\n","alphasEN_log = np.logspace(-3, 1, 30)\n","l1ratioEN_log = np.linspace(0, 1, 6)\n","param_gridEN_log = {\n","    'elasticnet__alpha': alphasEN_log,\n","    'elasticnet__l1_ratio': l1ratioEN_log\n","}\n","\n","GridEN_log, \\\n","BestParametresEN_log, \\\n","ScoresEN_log, \\\n","TotalGHGEmissions_pred_logEN, \\\n","figEN_log = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log,\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_logEN',\n","                         score=score,\n","                         param_grid=param_gridEN_log)\n","\n","print(BestParametresEN_log)\n","print(ScoresEN_log)\n","figEN_log.show()\n"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[2.092257977981455,2.0919688163666352,2.091571946993702,2.0910319304685205,2.0903025759802003,2.089322106375019,2.0880095978204065,2.0862633221840743,2.083958953579587,2.0809471527226107,2.077051300467726,2.0720651738006066,2.0657509204354008,2.0578406540036744,2.0480493297793685,2.0361096490127455,2.0218376317135673,2.0052276370340905,1.9865595233942244,1.9664851637618679,1.9460570019568386,1.9266741881824747,1.9099452312074754,1.897482535788044,1.8906465470397993,1.8902657393244418,1.896397805615838,1.9082415589591384,1.9242836763073208,1.9426488571322236]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[2.6379298004295357,2.637146334781211,2.636070849483997,2.634602156368249,2.6326082028318982,2.6299144868051743,2.626293326800713,2.62145476095363,2.615036996916483,2.606596165165777,2.5955956612993853,2.5813945428080665,2.563236696505285,2.54025000748557,2.5114763932275137,2.475962725657729,2.432938513961874,2.382080491840593,2.323819356472445,2.259598069402709,2.1919743387222583,2.124493146539291,2.0613248237534867,2.006727417043365,1.9644248020406705,1.9370247250575339,1.9256083041818448,1.929084628329845,1.9428321538645172,1.960919768947534]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"y":[1.5465861555333742,1.5467912979520593,1.5470730445034073,1.547461704568792,1.5479969491285024,1.5487297259448642,1.5497258688400999,1.5510718834145187,1.5528809102426906,1.5552981402794441,1.5585069396360665,1.5627358047931466,1.5682651443655167,1.575431300521779,1.584622266331223,1.596256572367762,1.6107367494652607,1.628374782227588,1.6492996903160033,1.6733722581210269,1.700139665191419,1.7288552298256583,1.758565638661464,1.7882376545327232,1.816868292038928,1.8435067535913496,1.867187307049831,1.8873984895884317,1.9057351987501243,1.9243779453169132]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.8351785148098618,1.8351930203842004,1.8352128897711468,1.8352403996778832,1.8352786281044364,1.8353315927512037,1.8354046429957218,1.8355051508147846,1.8356433094032287,1.835833034757787,1.836093118173379,1.8364486485320768,1.8369326628444211,1.8375879795965009,1.8384691750906292,1.839644717872676,1.8411994205113305,1.8432375881027312,1.8458873801150055,1.849306705343971,1.8536903489199636,1.8592768029832938,1.8663499954574247,1.8752240994162157,1.886193533929036,1.8994390550378815,1.9149113442126173,1.9322454709163999,1.9507603542600098,1.96956018863643],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.7898406381871481,1.7898362604253342,1.7898314611934147,1.789824346357398,1.7898124988685162,1.7897943296384629,1.7897698198250296,1.7897399531362512,1.7897064842829449,1.7896731111756616,1.7896472698081152,1.789642242677632,1.7896798047600488,1.7897936089021098,1.7900335837266965,1.7904715770590791,1.7912080150398675,1.7923784765422717,1.7941583411242235,1.7967638244614748,1.8004489909838388,1.8054995936151061,1.81222311610918,1.8209279327667838,1.8318770789533552,1.8452066860404384,1.8608241828992282,1.8783318372343247,1.8970269879794595,1.9159997098398185],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[3.1824649444308153,3.181191281317004,3.179442847967668,3.177052812300551,3.1738033902729574,3.169407737418771,3.163492062730056,3.1555787940339113,3.145069006463229,3.131224231506592,3.113147293303224,3.089761303245994,3.0597899321707374,3.021754061323919,2.9740187849760638,2.9149399880834985,2.8431536201214866,2.758009246166608,2.6600756211873295,2.5515693951619087,2.4365281807838697,2.32060150741942,2.210446513467165,2.1128190613082136,2.033498158761808,1.9762130740643866,1.9418380075110808,1.9282184387092944,1.9308525882856948,1.9442058315480732],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.7963683029957342,1.7963762972219461,1.7963873898536644,1.7964024003297774,1.7964225226919035,1.7964497023352721,1.796486889992218,1.796538163907053,1.7966090776612345,1.7967073600310153,1.7968438874549946,1.797034016873763,1.7972994412258911,1.7976707173516708,1.7981905961719538,1.7989182655056726,1.7999346637328788,1.8013492240423452,1.8033086777078597,1.8060083727757172,1.8097050596052877,1.8147263792649515,1.8214660221696002,1.8303468269740126,1.841735480683675,1.8558136981085052,1.8724480106389292,1.8911234838297355,1.9109877287631558,1.9310002033777214],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.001,0.0013738237958832624,0.0018873918221350976,0.002592943797404667,0.003562247890262444,0.004893900918477494,0.006723357536499335,0.009236708571873866,0.01268961003167922,0.017433288221999882,0.02395026619987486,0.03290344562312668,0.04520353656360243,0.06210169418915616,0.08531678524172806,0.11721022975334805,0.16102620275609392,0.2212216291070448,0.3039195382313198,0.41753189365604004,0.5736152510448681,0.7880462815669912,1.0826367338740541,1.4873521072935119,2.0433597178569416,2.8072162039411754,3.856620421163472,5.298316906283707,7.278953843983146,10],"xaxis":"x","y":[1.8574374894837167,1.857247222484692,1.8569851461826186,1.8566396936769938,1.8561958399631884,1.8556271697313853,1.854894573559007,1.8539545490283713,1.8527668900872987,1.851298026141997,1.8495249335989155,1.8474396576735674,1.8450527611759044,1.8423969028441727,1.8395345089315,1.8365736965428021,1.8336924391622738,1.8311636503164963,1.829367596836704,1.828777521066266,1.8299124294912332,1.8332666576296017,1.8392405088340056,1.8480947584749956,1.8599284828711222,1.8746561833709963,1.8919674828173334,1.9112885641059383,1.9317907222482837,1.952478352259074],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre<br>elasticnet__l1_ratio=0.0<br>en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","FigRMSEGRidEN_log = visuRMSEGrid(ElasticNet(), 'EN', alphasEN_log, 'alpha',\n","                                 GridEN_log, BestParametresEN_log,\n","                                 'elasticnet__l1_ratio')\n","FigRMSEGRidEN_log.show()\n","if write_data is True:\n","    FigRMSEGRidEN_log.write_image('./Figures/EmissionsGraphRMSEEN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.5 Modèle kNeighborsRegressor"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor()\n","0  kneighborsregressor__n_neighbors                     29\n","      KNeighborsRegressor()\n","R²                 0.438742\n","RMSE               1.560553\n","MAE                1.260167\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_logkNN=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.319788317567966,3.905416432847158,3.876877357642689,3.662335890372819,5.883546818688831,4.519239483001833,4.1009237726573495,4.885010658374142,5.784082952589399,3.092720697739841,5.388289709198326,4.340862222550784,4.906222611353024,5.09133918481004,4.184224627579118,5.483240887433034,3.9772520107853406,4.471713749087232,5.114014541274238,5.388719067171259,4.2610658661459855,4.744251625967528,4.335163137539703,4.4809023348841714,5.690636307762599,4.887689834705625,6.795686871749732,4.903567205280725,4.458171545571661,4.395268457447839,6.212795398843246,4.815071355674527,5.4331222856543775,5.696142928648658,4.803874995592301,4.830140196742362,8.131746705451553,4.280243283865311,5.215286446610997,3.7854383462983083,6.095248449761195,3.671888404140837,4.1947805999426215,5.950999759474032,5.212211411246866,5.089295251309117,5.483709359950318,5.321475750764041,4.237944442261181,4.57074759716525,3.4258222078141745,3.2750317572269356,8.913032836020559,5.562556085004288,4.461814951715921,5.240225192237957,7.327868873049777,4.0385813255238014,6.259828634045357,4.9811896969920895,6.34849840679773,6.696191770122426,4.134577532443071,6.5155981525004885,6.155672121745788,3.3455275459771134,5.877965393802021,5.646742797317509,3.3896372183885233,4.622368694733,3.7417293488606336,5.895083237531262,3.8301931473623965,3.7996306383478324,5.3484981362652935,5.997204784941494,7.489243164500848,3.982347309416364,5.028273357851902,4.041053898505135,4.981923994747749,6.474755903388306,6.679592439301381,7.0012598734538365,3.019216647225529,2.87483222952047,4.680851055222215,5.029706598826658,6.310292432517416,7.646185778770315,5.011010214813172,4.791158029196132,6.02623244452001,3.844170040917977,4.791158029196132,5.345220572349974,10.085746504301039,6.635186595059107,5.0835096171778,5.00382866918575,6.043159588889128,4.487348672403309,5.016321974641196,4.337061616108997,5.459427004277703,4.754955849458214,4.605599874744908,7.377345952250041,4.538969181353923,5.770184920355532,5.493818081163031,3.480970514609616,4.261770684105032,3.5854135537039427,4.850520158613734,4.643279347529621,3.831457270751718,8.412465969819504,5.926460534489706,5.48177518953616,3.3253038668231407,4.332400973126467,3.8937145834539084,7.299885616967701,5.370678002683496,5.125397067185976,3.4502084867931053,4.812778773497488,3.392611511880116,5.750545846794797,7.3836727245673694,3.4988532775264014,4.151632127810362,4.986523129247766,6.7658895934528225,4.3146538482370635,4.873975501921171,5.147039645677629,5.641654794778352,4.265345883259145,2.9419743994834926,7.180165668265011,6.015962670446703,5.466392886761763,5.284875733963785,4.664452442853703,3.316954777889498,4.687839067143453,4.160996938590666,3.6949490557356675,5.919498203992583,5.620433835204342,4.2610658661459855,4.8816003271750565,4.843605315196681,4.595328302952727,5.141520144260085,3.936920776798099,7.550373597218304,5.455601840606146,2.76396590686402,3.386679292092212,3.694949055735668,4.502697555676916,5.19602724875307,3.7773877830369194,7.71040112117272,6.644899159555472,4.667448273933359,5.135390459719924,5.025311536609159,3.6165718788441876,4.692371050238521,4.518436822711466,3.8566128099825514,6.151362446309842,3.7609512314524705,5.926460534489706,3.7929244349649354,2.695734132841369,5.388483754005623,7.152133792827706,4.517911821536969,4.597268643135578,3.638605208916672,6.351269483007494,2.705367237023447,5.06593737427845,3.441204678690819,5.623719738328816,4.416986370226378,5.137594967886685,4.0002899731294,6.399022872397201,5.364250104990189,5.232811268557224,4.2275303439998435,5.13829904227313,4.94557858634683,5.537355689927881,6.77641597438149,3.598921717159954,3.2750317572269356,3.3360821710774116,3.3025788521560897,3.290809403028697,3.5530168179734654,3.667105821616885,4.803093615423925,4.471074349139378,7.755150451536694,4.090109232403341,3.95284656355088,7.1622157244115465,4.832652520483768,8.351744346511994,4.241210113071796,3.1061006322310942,6.344974169926917,3.5441304191695338,5.004681678642116,4.875313572268101,4.201582165573935,4.835663182635237,3.3657882442967537,4.398044361156411,5.5914187058259515,3.7934207525713424,4.214527557401327,4.321997985225817,3.9773431536737904,4.915035545015274,4.340862222550784,5.457291634396162,4.142372006201574,3.447526623226807,4.180198061544927,5.252068183493826,6.742119486483311,5.118251349019467,6.385939026005089,4.3955645479808645,3.602066993039618,3.186281277063488,4.617067045167499,3.6684337425612585,7.334523459747461,4.484171828476788,7.363566796555875,5.219590057048086,4.4408810943617985,4.980060737398547,5.111652042115639,4.6193270572291585,6.2201474265126375,5.138223929842853,4.927814591836194,5.037295223328215,4.948808514703618,4.766665977797945,5.234414336741881,4.632385490622896,5.4416613656296144,4.568480179451414,2.7397289245090373,5.90230055640732,3.74546449102221,5.880973397075401,4.198963239600373,5.088086207037319,4.8427221381451,5.759493833599841,4.790619250835316,4.875804064263227,4.054765210365299,6.11953951265966,5.531624885897636,9.020140012607929,6.4089815755194355,4.537305226750484,4.686170241138966,3.8477610002691356,4.472619094063246,7.0525273706857785,5.09419181096893,5.389487475071301,4.543074463118863,5.371307842385187,4.047572564954764,4.992682834100835,7.669055607279923,4.334365349019591,4.94971816336795,4.071516181677369,4.983825226241407,5.972777117795819,3.476113276076363,4.950063404035824,4.502683732241417,3.075290723772752,5.20162962478469,6.239634077561312,5.898838900967733,4.4130870525012655,7.731438805041554,6.372368448504786,6.752951565353002,3.215247202002325,4.897900944653106,4.7431824264011215,3.5654223156808382,4.519172631028096,3.576953659095153,6.412360646854541,4.205465644305085,4.536010611591813,5.151678623243577,4.990308823575602,3.505653527224128,5.727660754309455,5.667722514485448,7.302843336547523,4.271781827962153,4.259425466586954,5.329352723482762,4.173199902701568,5.076169730474993,4.680319667948707,8.958282731699795,3.830669839695662,4.645122701127839,4.5406029798969145,4.509092908267362,6.461138467079018,5.808598287733297,5.101188134425157,7.170447003395469,7.011938417045933,4.342402824217271,3.3844955268978634,3.3753297632561394,3.251616618750237,3.8648115563536347,3.7026271507130084,6.388284849373444,6.27937154537283,5.207224112588902,4.684328893408245,4.1199213963806995,3.5427703083546787,3.965733981660001,4.2330266499996565,3.3208443212687118,5.0912757589689095,6.25209988435403,7.964667251525999,6.521787377392674,7.534140013504588,3.4446232502816567,6.125878905677817,4.925629554626035,3.4372205096222506,3.949467342318571,5.901340391576027,4.161530364677803,4.108911565281572,6.21629874992241,5.42687114331426,6.40822539325774,5.758913277541289,5.4331885034212455,4.893915220385525,5.977002727270486,6.341576709511726,7.202397146263255,4.664837570257674,5.510202716746361,5.476340751793774,6.332004948706281,5.004267378547468,3.6071547288214654,4.401550656783991,7.496499169152667,5.190646021366256,3.5631602997126697,6.6592460618787035,5.083053724890835,4.624733433571179,5.663631457230062,4.694791435069208,5.944549283673827,4.063284648453392,4.800902069147973,3.702396853758724,3.9239554043725904,5.003828669185752,4.5172545276296585,3.4540281459658124,3.447526623226807,3.9806425280845454,5.369300499598963,5.802820869489747,3.0228589331566225,3.9647043690982473,3.9033801352231214,4.886935333376118,4.347320928981585,5.42231187471297,3.916544536407698,4.700710483836191,3.2050364864381065,4.764037753616493,4.8398034403268735,5.949400148746667,4.961590519842205,3.7784409579497806,3.576607740583537,5.230999618476829,6.8760529442655685,6.444381899503276,4.5738816310813,3.5654223156808382,4.071747063571306,5.32584176990624,5.906314664272153,4.95867859700269,9.61365123255916,4.2789961904795595,5.998177088489064,4.899902270393022,4.913585139101018,6.681800549468637,5.071993687143434,9.128284370071944,7.726732250693329,4.065194310956586,7.753773001218652,3.9134562644187256,5.093581843091162,4.206090132044845,3.7253186319209934,3.3183155341213095,3.3439478092736423,2.968856558212737,7.576120187107787,4.487758503329223,4.840416792491567,3.679732298960404,4.903898063128877,5.4455500388715485,5.309291024708739,4.26557145076697,3.839349852382332,5.57721529785714,5.288968193216366,5.0566859321439495,3.39218168168219,6.3654700918507325,3.7271541775942096,2.739728924509037,4.415848631251966,3.667326266386283,6.080934931224386,4.89088520557909,6.965602948951287,4.6672063382682305,4.2330266499996565,4.534799563272599,4.614348421436271,5.053904491043083,3.506709885435413,6.222660420763949,8.28927952741788,7.410438715898635,3.019846173565005,3.9846716769216473,6.237209654225099,5.369625839403951,4.3809052928729475,4.786063949306792,4.807156524838432,5.168030313727163,4.774566890460792,3.929910606043679,4.888549980779661,3.973224967022936,5.695051711754833,4.529215980696995,7.441825919272947,4.3535769402617674,3.459847499713247,4.447203868499512,4.533342590492951,3.9153254197435383,4.735178060459267,7.384413812718898,5.160361722358324,5.436609009154356,7.860069898267994,6.2155047424034935,5.60761035497931,6.1612106192444,5.1336560099153035,5.263565390254806,6.598804479244125,6.063492401363839,4.814270823559168,7.476227222603216,5.103199643993075,4.302984733853128,3.2934261088994683,6.717970961721882,5.0855904699948375,4.516205329635789,5.699057786604784,3.5721697247625417,2.739728924509037,8.301519767547658,3.914617751599181,4.9963655944262335,6.827700140461187,7.528078391913906,4.534090266760702,3.7522080684933323,4.310516954599102,2.76396590686402,4.845479016737673,4.812587054365567,3.4377938441850415,4.411313188452808,5.4135300588656525,3.6550217638183153,4.1066075790173855,3.425822207814175,4.691886320368535,3.5104878261823247,4.801142165669787,3.4226304566921915,5.092700814288614,3.3168231528422423,4.994958685782221,4.616109817489174,3.6698291002228514,4.905514704617319,3.646739302361856,4.565365613294724,5.234692213216232,4.930931982867611,4.2218717026741786,7.27581499151256,6.2448534893116925,4.56566765167719,5.596971144433943,5.4168865800555555,3.9417212984081313,3.773834024600262,4.200123458852736,5.717328825580717,4.265345883259145,5.565336398249678,4.566629893983821,5.1267053065859445,5.36805297419623,5.161569987440384,6.742361513038305,5.580517666846207,5.446588458637631,7.94032856344226,4.9560032391192514,3.8760341172672823,3.669470972674688,7.866976687581843,3.144559821195194,3.4226304566921915,7.734105875876701,4.562221599343351,4.400162992867581,3.217207792661714,4.988542699065968,4.855442655298908,5.688759717241061,4.123550251391693,3.7293679822089016,4.537276535722911,3.7800799569605057,5.62699061302724,5.013276668525078,3.4633027596271737,5.6190527958297665,3.394283213703513,5.273272464362101,6.911939932380076,4.609982194048313,7.8176609289620504,4.969183422252273,4.809938971979358,7.09018169771711,4.225442878890209,5.278075984810878,4.708273769142289,5.79170364084542,3.2888481551206645,6.404491153281889,7.422316541313706,4.14308681349422,7.911718463992278,4.050678191749564,7.212431967472546,3.908310443331122,4.991814399394933,5.206834438259822,2.7397289245090373,5.085688330683796,5.445120251536228,8.268932682263344,6.209804898442939,3.305982102479326,5.777200315441819,3.9139665953784686,5.156657593637261,6.003023736275217,3.0786824686488448,4.755617486825394,4.933871227772688,4.385609238618186,3.389654250326199,4.969931192370153,3.7026271507130084,4.064929074906746,4.101739541778994,6.794246938878278,4.151632127810362,5.020631992442954,5.645139885872181,3.9645231036370294,4.256938907866099,7.275814991512559,4.73047865290541,5.928411070147998,4.5466653507181345,7.42806508186098,8.681883929798436,3.699740691591362,3.4530899998082565,3.6705083221446,4.806781665455366,4.458466456467959,4.758673301319773,4.496209125899494,3.7405315784543514,5.1954680395731225,7.509451841348617,5.5541931303771825,4.132281048972151,4.167117472986218,5.3350624588178395,4.8521202004195345,3.8295690059953973,6.439048492405884,3.980014249548465,4.2131651604715215,4.463193984507342,4.8300643538848345,4.616204227014912,5.311771694826676,4.771147318973774,3.4571278936142167,5.0383416310073,3.5383507656451014,3.981268759234968,3.565881858196637,4.3241239419353725,5.082597559728593,4.953078901630296,4.26210194400946,6.9165740148954304,8.62973027989813,4.717600320652771,5.324022783181097,4.18539062594442,4.161530364677803,4.674857772463958,3.589037920221377,5.187885948947585,5.152379139384103,4.009404265790231,7.306442770750262,5.58604123390245,5.340579073247027,6.027488277891404,3.5365353526963843,6.0524096188909375,4.308603879366553,4.92890349313437,6.965399673107335,6.000066893558729,7.7869121914517425,6.746300598072649,3.719095236275133,3.7049325509721047,4.6297511443497585,5.743017407891665,3.726799450231858,4.269150130955291,3.7537372277463468,4.479611509830366,4.472649090038603,3.872610523947085,3.713370346866004,5.329105453179785,4.310131866967815,9.849604175994335,4.471986545591495,8.583584305229067,5.578498634519437,3.513012546098908,3.6793344790174807,4.141585491769813,3.8937145834539084,7.441291858143973,7.453193801012778,6.95349318063542,4.211292405939301,4.298940913211901,5.381532307133522,5.252730708006178,2.5979020588074793,5.094080031510313,3.2345421568871227,8.299517433407424,4.614827174325878,3.214907787381265,7.809828180015532,5.044948848713282,5.193366301377051,6.267418602093405,7.860910108119915,2.76396590686402,5.109468917208344,5.511797644085166,4.180268020988553,5.170109697370076,3.7390542086204035,6.143665423255072,5.490463161027475,5.991354053680463,6.651623250340659,4.120758378452028,3.462969889594671,6.018735897144986,6.181981337550307,7.819683305885984,3.3679605888931237,8.52073567697011,3.965446690899557,5.029706598826658,4.5083745063617595,3.2447405510717604,5.319569652841211,5.974472037492328,3.2172077926617146,6.620044426599381,3.665282919361064,3.098533145519216,3.2049568728641296,4.499086320871943,4.889387993981915,6.3629494589918325,6.063065311280034,4.4927766430219584,5.4194523797609895,3.4258222078141745,4.152210215057079,5.34277233378597,4.589493163729831,3.756194652410121,2.94876063077431,4.966532947683507,7.029191417992233,8.623625029846602,5.597942529835707,3.942373663815829,3.366233291494569,4.534090266760702,5.125621030946036,5.074362995078728,5.824822037392858,4.3866416629110425,2.7639659068640197,4.216347303798087,6.072056633401662,5.365337862117322,4.842411357704732,4.812587054365567,7.591496212237212,4.468055888799135,4.441490259518377,6.030685684641611,9.599687929363744,4.582274123626882,5.0440901417647295,2.8913056590562185,5.910856830649521,4.4904528311551735,3.8121272045553694,5.225946312613837,3.719806266316218,4.100923772657349,4.429915028124287,4.944755478087377,4.656595085321891,7.406254448101324,3.4107797006741327,4.466384172517543,3.1198428588786338,5.665572572467513,3.4649778956009056,7.099477392893308,4.3520357374722485,4.358883295517903,5.138223929842851,8.560849036815132,2.8936472839479457,6.4055983758229935,3.2371926394503854,5.147171745066607,3.1180795119630655,6.970679034077193,5.493534523837354,4.469888800736663,6.025619946945829,7.6351090572464395,5.295171136478964,4.199201867850402,5.2056488719509595,4.100923772657349,3.948255531594614,3.7126475321835986,4.231056986477413,8.713151323660524,4.121324372238654,4.0787586027952205,5.591227583694467,5.602854470329775,7.088389160886855,3.5839824253059063,4.778283958080114,3.931077325748471,6.852030433604843,4.41374732430546,3.968649156592235,3.9546266075511523,6.9249160367439355,3.913456264418725,5.719951701858559,4.64229502678948,3.5173639591238945,5.103501597643591,7.531840955871229,2.8647245786687923,4.780458990644825,3.918059408355062,6.796108477696999,5.181100988685683,3.667105821616885,3.0020818362061368,7.865628949800037,5.777151574014187,4.205509351625443,5.598570657095706,3.7784379302042517,5.1042580529288655,5.5262848951136165,7.015442609222908,5.121763796728851,4.951499976225206,3.980642528084545,3.213036083086374,3.7038388552700856,4.700029767401376,6.21981923571771,6.392672792989874,4.778163497534118,4.577866219850941,3.1382303748333924,8.065771994471167,4.141585491769813,4.522873613784365,3.8514055888501666,4.3948030739837325,4.2666956474219795,4.7292643493458515,3.506709885435413,3.739881856445561,6.695146275968959,4.484746712723395,3.3423061818054327,5.938592109137422,4.4315155568529185,3.99002588008138,6.535307455085649,4.162136286297928,2.8958842652118255,4.805143470853333,7.965564361267311,5.431673693949883,5.973137154612556,4.407858318631248,4.3809052928729475,5.991814404322715,4.9587477426921485,4.7731551424769005,3.807697650093212,9.649070517822675,4.754779897116387,5.1688760205207585,5.25733521527861,3.9702540937304738,4.084223417348017,7.8823282421895815,7.619258487469394,4.976816896682427,5.0643204332469285,8.421298089326843,4.189489781897902,7.878920718636352,5.46490342835946,6.558882448710637,3.9356107635446422,7.9141270169079325,2.6546315804467118,5.372718716301042,4.071516181677369,4.15051863240733,4.239423045689072,5.028507893070609,4.948018554854628,8.725953937650177,3.534903899970099,6.370130826688125,4.419099661668953,5.708890648119546,3.9411304001531366,2.8730553605793685,6.372981844460306,7.028699755149071,5.29625096369947,4.8782088232635,4.0800313360328895,5.605962629602934,4.958678597002689,5.215192596323078,4.161530364677802,5.334606513582653,5.3391141491205865,7.127154434220834,9.793707515064018,3.8211705440489294,3.735471285647115,4.039109615067909,8.201849439163844,4.200520304468884,2.739728924509037,5.598570657095706,7.6563050427572685,5.761424953766411,4.152210215057078,2.629770166989586,7.3225245392301845,4.702456800176572,4.8545469337548415,3.224089052716171,7.9334442311134765,4.279220276215266,4.59121331924333,4.076826129445589,5.058633719059006,3.48254949259788,4.978036386075368,3.019216647225529,5.676693242753404,4.057989683138273,5.523604481012228,4.146515576569118,5.563920726142563,4.509092908267362,4.843613991085145,5.850563953289026,3.326821391824066,3.424829579534708,6.764866520018437,4.703682819623816,4.801614527738718,7.194959053809989,4.580784724327026,7.80876455934732,3.7561946524101213,4.728390039860783,4.539361486122933,4.322882237658643,6.15506608718454,5.650785691133391,5.825018175916785,3.8695407680917735,7.888457712260781,3.8881741336526714,4.731042481586498,4.867109990249312,3.6944066761380756,5.984097926036594,4.100923772657349,5.041593045171596,5.537683349451329,3.4968814694119534,4.737233652849654,4.095346835730744,2.7633203891119518,3.4258222078141745,4.200511629891596,3.908324750025034,7.01128200742529,4.383944148447217,5.810746866724553,3.9718023856784503,5.350339172068054,5.913643985597128,3.5509912099383563,3.699503734771152,3.905416432847158,5.862582719252037,4.9301062737346175,4.181478605808358,7.075012846521067,6.34557109341963,8.01368821445246,6.028128205383307,7.549253011578988,3.399500118448602,5.103735495269688,4.579770364353771,5.124519273530463,4.537669672262188,2.7122327474058747,5.425933408446268,4.045063608796452,5.171350117434704,5.258147816008714,6.966859977307236,4.851343548951636,3.820650437518821,6.033990715011036,5.316348141230785,4.092316991425188,3.630893151783156,6.607620462318875,3.1394831385362068,4.788093024347446,5.074365971374191,5.860295467545661,5.1190749936792415,4.9656896432009585,5.620311918554133,5.684485692145404,5.58155411473369,4.717410775970707,4.777615192676868,2.856148675690159,4.448498780292208,3.030715345850317,5.905700317441133,4.64536163401994,5.141895718298971,4.64229502678948,6.462808618571301,4.629751144349759,3.989079336524033,4.743997950784401,3.3686503044642815,5.029706598826658,8.043670460823954,3.8937145834539084,4.885026030595591,9.692124250779013,4.974509758755724,6.391633298371682,4.699363143607271,6.661743276372193,9.221231156303407,5.279789643347662,4.045063608796452,4.954447725163696,4.675648580344483,3.99002588008138,4.383876178229114,6.095248449761195,7.385046311753246,4.535126060544645,5.817740366795198,2.946610837935749,7.618518652168844,6.168043477640686,3.71884057632683,2.705367237023447,4.101837105179348,4.350114223891045,7.3882605851119365,7.294000522575475,4.514630415877459,6.282570757123406,4.884995468534353,7.2282136723841575,3.3896542503261986,7.800089122291312,4.137234824276161,4.0540897154353726,6.670212842994542,3.9870472835503774,5.1003929033670445,4.28000745757404,4.270925858811163,3.9264339845558407,3.793344613859148,4.936789497691623,4.220579440101954,8.1622117836714,6.150859734109297,5.103735495269688,3.738724345955604,5.667889170095864,6.419910219183191,4.47851581060179,5.323496231190714,4.530965284829305,4.504756630609021,4.280504162005852,4.893211368693128,6.237209654225099,3.4866048319527594,4.599409854910816,4.523556087000329,5.0861298956035395,3.657402314672255,2.8802143569696144,6.9315397810576425,5.680661576116751,4.939133535191334,6.237546915290257,4.236568638253996,7.066223812906004,4.9791253933947335,3.838581309937234,7.293876412820999,3.69268890801471,4.235938570452967,5.508122049023151,5.348293207835488,5.384488657677379,4.774273467665724,3.670239264269837,4.884113800804698,9.849604175994335,4.258481559247089,5.157638394961754,3.8937145834539084,3.3683863997473744,6.224874021326166,8.774294170734596,3.2183544564161934,3.7001125559651755,4.835009726995789,5.212369540430503,4.730424491885838,4.620566245383694,6.664262828102363,3.5521742501207134,5.507033761999227,5.875424857956463,4.965271728168068,3.366233291494568,6.152401743141345,5.055224110183781,2.949833753608076,4.82364533485405,4.441744665313306,4.9587477426921485,5.199528286659415,4.039975143417275,5.241266627949329,4.967391293411682,4.590892348834046,3.409914930862548,3.5276888308280188,3.4226304566921915,4.2623601010179,5.460573627403807,6.011192040337636,7.373510510315554,4.643105621530531,3.8742653517617938,3.9385508218060052,4.321997985225817,4.928179195268192,5.88801950688263,3.0153711952231754,4.970247603574005,3.6749909792841504,5.962082482918543,4.580861597776883,5.149123485660274,3.7800799569605057,5.126655445417906,6.571039908609864,4.267744556526927,8.004828193796513,4.0787586027952205,5.156327178563779,5.8803423449080325,3.441961789161059,3.6480650229857092,3.8000651886869194,4.639572059301344,3.4258222078141745,5.149595707238777,6.351269483007494,5.6266071840691385,4.419092908608167,5.110603081196561,4.556212410060727,5.238706420437426,4.17368457145941,5.451923303572129,4.3766200353583695,5.415339385974344,4.517885394010385,3.7569100773713595,2.9824337284967894,6.475851762634955,3.7497908356609106,5.296417812859338,4.244305592895034,5.832716943885451,5.933310645224329,4.9036674179631214,4.290084043686603,4.1353148996113145,3.9083104433311227,6.165878496214605,6.235767788531102,7.919479255919394,6.339857942473396,4.8305771837268505,4.930276761330883,6.109640509041937,4.337253808896417,6.179212781602811,3.9483749773994816,3.4502084867931044,5.662573548179286,6.624816630639935,4.1275623867071225,7.025892487750158,9.655001020671813,5.43011526872457,3.8192152827052883,3.5662160148426203,2.999712948676772,4.9419216214099,3.8652026387896448,3.7395541017536034,4.153272655634091,3.946197988702813,6.487331133098773,5.181845751778979,4.986523129247766,2.915551780302607,6.852297504806087,7.584222234145706,3.1002249151143038,5.8793585806163655,5.751372727815277,7.835838706301457,6.258000779570387,3.368619679682099,4.1984093119849994,8.95562291387533,4.446776456324991,4.314502589403731,5.931164820569535,4.043227467822889,4.946488949465121,5.873015050606288,4.008117537834433,5.900164022307556,5.84812194111852,5.323784635881546,4.563779852169009,4.911790868058738,4.751480793806406,6.7931691665388065,3.1390040198386213,5.032262821820385,3.4489014424594693,6.304573812057997,3.320844321268712,6.083561821962587,2.7878820108176914,3.9175061244193548,5.924921022089512,3.9552083409697656],"xaxis":"x","y":[4.9676297534661975,5.60584986719498,2.950468414150123,2.7355221772965375,7.258707352196828,3.0925457415435655,4.178714641175443,3.4154882710497003,6.224387627929912,2.634593268445757,7.244982170376553,5.597531174224212,4.210232990095849,3.594548549550354,2.526068811667588,6.215484464192794,4.969933274697856,5.912649864897204,5.111865963867557,5.042644337408494,7.864928972289788,3.382667252745041,3.189033824390017,5.705977901682522,3.9717734471933728,5.119771161952904,3.496973580998276,5.058749412335524,5.968551603363945,3.1160319934471103,4.464668267003444,5.192194165283345,6.73226919950145,5.39746072605472,8.171076693915895,6.43095427138573,7.132782457298859,4.294988267691446,4.708739041359579,3.4646682670034443,5.450881315273369,4.95093492831454,4.20006486151431,8.047505671251574,3.605257262939004,3.261530815243406,9.159820892013935,3.8728287595348854,3.8298495598446904,1.9671686075326276,2.9126498648972037,5.648177795724818,10.257022407964342,4.887037791419197,5.718087583960517,7.540941686758214,7.156841524588401,2.0942360698457656,6.074462620704536,6.428778891337586,6.464504909131913,6.588864518190148,5.629648044187436,4.795974694206668,5.676380255466924,2.811471030529836,3.9882302230604574,8.45314714555533,2.321928094887362,3.980939266085512,2.5801454844233804,4.41886457739032,2.8972404255747994,4.341985747228616,6.561784353081841,5.751410160064371,8.545157551729474,2.7676547982373463,3.773996325111173,2.7676547982373463,3.258518924711301,5.596637034102074,7.019368330478469,6.649471757181635,5.275007047499869,2.0071955014042038,3.700439718141092,5.320845667645722,5.331633567171029,8.061668207389335,2.786596361890807,4.94673086014031,5.649040565634448,2.269033146455237,3.292781749227846,3.683696454306516,13.390111171989,8.064904402198286,3.727920454563199,2.8155754288625725,4.253232938538032,3.0373822220030804,6.01992426090841,3.2524762141352173,7.125981653854716,4.273515889702116,4.92552476974757,7.778274295338579,6.135452784185173,9.20006486151431,3.6971065744769747,3.2418401835646704,2.4222330006830477,2.6229303509201767,5.57198012018773,5.0539801681876515,2.6915341649192004,7.510803720358323,3.01435529297707,6.045049740837989,2.7803100990433753,4.658211482751795,4.595145567990858,7.485507332459785,5.342341397431548,4.129283016944966,2.454175893185802,5.96254902292306,2.3305584000308026,3.292781749227846,6.290756138396673,2.817623257511431,5.532940288372874,3.2883585621936606,8.008820757716158,1.6644828403646825,5.284292026394312,3.598126959919604,6.237448995639321,4.222650022451478,4.380590934265922,8.669593751188334,7.720552091605108,5.548744859937229,1.7484612330040354,4.524815928357506,7.639376888638463,2.851998837112446,3.218781167784069,2.7990873060740036,3.868884273028774,8.043027283594547,7.260778431893426,4.038260575175349,3.040892430646901,2.7949356628035362,5.4741115139424075,5.952799477899938,7.323370069061268,5.040892430646901,3.4208865749755315,6.136683577697236,4.255500733148386,3.228049047884462,4.6194130105979365,5.623515741490549,7.217424613813984,6.179311989210016,4.3139713710590195,5.764473550992666,4.221103725367875,2.4594316186372973,6.548282482376233,5.9902741833186015,2.5360529002402097,4.720825666089835,6.512700963425246,6.664198369291911,2.881664619320345,1.9634741239748859,6.013685569928653,6.026578770127714,4.532940288372874,7.597158683183727,2.711494906650088,6.791683858152045,2.1009776477248208,6.090641751917847,5.040892430646901,6.327328082629952,3.2942531364445142,2.9467308601403097,3.7366048753142485,4.922673592849446,3.71699089440494,4.15623479785027,6.872951851130105,2.8933622107638715,7.561631630375841,5.341985747228616,4.993221467368938,6.309067020588001,2.414135532984451,2.493134922305505,4.161081482277184,2.144046369616707,3.486714373030702,2.8011586560936985,5.648465443027314,4.399171093819823,7.581803148516525,3.1160319934471103,2.533563348214512,9.967312731471658,5.687060688339892,6.954079959320971,7.084170591577843,2.722466024471091,4.826802684285827,3.01435529297707,1.6040713236688608,5.5109619192773796,2.1731274334806563,5.425593577653059,2.8797057662822882,2.7803100990433753,3.303050084681673,2.3132458517875616,7.006298023900369,5.279842693520348,2.3950627995175777,7.216163813388774,2.786596361890807,3.5298209465286954,2.7420062108667365,5.079378111118652,5.1622906135457995,5.839203788096944,9.354249381945241,6.058532970201611,6.614857050384495,2.1667154449664223,2.4982508675278257,3.3405622690264134,5.415826257472914,3.9791107550157854,5.338780943894699,1.9335726382610239,7.164705840182799,6.12866458654003,2.933572638261024,3.0548484769956197,6.390598905552461,3.2764966656403565,8.007363716298686,6.558114538598396,3.674686619927999,5.28835856219366,4.189033824390017,5.480265122054463,4.2787282129389395,3.776103988073164,4.82273014794452,2.9467308601403097,2.2898344651775093,6.1705259991768475,1.4750848829487828,7.264067216098838,5.6845382485777325,4.534808661231747,3.7070829917717063,4.919340082442012,4.458119481174506,6.288173968112302,2.27500704749987,6.5249725982866265,3.940166750482817,9.715601700373337,7.829278760612363,4.821199978056451,5.875042803002589,2.6229303509201767,2.942983598187102,4.445594291341182,7.474030370207135,5.92148393743373,3.4369613378336026,3.2078928516413328,4.095080491901415,4.422233000683048,7.424922088210688,5.185470146066942,4.955126781261366,4.3729520979118295,4.0522419811811385,6.067165427017112,2.2898344651775093,3.0806576633452254,3.940166750482817,3.127633279725874,7.152994605492435,5.573192723781613,4.4121040446775694,2.2479275134435857,7.886854944198882,7.9329824099498465,7.25691629667084,6.670585440262214,5.845740932614582,2.9616233282869446,2.5459683691052923,7.959132576789657,4.653633311386255,7.406162478777607,5.588264894307497,3.8063240573900288,6.716579420796345,4.984589350362456,2.269033146455237,6.929672661579345,7.060371694122218,6.746447139362815,5.521364878446219,2.9873208659292536,5.970623614470605,2.37851162325373,4.667892125230234,3.192194165283345,6.816215687622865,2.851998837112446,3.3854310371935203,4.634593268445757,2.5558161550616396,8.990671283975267,9.114445113289024,3.389566811762726,6.9532652390148435,6.705563274566321,5.672425341971495,2.280956313831056,2.137503523749935,2.2203299548795554,6.561173365231147,2.9726926540042644,5.078951341394822,5.0759604132434015,3.8509993947164736,8.693835392482516,2.845991770664573,2.8399595874895316,6.111448698487674,0.7484612330040358,4.125155131322289,3.8953026213333066,8.45889870671479,9.650854294025212,8.19120730223037,6.534964249804655,2.5160151470036647,3.0373822220030804,4.701549056943071,2.9541963103868754,4.903520373723376,7.848935855524565,2.0036022366801953,5.660780334032546,8.418822422291017,5.254745197629213,8.4696011200494,6.567880042052731,6.557808298936688,3.6769443591069124,5.651051691178929,7.279842693520348,6.425089989874059,2.811471030529836,5.4799419014771305,5.184280294419382,5.713420884868084,5.452858964713811,2.4195388915137843,4.178714641175443,7.686990680088471,5.3847405872923835,2.722466024471091,7.554281862626391,3.955126781261366,5.192194165283345,9.071542807992602,5.765004246250036,5.318316841334983,3.1953475983222193,5.2671612180222604,2.269033146455237,2.702657543390911,5.6937657122177825,8.574290766120185,3.554588851677637,2.4059923596758366,3.0214797274104517,1.3785116232537298,6.010108453474289,2.438292851579147,3.089159131911238,3.0250287944915226,4.571676809970931,3.24031432933371,3.261530815243406,2.6712933724815757,3.6542063779442917,2.9873208659292536,5.8060662260890314,4.371558862611963,6.4984104488609775,6.229587922740652,2.2957230245399685,5.178714641175443,5.463687842201262,4.884597920990064,6.157852169141737,3.4672794804599825,2.867896463992655,2.611172380044005,5.535430914871279,0,3.4776773275653072,9.582254908357664,3.950468414150123,4.8894735425311096,4.850999394716474,3.300123724569014,8.009660689646706,7.626950122372515,9.04105676230812,6.564835417010355,3.3175935046234715,7.380331184220349,2.0214797274104517,4.235727059838059,4.334139178838237,5.692092375427572,5.592457037268081,1.996388746447621,2.070389327891398,5.8052924556007115,6.60584986719498,4.088311235888661,2.521050736900963,3.5310694927259543,7.058316495590823,4.044394119358453,3.0513721017210256,6.008092420948722,5.605257262939004,8.220281580441913,6.088735246190284,2.137503523749935,6.637204481902295,0.8155754288625726,2.2387868595871168,5.676662334857967,1.7865963618908067,8.05642095591347,7.5673476962842585,7.295355692891673,6.394033895367782,6.582706526780788,4.43095427138573,4.915520900751958,3.710393192170516,3.8845979209900645,5.135042286233651,10.267009017093484,5.336640446408735,3.0992952043377753,6.836050355058069,7.896998342338911,6.558114538598396,5.10936055940423,5.85872702377863,3.785550551739256,3.8042601156347384,2.7484612330040354,2.883620816285671,3.576522137920503,4.104336659814735,5.86195536414487,4.294253136444514,8.44136727845972,5.497931651896279,7.5808238499627985,2.8278190246173196,6.260590274730996,2.229587922740652,6.218781167784069,10.212070748654236,4.400537929583729,6.543805175964397,7.563234413022304,6.203788453308596,6.676803353873854,4.070389327891398,6.340918270272116,2.063502942306158,7.151270288790165,4.228049047884462,4.563768278452033,5.4329594072761065,3.9373443921502322,5.580748491763774,4.279471295644468,7.959132576789657,5.739578112048051,2.7463127664254587,5.707635219545296,2.0250287944915226,3.4776773275653072,7.925940095994448,4.478971805032942,3.24031432933371,6.836807807844575,6.2502036564992975,3.82273014794452,0.918386234446348,3.258518924711301,2.1634987322828794,2.662205499653617,6.687060688339892,2.2357270598380583,5.0259146987507926,5.443606651475615,2.980025300238734,3.1193561770396756,2.8399595874895316,4.914564523493939,2.782408564927373,6.282625134191674,1.2016338611696504,5.269781238274379,3.0513721017210256,4.649615459063409,4.896271848807375,4.721919445551544,3.6182386555954547,5.045268215138534,5.020591094773247,5.305240965954483,5.663913842115978,3.3881895371560837,8.836744701966083,6.976363635732762,3.925049964727359,6.802451741275622,4.664482840364682,2.3589588258323295,6.127014141582809,7.1945598860226125,4.475084882948782,4.301587646603187,3.7441610955704103,4.05484847699562,5.464668267003444,7.33360262826828,5.6082178530214595,7.1320626778543845,4.993674361750585,5.522620761042079,7.366234775533047,7.166815851761945,3.7687136570304847,2.321928094887362,6.7873802222963375,4.788685710613533,6.025250321561575,8.162995472417697,3.4019034716079584,6.238786859587116,2.3248106034204836,7.831687267083084,3.4802651220544627,3.871843648509318,6.548128323585586,3.0268000593437154,3.1953475983222193,6.2600256559614555,5.931210274803932,7.80967167790706,3.169925001442312,3.894332742277694,2.298658315564515,6.215678596607928,7.873013392989595,7.709152756902506,8.461602224618334,4.150559676575382,7.653203362033977,9.24186401223682,5.725468512632615,4.0925457415435655,3.575312330687437,6.519321734629434,2.5185351389821804,6.137913322088407,7.904544309292717,2.568032104771279,9.816679697610459,0.765534746362977,8.646594709581654,5.037821465434967,4.388878338811989,7.9616812016062815,1.7782085763980877,8.912589990852627,3.5008020530571575,7.473786911614368,7.182890892745479,2.6205864104518777,6.628919373511362,4.842476062993939,6.013462259806563,7.1851727751412495,4.8359240742543745,4.918386234446348,4.783980413683795,4.285402218862249,6.340918270272116,3.5570424152637194,4.030336078370959,0.8073549220576041,2.2957230245399685,8.893574570329104,2.3923174227787602,7.015805293913353,8.764407200357251,6.147306698780294,5.502075956045791,8.694706111949401,2.440952198029637,8.447992374957895,4.943921326553485,6.072963271555224,8.18685706717715,5.508111681193465,2.327687364176047,2.6322682154995127,5.853496703702325,2.4724877714627436,3.412781525338476,5.751677945687579,5.8774988727835344,5.890446692679906,8.622783966156124,6.203396948546115,5.524189078449365,4.679198570566922,2.3757345385831563,7.809543066804601,3.5716768099709313,6.39694722007283,3.097610796626422,4.9415758618182375,6.668317719915721,3.0925457415435655,6.941106310946432,7.471431358002323,4.633431210355632,2.454175893185802,5.760487101811031,4.112700132749362,2.560714954474479,2.885574364371426,5.8060662260890314,5.969012307516316,3.0214797274104517,4.37016428054021,3.1811025507537978,9.463033855317493,5.102238193930738,7.537684337147519,5.960001932068081,4.451540833017832,5.931683057059805,4.126807703142035,9.109647857806625,8.028624401408491,3.2645364309990255,7.394977085526471,6.152589069287125,6.022367813028454,4.51853513898218,3.344828496997441,6.810314561497258,3.0036022366801953,7.890811454817677,8.317774372804099,7.237735785231934,8.515305605451724,8.213638902198852,5.154615611326894,2.039138393906958,7.296273846706719,6.905206468795705,2.3673710656485296,2.0942360698457656,2.9145645234939397,2.2172307162206693,5.641256998726767,3.303050084681673,2.62993940943954,7.220233204382192,6.489767697008256,10.809687753488749,5.260778431893426,8.11134436328308,4.9941271140031285,2.7070829917717063,5.856985689782205,6.381629467033664,4.493134922305505,10.642557179595622,5.439955517478156,6.790772037862,2.5058909297299574,4.37920505996061,6.33646192818282,2.9523335663696857,2.811471030529836,5.323730337521348,2.763411574470007,8.089159131911238,6.435461914479276,2.4646682670034443,7.870364719583405,3.396433531250992,7.305696981080096,4.020591094773247,5.187451054027326,2.298658315564515,6.038918989292302,5.369815424283912,4.422905742612183,4.024142345897803,4.926948247949772,5.348728154231077,6.907251224292033,4.861459166361513,6.596487956856693,4.593353770980297,5.9502351004856875,7.814614507639093,6.638797899951696,8.79720757285578,2.176322772640463,7.7974669920591575,2.7398481026993275,3.1538053360790355,3.6701605141266334,2.513490745588118,5.307064162255372,5.076815597050831,2.533563348214512,6.678494507773734,2.7092906357233577,2.7355221772965375,2.782408564927373,7.039796407620025,5.938521045865967,4.733354340613827,6.132988042627442,7.991691898336964,6.419033185471022,2.1858665453113337,5.449891473010944,2.6064422281316078,3.7311832415722,4.332707933640605,6.56498780095989,5.434294617959925,5.893362210763872,10.084529386649065,7.932923373835633,3.0823619695574735,2.440952198029637,3.1842802944193824,6.1430260038899736,6.387500406480984,6.681449265314958,4.324810603420484,2.327687364176047,2.722466024471091,5.569551851083613,7.527555198794536,6.632559052117953,2.9145645234939397,6.135452784185173,5.424922088210688,2.2357270598380583,2.8011586560936985,9.8361608416947,7.193968819421969,3.097610796626422,2.655351828612554,5.553360503335328,3.169925001442312,6.173127433480656,3.817623257511431,5.778208576398088,5.7554217347342425,8.571145863602528,5.470862199413702,3.1602748314085933,8.3389146195993,2.440952198029637,3.982765462583646,2.140778655782796,4.3197624276692475,2.292781749227846,6.864928972289788,5.969933274697856,4.950468414150123,3.4581194811745064,9.054360117597708,2.232660756790275,5.648753032989572,4.827310943949667,6.609104846618959,2.920293300211007,5.726013748860151,4.484138131201669,6.010779838753242,6.51317488460363,8.731081391094504,7.529742876502924,2.7782085763980877,6.546122758857796,4.635173946674924,5.533563348214512,6.2992080183872785,2.657640005207824,11.260084480727762,3.442280035252584,5.339850002884624,6.091065077930535,7.352175878326325,7.78959885005233,3.2418401835646704,4.584962500721156,5.035183996618336,4.898692072560488,4.393690764187454,6.686360452903688,4.792855352362489,4.044394119358453,5.077670274232752,7.061344188288391,4.06522762277562,6.802968651005583,3.412781525338476,7.953207027098,3.5643781685650637,2.790772037862,3.350497247084133,8.870919495610579,4.663913842115978,2.2234225499349374,2.6667565918848033,9.754286253961022,6.500324049284198,7.87670110864523,1.4168397419128291,4.017921907997263,6.177519202597814,5.47800105583793,8.031053274444373,7.831180547126736,6.461724991502711,3.948600847493356,2.319039815562536,4.898208352508718,6.20945336562895,7.562776656791846,5.542258049766918,2.6016965164809576,6.711081862306696,2.0565835283663674,8.630303533275018,4.854494418154875,3.4892860226258766,4.601102203376409,5.932391940068527,6.822602695756541,3.039138393906958,4.397118409042582,4.078951341394822,6.198886990956932,2.298658315564515,7.472731449369055,5.0669502439246275,5.059614856297222,2.9708536543404835,6.608661417987563,5.51443791382931,2.176322772640463,5.642412772905056,6.8820316083521,5.234577959784565,7.163498732282879,3.212569338850806,2.2898344651775093,8.307656205306488,3.423578170981797,1.3840498067951599,2.7676547982373463,10.679004987548526,3.2645364309990255,3.9990980337056072,5.649040565634448,4.793375711264754,4.203201156316611,7.668814088419117,5.911931212309049,3.4581194811745064,5.445263208140443,8.913188619508889,4.936402377725063,8.645802514606062,6.207111961207714,5.749266083814806,4.4376272483189485,8.580786171355323,2.189033824390017,2.341985747228616,4.06522762277562,4.804260115634738,4.849999259466098,5.075532631167357,4.056583528366367,9.188044796421696,3.8439838440483265,4.634012356402116,4.8104431038451745,8.686815644595628,2.521050736900963,4.016139702655253,6.212180210044431,7.545505100691072,3.904002316283692,3.71699089440494,2.1667154449664223,4.381283372503783,6.564835417010355,6.563005553062399,5.911212201558612,6.247547806890298,5.477677327565307,8.292781749227846,13.587327482778312,2.9011082430145123,2.845991770664573,3.7644735509926663,9.182369521950886,5.921959917326167,1.9297909977185974,6.910972451655129,8.149950301681047,4.3729520979118295,5.38404980679516,2.063502942306158,5.977508904540138,1.3673710656485294,4.067810783789621,2.1176950426697547,9.199893148608071,3.7081872360207084,4.628773595201645,2.488000770834068,6.501439145158873,6.706254253573203,3.0908534304511135,5.86616616903904,4.155425431747197,6.060695931687554,4.570462931026041,3.2357270598380583,5.341274183692174,6.656925339794245,5.588564737401351,6.353146825498083,3.48155728070859,2.4698859762744636,4.872828759534886,4.34766565630097,2.877744249949002,6.019479533703628,3.465974464504069,7.536363792395467,5.132576842670009,2.0531113364595623,6.283366212949919,3.699329525673156,8.199279721336625,6.8203066288938246,6.068240861312821,4.598126959919604,7.979739567128753,6.258895755075436,7.258707352196828,5.394376944957055,3.0125686735030555,6.862203399053224,2.8318772411916733,5.164705840182799,6.431288654291593,4.375734538583156,4.8594726668519375,2.9392265777282085,4.302319050948834,4.397118409042582,5.169123281476757,3.761285273361619,7.240600549955795,3.7387678368001915,5.678916986679839,4.3334237337251915,5.530445354692906,8.431288654291592,2.521050736900963,1.2265085298086797,2.2539892662307865,5.23687524536694,2.298658315564515,6.398316163400203,8.348374075191163,6.043300754364182,9.401604869714957,4.534186139090097,9.069933046375542,5.486714373030702,4.96624587322494,6.307246355226617,5.51127826508303,5.970853654340483,5.342341397431548,6.219555769166913,2.319039815562536,7.516566771496865,6.652773284510777,9.109908988534402,5.338067797542616,2.9981955031532523,6.506843098969815,3.04788732939655,2.526068811667588,3.414135532984451,6.295723024539968,2.257010618206024,5.500164679492167,5.333065878076355,5.253232938538032,3.619413010597937,4.695437110405369,4.37364821133469,8.356407951394893,6.039796407620025,5.908332569782425,1.74416109557041,5.666472568842073,5.9586107120558465,2.430285272977781,8.487277313213783,2.927896453728821,6.9920886090139955,5.7615512324444795,5.906409617094016,2.572889668420581,2.9763636357327616,3.311793717753649,5.325530331567558,6.075532631167357,8.785648628652076,5.301953395126096,6.687620632084895,10.60719158640291,5.2543672814408335,6.976821852360685,2.655351828612554,8.033092575523664,11.658760423755194,6.231317197152663,5.008092420948722,7.824449651364088,2.8155754288625725,5.656496370553495,3.218781167784069,6.101187815259738,8.328001662149257,5.576522137920503,7.176621973161577,2.0565835283663674,6.5787877956769725,6.602142090697982,2.7441610955704103,4.727375938594897,2.2172307162206693,4.552131108253784,10.754979337466484,5.651338766105931,7.449148645375437,8.21179097625254,5.769771739249448,7.09876903815488,2.5921580021253603,7.971830914780502,6.234194722953812,2.1984941536390834,6.955824240806441,0.16349873228287956,4.680324356844016,2.5801454844233804,5.583458910130768,4.845991770664573,2.7070829917717063,4.18824265626441,7.27733390042982,9.535217045453418,6.304876050044889,6.982080909763738,3.0925457415435655,7.051372101721025,7.022922589055629,2.9392265777282085,6.653203362033977,8.22838581704935,4.960697039304312,5.564378168565064,2.782408564927373,0,3.9354597478052895,5.525755692829486,1.0976107966264221,7.211790976252541,4.477677327565307,2.4436066514756147,7.966303561412599,6.19021976366016,5.592457037268081,10.665637927637095,3.277984747299765,7.8269297659895765,3.5643781685650637,5.612057559563065,8.766462652425766,2.662205499653617,6.920412408160787,5.290940402403678,7.502871574574138,5.2539892662307865,3.1984941536390834,3.0089887832272546,7.2259304111710545,0,4.919340082442012,5.334496768390418,4.654206377944292,2.327687364176047,6.705839705883682,11.02948049374083,2.4594316186372973,6.294253136444514,7.172927489603836,5.573192723781613,5.5804470195986635,6.341274183692174,6.768713657030485,4.5855634984556914,5.609400390006429,6.10055722078293,6.140165134599453,2.37851162325373,4.726831217032493,3.127633279725874,2.414135532984451,4.561326136453918,2.211012193485512,3.6147098441152083,5.901832305785489,5.903279342056139,3.84197311892718,3.0443941193584534,4.77663042305105,2.7949356628035362,2.5897634869849773,2.4436066514756147,2.5160151470036647,4.623515741490549,6.6059979802325195,9.051127353591394,3.017921907997262,3.9212458885855885,2.2387868595871168,5.621465834460144,6.001351892999959,6.605701738949982,2.6530600171045644,3.4698859762744636,2.456806149230474,4.528571318870758,10.08413071995014,3.1953475983222193,6.061128135118254,4.434961760187254,8.556735947914529,6.821837746088096,5.895302621333307,4.783456654360239,6.1147834472498515,8.8634739090894,2.6182386555954547,5.862203399053224,3.319039815562536,6.531225485053606,2.2265085298086795,5.50303064564116,6.549977142771825,2.582556003014061,0.45417589318580215,8.258094872866513,5.864186144654281,0,4.110196177754199,5.541948425333733,5.139960569545456,5.166715444966422,4.661635602335959,2.6016965164809576,1.0565835283663676,4.867402305727543,6.390082950951663,6.790641730763425,4.185073637876762,5.681168120416691,4.409390936137702,5.399854673572431,2.752748591407134,5.553974808237472,4.238786859587116,6.451211111832329,3.2555007331483865,8.627898615871748,4.5588798534754424,5.639521599510833,5.29020320512607,8.07804403634318,5.592756010441026,6.952217064973338,2.6392321632492775,3.2942531364445142,5.633721812641012,9.325777655214823,5.554588851677638,8.364703739582406,12.206535173103617,6.2268938135713885,6.058532970201611,2.996388746447621,2.169925001442312,3.2539892662307865,2.3645724322958563,2.996388746447621,2.5084286525318573,5.370861740085285,7.2461230165968376,3.19377174339668,2.4276061727818994,5.25285462595139,8.366191054189278,7.062963556559086,2.0531113364595623,7.846744023244868,3.581351247168777,6.948834425578686,6.412442824775613,2.899175630480513,4.786073551578548,8.971342367308225,7.2193621578077165,2.885574364371426,7.801029283808189,3.7813597135246595,5.6884601404244926,3.361768359419153,2.280956313831056,8.531927241876955,7.295631200395783,5.566206293465506,3.9990980337056072,6.145066014185894,3.0373822220030804,6.666614587352882,2.655351828612554,5.343763122149601,6.696689389537669,4.609991295212678,2.3950627995175777,7.005961325845812,2.2957230245399685,3.7548875021634687,6.883987308105472,3.2433644256936605],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_logkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle kNN\n","# réglage des paramètre pour la gridsearch\n","n_neighbors_log = np.linspace(1, 100, dtype=int)\n","param_gridkNN_log = {'kneighborsregressor__n_neighbors': n_neighbors_log}\n","\n","\n","GridkNN_log, \\\n","BestParametreskNN_log, \\\n","ScoreskNN_log, \\\n","TotalGHGEmissions_pred_logkNN_log, \\\n","figkNN_log = reg_modelGrid(model=KNeighborsRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log,\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_logkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN_log)\n","\n","print(BestParametreskNN_log)\n","print(ScoreskNN_log)\n","figkNN_log.show()\n"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.060618692608231,1.716881500743601,1.6501148191475772,1.608878566240002,1.5877564015406107,1.5798768555669225,1.570829674059032,1.56486808561312,1.561465457792913,1.5592527865766486,1.5573443739541397,1.55557923942769,1.5554690438091103,1.5532346920794746,1.5524248502219649,1.5538861672780375,1.5534525840672084,1.5529662314651795,1.5536761438235893,1.5534596061095076,1.5532105665866123,1.553347942025975,1.5539186547355588,1.5541375021028636,1.5548052822502534,1.554752818736094,1.5554186325144161,1.55566555146288,1.5554506390240612,1.5556410604110487,1.5548667734271642,1.555442300588055,1.5559889557134843,1.557021669080526,1.5571749428417239,1.5574706634714943,1.5577658594206254,1.5581597873790547,1.5580540724710001,1.5579434484221206,1.5577162208946143,1.5577649891823007,1.5581612862231509,1.5590541962899678,1.5585032141701958,1.5591245314641562,1.559549879271622,1.5601470629716758,1.5607005931244564,1.5610669270823785]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.094540162324746,1.7441371748442602,1.6787637276918133,1.6372269863220916,1.6143067344737274,1.603188598034606,1.595105981167696,1.5901132197342998,1.5852839658510096,1.5810815417145148,1.5796596876658657,1.5782516386699978,1.579660156217532,1.5759349128006643,1.5736003286774995,1.5737740171457761,1.573084886402356,1.5714723684307421,1.5717997399761607,1.5711753484653765,1.5703790173588197,1.5701857532691001,1.5713499423067774,1.5710963010194219,1.5720866012969748,1.5726493494092062,1.573189116585993,1.5738448643269813,1.5735240103936858,1.5734379761589516,1.5721089687293817,1.5724122707258315,1.5726536143794283,1.5738701241641004,1.5741276521125145,1.5741978698535757,1.574823828990299,1.5755462494995327,1.575398578947869,1.5745821093101033,1.574482266296887,1.5743286786928703,1.5745472187081704,1.576382454489754,1.5762669229989101,1.5771841792255152,1.577782264901222,1.5783171043735005,1.5790531845384395,1.5791796683368486]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[2.026697222891716,1.6896258266429416,1.621465910603341,1.5805301461579124,1.561206068607494,1.556565113099239,1.546553366950368,1.5396229514919404,1.5376469497348164,1.5374240314387824,1.5350290602424137,1.532906840185382,1.5312779314006886,1.530534471358285,1.5312493717664302,1.533998317410299,1.5338202817320608,1.534460094499617,1.535552547671018,1.5357438637536387,1.5360421158144049,1.53651013078285,1.5364873671643402,1.5371787031863053,1.537523963203532,1.5368562880629817,1.5376481484428393,1.5374862385987789,1.5373772676544366,1.5378441446631457,1.5376245781249467,1.5384723304502788,1.5393242970475403,1.5401732139969517,1.5402222335709332,1.540743457089413,1.5407078898509519,1.5407733252585767,1.5407095659941312,1.541304787534138,1.5409501754923416,1.541201299671731,1.5417753537381313,1.5417259380901815,1.5407395053414814,1.5410648837027972,1.541317493642022,1.5419770215698512,1.5423480017104734,1.5429541858279083]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.088733003335068,1.7574090633338757,1.6844250185834846,1.6405319883502207,1.6132861272526682,1.6060405555218553,1.5979738100569965,1.597530596166698,1.5901826888738682,1.5842933054726733,1.5860481020058512,1.5864475995863088,1.5874414832827013,1.5833698491718406,1.581471018878996,1.5793136515282702,1.5778960591418898,1.5792728877423041,1.5800438624419166,1.5808078758011377,1.5792087816006628,1.5783103447065325,1.5802561051286614,1.5799574477119236,1.5822954090776267,1.5823270961186908,1.5816357838945747,1.5831444457329562,1.582628795514786,1.5817020344211843,1.57919496955783,1.5787009269329528,1.5785835014515555,1.5794809842263002,1.5805369842220784,1.5805574685460153,1.580721481005521,1.5820976275752032,1.5816476113116102,1.5801368168404133,1.5804149636340905,1.5795441372167789,1.5796355687204309,1.5805672982191645,1.581084893094063,1.5820212953368182,1.5823642312315997,1.5822404531285232,1.5827691831999646,1.5817556937393034],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[1.9948348794838433,1.6910757766476463,1.6414430224592307,1.590394805228895,1.565545448917508,1.5552732742852937,1.5467771434003155,1.545415642529146,1.5454147579864395,1.5477662283087477,1.543770994362499,1.542617526852537,1.5389136260539285,1.5341007762296355,1.5342842600369264,1.5346960670149057,1.5326208642086199,1.5332796347683897,1.5380777802532681,1.5410013746717575,1.5432368870225859,1.5466087667408686,1.5490266326011848,1.5476611216438811,1.5489619595378952,1.549286211011404,1.5497673087310255,1.5486094469749225,1.5487116327385648,1.5483698547140567,1.547104201522647,1.5470986205678414,1.548674133064547,1.5500535251724905,1.5499412656250529,1.549602213069178,1.5493658506638857,1.548696581098881,1.5482492131245504,1.548417928731128,1.5482205165769933,1.5496041188654155,1.550450775970039,1.5522190415405945,1.5510516176866174,1.5524280580376597,1.5532418818250717,1.554515238578984,1.5545971404214798,1.5550164819086614],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.0775541992274573,1.6837391639150814,1.607856674839868,1.5795999875668683,1.564990920604417,1.5618756695850307,1.5565998508053862,1.548224054035803,1.5517842317555623,1.5536601443801135,1.5518577036055088,1.5486202315482196,1.5463213211662028,1.5450800801941134,1.5475038389634899,1.5487299135405297,1.5488985454120252,1.5462518907167644,1.5449274145718321,1.5435668613101778,1.5444359796804261,1.5453709840126646,1.5458771614459523,1.5438238593119955,1.544165715365392,1.5447512925155824,1.5472770971518017,1.5462671253579734,1.547667437356342,1.5444432871672362,1.5432444825369667,1.5439517347995915,1.5436111534823571,1.5429533447865866,1.5428662682936332,1.542858868868929,1.543105132522467,1.5437850865667586,1.5440631419128923,1.544024199219867,1.542828519614043,1.5429841095039758,1.543102872520569,1.543351616811617,1.5425233547047936,1.5421063914099675,1.5413494826731262,1.5410607878334706,1.5406077658449977,1.5406382114165789],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.0791602035377816,1.717789091966209,1.6368892389728882,1.5878848557944478,1.568564765946388,1.5660960911537491,1.5502563915320715,1.539581114211134,1.5311967722296567,1.5273388568344957,1.5260166706662799,1.52421165983479,1.5247083380033826,1.5269948876121047,1.5267707083615538,1.5317164021623744,1.5330130917482168,1.53587137017078,1.5349856664964288,1.5342913856052154,1.5323356243537445,1.530435814545626,1.5292283937309714,1.5325601519956684,1.533029164941842,1.5308406513477206,1.5302828514812423,1.5314656714017767,1.53030109969712,1.5331715933024308,1.5338606533428805,1.5351367991101796,1.5362744584643446,1.5380421541371065,1.5384821442284478,1.5399860623670576,1.5398962446833349,1.540190057404935,1.5400499143518471,1.54115628626331,1.5415727144608955,1.5408577548916473,1.5415705202199106,1.5402699605687433,1.539478536176635,1.5398904129164497,1.5407866400410586,1.5419250090445793,1.5434308838889108,1.5447056022911652],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[2.062811177457004,1.7343944078551916,1.679960140882414,1.6459811942595783,1.626394744982074,1.6100986872886835,1.6025411745003897,1.5935890211228196,1.5887488381190384,1.5832053978872132,1.57902839913056,1.5759991793165946,1.5799604505393368,1.5766278671896783,1.5720944248688584,1.5749748021441075,1.5748343598252912,1.5701553739276586,1.5703459953545016,1.5676305331592497,1.5668355602756416,1.566013800124184,1.565204980771024,1.5666849298508483,1.565574162328511,1.5665588426870705,1.5681301213134369,1.5688410678467715,1.5679442298134938,1.5705185324503348,1.5709295601754962,1.5723234215297106,1.5728015321046174,1.5745783370801467,1.574048051839407,1.57434870450629,1.5757405882279187,1.5760295842494958,1.5762604816540997,1.5759820110558846,1.5755443901870485,1.5758348254336862,1.576046693684805,1.5788630643097197,1.5783776691888707,1.5791764996198858,1.5800071605872539,1.5809938262728223,1.5820979922669292,1.5832186460561832],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de n neighbors"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour les paramètres de GridSearchCV\n","FigRMSEGRidkNN_log = visuRMSEGrid(KNeighborsRegressor(), 'kNN',\n","                                  n_neighbors_log, 'n neighbors', GridkNN_log)\n","FigRMSEGRidkNN_log.show()\n","if write_data is True:\n","    FigRMSEGRidkNN_log.write_image('./Figures/EmissionsGraphRMSEkNN_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.6 Modèle RandomForestRegressor"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                    1000\n","1  randomforestregressor__max_features                    auto\n","      RandomForestRegressor()\n","R²                   0.652515\n","RMSE                 1.227907\n","MAE                  0.900115\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_pred_log_logRF=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.297616888633345,5.122452038670764,3.6004600462636906,3.319298962795901,5.072112164803708,5.3268081049001275,3.4858671602216766,4.326358077607432,5.768519614063144,3.1122853978362763,6.27313470969977,5.766784976087246,4.1467479408986,4.18295033059175,3.212982355786819,6.012412464424027,5.162519712932909,5.209812389102345,5.193412306635096,6.341133088397506,4.665709193010539,3.9409636217388035,3.742773701419263,4.613743490504452,4.730398963423362,4.448013325269053,7.6289371606553615,5.016946523373089,4.980148281527235,4.49944711396261,5.200664848291822,5.362881948255629,6.453564510225078,5.838375740115963,6.783648292252501,5.71266662377185,7.7479368972928055,4.033010035474121,5.055918669095063,3.3272269258743976,5.199943646628098,5.111915213179157,4.435115601077897,7.066518725929403,4.454748065065733,3.911028397669494,7.676175136901078,4.558207688237289,4.265798272579069,3.6541782984071713,3.162399530390176,2.820829623662697,9.246716640636514,5.510118543877764,4.989054306325018,6.565795587989098,7.081942470710973,2.623760914672171,5.925092739315282,4.696994908967959,6.399519484699563,6.733574807948668,3.412306574222831,6.0927644389631395,5.793935562988212,3.3345444737877035,4.710252250986432,5.1597129468808465,4.967938749982849,4.410943769960043,2.933418167785802,5.07506167663785,3.3897514641583606,3.247467426132734,5.863715773354807,5.908882795331251,8.27498871463729,4.290496439154893,4.459523589093023,2.8981789645234195,3.588396353469923,5.795781160293966,6.545121007103463,6.56302641214363,3.487284962489656,2.506174471886261,4.569410521043956,4.212228271943591,5.815394198579455,8.470150339635575,6.300158338671887,4.798358551814711,6.87873571199184,3.125129713454029,4.220988854826894,5.279934740503821,12.275146275923174,6.138159403038613,4.940770790596987,3.989284438580867,5.0856755389934145,3.078673229047437,5.773190447977675,3.7849566458249835,5.6120726896563875,4.675172579773116,4.035382367738259,7.615724375357354,5.467969938980351,6.676807521780627,3.992125714060375,3.5311637410786285,3.7404862093046183,2.522151058367294,5.23512081365582,4.60730616751766,3.0437690672139124,7.606176810234927,4.493270942107723,6.973615582563092,2.5083590260169117,6.361625291952998,4.0869290472591935,8.504329610995047,5.409764533202916,4.736793501534349,3.6877422845180376,5.560237694907448,2.4304012388925207,4.610753166296368,6.720754506413302,3.2937332403492547,4.615010373641206,3.6624797252731756,7.733736357556418,2.497701616745331,4.657679083976795,4.964001279799015,5.690487987770055,4.513663423133813,3.260272244583603,6.9868992523587155,7.100629890920912,5.274127203062815,4.390021901599322,4.743844779680479,5.470230116580868,4.284098939247489,4.318643330649641,2.7743741482853914,6.85242658305355,6.902961370473774,6.404384642247487,3.473207214819819,4.218012783500714,5.4234842025910925,5.1841481555438955,5.921486691778274,7.95876151877927,4.564261717540097,2.434087294012014,5.216236295467969,3.4385068086789237,3.947730552015345,6.506758498770095,5.580244593549047,7.435828535734695,6.255365816167261,5.109153824829134,5.9998667098484555,4.037714771663819,4.158736491740751,6.16602136003983,5.283203607823856,3.7437049477714734,6.325332682346553,4.855446174505464,5.749713258041518,3.273993094983525,2.481685311518564,6.043136968250235,6.242049631291485,4.812970073925373,5.416787059745538,3.0882012650207007,5.597104773636747,2.9591034590757284,5.370627724411924,4.61302033484617,6.168101719040077,4.152296989581298,5.038704457937904,4.46102950485672,7.030852684547241,4.127250335037665,4.723959421608669,5.333798731312923,6.016625888014134,6.290116894775009,5.973979627980814,6.53807390614732,4.57017746256745,3.1842050040694256,2.8654336954369968,3.6372738419558073,2.4947414046205014,3.666195978789829,2.6732062494143034,4.128142875264492,4.468587774991903,7.778185701698065,3.43947703726003,3.399433991233946,7.680162657103682,5.581714027355824,7.898918574354431,4.85997594049823,3.2289577897411306,7.037395715119672,3.0693823987134947,5.153650099478361,5.331731118344118,4.229353292571135,4.980474639949757,2.9854714591290095,2.6559033658572373,4.979605075587812,3.163195653804648,5.080717388261889,4.177212788317844,2.85601844467347,5.09453649128375,3.7126633708956183,4.955016054675931,3.0622824649546385,4.189759100272552,3.8489297955034507,4.759010082131395,6.540595423365424,5.110533064023043,6.371692350287049,5.996471977640685,2.945949837274386,3.700358526207771,5.479305485396552,3.5769157317020936,6.483979059215213,3.1021479690221816,7.238972033002676,6.079403612840257,4.58503422365614,4.505170159029566,6.045036998573201,4.469437409056619,6.853555099161969,6.334361287763938,5.403486727879223,4.162639004741621,5.184983261362121,4.905277682524807,4.696908897948385,4.396717882009174,4.839680890717759,3.874241251350224,3.472977536071914,6.10394823293253,2.501494509323223,6.896790107726605,4.541866145560941,6.844379534198886,4.149928678928901,4.782549180899301,4.416174167652303,5.384742223619124,4.0734709372162845,6.348712516759372,5.510802879688411,10.354871783931332,6.999932148123788,5.0712034476458925,4.902300024546127,2.7074326989267785,4.045126594485138,5.655379671839092,6.765091815529513,5.903430198942615,4.0650749658819345,4.320098648079232,3.575852999904791,5.341858977342862,7.537405544002271,4.00371659578531,5.043381798267594,5.234589441278427,4.3420418128536795,5.986576926246658,3.680220229098579,3.8062373246607684,3.728072240641546,2.865387329423565,6.732554227877655,6.15902365529676,6.336572092890494,4.82968333247733,7.496007838812262,6.650687256837988,7.537536683028285,4.757213127415551,4.119522936284876,3.849586712714292,2.771615247205578,6.045937980835195,5.101601384442246,7.409882132529178,4.40092413515786,5.348155561741544,6.392781297205748,5.268151214761171,2.7010605882053818,5.947569663356946,4.612643163433079,6.705157538347875,5.0062681793941355,4.220632317035262,6.092470197693839,2.8122791188293688,4.757967207543721,2.7809965834707064,7.771223051112983,3.7689567562981936,3.4674966031449963,4.371945601358763,5.068958936138727,6.624130935528405,7.319351802526058,4.391718243320469,6.954993489505789,7.318384030390083,4.870322082770038,2.5717473781675433,2.497912416071798,3.470939899963252,5.148270105902203,3.531571629944025,5.358141581559544,5.589570501919188,4.186784330566949,7.1774317291040015,3.989580609668055,3.474884792584373,4.881623192346306,1.7506095947255753,2.2589354567646422,4.531216181738207,4.195574674910408,8.759568022764611,6.761139909357102,6.996127724167578,2.7652683296169482,4.651906557161227,4.358071732710038,3.8494895561829616,5.031751814896788,5.9871319125004945,4.41855696564441,4.387884488904779,6.405070725246645,5.024277511468655,6.542867595225811,6.099036552220441,6.3067555054193045,4.295651918932508,5.777714134671907,6.5084245368147196,6.576997539505418,4.219094302475451,7.026987274918531,4.823833963296635,5.714895988005483,5.343594329633362,2.7247267221575213,4.23975312928132,6.696242274041521,5.426061622652343,3.077052346608406,7.1924901626867745,4.604697682077979,4.755749798116267,7.458088383381138,5.654325288209668,5.3353731352804665,4.840837503824419,4.983602199751173,3.251822917073844,3.513241103064501,6.565699809775375,6.396600638415026,3.6405546410938556,4.935684327614654,3.703036813830773,5.408083665748585,6.119210935002261,3.3979304474597347,5.44887571393584,3.4608314418920836,4.263855782925133,3.967360004566051,4.890743423845573,3.938767562885414,3.52408152944076,3.0970761206878294,5.113177653988465,4.987524544182188,6.290940315334693,4.473805224874728,2.946233986276718,4.731584869484376,5.135539670363996,5.940243453608523,6.156212722826267,2.878752565895965,3.406725682045344,4.82035393864203,5.4902309979058685,6.350135417045824,5.854310413848725,10.382464946241726,4.0308516872524915,5.260255807472011,5.034927778169032,2.840606849580062,7.809967379931527,5.4269819235276255,9.066598562297607,7.405481794713393,3.872137717310726,8.020721852386037,3.1368912409491236,5.532541586658502,3.6375596471156864,3.509053159427854,4.374386136568397,2.262451343894132,2.5947110926412233,7.553243243557204,6.556297594314972,4.600861046559744,2.9558482881205648,4.271465159349141,6.527483757168133,6.592259490763433,3.2749153348578086,4.787884481266361,5.732946519135151,6.856496309038436,4.525407365703364,3.8464760394594215,6.51656689391386,1.1314228853143176,2.359836832053346,5.187760275967404,2.588025762663905,6.024488360400303,5.970416645720231,7.282743568290443,5.316844715045423,1.6707228667947611,3.9097221819395864,4.896502138493605,3.95360255974051,3.7329777162171007,5.315079119157235,9.917780715445481,5.972813009131595,2.8578327704198476,5.282596644265286,6.252344092749464,5.814860308954743,3.547332597552783,5.5826966428591085,4.4336879481619205,4.307416785498053,3.9797903751800674,4.485722400393354,5.272583295685074,3.411126215570729,6.001286381770977,5.429721672740459,7.916280242406897,5.725296153422577,5.155268755425171,4.371125999236564,5.220101118307977,5.963733979468125,6.221097559721156,7.929574005145436,4.619971943300358,6.466099963058958,8.353988016384722,7.250757950539132,6.2464763486808135,5.375174873261209,5.1915514096621225,3.1802569192154513,6.855680426354253,6.055320330505401,4.708998827829765,6.6361240385472975,4.373336761287565,4.530979091117457,3.4594295918406983,7.762658948674022,5.750635043384111,3.6490292566575184,6.737120992301421,2.6499836110652537,2.4010075981891594,9.528864883806555,3.9572130017005875,4.160362900208988,7.665415539565456,7.355117497276091,4.6195427574285635,2.8851257630775486,4.366308085977451,2.832457115498946,4.321723233249893,5.892086035564776,2.997908614886012,4.43877114899587,5.702985346228796,4.658063180522536,4.722124608293657,2.465449996465708,4.709123303465155,3.7821991214674098,5.258490812956432,3.1568955874322655,6.46176565007583,3.386867564879068,5.1889317210091,5.220130258330323,3.0428874968243815,4.840126158989848,4.103032291142724,4.784015086703979,5.2701767709559455,5.139796140874853,4.557107436593446,7.072311940476611,6.718115059933009,4.192578164869071,6.130162098923512,4.269930297478793,2.6423744536716223,4.354116780158401,5.9014424021075245,5.897634246964475,4.519793751836586,4.0363797690538155,5.385739105836537,5.475615438216765,5.839011502311969,6.152946243193769,6.425087719720538,5.8223406607187815,6.3971400868315795,7.648805983792907,5.298422879730805,3.396069453780608,3.099152801872106,7.540098056593354,4.142063567021635,3.3272724392266935,8.177374178532787,3.4497489239507946,5.853793328355393,3.5269823103341307,6.443611688719505,3.944043740056692,4.600373327451503,4.382308251088211,3.3773997935998854,4.798103429493396,5.559623632414512,6.91953625575371,4.525258891815613,3.7503521284206136,7.343543212572741,2.336543474678748,5.755366176448497,8.120030312629332,5.548977736752014,8.438040408172219,5.631608402216029,5.420500722138058,8.75255314800042,4.663969033603495,5.001145343208766,4.663316495458797,5.613202537703893,2.785372617697112,6.273858818351235,7.549869155176145,2.6877497479992063,8.819778730080447,2.8366036390639517,7.836726632439211,4.010343313161869,6.370827849610246,5.185809229223773,2.2666065286502586,6.044502465592331,4.385049276606711,9.378782066648414,6.490093410160106,4.028081551789321,6.732912477474182,4.07016850039825,5.701956699328325,5.713419533281321,4.6760310591897145,4.580102639220578,4.801415712010022,3.986511642520592,5.397720615881872,4.091565885819056,3.6348703487436835,2.140251361768407,2.359984216633305,8.274334395238965,3.0340750772396663,6.07973517898412,5.828601366857306,5.460153449179231,4.736647490111305,7.0168174120269216,5.199358098573558,5.268219497230147,4.340693516804009,6.96208343221073,8.827852973143445,4.208461733909465,3.211202967408571,3.020407649738827,5.815195823560867,3.34021333426577,4.560390063658364,5.089009492517869,4.831871989326988,5.785869106220555,8.452380133095023,5.675241656033833,5.590896235774532,3.377449550603244,4.635526641862976,7.150031907020565,3.1516845756540275,6.79751180916615,3.820128148570977,5.0522002267263355,5.625276153399888,3.8040847520380834,4.891340907845589,6.256659877816915,4.4019711163993165,3.0170270849634595,5.929374316306177,3.6108601353649976,4.213635307963924,2.9338150784386783,4.279696078644317,5.674315215907524,3.7924996400337108,4.692357928269407,7.7326872051566395,9.196532576603035,4.257376330568009,5.004650816094102,5.247455831753655,3.948597261595783,6.1752625133107975,3.6787804858511017,5.802124581386445,6.962088469474686,4.236976561240271,8.435984835116557,5.442138620017073,6.036369887268134,6.86320783612408,3.344331086304502,6.644305344847551,3.4028615541472145,6.932184797860471,6.070418576156257,6.179381200388913,7.71868239609187,7.576451984563645,4.414894759254314,3.7351540567139057,5.987589954782937,6.277659178898543,3.0022107491544956,2.910479201177777,3.7042409520963613,3.5239311851918105,4.733435762485592,3.1082482437049817,3.1723876830171656,6.298145906622295,1.5668293382068745,10.039961253428642,5.4551482944170555,8.143565679935637,5.2789066654491945,4.77891294694277,5.191262282035106,5.053118678680559,3.453339309233303,9.366040557165087,6.065478793219581,6.116713788768605,2.8227509635665253,3.3331063533047733,6.136213020509,3.5019444339336054,2.1355113542691995,4.7430115071548835,3.1069477808390213,8.065970758029364,5.47357443456584,2.5681200979779377,7.285698168808081,4.951870630719621,5.422681268029594,5.446219278859315,7.957376603328452,3.5375067976297485,5.162604215765423,5.993116375079316,3.8352468019266066,4.746931701704988,4.58341401356088,5.864552374280367,5.074578301654911,5.34231297409412,6.442995629496079,3.892017706764401,4.416329131028715,6.552843656008735,6.506706180189022,8.442808556686533,2.95068139518232,9.463547844577617,3.3927943847682007,3.702522639396269,4.1932801484099285,5.002393660067377,5.721345176319355,4.601192780459784,5.14561770813318,6.805518311620767,4.668213107042096,4.616578159920519,3.6420460016491067,6.631506874598976,5.033629169355679,6.154908583321045,6.201834529562911,5.9691218873893614,5.4525675739122486,2.5125132844636346,5.678988341044827,3.5390147585163003,3.617956338690875,3.9026449873876086,4.535129924870375,6.294421435265675,5.926835501749432,9.41165851000855,6.352847722453884,5.855275281125501,2.5300221676664325,4.18900443204895,4.953898572820358,6.029961377363957,5.609692385196161,4.335647057346007,2.270071346545163,3.9299123874741637,5.64059825083683,6.561244348534114,5.935754619226933,3.8443462228317573,6.795809282559559,4.954329237189709,4.161233008328398,2.9331305396234195,9.740289808302165,4.6891061489161965,4.192820055414472,3.260395357183688,5.815383336602283,4.0821397809901745,5.995746320814809,5.224549889483576,4.959273573044155,5.377315645424002,5.735616656517053,5.2944931525804995,5.061967282807817,7.869502878936163,2.711485812510304,3.939487533113229,2.275760981652449,4.877690113353054,2.7889573436017923,7.720217542199708,5.557767797079014,5.125052352434585,4.952363317336088,9.272940049516649,4.9476682783047945,5.575087169488717,3.7549159629682274,5.635070844907783,2.705319347545463,6.592801740796143,5.2929538817441415,4.245597722919849,7.536113140477834,8.042065757391654,6.511401995062392,3.065489882042514,6.326842384035909,4.967681074537941,3.720863449725188,5.345474077482789,4.399399523481437,10.329037288357036,4.111390891320159,5.0113562017983915,5.92359333063368,7.152519002746972,7.0178829481268155,4.843757758325081,4.64332547730084,4.075086118899581,5.684673357921563,4.576248863485329,5.1755709050532985,4.605514756960798,4.774569772276704,3.9197727642070634,6.286592848866886,5.190725902904166,5.778326780104886,4.971807665294027,7.614869275871257,2.894545459638159,5.309232604940544,3.2196654175338746,8.14210113782319,4.72774966498235,3.571954535508053,3.127760597998633,8.902726745746733,5.862970072597972,6.867545487505236,3.577464160023894,4.042048190565268,5.484287100456819,6.111601560468877,6.7475389539119615,5.272917822302005,6.047683018583436,3.7976594312050014,2.369575264640262,4.303214739822404,5.292378807851633,6.844299251008058,5.778797697197878,3.3765231542741643,5.504480898159696,2.91723761650362,8.978771609305968,4.553020795451451,4.453463067301214,3.792014756678971,5.588403063174823,5.601446269233885,3.277433946050137,3.8651594566577456,4.026743076218789,6.169006318218519,4.382739293946532,5.253518284177827,5.535991627950442,4.312082721046022,3.8921794516352772,6.169777826820942,4.254186446908665,2.8370533629144883,4.951661188596915,8.096572173639897,6.9452795539336485,6.641970171383234,3.836178810648593,3.547332597552783,7.799772967097376,3.808812503279923,2.981416084041517,4.813979864078165,9.190898284669906,4.077864706334199,4.103178608497099,5.611049401860382,3.468094569407474,4.074869145601992,7.410627098986079,7.649108027920417,3.935992276605551,4.603687130322568,8.870660009009018,5.181416005126279,8.429410412980564,5.319262284596187,5.927423271939553,4.614131327099226,8.144103316671687,2.9309360678850114,3.6474255229300128,4.666327819313094,4.561449713058162,4.087988925053528,4.328441405354701,4.6286191103444185,9.138255380303626,4.4160482252829585,5.867111068522613,4.74873224387243,5.703612899283014,3.648868919393954,3.2685865848298135,7.358135325318107,7.7068135133056215,5.012099147818706,5.021243451077967,3.3939951197410734,5.398299285364039,6.166572641078057,5.072552682682097,3.975388418852013,5.500691527066883,5.610481125626539,7.7147338195697435,12.721989035038083,3.4490352048784993,1.9802336127788445,3.468928867616811,9.200568553649962,4.02368042431924,2.1860191210046285,6.0643767749896345,7.977792540979757,5.092058702427544,5.668338603663003,2.301414170614177,6.207305661602966,2.740185739370269,5.345760638276608,2.3407440953275453,8.030656899816226,4.077551459312194,4.537843266018632,4.067273506957445,5.558531285277003,5.426318387505267,3.9893013873710568,2.7919267955949802,4.4797035130342495,4.0794155513457335,5.642137520162378,4.186135965747527,5.584579972454675,5.2404151011688995,4.662940696234383,6.325507010188327,3.348922593212236,3.597916444066182,6.066506758567954,4.31140644364102,3.362359967708991,6.746485887325049,4.991194094665159,7.487791503993111,4.245562526530603,3.1339375069285977,5.735262802077375,3.6293770185080394,5.321636613411039,6.150690324748843,5.8101532023092854,3.488252034837491,8.305498801807412,5.550712434340855,6.28161380930767,4.391933255606427,3.996107030586662,5.519917107027166,4.319415207341825,6.058492749823281,4.654402257978408,3.442699423504542,4.899106081103115,3.3916636364214217,2.907370048795948,3.6069700126324613,4.233860077025677,4.184211132590622,7.147743038016605,4.624847884388043,5.507245461384472,4.400523643374989,4.512437097947032,6.355643494755823,3.5460844803057916,2.8435616302750306,5.016517185503988,5.697363746014135,3.871238234648434,5.702047734903441,7.940965522165656,4.390435306199063,8.962948352932013,7.687779959207263,8.707572844612834,4.352528179324203,5.211422138959151,6.930064365765119,5.3689416665124545,4.664428861323008,4.23180293429518,4.783380432012223,2.791685218472215,6.678950327462081,5.066773896638143,6.868398722391649,4.690389686363108,3.7920731949456816,5.8770537906239335,5.207478894936286,5.53586833751408,3.7241638495745275,6.271028319267835,3.2558462438738927,5.527087651675574,5.892323276394728,5.291897570395287,4.4261668199708835,5.8978774259285265,5.968676267735085,4.95457349970649,6.479656855098717,4.289063886667109,3.335208678567089,4.884897692347251,4.775889767186407,3.868623496678436,3.9339006600561883,3.6487373148653903,6.280455719706067,5.386448557761657,6.691348135716632,4.610225365255083,3.730655414902273,4.066824050355358,4.584167737237528,5.125471568052544,9.348912314293946,4.362487550978183,5.8660226733849505,9.240931368194964,4.443663211950249,6.514617908124408,3.528173405815865,6.523065987694477,12.037015897020632,6.001485061736483,4.269355598576488,7.201846437830194,3.435040965287618,5.076431660135162,4.045441658421876,2.845341628688078,7.919696031203974,4.837245012793628,6.762177113193301,2.2967656000429626,7.677906428415627,6.266501138695573,4.29496840712652,2.255422707816347,3.5706288456212376,3.675300623270868,9.376858938004045,6.207046926516848,6.049147353374743,5.134676038633564,4.623145731761123,7.920226455028911,2.8936357473080907,7.7875063122944015,5.910198706340255,4.949969156194532,6.100001826150847,1.7623501741968584,4.949481177122737,2.9371151752015585,5.619107377915387,3.555412480375561,3.07360134895741,5.628390594688229,5.849832848868015,9.188197608985373,6.272839021853058,5.971768700277189,3.6389920547599006,6.43040685062912,6.684147111983986,3.6994801846889738,5.563257267573595,6.870408574032976,5.376445952648356,4.336325539114832,4.296804944576261,7.299388908552171,3.885901999347552,4.181904364658396,2.3778136703546613,5.557313875051275,3.585845997076431,3.811100984059065,7.695070480062923,6.581307940243165,4.745124576965164,8.842062621223299,2.8349024960652014,7.478113199069407,5.020543649179201,4.237158626203099,8.396027194406035,3.484288769551775,5.435667732949578,5.0576781555819466,5.7214081216283414,5.7255040692120245,3.8874491353502556,3.485211029335494,5.449153166096855,9.98173831464795,4.843405097026244,5.30355962661678,3.137111085450664,2.496993962512727,6.674891088951077,9.317245585918517,2.39658680982897,3.9505014240570056,5.524755226397324,5.217888937803943,4.468498791323273,5.637903018482402,6.133662742383791,3.0954724705226555,5.350405949392887,6.516883293628461,5.755568634313574,2.4991679506336992,5.865225249620522,4.046013172747995,2.8881713375220817,4.105076631201051,3.41316455963196,3.888583939105426,5.140550490997647,5.513149753505044,4.471852177930945,3.929105722844942,4.798588853344738,2.868337137082003,2.7853671819712993,3.267343113761848,3.2534300800463978,5.099385847923442,6.20092010661126,7.555215869095159,4.1079295846440615,3.5016607116929634,2.357388998956738,5.063690427557493,5.286405389504822,5.594086608319073,2.831951268730947,5.7360351578171205,3.359174775639334,6.204606308134425,8.041447920227842,3.8453402808522235,5.727905507389194,4.393762618401087,7.674364934061711,5.977315692701076,7.11843206422977,4.868124919720712,5.524754368308534,7.582080164834562,3.0002735494767356,3.381667027447267,4.191051933925969,5.762826763119871,2.7894775137014127,5.145034092611423,6.149593207691148,3.59773362602962,2.5991737111374746,5.963940925619258,5.8773411007756025,6.518450874689542,4.006328274896713,5.293105149253779,3.9102181834254033,5.644292768512132,4.367018735704972,3.9704700553704657,1.7848141146454917,5.573848136720124,4.589825820969181,4.964502777625057,4.039434039897194,5.585358100140035,4.950210221101148,4.174015796271415,3.137075256898546,3.4719202822788273,4.316490450044348,6.002045091072202,4.958108845207032,8.11665913348131,5.724786984756039,5.492109377149341,6.317799519615691,6.471496327066043,5.401687590874674,6.996165880009449,2.830005521864396,3.763732087229161,5.708112197183218,8.102793335418921,4.838735124713684,7.819127064805674,10.827813493605102,6.089991459543775,3.4259861947164296,3.5335944929247414,3.1630459583655113,3.630169840324237,3.006287746671116,3.351646647980818,4.570618079772991,4.8344641143937395,6.620506670121367,4.392825602195024,3.1834273511337012,4.814899784033033,7.425468497186173,7.055571471242438,3.766143241115683,7.2947486021622945,4.83375093398448,7.289956096478229,6.720933880552218,3.5547956721609824,5.463942757337777,9.186452434681284,5.687713043702959,4.231555901370673,6.396986733285589,2.964584368250535,5.769655107218985,4.439537939926933,2.5565945158681935,8.01674519516312,6.889828231477674,5.904109391108383,3.643728638027957,5.791020002640558,3.381451161448422,6.579534179145619,5.077681655451447,4.751849031552641,5.395774462210273,5.496200992049497,2.3712665684351673,6.71439359401731,2.7954900880870843,3.541512579939742,6.047497263203454,3.5686877273944773],"xaxis":"x","y":[4.9676297534661975,5.60584986719498,2.950468414150123,2.7355221772965375,7.258707352196828,3.0925457415435655,4.178714641175443,3.4154882710497003,6.224387627929912,2.634593268445757,7.244982170376553,5.597531174224212,4.210232990095849,3.594548549550354,2.526068811667588,6.215484464192794,4.969933274697856,5.912649864897204,5.111865963867557,5.042644337408494,7.864928972289788,3.382667252745041,3.189033824390017,5.705977901682522,3.9717734471933728,5.119771161952904,3.496973580998276,5.058749412335524,5.968551603363945,3.1160319934471103,4.464668267003444,5.192194165283345,6.73226919950145,5.39746072605472,8.171076693915895,6.43095427138573,7.132782457298859,4.294988267691446,4.708739041359579,3.4646682670034443,5.450881315273369,4.95093492831454,4.20006486151431,8.047505671251574,3.605257262939004,3.261530815243406,9.159820892013935,3.8728287595348854,3.8298495598446904,1.9671686075326276,2.9126498648972037,5.648177795724818,10.257022407964342,4.887037791419197,5.718087583960517,7.540941686758214,7.156841524588401,2.0942360698457656,6.074462620704536,6.428778891337586,6.464504909131913,6.588864518190148,5.629648044187436,4.795974694206668,5.676380255466924,2.811471030529836,3.9882302230604574,8.45314714555533,2.321928094887362,3.980939266085512,2.5801454844233804,4.41886457739032,2.8972404255747994,4.341985747228616,6.561784353081841,5.751410160064371,8.545157551729474,2.7676547982373463,3.773996325111173,2.7676547982373463,3.258518924711301,5.596637034102074,7.019368330478469,6.649471757181635,5.275007047499869,2.0071955014042038,3.700439718141092,5.320845667645722,5.331633567171029,8.061668207389335,2.786596361890807,4.94673086014031,5.649040565634448,2.269033146455237,3.292781749227846,3.683696454306516,13.390111171989,8.064904402198286,3.727920454563199,2.8155754288625725,4.253232938538032,3.0373822220030804,6.01992426090841,3.2524762141352173,7.125981653854716,4.273515889702116,4.92552476974757,7.778274295338579,6.135452784185173,9.20006486151431,3.6971065744769747,3.2418401835646704,2.4222330006830477,2.6229303509201767,5.57198012018773,5.0539801681876515,2.6915341649192004,7.510803720358323,3.01435529297707,6.045049740837989,2.7803100990433753,4.658211482751795,4.595145567990858,7.485507332459785,5.342341397431548,4.129283016944966,2.454175893185802,5.96254902292306,2.3305584000308026,3.292781749227846,6.290756138396673,2.817623257511431,5.532940288372874,3.2883585621936606,8.008820757716158,1.6644828403646825,5.284292026394312,3.598126959919604,6.237448995639321,4.222650022451478,4.380590934265922,8.669593751188334,7.720552091605108,5.548744859937229,1.7484612330040354,4.524815928357506,7.639376888638463,2.851998837112446,3.218781167784069,2.7990873060740036,3.868884273028774,8.043027283594547,7.260778431893426,4.038260575175349,3.040892430646901,2.7949356628035362,5.4741115139424075,5.952799477899938,7.323370069061268,5.040892430646901,3.4208865749755315,6.136683577697236,4.255500733148386,3.228049047884462,4.6194130105979365,5.623515741490549,7.217424613813984,6.179311989210016,4.3139713710590195,5.764473550992666,4.221103725367875,2.4594316186372973,6.548282482376233,5.9902741833186015,2.5360529002402097,4.720825666089835,6.512700963425246,6.664198369291911,2.881664619320345,1.9634741239748859,6.013685569928653,6.026578770127714,4.532940288372874,7.597158683183727,2.711494906650088,6.791683858152045,2.1009776477248208,6.090641751917847,5.040892430646901,6.327328082629952,3.2942531364445142,2.9467308601403097,3.7366048753142485,4.922673592849446,3.71699089440494,4.15623479785027,6.872951851130105,2.8933622107638715,7.561631630375841,5.341985747228616,4.993221467368938,6.309067020588001,2.414135532984451,2.493134922305505,4.161081482277184,2.144046369616707,3.486714373030702,2.8011586560936985,5.648465443027314,4.399171093819823,7.581803148516525,3.1160319934471103,2.533563348214512,9.967312731471658,5.687060688339892,6.954079959320971,7.084170591577843,2.722466024471091,4.826802684285827,3.01435529297707,1.6040713236688608,5.5109619192773796,2.1731274334806563,5.425593577653059,2.8797057662822882,2.7803100990433753,3.303050084681673,2.3132458517875616,7.006298023900369,5.279842693520348,2.3950627995175777,7.216163813388774,2.786596361890807,3.5298209465286954,2.7420062108667365,5.079378111118652,5.1622906135457995,5.839203788096944,9.354249381945241,6.058532970201611,6.614857050384495,2.1667154449664223,2.4982508675278257,3.3405622690264134,5.415826257472914,3.9791107550157854,5.338780943894699,1.9335726382610239,7.164705840182799,6.12866458654003,2.933572638261024,3.0548484769956197,6.390598905552461,3.2764966656403565,8.007363716298686,6.558114538598396,3.674686619927999,5.28835856219366,4.189033824390017,5.480265122054463,4.2787282129389395,3.776103988073164,4.82273014794452,2.9467308601403097,2.2898344651775093,6.1705259991768475,1.4750848829487828,7.264067216098838,5.6845382485777325,4.534808661231747,3.7070829917717063,4.919340082442012,4.458119481174506,6.288173968112302,2.27500704749987,6.5249725982866265,3.940166750482817,9.715601700373337,7.829278760612363,4.821199978056451,5.875042803002589,2.6229303509201767,2.942983598187102,4.445594291341182,7.474030370207135,5.92148393743373,3.4369613378336026,3.2078928516413328,4.095080491901415,4.422233000683048,7.424922088210688,5.185470146066942,4.955126781261366,4.3729520979118295,4.0522419811811385,6.067165427017112,2.2898344651775093,3.0806576633452254,3.940166750482817,3.127633279725874,7.152994605492435,5.573192723781613,4.4121040446775694,2.2479275134435857,7.886854944198882,7.9329824099498465,7.25691629667084,6.670585440262214,5.845740932614582,2.9616233282869446,2.5459683691052923,7.959132576789657,4.653633311386255,7.406162478777607,5.588264894307497,3.8063240573900288,6.716579420796345,4.984589350362456,2.269033146455237,6.929672661579345,7.060371694122218,6.746447139362815,5.521364878446219,2.9873208659292536,5.970623614470605,2.37851162325373,4.667892125230234,3.192194165283345,6.816215687622865,2.851998837112446,3.3854310371935203,4.634593268445757,2.5558161550616396,8.990671283975267,9.114445113289024,3.389566811762726,6.9532652390148435,6.705563274566321,5.672425341971495,2.280956313831056,2.137503523749935,2.2203299548795554,6.561173365231147,2.9726926540042644,5.078951341394822,5.0759604132434015,3.8509993947164736,8.693835392482516,2.845991770664573,2.8399595874895316,6.111448698487674,0.7484612330040358,4.125155131322289,3.8953026213333066,8.45889870671479,9.650854294025212,8.19120730223037,6.534964249804655,2.5160151470036647,3.0373822220030804,4.701549056943071,2.9541963103868754,4.903520373723376,7.848935855524565,2.0036022366801953,5.660780334032546,8.418822422291017,5.254745197629213,8.4696011200494,6.567880042052731,6.557808298936688,3.6769443591069124,5.651051691178929,7.279842693520348,6.425089989874059,2.811471030529836,5.4799419014771305,5.184280294419382,5.713420884868084,5.452858964713811,2.4195388915137843,4.178714641175443,7.686990680088471,5.3847405872923835,2.722466024471091,7.554281862626391,3.955126781261366,5.192194165283345,9.071542807992602,5.765004246250036,5.318316841334983,3.1953475983222193,5.2671612180222604,2.269033146455237,2.702657543390911,5.6937657122177825,8.574290766120185,3.554588851677637,2.4059923596758366,3.0214797274104517,1.3785116232537298,6.010108453474289,2.438292851579147,3.089159131911238,3.0250287944915226,4.571676809970931,3.24031432933371,3.261530815243406,2.6712933724815757,3.6542063779442917,2.9873208659292536,5.8060662260890314,4.371558862611963,6.4984104488609775,6.229587922740652,2.2957230245399685,5.178714641175443,5.463687842201262,4.884597920990064,6.157852169141737,3.4672794804599825,2.867896463992655,2.611172380044005,5.535430914871279,0,3.4776773275653072,9.582254908357664,3.950468414150123,4.8894735425311096,4.850999394716474,3.300123724569014,8.009660689646706,7.626950122372515,9.04105676230812,6.564835417010355,3.3175935046234715,7.380331184220349,2.0214797274104517,4.235727059838059,4.334139178838237,5.692092375427572,5.592457037268081,1.996388746447621,2.070389327891398,5.8052924556007115,6.60584986719498,4.088311235888661,2.521050736900963,3.5310694927259543,7.058316495590823,4.044394119358453,3.0513721017210256,6.008092420948722,5.605257262939004,8.220281580441913,6.088735246190284,2.137503523749935,6.637204481902295,0.8155754288625726,2.2387868595871168,5.676662334857967,1.7865963618908067,8.05642095591347,7.5673476962842585,7.295355692891673,6.394033895367782,6.582706526780788,4.43095427138573,4.915520900751958,3.710393192170516,3.8845979209900645,5.135042286233651,10.267009017093484,5.336640446408735,3.0992952043377753,6.836050355058069,7.896998342338911,6.558114538598396,5.10936055940423,5.85872702377863,3.785550551739256,3.8042601156347384,2.7484612330040354,2.883620816285671,3.576522137920503,4.104336659814735,5.86195536414487,4.294253136444514,8.44136727845972,5.497931651896279,7.5808238499627985,2.8278190246173196,6.260590274730996,2.229587922740652,6.218781167784069,10.212070748654236,4.400537929583729,6.543805175964397,7.563234413022304,6.203788453308596,6.676803353873854,4.070389327891398,6.340918270272116,2.063502942306158,7.151270288790165,4.228049047884462,4.563768278452033,5.4329594072761065,3.9373443921502322,5.580748491763774,4.279471295644468,7.959132576789657,5.739578112048051,2.7463127664254587,5.707635219545296,2.0250287944915226,3.4776773275653072,7.925940095994448,4.478971805032942,3.24031432933371,6.836807807844575,6.2502036564992975,3.82273014794452,0.918386234446348,3.258518924711301,2.1634987322828794,2.662205499653617,6.687060688339892,2.2357270598380583,5.0259146987507926,5.443606651475615,2.980025300238734,3.1193561770396756,2.8399595874895316,4.914564523493939,2.782408564927373,6.282625134191674,1.2016338611696504,5.269781238274379,3.0513721017210256,4.649615459063409,4.896271848807375,4.721919445551544,3.6182386555954547,5.045268215138534,5.020591094773247,5.305240965954483,5.663913842115978,3.3881895371560837,8.836744701966083,6.976363635732762,3.925049964727359,6.802451741275622,4.664482840364682,2.3589588258323295,6.127014141582809,7.1945598860226125,4.475084882948782,4.301587646603187,3.7441610955704103,4.05484847699562,5.464668267003444,7.33360262826828,5.6082178530214595,7.1320626778543845,4.993674361750585,5.522620761042079,7.366234775533047,7.166815851761945,3.7687136570304847,2.321928094887362,6.7873802222963375,4.788685710613533,6.025250321561575,8.162995472417697,3.4019034716079584,6.238786859587116,2.3248106034204836,7.831687267083084,3.4802651220544627,3.871843648509318,6.548128323585586,3.0268000593437154,3.1953475983222193,6.2600256559614555,5.931210274803932,7.80967167790706,3.169925001442312,3.894332742277694,2.298658315564515,6.215678596607928,7.873013392989595,7.709152756902506,8.461602224618334,4.150559676575382,7.653203362033977,9.24186401223682,5.725468512632615,4.0925457415435655,3.575312330687437,6.519321734629434,2.5185351389821804,6.137913322088407,7.904544309292717,2.568032104771279,9.816679697610459,0.765534746362977,8.646594709581654,5.037821465434967,4.388878338811989,7.9616812016062815,1.7782085763980877,8.912589990852627,3.5008020530571575,7.473786911614368,7.182890892745479,2.6205864104518777,6.628919373511362,4.842476062993939,6.013462259806563,7.1851727751412495,4.8359240742543745,4.918386234446348,4.783980413683795,4.285402218862249,6.340918270272116,3.5570424152637194,4.030336078370959,0.8073549220576041,2.2957230245399685,8.893574570329104,2.3923174227787602,7.015805293913353,8.764407200357251,6.147306698780294,5.502075956045791,8.694706111949401,2.440952198029637,8.447992374957895,4.943921326553485,6.072963271555224,8.18685706717715,5.508111681193465,2.327687364176047,2.6322682154995127,5.853496703702325,2.4724877714627436,3.412781525338476,5.751677945687579,5.8774988727835344,5.890446692679906,8.622783966156124,6.203396948546115,5.524189078449365,4.679198570566922,2.3757345385831563,7.809543066804601,3.5716768099709313,6.39694722007283,3.097610796626422,4.9415758618182375,6.668317719915721,3.0925457415435655,6.941106310946432,7.471431358002323,4.633431210355632,2.454175893185802,5.760487101811031,4.112700132749362,2.560714954474479,2.885574364371426,5.8060662260890314,5.969012307516316,3.0214797274104517,4.37016428054021,3.1811025507537978,9.463033855317493,5.102238193930738,7.537684337147519,5.960001932068081,4.451540833017832,5.931683057059805,4.126807703142035,9.109647857806625,8.028624401408491,3.2645364309990255,7.394977085526471,6.152589069287125,6.022367813028454,4.51853513898218,3.344828496997441,6.810314561497258,3.0036022366801953,7.890811454817677,8.317774372804099,7.237735785231934,8.515305605451724,8.213638902198852,5.154615611326894,2.039138393906958,7.296273846706719,6.905206468795705,2.3673710656485296,2.0942360698457656,2.9145645234939397,2.2172307162206693,5.641256998726767,3.303050084681673,2.62993940943954,7.220233204382192,6.489767697008256,10.809687753488749,5.260778431893426,8.11134436328308,4.9941271140031285,2.7070829917717063,5.856985689782205,6.381629467033664,4.493134922305505,10.642557179595622,5.439955517478156,6.790772037862,2.5058909297299574,4.37920505996061,6.33646192818282,2.9523335663696857,2.811471030529836,5.323730337521348,2.763411574470007,8.089159131911238,6.435461914479276,2.4646682670034443,7.870364719583405,3.396433531250992,7.305696981080096,4.020591094773247,5.187451054027326,2.298658315564515,6.038918989292302,5.369815424283912,4.422905742612183,4.024142345897803,4.926948247949772,5.348728154231077,6.907251224292033,4.861459166361513,6.596487956856693,4.593353770980297,5.9502351004856875,7.814614507639093,6.638797899951696,8.79720757285578,2.176322772640463,7.7974669920591575,2.7398481026993275,3.1538053360790355,3.6701605141266334,2.513490745588118,5.307064162255372,5.076815597050831,2.533563348214512,6.678494507773734,2.7092906357233577,2.7355221772965375,2.782408564927373,7.039796407620025,5.938521045865967,4.733354340613827,6.132988042627442,7.991691898336964,6.419033185471022,2.1858665453113337,5.449891473010944,2.6064422281316078,3.7311832415722,4.332707933640605,6.56498780095989,5.434294617959925,5.893362210763872,10.084529386649065,7.932923373835633,3.0823619695574735,2.440952198029637,3.1842802944193824,6.1430260038899736,6.387500406480984,6.681449265314958,4.324810603420484,2.327687364176047,2.722466024471091,5.569551851083613,7.527555198794536,6.632559052117953,2.9145645234939397,6.135452784185173,5.424922088210688,2.2357270598380583,2.8011586560936985,9.8361608416947,7.193968819421969,3.097610796626422,2.655351828612554,5.553360503335328,3.169925001442312,6.173127433480656,3.817623257511431,5.778208576398088,5.7554217347342425,8.571145863602528,5.470862199413702,3.1602748314085933,8.3389146195993,2.440952198029637,3.982765462583646,2.140778655782796,4.3197624276692475,2.292781749227846,6.864928972289788,5.969933274697856,4.950468414150123,3.4581194811745064,9.054360117597708,2.232660756790275,5.648753032989572,4.827310943949667,6.609104846618959,2.920293300211007,5.726013748860151,4.484138131201669,6.010779838753242,6.51317488460363,8.731081391094504,7.529742876502924,2.7782085763980877,6.546122758857796,4.635173946674924,5.533563348214512,6.2992080183872785,2.657640005207824,11.260084480727762,3.442280035252584,5.339850002884624,6.091065077930535,7.352175878326325,7.78959885005233,3.2418401835646704,4.584962500721156,5.035183996618336,4.898692072560488,4.393690764187454,6.686360452903688,4.792855352362489,4.044394119358453,5.077670274232752,7.061344188288391,4.06522762277562,6.802968651005583,3.412781525338476,7.953207027098,3.5643781685650637,2.790772037862,3.350497247084133,8.870919495610579,4.663913842115978,2.2234225499349374,2.6667565918848033,9.754286253961022,6.500324049284198,7.87670110864523,1.4168397419128291,4.017921907997263,6.177519202597814,5.47800105583793,8.031053274444373,7.831180547126736,6.461724991502711,3.948600847493356,2.319039815562536,4.898208352508718,6.20945336562895,7.562776656791846,5.542258049766918,2.6016965164809576,6.711081862306696,2.0565835283663674,8.630303533275018,4.854494418154875,3.4892860226258766,4.601102203376409,5.932391940068527,6.822602695756541,3.039138393906958,4.397118409042582,4.078951341394822,6.198886990956932,2.298658315564515,7.472731449369055,5.0669502439246275,5.059614856297222,2.9708536543404835,6.608661417987563,5.51443791382931,2.176322772640463,5.642412772905056,6.8820316083521,5.234577959784565,7.163498732282879,3.212569338850806,2.2898344651775093,8.307656205306488,3.423578170981797,1.3840498067951599,2.7676547982373463,10.679004987548526,3.2645364309990255,3.9990980337056072,5.649040565634448,4.793375711264754,4.203201156316611,7.668814088419117,5.911931212309049,3.4581194811745064,5.445263208140443,8.913188619508889,4.936402377725063,8.645802514606062,6.207111961207714,5.749266083814806,4.4376272483189485,8.580786171355323,2.189033824390017,2.341985747228616,4.06522762277562,4.804260115634738,4.849999259466098,5.075532631167357,4.056583528366367,9.188044796421696,3.8439838440483265,4.634012356402116,4.8104431038451745,8.686815644595628,2.521050736900963,4.016139702655253,6.212180210044431,7.545505100691072,3.904002316283692,3.71699089440494,2.1667154449664223,4.381283372503783,6.564835417010355,6.563005553062399,5.911212201558612,6.247547806890298,5.477677327565307,8.292781749227846,13.587327482778312,2.9011082430145123,2.845991770664573,3.7644735509926663,9.182369521950886,5.921959917326167,1.9297909977185974,6.910972451655129,8.149950301681047,4.3729520979118295,5.38404980679516,2.063502942306158,5.977508904540138,1.3673710656485294,4.067810783789621,2.1176950426697547,9.199893148608071,3.7081872360207084,4.628773595201645,2.488000770834068,6.501439145158873,6.706254253573203,3.0908534304511135,5.86616616903904,4.155425431747197,6.060695931687554,4.570462931026041,3.2357270598380583,5.341274183692174,6.656925339794245,5.588564737401351,6.353146825498083,3.48155728070859,2.4698859762744636,4.872828759534886,4.34766565630097,2.877744249949002,6.019479533703628,3.465974464504069,7.536363792395467,5.132576842670009,2.0531113364595623,6.283366212949919,3.699329525673156,8.199279721336625,6.8203066288938246,6.068240861312821,4.598126959919604,7.979739567128753,6.258895755075436,7.258707352196828,5.394376944957055,3.0125686735030555,6.862203399053224,2.8318772411916733,5.164705840182799,6.431288654291593,4.375734538583156,4.8594726668519375,2.9392265777282085,4.302319050948834,4.397118409042582,5.169123281476757,3.761285273361619,7.240600549955795,3.7387678368001915,5.678916986679839,4.3334237337251915,5.530445354692906,8.431288654291592,2.521050736900963,1.2265085298086797,2.2539892662307865,5.23687524536694,2.298658315564515,6.398316163400203,8.348374075191163,6.043300754364182,9.401604869714957,4.534186139090097,9.069933046375542,5.486714373030702,4.96624587322494,6.307246355226617,5.51127826508303,5.970853654340483,5.342341397431548,6.219555769166913,2.319039815562536,7.516566771496865,6.652773284510777,9.109908988534402,5.338067797542616,2.9981955031532523,6.506843098969815,3.04788732939655,2.526068811667588,3.414135532984451,6.295723024539968,2.257010618206024,5.500164679492167,5.333065878076355,5.253232938538032,3.619413010597937,4.695437110405369,4.37364821133469,8.356407951394893,6.039796407620025,5.908332569782425,1.74416109557041,5.666472568842073,5.9586107120558465,2.430285272977781,8.487277313213783,2.927896453728821,6.9920886090139955,5.7615512324444795,5.906409617094016,2.572889668420581,2.9763636357327616,3.311793717753649,5.325530331567558,6.075532631167357,8.785648628652076,5.301953395126096,6.687620632084895,10.60719158640291,5.2543672814408335,6.976821852360685,2.655351828612554,8.033092575523664,11.658760423755194,6.231317197152663,5.008092420948722,7.824449651364088,2.8155754288625725,5.656496370553495,3.218781167784069,6.101187815259738,8.328001662149257,5.576522137920503,7.176621973161577,2.0565835283663674,6.5787877956769725,6.602142090697982,2.7441610955704103,4.727375938594897,2.2172307162206693,4.552131108253784,10.754979337466484,5.651338766105931,7.449148645375437,8.21179097625254,5.769771739249448,7.09876903815488,2.5921580021253603,7.971830914780502,6.234194722953812,2.1984941536390834,6.955824240806441,0.16349873228287956,4.680324356844016,2.5801454844233804,5.583458910130768,4.845991770664573,2.7070829917717063,4.18824265626441,7.27733390042982,9.535217045453418,6.304876050044889,6.982080909763738,3.0925457415435655,7.051372101721025,7.022922589055629,2.9392265777282085,6.653203362033977,8.22838581704935,4.960697039304312,5.564378168565064,2.782408564927373,0,3.9354597478052895,5.525755692829486,1.0976107966264221,7.211790976252541,4.477677327565307,2.4436066514756147,7.966303561412599,6.19021976366016,5.592457037268081,10.665637927637095,3.277984747299765,7.8269297659895765,3.5643781685650637,5.612057559563065,8.766462652425766,2.662205499653617,6.920412408160787,5.290940402403678,7.502871574574138,5.2539892662307865,3.1984941536390834,3.0089887832272546,7.2259304111710545,0,4.919340082442012,5.334496768390418,4.654206377944292,2.327687364176047,6.705839705883682,11.02948049374083,2.4594316186372973,6.294253136444514,7.172927489603836,5.573192723781613,5.5804470195986635,6.341274183692174,6.768713657030485,4.5855634984556914,5.609400390006429,6.10055722078293,6.140165134599453,2.37851162325373,4.726831217032493,3.127633279725874,2.414135532984451,4.561326136453918,2.211012193485512,3.6147098441152083,5.901832305785489,5.903279342056139,3.84197311892718,3.0443941193584534,4.77663042305105,2.7949356628035362,2.5897634869849773,2.4436066514756147,2.5160151470036647,4.623515741490549,6.6059979802325195,9.051127353591394,3.017921907997262,3.9212458885855885,2.2387868595871168,5.621465834460144,6.001351892999959,6.605701738949982,2.6530600171045644,3.4698859762744636,2.456806149230474,4.528571318870758,10.08413071995014,3.1953475983222193,6.061128135118254,4.434961760187254,8.556735947914529,6.821837746088096,5.895302621333307,4.783456654360239,6.1147834472498515,8.8634739090894,2.6182386555954547,5.862203399053224,3.319039815562536,6.531225485053606,2.2265085298086795,5.50303064564116,6.549977142771825,2.582556003014061,0.45417589318580215,8.258094872866513,5.864186144654281,0,4.110196177754199,5.541948425333733,5.139960569545456,5.166715444966422,4.661635602335959,2.6016965164809576,1.0565835283663676,4.867402305727543,6.390082950951663,6.790641730763425,4.185073637876762,5.681168120416691,4.409390936137702,5.399854673572431,2.752748591407134,5.553974808237472,4.238786859587116,6.451211111832329,3.2555007331483865,8.627898615871748,4.5588798534754424,5.639521599510833,5.29020320512607,8.07804403634318,5.592756010441026,6.952217064973338,2.6392321632492775,3.2942531364445142,5.633721812641012,9.325777655214823,5.554588851677638,8.364703739582406,12.206535173103617,6.2268938135713885,6.058532970201611,2.996388746447621,2.169925001442312,3.2539892662307865,2.3645724322958563,2.996388746447621,2.5084286525318573,5.370861740085285,7.2461230165968376,3.19377174339668,2.4276061727818994,5.25285462595139,8.366191054189278,7.062963556559086,2.0531113364595623,7.846744023244868,3.581351247168777,6.948834425578686,6.412442824775613,2.899175630480513,4.786073551578548,8.971342367308225,7.2193621578077165,2.885574364371426,7.801029283808189,3.7813597135246595,5.6884601404244926,3.361768359419153,2.280956313831056,8.531927241876955,7.295631200395783,5.566206293465506,3.9990980337056072,6.145066014185894,3.0373822220030804,6.666614587352882,2.655351828612554,5.343763122149601,6.696689389537669,4.609991295212678,2.3950627995175777,7.005961325845812,2.2957230245399685,3.7548875021634687,6.883987308105472,3.2433644256936605],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_pred_log_logRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle RandomForestRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsRF_log = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF_log = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF_log,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF_log, \\\n","BestParametresRF_log, \\\n","ScoresRF_log, \\\n","TotalGHGEmissions_pred_logRF_log, \\\n","figRF_log = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log.ravel(),\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_pred_log_logRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF_log)\n","print(ScoresRF_log)\n","figRF_log.show()\n"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.8488129348849516,1.5903425849610888,1.4542083083287625,1.3484849538713444,1.3189227604439486,1.300383324414776,1.300784462433422,1.2964228289843676,1.2941290865954072,1.2920487018047393]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.899566954868825,1.6334244991101825,1.4813816404449176,1.3567936614869927,1.3307576756462018,1.314004480498304,1.315957138027594,1.312589164760712,1.304198061487071,1.3048093601037407]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[1.7980589149010782,1.5472606708119951,1.4270349762126073,1.340176246255696,1.3070878452416954,1.286762168331248,1.2856117868392498,1.2802564932080231,1.2840601117037433,1.2792880435057379]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.845542326055504,1.6396109949010038,1.4857556797579723,1.3478593174547149,1.309570406990059,1.298773699351253,1.2918586771509029,1.2889686030226104,1.2878066169379774,1.2845230998379702],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.7946649217355053,1.5263366232896585,1.473495893711545,1.3377113401904581,1.323528472979864,1.3087595847353608,1.3071765412009049,1.306724585016115,1.2997356738620445,1.2991907223933308],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.9140932737376364,1.6310694552126734,1.4117370846496382,1.3498230699350415,1.3134656689974618,1.2942853334951157,1.2873891201041232,1.276473386762351,1.281593947058433,1.2772350892279536],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.7917561760997038,1.5578579669119872,1.4344392444401255,1.344126506698316,1.3080588967337725,1.2798312155277596,1.289719803808373,1.2876411214516958,1.290992699198297,1.2860162975636422],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[1.8980079767964075,1.5968378844901219,1.4656136390845313,1.3629045350781919,1.3399903565185856,1.3202667889643909,1.327778169902805,1.3223064486690645,1.3105164959202842,1.3132783000007995],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre<br>randomforestregressor__max_features=auto<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE RandomForestRegressor\n","# pour le meilleur paramètre max features\n","FigRMSEGRidRF_log = visuRMSEGrid(RandomForestRegressor(), 'RF',\n","                                 n_estimatorsRF_log, 'n estimators',\n","                                 GridRF_log, BestParametresRF_log,\n","                                 'randomforestregressor__max_features')\n","FigRMSEGRidRF_log.show()\n","if write_data is True:\n","    FigRMSEGRidRF_log.write_image('./Figures/EmissionsGraphRMSERF_log.pdf')\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2.7 Modèle AdaboostRegressor"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         paramètre AdaBoostRegressor()\n","0  adaboostregressor__n_estimators                   6\n","1          adaboostregressor__loss         exponential\n","      AdaBoostRegressor()\n","R²               0.438320\n","RMSE             1.561140\n","MAE              1.280275\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"TotalGHGEmissions_predAB_log=%{x}<br>TotalGHGEmissions_test_log=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[4.409575735697215,3.9847313251863405,4.503196188718829,3.7585638894729305,5.283296497963151,4.924140471506023,3.9847313251863405,4.503196188718829,6.192601903759075,3.7585638894729305,5.211643197410187,4.409575735697215,4.293833752772434,5.165035297481078,4.293833752772434,5.778156997308803,4.409575735697215,4.504567349914338,5.283296497963151,5.594881113962358,4.293833752772434,4.504567349914338,3.7585638894729305,4.504567349914338,5.283296497963151,5.165035297481078,6.96075595505999,4.504567349914338,5.165035297481078,4.293833752772434,6.192601903759075,5.165035297481078,5.594881113962358,5.283296497963151,5.165035297481078,5.211643197410187,9.66622749851968,4.475519765442538,5.163142389294744,4.409575735697215,5.618575918388373,4.503196188718829,4.293833752772434,6.18977612590711,5.211643197410187,4.503196188718829,5.163142389294744,5.211643197410187,4.293833752772434,4.573662786751511,3.7239667426766574,3.7585638894729305,8.266654263550821,5.594881113962358,4.504567349914338,5.163142389294744,7.085501510816962,3.9847313251863405,6.192601903759075,5.163142389294744,6.395823386251884,6.96075595505999,4.293833752772434,6.9937992619773945,6.192601903759075,3.7239667426766574,5.211643197410187,5.594881113962358,3.7239667426766574,4.475519765442538,4.293833752772434,6.192601903759075,4.409575735697215,4.293833752772434,5.618575918388373,6.192601903759075,8.01798504096251,4.503196188718829,5.165035297481078,4.409575735697215,5.165035297481078,6.96075595505999,5.778156997308803,6.96075595505999,3.7585638894729305,3.7239667426766574,4.475519765442538,4.772912312258713,6.192601903759075,7.946111769316279,4.503196188718829,4.504567349914338,6.192601903759075,3.7585638894729305,4.504567349914338,5.283296497963151,10.08536655162553,6.192601903759075,5.165035297481078,4.503196188718829,6.192601903759075,4.503196188718829,5.211643197410187,4.504567349914338,5.778156997308803,4.503196188718829,5.165035297481078,7.085501510816962,3.9847313251863405,6.192601903759075,4.924140471506023,4.503196188718829,3.9847313251863405,3.7239667426766574,4.293833752772434,4.409575735697215,3.7585638894729305,8.266654263550821,6.192601903759075,6.192601903759075,3.7239667426766574,4.409575735697215,4.293833752772434,7.946111769316279,5.283296497963151,5.165035297481078,3.7585638894729305,4.503196188718829,3.7239667426766574,6.96075595505999,6.96075595505999,3.7585638894729305,3.9847313251863405,4.409575735697215,6.96075595505999,4.409575735697215,4.503196188718829,5.165035297481078,6.192601903759075,4.293833752772434,3.7585638894729305,7.085501510816962,6.192601903759075,4.924140471506023,4.503196188718829,5.165035297481078,3.7585638894729305,4.772912312258713,4.504567349914338,4.293833752772434,6.18977612590711,5.283296497963151,4.293833752772434,4.503196188718829,4.504567349914338,4.504567349914338,4.573662786751511,3.9847313251863405,8.01798504096251,5.165035297481078,3.7239667426766574,3.7239667426766574,4.293833752772434,4.504567349914338,4.503196188718829,4.409575735697215,8.01798504096251,6.143062805227293,5.211643197410187,5.283296497963151,4.503196188718829,3.7585638894729305,4.924140471506023,5.165035297481078,4.293833752772434,6.9937992619773945,4.293833752772434,6.192601903759075,3.7239667426766574,3.7239667426766574,5.594881113962358,6.9937992619773945,3.7239667426766574,5.165035297481078,3.7585638894729305,6.192601903759075,3.7239667426766574,4.475519765442538,3.7585638894729305,6.18977612590711,5.165035297481078,5.283296497963151,4.503196188718829,6.192601903759075,3.7585638894729305,5.618575918388373,4.504567349914338,4.503196188718829,4.924140471506023,5.283296497963151,6.96075595505999,3.9847313251863405,3.7585638894729305,3.7585638894729305,3.7585638894729305,3.7239667426766574,3.9847313251863405,4.293833752772434,5.165035297481078,5.165035297481078,7.120643219576693,4.293833752772434,3.9847313251863405,7.085501510816962,5.165035297481078,8.266654263550821,5.165035297481078,3.7585638894729305,6.9937992619773945,3.7585638894729305,4.475519765442538,5.165035297481078,4.293833752772434,4.475519765442538,3.7585638894729305,3.9847313251863405,5.594881113962358,4.409575735697215,4.503196188718829,4.772912312258713,3.9847313251863405,5.165035297481078,4.293833752772434,5.594881113962358,3.9847313251863405,3.7239667426766574,4.409575735697215,4.475519765442538,6.96075595505999,4.503196188718829,6.192601903759075,3.9847313251863405,3.7585638894729305,3.7239667426766574,4.475519765442538,4.409575735697215,6.9937992619773945,4.573662786751511,7.120643219576693,5.163142389294744,4.924140471506023,4.475519765442538,5.283296497963151,5.165035297481078,6.192601903759075,5.165035297481078,5.283296497963151,5.618575918388373,5.211643197410187,4.503196188718829,5.165035297481078,4.409575735697215,4.924140471506023,4.924140471506023,3.7239667426766574,6.192601903759075,3.7585638894729305,6.192601903759075,4.293833752772434,4.503196188718829,5.165035297481078,5.594881113962358,4.924140471506023,4.772912312258713,4.293833752772434,5.283296497963151,6.18977612590711,9.551148878163271,6.192601903759075,4.573662786751511,5.165035297481078,4.503196188718829,4.293833752772434,6.395823386251884,5.594881113962358,5.163142389294744,4.504567349914338,4.573662786751511,4.409575735697215,4.573662786751511,8.266654263550821,3.7585638894729305,4.503196188718829,4.293833752772434,5.165035297481078,6.192601903759075,3.7239667426766574,5.165035297481078,4.504567349914338,3.7585638894729305,5.594881113962358,7.085501510816962,5.618575918388373,4.409575735697215,7.946111769316279,5.283296497963151,7.085501510816962,3.7585638894729305,5.165035297481078,4.772912312258713,3.7585638894729305,4.293833752772434,4.503196188718829,6.9937992619773945,4.293833752772434,4.504567349914338,5.778156997308803,5.165035297481078,3.7585638894729305,4.503196188718829,6.18977612590711,7.085501510816962,4.293833752772434,4.504567349914338,4.503196188718829,3.9847313251863405,4.503196188718829,5.165035297481078,8.266654263550821,4.504567349914338,4.772912312258713,3.9847313251863405,4.503196188718829,6.395823386251884,6.18977612590711,5.165035297481078,7.085501510816962,6.96075595505999,5.165035297481078,3.7585638894729305,3.9847313251863405,3.7239667426766574,4.409575735697215,4.503196188718829,5.778156997308803,5.778156997308803,4.504567349914338,4.573662786751511,4.293833752772434,3.9847313251863405,4.293833752772434,4.409575735697215,3.7239667426766574,5.165035297481078,6.192601903759075,8.266654263550821,6.9937992619773945,8.266654263550821,3.9847313251863405,6.192601903759075,4.503196188718829,4.503196188718829,4.503196188718829,6.192601903759075,3.9847313251863405,4.293833752772434,6.96075595505999,5.165035297481078,6.96075595505999,5.283296497963151,5.163142389294744,4.503196188718829,6.192601903759075,6.9937992619773945,6.9937992619773945,4.504567349914338,5.594881113962358,4.924140471506023,6.192601903759075,5.283296497963151,3.9847313251863405,4.475519765442538,6.96075595505999,5.618575918388373,4.293833752772434,6.96075595505999,5.165035297481078,4.475519765442538,6.96075595505999,4.503196188718829,5.283296497963151,4.504567349914338,5.165035297481078,4.503196188718829,4.409575735697215,4.503196188718829,4.504567349914338,3.9847313251863405,3.7239667426766574,4.503196188718829,4.323773874187903,6.18977612590711,3.7239667426766574,4.504567349914338,4.503196188718829,4.503196188718829,4.504567349914338,5.594881113962358,4.504567349914338,5.165035297481078,4.503196188718829,4.924140471506023,4.503196188718829,6.192601903759075,5.165035297481078,3.7239667426766574,4.504567349914338,4.503196188718829,6.395823386251884,6.143062805227293,5.165035297481078,3.7585638894729305,3.9847313251863405,4.503196188718829,5.163142389294744,5.165035297481078,9.66622749851968,4.503196188718829,4.503196188718829,4.475519765442538,4.503196188718829,6.192601903759075,5.618575918388373,10.08536655162553,8.01798504096251,3.7585638894729305,8.01798504096251,4.293833752772434,4.503196188718829,4.293833752772434,4.504567349914338,3.7585638894729305,3.9847313251863405,3.7239667426766574,8.01798504096251,5.165035297481078,5.165035297481078,3.7585638894729305,4.503196188718829,5.163142389294744,4.503196188718829,4.504567349914338,4.293833752772434,6.143062805227293,5.594881113962358,5.165035297481078,3.7239667426766574,6.96075595505999,4.293833752772434,3.7239667426766574,4.504567349914338,4.503196188718829,6.192601903759075,4.293833752772434,7.085501510816962,4.475519765442538,4.409575735697215,3.9847313251863405,4.293833752772434,5.211643197410187,3.7585638894729305,5.283296497963151,8.266654263550821,6.96075595505999,3.7239667426766574,4.409575735697215,6.192601903759075,5.594881113962358,4.293833752772434,5.165035297481078,5.165035297481078,5.165035297481078,4.503196188718829,4.924140471506023,5.165035297481078,4.293833752772434,5.283296497963151,4.293833752772434,6.96075595505999,5.165035297481078,4.409575735697215,4.409575735697215,4.475519765442538,3.9847313251863405,4.573662786751511,7.946111769316279,4.772912312258713,4.573662786751511,8.266654263550821,6.192601903759075,5.283296497963151,6.96075595505999,4.924140471506023,4.409575735697215,6.96075595505999,6.192601903759075,4.924140471506023,7.085501510816962,5.618575918388373,3.9847313251863405,3.7585638894729305,7.085501510816962,5.165035297481078,4.504567349914338,6.192601903759075,3.7585638894729305,3.7239667426766574,8.428749179186013,3.9847313251863405,5.165035297481078,6.395823386251884,8.01798504096251,4.475519765442538,4.293833752772434,4.772912312258713,3.7239667426766574,5.165035297481078,4.503196188718829,3.7239667426766574,4.504567349914338,5.211643197410187,5.165035297481078,4.503196188718829,3.7239667426766574,4.924140471506023,3.7585638894729305,4.475519765442538,3.7585638894729305,5.618575918388373,4.503196188718829,4.503196188718829,5.165035297481078,4.409575735697215,4.924140471506023,3.7585638894729305,4.504567349914338,5.211643197410187,5.165035297481078,3.9847313251863405,6.9937992619773945,6.192601903759075,4.409575735697215,5.778156997308803,5.165035297481078,3.7239667426766574,3.9847313251863405,4.503196188718829,5.618575918388373,4.293833752772434,6.18977612590711,4.409575735697215,5.211643197410187,5.618575918388373,5.618575918388373,6.96075595505999,5.283296497963151,5.618575918388373,8.266654263550821,5.165035297481078,4.503196188718829,3.9847313251863405,8.01798504096251,3.7239667426766574,3.7585638894729305,8.01798504096251,3.9847313251863405,4.503196188718829,3.7239667426766574,4.503196188718829,4.924140471506023,5.594881113962358,4.409575735697215,4.503196188718829,4.504567349914338,3.9847313251863405,6.192601903759075,4.504567349914338,4.503196188718829,6.192601903759075,3.7239667426766574,5.283296497963151,6.395823386251884,5.165035297481078,8.01798504096251,5.211643197410187,4.924140471506023,8.01798504096251,4.504567349914338,6.18977612590711,5.165035297481078,6.192601903759075,3.7585638894729305,6.192601903759075,8.266654263550821,4.409575735697215,8.266654263550821,4.293833752772434,6.96075595505999,4.293833752772434,4.409575735697215,4.503196188718829,3.7239667426766574,4.293833752772434,5.165035297481078,8.266654263550821,6.192601903759075,3.9847313251863405,5.618575918388373,4.293833752772434,5.165035297481078,6.192601903759075,3.7239667426766574,5.165035297481078,4.504567349914338,4.293833752772434,3.7239667426766574,5.165035297481078,4.503196188718829,4.293833752772434,3.9847313251863405,6.96075595505999,3.9847313251863405,4.475519765442538,6.192601903759075,3.9847313251863405,5.165035297481078,7.085501510816962,4.504567349914338,5.594881113962358,4.573662786751511,7.393236219418359,10.08536655162553,4.504567349914338,3.7239667426766574,4.503196188718829,5.211643197410187,3.7239667426766574,4.772912312258713,5.165035297481078,4.409575735697215,5.778156997308803,7.085501510816962,5.594881113962358,4.503196188718829,3.9847313251863405,5.594881113962358,5.165035297481078,4.409575735697215,6.192601903759075,4.293833752772434,4.504567349914338,4.504567349914338,5.165035297481078,5.165035297481078,5.594881113962358,3.7585638894729305,3.9847313251863405,5.594881113962358,3.9847313251863405,4.409575735697215,4.503196188718829,4.475519765442538,5.165035297481078,4.503196188718829,4.503196188718829,6.96075595505999,10.08536655162553,5.165035297481078,5.211643197410187,3.9847313251863405,3.9847313251863405,4.924140471506023,3.9847313251863405,5.163142389294744,5.211643197410187,4.409575735697215,7.946111769316279,5.778156997308803,5.163142389294744,5.618575918388373,3.7585638894729305,6.192601903759075,3.7585638894729305,4.503196188718829,6.192601903759075,6.192601903759075,8.266654263550821,6.192601903759075,3.7239667426766574,4.293833752772434,4.409575735697215,5.778156997308803,4.409575735697215,4.409575735697215,3.7585638894729305,3.9847313251863405,4.503196188718829,4.409575735697215,3.7585638894729305,6.18977612590711,4.409575735697215,10.08536655162553,4.475519765442538,9.66622749851968,5.211643197410187,3.7239667426766574,3.9847313251863405,4.503196188718829,4.293833752772434,7.085501510816962,7.085501510816962,6.395823386251884,4.409575735697215,4.409575735697215,6.18977612590711,5.165035297481078,3.7239667426766574,4.475519765442538,3.7585638894729305,8.266654263550821,4.293833752772434,3.7585638894729305,8.266654263550821,5.211643197410187,4.924140471506023,6.192601903759075,7.946111769316279,3.7239667426766574,4.503196188718829,5.283296497963151,3.9847313251863405,4.503196188718829,4.924140471506023,6.192601903759075,6.18977612590711,6.192601903759075,6.395823386251884,4.504567349914338,4.409575735697215,6.192601903759075,6.9937992619773945,8.01798504096251,3.7585638894729305,8.266654263550821,4.504567349914338,4.772912312258713,4.503196188718829,3.7239667426766574,6.192601903759075,5.618575918388373,3.7239667426766574,6.192601903759075,3.7585638894729305,3.7585638894729305,3.7585638894729305,4.503196188718829,5.165035297481078,6.192601903759075,6.192601903759075,4.409575735697215,5.594881113962358,3.7239667426766574,3.9847313251863405,4.503196188718829,4.409575735697215,4.409575735697215,3.7585638894729305,4.475519765442538,6.395823386251884,8.266654263550821,5.211643197410187,4.924140471506023,3.7239667426766574,4.475519765442538,5.618575918388373,5.594881113962358,5.618575918388373,4.504567349914338,3.7239667426766574,4.503196188718829,6.192601903759075,4.503196188718829,4.573662786751511,4.503196188718829,7.085501510816962,5.165035297481078,4.293833752772434,4.409575735697215,9.66622749851968,5.165035297481078,4.924140471506023,3.7239667426766574,5.211643197410187,4.503196188718829,3.9847313251863405,5.283296497963151,4.293833752772434,3.9847313251863405,5.165035297481078,5.165035297481078,5.283296497963151,6.96075595505999,3.7585638894729305,4.503196188718829,3.7239667426766574,5.283296497963151,3.9847313251863405,6.395823386251884,4.409575735697215,4.573662786751511,5.165035297481078,8.266654263550821,3.7239667426766574,5.283296497963151,4.503196188718829,4.573662786751511,3.7585638894729305,7.085501510816962,6.192601903759075,4.475519765442538,6.96075595505999,7.085501510816962,5.283296497963151,3.9847313251863405,4.573662786751511,3.9847313251863405,4.504567349914338,4.504567349914338,4.504567349914338,10.08536655162553,4.409575735697215,3.9847313251863405,5.778156997308803,6.192601903759075,7.085501510816962,4.409575735697215,4.293833752772434,4.293833752772434,6.96075595505999,3.7585638894729305,4.409575735697215,4.293833752772434,5.618575918388373,4.293833752772434,5.211643197410187,4.409575735697215,4.293833752772434,5.211643197410187,8.266654263550821,3.7585638894729305,4.504567349914338,4.503196188718829,6.395823386251884,5.165035297481078,4.293833752772434,3.7239667426766574,8.01798504096251,5.594881113962358,4.503196188718829,5.594881113962358,4.293833752772434,5.211643197410187,6.18977612590711,7.085501510816962,4.503196188718829,4.573662786751511,4.503196188718829,3.7239667426766574,4.293833752772434,4.772912312258713,6.96075595505999,6.192601903759075,4.503196188718829,5.165035297481078,3.9847313251863405,8.266654263550821,4.503196188718829,5.165035297481078,3.9847313251863405,4.409575735697215,4.504567349914338,4.504567349914338,3.7585638894729305,4.504567349914338,6.395823386251884,4.293833752772434,3.7585638894729305,5.778156997308803,4.772912312258713,3.9847313251863405,6.143062805227293,3.9847313251863405,3.7239667426766574,4.475519765442538,8.01798504096251,5.594881113962358,6.192601903759075,3.7585638894729305,4.293833752772434,6.192601903759075,5.165035297481078,4.475519765442538,3.7585638894729305,9.66622749851968,4.504567349914338,5.618575918388373,4.503196188718829,4.293833752772434,4.503196188718829,8.266654263550821,7.120643219576693,4.924140471506023,4.504567349914338,9.66622749851968,4.293833752772434,8.01798504096251,5.594881113962358,6.143062805227293,4.409575735697215,8.266654263550821,3.7239667426766574,4.503196188718829,4.293833752772434,4.504567349914338,4.504567349914338,4.475519765442538,4.503196188718829,8.266654263550821,4.409575735697215,6.192601903759075,4.475519765442538,6.192601903759075,4.293833752772434,3.7239667426766574,6.192601903759075,7.120643219576693,5.211643197410187,5.165035297481078,4.409575735697215,6.192601903759075,5.165035297481078,4.573662786751511,3.9847313251863405,5.594881113962358,5.211643197410187,7.946111769316279,10.08536655162553,4.504567349914338,3.9847313251863405,4.503196188718829,8.266654263550821,5.165035297481078,3.7239667426766574,5.594881113962358,7.085501510816962,6.192601903759075,3.9847313251863405,3.7239667426766574,7.085501510816962,4.772912312258713,5.283296497963151,3.7239667426766574,8.266654263550821,4.293833752772434,5.165035297481078,3.9847313251863405,5.165035297481078,3.9847313251863405,4.924140471506023,3.7585638894729305,6.18977612590711,4.504567349914338,5.594881113962358,4.293833752772434,5.594881113962358,4.503196188718829,4.573662786751511,5.618575918388373,3.7239667426766574,3.7585638894729305,6.9937992619773945,4.503196188718829,4.504567349914338,7.085501510816962,4.504567349914338,8.01798504096251,4.409575735697215,4.475519765442538,4.409575735697215,5.165035297481078,6.192601903759075,5.211643197410187,5.283296497963151,4.409575735697215,8.01798504096251,4.293833752772434,4.573662786751511,4.573662786751511,3.7585638894729305,6.192601903759075,3.9847313251863405,5.594881113962358,5.163142389294744,3.7239667426766574,4.475519765442538,4.503196188718829,3.7239667426766574,3.7239667426766574,4.924140471506023,3.9847313251863405,7.085501510816962,4.409575735697215,6.192601903759075,4.293833752772434,4.573662786751511,5.936529318612487,3.7585638894729305,3.7585638894729305,3.9847313251863405,6.18977612590711,4.924140471506023,5.165035297481078,6.96075595505999,5.778156997308803,8.266654263550821,5.618575918388373,8.01798504096251,4.924140471506023,5.165035297481078,5.165035297481078,5.211643197410187,5.165035297481078,3.7239667426766574,4.503196188718829,4.293833752772434,4.503196188718829,5.283296497963151,7.085501510816962,5.165035297481078,3.7585638894729305,5.618575918388373,5.594881113962358,4.293833752772434,4.503196188718829,6.192601903759075,3.9847313251863405,4.573662786751511,5.165035297481078,6.18977612590711,5.165035297481078,5.165035297481078,5.211643197410187,5.594881113962358,5.594881113962358,5.165035297481078,4.503196188718829,3.7239667426766574,4.504567349914338,3.7239667426766574,6.192601903759075,4.503196188718829,5.283296497963151,4.409575735697215,6.96075595505999,4.409575735697215,4.504567349914338,4.504567349914338,3.7585638894729305,4.772912312258713,9.66622749851968,4.293833752772434,5.283296497963151,9.66622749851968,4.924140471506023,5.778156997308803,3.7239667426766574,6.9937992619773945,10.08536655162553,5.778156997308803,4.293833752772434,4.503196188718829,4.503196188718829,3.9847313251863405,4.504567349914338,5.618575918388373,8.01798504096251,3.7585638894729305,6.18977612590711,3.7239667426766574,8.266654263550821,5.618575918388373,4.409575735697215,3.7239667426766574,4.293833752772434,4.503196188718829,8.01798504096251,8.01798504096251,4.409575735697215,6.192601903759075,5.165035297481078,7.120643219576693,3.7239667426766574,7.946111769316279,4.293833752772434,3.9847313251863405,6.395823386251884,4.409575735697215,4.475519765442538,4.293833752772434,3.9847313251863405,4.293833752772434,4.503196188718829,5.211643197410187,4.504567349914338,8.266654263550821,6.18977612590711,5.165035297481078,4.503196188718829,5.163142389294744,6.96075595505999,4.504567349914338,5.283296497963151,4.503196188718829,4.924140471506023,5.165035297481078,4.772912312258713,6.192601903759075,3.9847313251863405,5.165035297481078,4.409575735697215,4.573662786751511,4.503196188718829,3.7239667426766574,7.120643219576693,5.594881113962358,5.165035297481078,6.96075595505999,4.503196188718829,7.120643219576693,5.165035297481078,4.293833752772434,6.96075595505999,4.503196188718829,4.503196188718829,5.283296497963151,5.594881113962358,5.594881113962358,5.165035297481078,3.7585638894729305,5.165035297481078,10.08536655162553,4.293833752772434,5.594881113962358,4.293833752772434,3.9847313251863405,6.192601903759075,10.08536655162553,3.7239667426766574,4.504567349914338,5.618575918388373,4.503196188718829,4.924140471506023,4.293833752772434,6.143062805227293,3.9847313251863405,6.192601903759075,5.211643197410187,4.503196188718829,3.7239667426766574,5.778156997308803,4.503196188718829,3.7239667426766574,4.504567349914338,3.9847313251863405,5.165035297481078,4.503196188718829,4.409575735697215,5.283296497963151,4.924140471506023,4.924140471506023,3.7239667426766574,3.9847313251863405,3.7585638894729305,4.293833752772434,5.283296497963151,6.143062805227293,7.946111769316279,4.504567349914338,4.503196188718829,3.9847313251863405,4.772912312258713,4.573662786751511,5.594881113962358,3.7239667426766574,5.165035297481078,4.503196188718829,6.192601903759075,4.924140471506023,5.165035297481078,3.9847313251863405,4.475519765442538,6.96075595505999,4.409575735697215,8.01798504096251,3.9847313251863405,5.211643197410187,6.96075595505999,3.7585638894729305,3.9847313251863405,4.504567349914338,5.165035297481078,3.7239667426766574,5.618575918388373,6.192601903759075,5.165035297481078,4.772912312258713,5.618575918388373,4.475519765442538,4.503196188718829,4.409575735697215,5.778156997308803,4.409575735697215,5.594881113962358,4.504567349914338,3.9847313251863405,3.7239667426766574,6.96075595505999,3.9847313251863405,6.192601903759075,4.293833752772434,6.192601903759075,5.778156997308803,5.165035297481078,3.7585638894729305,4.293833752772434,4.293833752772434,6.143062805227293,6.192601903759075,8.01798504096251,6.18977612590711,5.165035297481078,5.594881113962358,4.504567349914338,5.165035297481078,6.96075595505999,3.9847313251863405,3.7585638894729305,5.594881113962358,6.96075595505999,4.293833752772434,7.085501510816962,9.66622749851968,5.594881113962358,4.293833752772434,4.503196188718829,3.7239667426766574,4.772912312258713,3.9847313251863405,4.503196188718829,3.9847313251863405,4.504567349914338,6.9937992619773945,5.618575918388373,4.409575735697215,3.7239667426766574,7.085501510816962,7.085501510816962,3.7239667426766574,6.192601903759075,5.594881113962358,8.266654263550821,6.192601903759075,3.7585638894729305,4.409575735697215,10.08536655162553,4.772912312258713,4.409575735697215,6.192601903759075,4.293833752772434,5.165035297481078,5.165035297481078,3.9847313251863405,5.618575918388373,6.143062805227293,5.211643197410187,4.409575735697215,4.503196188718829,4.503196188718829,6.9937992619773945,3.7239667426766574,5.165035297481078,3.7239667426766574,6.192601903759075,3.7239667426766574,6.18977612590711,3.7239667426766574,4.293833752772434,5.211643197410187,4.409575735697215],"xaxis":"x","y":[4.9676297534661975,5.60584986719498,2.950468414150123,2.7355221772965375,7.258707352196828,3.0925457415435655,4.178714641175443,3.4154882710497003,6.224387627929912,2.634593268445757,7.244982170376553,5.597531174224212,4.210232990095849,3.594548549550354,2.526068811667588,6.215484464192794,4.969933274697856,5.912649864897204,5.111865963867557,5.042644337408494,7.864928972289788,3.382667252745041,3.189033824390017,5.705977901682522,3.9717734471933728,5.119771161952904,3.496973580998276,5.058749412335524,5.968551603363945,3.1160319934471103,4.464668267003444,5.192194165283345,6.73226919950145,5.39746072605472,8.171076693915895,6.43095427138573,7.132782457298859,4.294988267691446,4.708739041359579,3.4646682670034443,5.450881315273369,4.95093492831454,4.20006486151431,8.047505671251574,3.605257262939004,3.261530815243406,9.159820892013935,3.8728287595348854,3.8298495598446904,1.9671686075326276,2.9126498648972037,5.648177795724818,10.257022407964342,4.887037791419197,5.718087583960517,7.540941686758214,7.156841524588401,2.0942360698457656,6.074462620704536,6.428778891337586,6.464504909131913,6.588864518190148,5.629648044187436,4.795974694206668,5.676380255466924,2.811471030529836,3.9882302230604574,8.45314714555533,2.321928094887362,3.980939266085512,2.5801454844233804,4.41886457739032,2.8972404255747994,4.341985747228616,6.561784353081841,5.751410160064371,8.545157551729474,2.7676547982373463,3.773996325111173,2.7676547982373463,3.258518924711301,5.596637034102074,7.019368330478469,6.649471757181635,5.275007047499869,2.0071955014042038,3.700439718141092,5.320845667645722,5.331633567171029,8.061668207389335,2.786596361890807,4.94673086014031,5.649040565634448,2.269033146455237,3.292781749227846,3.683696454306516,13.390111171989,8.064904402198286,3.727920454563199,2.8155754288625725,4.253232938538032,3.0373822220030804,6.01992426090841,3.2524762141352173,7.125981653854716,4.273515889702116,4.92552476974757,7.778274295338579,6.135452784185173,9.20006486151431,3.6971065744769747,3.2418401835646704,2.4222330006830477,2.6229303509201767,5.57198012018773,5.0539801681876515,2.6915341649192004,7.510803720358323,3.01435529297707,6.045049740837989,2.7803100990433753,4.658211482751795,4.595145567990858,7.485507332459785,5.342341397431548,4.129283016944966,2.454175893185802,5.96254902292306,2.3305584000308026,3.292781749227846,6.290756138396673,2.817623257511431,5.532940288372874,3.2883585621936606,8.008820757716158,1.6644828403646825,5.284292026394312,3.598126959919604,6.237448995639321,4.222650022451478,4.380590934265922,8.669593751188334,7.720552091605108,5.548744859937229,1.7484612330040354,4.524815928357506,7.639376888638463,2.851998837112446,3.218781167784069,2.7990873060740036,3.868884273028774,8.043027283594547,7.260778431893426,4.038260575175349,3.040892430646901,2.7949356628035362,5.4741115139424075,5.952799477899938,7.323370069061268,5.040892430646901,3.4208865749755315,6.136683577697236,4.255500733148386,3.228049047884462,4.6194130105979365,5.623515741490549,7.217424613813984,6.179311989210016,4.3139713710590195,5.764473550992666,4.221103725367875,2.4594316186372973,6.548282482376233,5.9902741833186015,2.5360529002402097,4.720825666089835,6.512700963425246,6.664198369291911,2.881664619320345,1.9634741239748859,6.013685569928653,6.026578770127714,4.532940288372874,7.597158683183727,2.711494906650088,6.791683858152045,2.1009776477248208,6.090641751917847,5.040892430646901,6.327328082629952,3.2942531364445142,2.9467308601403097,3.7366048753142485,4.922673592849446,3.71699089440494,4.15623479785027,6.872951851130105,2.8933622107638715,7.561631630375841,5.341985747228616,4.993221467368938,6.309067020588001,2.414135532984451,2.493134922305505,4.161081482277184,2.144046369616707,3.486714373030702,2.8011586560936985,5.648465443027314,4.399171093819823,7.581803148516525,3.1160319934471103,2.533563348214512,9.967312731471658,5.687060688339892,6.954079959320971,7.084170591577843,2.722466024471091,4.826802684285827,3.01435529297707,1.6040713236688608,5.5109619192773796,2.1731274334806563,5.425593577653059,2.8797057662822882,2.7803100990433753,3.303050084681673,2.3132458517875616,7.006298023900369,5.279842693520348,2.3950627995175777,7.216163813388774,2.786596361890807,3.5298209465286954,2.7420062108667365,5.079378111118652,5.1622906135457995,5.839203788096944,9.354249381945241,6.058532970201611,6.614857050384495,2.1667154449664223,2.4982508675278257,3.3405622690264134,5.415826257472914,3.9791107550157854,5.338780943894699,1.9335726382610239,7.164705840182799,6.12866458654003,2.933572638261024,3.0548484769956197,6.390598905552461,3.2764966656403565,8.007363716298686,6.558114538598396,3.674686619927999,5.28835856219366,4.189033824390017,5.480265122054463,4.2787282129389395,3.776103988073164,4.82273014794452,2.9467308601403097,2.2898344651775093,6.1705259991768475,1.4750848829487828,7.264067216098838,5.6845382485777325,4.534808661231747,3.7070829917717063,4.919340082442012,4.458119481174506,6.288173968112302,2.27500704749987,6.5249725982866265,3.940166750482817,9.715601700373337,7.829278760612363,4.821199978056451,5.875042803002589,2.6229303509201767,2.942983598187102,4.445594291341182,7.474030370207135,5.92148393743373,3.4369613378336026,3.2078928516413328,4.095080491901415,4.422233000683048,7.424922088210688,5.185470146066942,4.955126781261366,4.3729520979118295,4.0522419811811385,6.067165427017112,2.2898344651775093,3.0806576633452254,3.940166750482817,3.127633279725874,7.152994605492435,5.573192723781613,4.4121040446775694,2.2479275134435857,7.886854944198882,7.9329824099498465,7.25691629667084,6.670585440262214,5.845740932614582,2.9616233282869446,2.5459683691052923,7.959132576789657,4.653633311386255,7.406162478777607,5.588264894307497,3.8063240573900288,6.716579420796345,4.984589350362456,2.269033146455237,6.929672661579345,7.060371694122218,6.746447139362815,5.521364878446219,2.9873208659292536,5.970623614470605,2.37851162325373,4.667892125230234,3.192194165283345,6.816215687622865,2.851998837112446,3.3854310371935203,4.634593268445757,2.5558161550616396,8.990671283975267,9.114445113289024,3.389566811762726,6.9532652390148435,6.705563274566321,5.672425341971495,2.280956313831056,2.137503523749935,2.2203299548795554,6.561173365231147,2.9726926540042644,5.078951341394822,5.0759604132434015,3.8509993947164736,8.693835392482516,2.845991770664573,2.8399595874895316,6.111448698487674,0.7484612330040358,4.125155131322289,3.8953026213333066,8.45889870671479,9.650854294025212,8.19120730223037,6.534964249804655,2.5160151470036647,3.0373822220030804,4.701549056943071,2.9541963103868754,4.903520373723376,7.848935855524565,2.0036022366801953,5.660780334032546,8.418822422291017,5.254745197629213,8.4696011200494,6.567880042052731,6.557808298936688,3.6769443591069124,5.651051691178929,7.279842693520348,6.425089989874059,2.811471030529836,5.4799419014771305,5.184280294419382,5.713420884868084,5.452858964713811,2.4195388915137843,4.178714641175443,7.686990680088471,5.3847405872923835,2.722466024471091,7.554281862626391,3.955126781261366,5.192194165283345,9.071542807992602,5.765004246250036,5.318316841334983,3.1953475983222193,5.2671612180222604,2.269033146455237,2.702657543390911,5.6937657122177825,8.574290766120185,3.554588851677637,2.4059923596758366,3.0214797274104517,1.3785116232537298,6.010108453474289,2.438292851579147,3.089159131911238,3.0250287944915226,4.571676809970931,3.24031432933371,3.261530815243406,2.6712933724815757,3.6542063779442917,2.9873208659292536,5.8060662260890314,4.371558862611963,6.4984104488609775,6.229587922740652,2.2957230245399685,5.178714641175443,5.463687842201262,4.884597920990064,6.157852169141737,3.4672794804599825,2.867896463992655,2.611172380044005,5.535430914871279,0,3.4776773275653072,9.582254908357664,3.950468414150123,4.8894735425311096,4.850999394716474,3.300123724569014,8.009660689646706,7.626950122372515,9.04105676230812,6.564835417010355,3.3175935046234715,7.380331184220349,2.0214797274104517,4.235727059838059,4.334139178838237,5.692092375427572,5.592457037268081,1.996388746447621,2.070389327891398,5.8052924556007115,6.60584986719498,4.088311235888661,2.521050736900963,3.5310694927259543,7.058316495590823,4.044394119358453,3.0513721017210256,6.008092420948722,5.605257262939004,8.220281580441913,6.088735246190284,2.137503523749935,6.637204481902295,0.8155754288625726,2.2387868595871168,5.676662334857967,1.7865963618908067,8.05642095591347,7.5673476962842585,7.295355692891673,6.394033895367782,6.582706526780788,4.43095427138573,4.915520900751958,3.710393192170516,3.8845979209900645,5.135042286233651,10.267009017093484,5.336640446408735,3.0992952043377753,6.836050355058069,7.896998342338911,6.558114538598396,5.10936055940423,5.85872702377863,3.785550551739256,3.8042601156347384,2.7484612330040354,2.883620816285671,3.576522137920503,4.104336659814735,5.86195536414487,4.294253136444514,8.44136727845972,5.497931651896279,7.5808238499627985,2.8278190246173196,6.260590274730996,2.229587922740652,6.218781167784069,10.212070748654236,4.400537929583729,6.543805175964397,7.563234413022304,6.203788453308596,6.676803353873854,4.070389327891398,6.340918270272116,2.063502942306158,7.151270288790165,4.228049047884462,4.563768278452033,5.4329594072761065,3.9373443921502322,5.580748491763774,4.279471295644468,7.959132576789657,5.739578112048051,2.7463127664254587,5.707635219545296,2.0250287944915226,3.4776773275653072,7.925940095994448,4.478971805032942,3.24031432933371,6.836807807844575,6.2502036564992975,3.82273014794452,0.918386234446348,3.258518924711301,2.1634987322828794,2.662205499653617,6.687060688339892,2.2357270598380583,5.0259146987507926,5.443606651475615,2.980025300238734,3.1193561770396756,2.8399595874895316,4.914564523493939,2.782408564927373,6.282625134191674,1.2016338611696504,5.269781238274379,3.0513721017210256,4.649615459063409,4.896271848807375,4.721919445551544,3.6182386555954547,5.045268215138534,5.020591094773247,5.305240965954483,5.663913842115978,3.3881895371560837,8.836744701966083,6.976363635732762,3.925049964727359,6.802451741275622,4.664482840364682,2.3589588258323295,6.127014141582809,7.1945598860226125,4.475084882948782,4.301587646603187,3.7441610955704103,4.05484847699562,5.464668267003444,7.33360262826828,5.6082178530214595,7.1320626778543845,4.993674361750585,5.522620761042079,7.366234775533047,7.166815851761945,3.7687136570304847,2.321928094887362,6.7873802222963375,4.788685710613533,6.025250321561575,8.162995472417697,3.4019034716079584,6.238786859587116,2.3248106034204836,7.831687267083084,3.4802651220544627,3.871843648509318,6.548128323585586,3.0268000593437154,3.1953475983222193,6.2600256559614555,5.931210274803932,7.80967167790706,3.169925001442312,3.894332742277694,2.298658315564515,6.215678596607928,7.873013392989595,7.709152756902506,8.461602224618334,4.150559676575382,7.653203362033977,9.24186401223682,5.725468512632615,4.0925457415435655,3.575312330687437,6.519321734629434,2.5185351389821804,6.137913322088407,7.904544309292717,2.568032104771279,9.816679697610459,0.765534746362977,8.646594709581654,5.037821465434967,4.388878338811989,7.9616812016062815,1.7782085763980877,8.912589990852627,3.5008020530571575,7.473786911614368,7.182890892745479,2.6205864104518777,6.628919373511362,4.842476062993939,6.013462259806563,7.1851727751412495,4.8359240742543745,4.918386234446348,4.783980413683795,4.285402218862249,6.340918270272116,3.5570424152637194,4.030336078370959,0.8073549220576041,2.2957230245399685,8.893574570329104,2.3923174227787602,7.015805293913353,8.764407200357251,6.147306698780294,5.502075956045791,8.694706111949401,2.440952198029637,8.447992374957895,4.943921326553485,6.072963271555224,8.18685706717715,5.508111681193465,2.327687364176047,2.6322682154995127,5.853496703702325,2.4724877714627436,3.412781525338476,5.751677945687579,5.8774988727835344,5.890446692679906,8.622783966156124,6.203396948546115,5.524189078449365,4.679198570566922,2.3757345385831563,7.809543066804601,3.5716768099709313,6.39694722007283,3.097610796626422,4.9415758618182375,6.668317719915721,3.0925457415435655,6.941106310946432,7.471431358002323,4.633431210355632,2.454175893185802,5.760487101811031,4.112700132749362,2.560714954474479,2.885574364371426,5.8060662260890314,5.969012307516316,3.0214797274104517,4.37016428054021,3.1811025507537978,9.463033855317493,5.102238193930738,7.537684337147519,5.960001932068081,4.451540833017832,5.931683057059805,4.126807703142035,9.109647857806625,8.028624401408491,3.2645364309990255,7.394977085526471,6.152589069287125,6.022367813028454,4.51853513898218,3.344828496997441,6.810314561497258,3.0036022366801953,7.890811454817677,8.317774372804099,7.237735785231934,8.515305605451724,8.213638902198852,5.154615611326894,2.039138393906958,7.296273846706719,6.905206468795705,2.3673710656485296,2.0942360698457656,2.9145645234939397,2.2172307162206693,5.641256998726767,3.303050084681673,2.62993940943954,7.220233204382192,6.489767697008256,10.809687753488749,5.260778431893426,8.11134436328308,4.9941271140031285,2.7070829917717063,5.856985689782205,6.381629467033664,4.493134922305505,10.642557179595622,5.439955517478156,6.790772037862,2.5058909297299574,4.37920505996061,6.33646192818282,2.9523335663696857,2.811471030529836,5.323730337521348,2.763411574470007,8.089159131911238,6.435461914479276,2.4646682670034443,7.870364719583405,3.396433531250992,7.305696981080096,4.020591094773247,5.187451054027326,2.298658315564515,6.038918989292302,5.369815424283912,4.422905742612183,4.024142345897803,4.926948247949772,5.348728154231077,6.907251224292033,4.861459166361513,6.596487956856693,4.593353770980297,5.9502351004856875,7.814614507639093,6.638797899951696,8.79720757285578,2.176322772640463,7.7974669920591575,2.7398481026993275,3.1538053360790355,3.6701605141266334,2.513490745588118,5.307064162255372,5.076815597050831,2.533563348214512,6.678494507773734,2.7092906357233577,2.7355221772965375,2.782408564927373,7.039796407620025,5.938521045865967,4.733354340613827,6.132988042627442,7.991691898336964,6.419033185471022,2.1858665453113337,5.449891473010944,2.6064422281316078,3.7311832415722,4.332707933640605,6.56498780095989,5.434294617959925,5.893362210763872,10.084529386649065,7.932923373835633,3.0823619695574735,2.440952198029637,3.1842802944193824,6.1430260038899736,6.387500406480984,6.681449265314958,4.324810603420484,2.327687364176047,2.722466024471091,5.569551851083613,7.527555198794536,6.632559052117953,2.9145645234939397,6.135452784185173,5.424922088210688,2.2357270598380583,2.8011586560936985,9.8361608416947,7.193968819421969,3.097610796626422,2.655351828612554,5.553360503335328,3.169925001442312,6.173127433480656,3.817623257511431,5.778208576398088,5.7554217347342425,8.571145863602528,5.470862199413702,3.1602748314085933,8.3389146195993,2.440952198029637,3.982765462583646,2.140778655782796,4.3197624276692475,2.292781749227846,6.864928972289788,5.969933274697856,4.950468414150123,3.4581194811745064,9.054360117597708,2.232660756790275,5.648753032989572,4.827310943949667,6.609104846618959,2.920293300211007,5.726013748860151,4.484138131201669,6.010779838753242,6.51317488460363,8.731081391094504,7.529742876502924,2.7782085763980877,6.546122758857796,4.635173946674924,5.533563348214512,6.2992080183872785,2.657640005207824,11.260084480727762,3.442280035252584,5.339850002884624,6.091065077930535,7.352175878326325,7.78959885005233,3.2418401835646704,4.584962500721156,5.035183996618336,4.898692072560488,4.393690764187454,6.686360452903688,4.792855352362489,4.044394119358453,5.077670274232752,7.061344188288391,4.06522762277562,6.802968651005583,3.412781525338476,7.953207027098,3.5643781685650637,2.790772037862,3.350497247084133,8.870919495610579,4.663913842115978,2.2234225499349374,2.6667565918848033,9.754286253961022,6.500324049284198,7.87670110864523,1.4168397419128291,4.017921907997263,6.177519202597814,5.47800105583793,8.031053274444373,7.831180547126736,6.461724991502711,3.948600847493356,2.319039815562536,4.898208352508718,6.20945336562895,7.562776656791846,5.542258049766918,2.6016965164809576,6.711081862306696,2.0565835283663674,8.630303533275018,4.854494418154875,3.4892860226258766,4.601102203376409,5.932391940068527,6.822602695756541,3.039138393906958,4.397118409042582,4.078951341394822,6.198886990956932,2.298658315564515,7.472731449369055,5.0669502439246275,5.059614856297222,2.9708536543404835,6.608661417987563,5.51443791382931,2.176322772640463,5.642412772905056,6.8820316083521,5.234577959784565,7.163498732282879,3.212569338850806,2.2898344651775093,8.307656205306488,3.423578170981797,1.3840498067951599,2.7676547982373463,10.679004987548526,3.2645364309990255,3.9990980337056072,5.649040565634448,4.793375711264754,4.203201156316611,7.668814088419117,5.911931212309049,3.4581194811745064,5.445263208140443,8.913188619508889,4.936402377725063,8.645802514606062,6.207111961207714,5.749266083814806,4.4376272483189485,8.580786171355323,2.189033824390017,2.341985747228616,4.06522762277562,4.804260115634738,4.849999259466098,5.075532631167357,4.056583528366367,9.188044796421696,3.8439838440483265,4.634012356402116,4.8104431038451745,8.686815644595628,2.521050736900963,4.016139702655253,6.212180210044431,7.545505100691072,3.904002316283692,3.71699089440494,2.1667154449664223,4.381283372503783,6.564835417010355,6.563005553062399,5.911212201558612,6.247547806890298,5.477677327565307,8.292781749227846,13.587327482778312,2.9011082430145123,2.845991770664573,3.7644735509926663,9.182369521950886,5.921959917326167,1.9297909977185974,6.910972451655129,8.149950301681047,4.3729520979118295,5.38404980679516,2.063502942306158,5.977508904540138,1.3673710656485294,4.067810783789621,2.1176950426697547,9.199893148608071,3.7081872360207084,4.628773595201645,2.488000770834068,6.501439145158873,6.706254253573203,3.0908534304511135,5.86616616903904,4.155425431747197,6.060695931687554,4.570462931026041,3.2357270598380583,5.341274183692174,6.656925339794245,5.588564737401351,6.353146825498083,3.48155728070859,2.4698859762744636,4.872828759534886,4.34766565630097,2.877744249949002,6.019479533703628,3.465974464504069,7.536363792395467,5.132576842670009,2.0531113364595623,6.283366212949919,3.699329525673156,8.199279721336625,6.8203066288938246,6.068240861312821,4.598126959919604,7.979739567128753,6.258895755075436,7.258707352196828,5.394376944957055,3.0125686735030555,6.862203399053224,2.8318772411916733,5.164705840182799,6.431288654291593,4.375734538583156,4.8594726668519375,2.9392265777282085,4.302319050948834,4.397118409042582,5.169123281476757,3.761285273361619,7.240600549955795,3.7387678368001915,5.678916986679839,4.3334237337251915,5.530445354692906,8.431288654291592,2.521050736900963,1.2265085298086797,2.2539892662307865,5.23687524536694,2.298658315564515,6.398316163400203,8.348374075191163,6.043300754364182,9.401604869714957,4.534186139090097,9.069933046375542,5.486714373030702,4.96624587322494,6.307246355226617,5.51127826508303,5.970853654340483,5.342341397431548,6.219555769166913,2.319039815562536,7.516566771496865,6.652773284510777,9.109908988534402,5.338067797542616,2.9981955031532523,6.506843098969815,3.04788732939655,2.526068811667588,3.414135532984451,6.295723024539968,2.257010618206024,5.500164679492167,5.333065878076355,5.253232938538032,3.619413010597937,4.695437110405369,4.37364821133469,8.356407951394893,6.039796407620025,5.908332569782425,1.74416109557041,5.666472568842073,5.9586107120558465,2.430285272977781,8.487277313213783,2.927896453728821,6.9920886090139955,5.7615512324444795,5.906409617094016,2.572889668420581,2.9763636357327616,3.311793717753649,5.325530331567558,6.075532631167357,8.785648628652076,5.301953395126096,6.687620632084895,10.60719158640291,5.2543672814408335,6.976821852360685,2.655351828612554,8.033092575523664,11.658760423755194,6.231317197152663,5.008092420948722,7.824449651364088,2.8155754288625725,5.656496370553495,3.218781167784069,6.101187815259738,8.328001662149257,5.576522137920503,7.176621973161577,2.0565835283663674,6.5787877956769725,6.602142090697982,2.7441610955704103,4.727375938594897,2.2172307162206693,4.552131108253784,10.754979337466484,5.651338766105931,7.449148645375437,8.21179097625254,5.769771739249448,7.09876903815488,2.5921580021253603,7.971830914780502,6.234194722953812,2.1984941536390834,6.955824240806441,0.16349873228287956,4.680324356844016,2.5801454844233804,5.583458910130768,4.845991770664573,2.7070829917717063,4.18824265626441,7.27733390042982,9.535217045453418,6.304876050044889,6.982080909763738,3.0925457415435655,7.051372101721025,7.022922589055629,2.9392265777282085,6.653203362033977,8.22838581704935,4.960697039304312,5.564378168565064,2.782408564927373,0,3.9354597478052895,5.525755692829486,1.0976107966264221,7.211790976252541,4.477677327565307,2.4436066514756147,7.966303561412599,6.19021976366016,5.592457037268081,10.665637927637095,3.277984747299765,7.8269297659895765,3.5643781685650637,5.612057559563065,8.766462652425766,2.662205499653617,6.920412408160787,5.290940402403678,7.502871574574138,5.2539892662307865,3.1984941536390834,3.0089887832272546,7.2259304111710545,0,4.919340082442012,5.334496768390418,4.654206377944292,2.327687364176047,6.705839705883682,11.02948049374083,2.4594316186372973,6.294253136444514,7.172927489603836,5.573192723781613,5.5804470195986635,6.341274183692174,6.768713657030485,4.5855634984556914,5.609400390006429,6.10055722078293,6.140165134599453,2.37851162325373,4.726831217032493,3.127633279725874,2.414135532984451,4.561326136453918,2.211012193485512,3.6147098441152083,5.901832305785489,5.903279342056139,3.84197311892718,3.0443941193584534,4.77663042305105,2.7949356628035362,2.5897634869849773,2.4436066514756147,2.5160151470036647,4.623515741490549,6.6059979802325195,9.051127353591394,3.017921907997262,3.9212458885855885,2.2387868595871168,5.621465834460144,6.001351892999959,6.605701738949982,2.6530600171045644,3.4698859762744636,2.456806149230474,4.528571318870758,10.08413071995014,3.1953475983222193,6.061128135118254,4.434961760187254,8.556735947914529,6.821837746088096,5.895302621333307,4.783456654360239,6.1147834472498515,8.8634739090894,2.6182386555954547,5.862203399053224,3.319039815562536,6.531225485053606,2.2265085298086795,5.50303064564116,6.549977142771825,2.582556003014061,0.45417589318580215,8.258094872866513,5.864186144654281,0,4.110196177754199,5.541948425333733,5.139960569545456,5.166715444966422,4.661635602335959,2.6016965164809576,1.0565835283663676,4.867402305727543,6.390082950951663,6.790641730763425,4.185073637876762,5.681168120416691,4.409390936137702,5.399854673572431,2.752748591407134,5.553974808237472,4.238786859587116,6.451211111832329,3.2555007331483865,8.627898615871748,4.5588798534754424,5.639521599510833,5.29020320512607,8.07804403634318,5.592756010441026,6.952217064973338,2.6392321632492775,3.2942531364445142,5.633721812641012,9.325777655214823,5.554588851677638,8.364703739582406,12.206535173103617,6.2268938135713885,6.058532970201611,2.996388746447621,2.169925001442312,3.2539892662307865,2.3645724322958563,2.996388746447621,2.5084286525318573,5.370861740085285,7.2461230165968376,3.19377174339668,2.4276061727818994,5.25285462595139,8.366191054189278,7.062963556559086,2.0531113364595623,7.846744023244868,3.581351247168777,6.948834425578686,6.412442824775613,2.899175630480513,4.786073551578548,8.971342367308225,7.2193621578077165,2.885574364371426,7.801029283808189,3.7813597135246595,5.6884601404244926,3.361768359419153,2.280956313831056,8.531927241876955,7.295631200395783,5.566206293465506,3.9990980337056072,6.145066014185894,3.0373822220030804,6.666614587352882,2.655351828612554,5.343763122149601,6.696689389537669,4.609991295212678,2.3950627995175777,7.005961325845812,2.2957230245399685,3.7548875021634687,6.883987308105472,3.2433644256936605],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle AdaBoostRegressor()<br>vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"TotalGHGEmissions_predAB_log"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"TotalGHGEmissions_test_log"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# modèle AdaBoostRegressor\n","# réglage des paramètre pour la gridsearch\n","n_estimatorsAB_log = np.logspace(0, 2, 30, dtype=int)\n","param_gridAB_log = {\n","    'adaboostregressor__n_estimators': n_estimatorsAB_log,\n","    'adaboostregressor__loss': ['linear', 'square', 'exponential']\n","}\n","\n","GridAB_log, \\\n","BestParametresAB_log, \\\n","ScoresAB_log, \\\n","TotalGHGEmissions_pred_logAB, \\\n","figAB_log = reg_modelGrid(model=AdaBoostRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBNumM_train,\n","                         X_test=BEBNumM_test,\n","                         y_train=TotalGHGEmissions_train_log.ravel(),\n","                         y_test=TotalGHGEmissions_test_log,\n","                         y_test_name='TotalGHGEmissions_test_log',\n","                         y_pred_name='TotalGHGEmissions_predAB_log',\n","                         score=score,\n","                         param_grid=param_gridAB_log)\n","\n","print(BestParametresAB_log)\n","print(ScoresAB_log)\n","figAB_log.show()\n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.578263381246869,1.5867031508961382,1.5855603326629437,1.5828322333540568,1.582044678161544,1.5835104559393327,1.5761334113018464,1.5636524762389605,1.563621693383111,1.5624661023039774,1.562279379242657,1.5648673254867513,1.5579269162997307,1.5613829581436358,1.558582229256681,1.5621134445416036,1.5658880390850451,1.5691522690610324,1.578863561122797,1.5869832295509965,1.5867895992073107,1.5955746072178802,1.5890947666432766,1.5974672548338706,1.5967596517090803,1.6030196858824155,1.6038234282687671,1.609847416435549,1.6094492480398395,1.6101790560489317]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.615199381243297,1.6132656495496034,1.6221878568711583,1.6116007257199865,1.60903707584579,1.6119772888469557,1.6065685286595373,1.5980103119912705,1.594403426069137,1.593285271215268,1.5860919162274831,1.5939652107796811,1.5891205497952896,1.5926014118064076,1.589268118513315,1.594824788726524,1.595233616810513,1.5981117687282689,1.6080573420810396,1.619515138942843,1.6161307965729308,1.626678959840417,1.6191384570160046,1.6265140221554895,1.6253571737719463,1.6281409858940012,1.6285371707460228,1.6366970553641653,1.636816450161993,1.6340373086935203]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"y":[1.5413273812504409,1.560140652242673,1.5489328084547291,1.554063740988127,1.5550522804772982,1.5550436230317097,1.5456982939441555,1.5292946404866505,1.532839960697085,1.5316469333926868,1.538466842257831,1.5357694401938216,1.5267332828041718,1.530164504480864,1.5278963400000471,1.5294021003566833,1.5365424613595773,1.540192769393796,1.5496697801645545,1.5544513201591499,1.5574484018416905,1.5644702545953435,1.5590510762705487,1.5684204875122518,1.5681621296462143,1.5778983858708298,1.5791096857915115,1.5829977775069326,1.5820820459176859,1.5863208034043432]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.6196272313458222,1.6004876089927178,1.6227170204237131,1.609659540638452,1.6032680206741792,1.5931187514747036,1.6038160745040373,1.601883926779716,1.5816609433313278,1.586624331994032,1.585629734479325,1.5945817200114256,1.5916411581471237,1.5955389109195985,1.583293239234723,1.5935059274229517,1.593638104038709,1.5935678255404642,1.6058007743633425,1.6144021956926515,1.6099637186659421,1.6220000673198691,1.6097594104249744,1.6202746580406995,1.6246500806472415,1.6190583513536323,1.6181553820245562,1.6248629070881082,1.6240302320651498,1.62520755585076],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.535222657319271,1.5685247773545905,1.5438602601379536,1.5510575717054857,1.5427117351565045,1.5410719628469338,1.5293004175951466,1.519293687396055,1.539018207844752,1.5155033111217515,1.5264648764917912,1.5218850893025624,1.5234637878365975,1.5215513089466155,1.5142990906916771,1.5112590316989631,1.5309325817560602,1.5288820199850088,1.5345374834159535,1.540535028339313,1.5425778530937753,1.5502484497169362,1.5433667031614515,1.554773904531847,1.555385130526979,1.5693306304131607,1.5633687092618562,1.5739960812121399,1.5732007265648127,1.574260866544234],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5414647924175173,1.5495519259141244,1.5489960362292343,1.5494794461268004,1.5627470192839694,1.56786749434507,1.558691142002665,1.539654210633877,1.5281203710257258,1.5463506074687825,1.548183012844574,1.5480434703761068,1.5252127665722557,1.5348331799174422,1.536555910929424,1.5444025204700813,1.5367652258569242,1.5520757236213052,1.5600421255617876,1.5687910627758417,1.568987932823945,1.5769243510159425,1.568915084835593,1.5776562848746887,1.5804009978043454,1.5853903152321271,1.5913486015653642,1.5835843518415158,1.5901402044483186,1.5923651884801624],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.5733810823700902,1.5879289277699118,1.579595371877339,1.5852052605001645,1.5838162993133653,1.5885255369598272,1.575747088758185,1.5518520535870506,1.5558297449115197,1.5602133752476905,1.560462800503411,1.5603272150594123,1.5533188757432386,1.556333954923245,1.5595557808442349,1.5610286548320893,1.563979062318681,1.561726371566205,1.580218632621136,1.5795294025315463,1.5869522166296486,1.5916369914243274,1.594918331078027,1.5986706534845603,1.5906279489409687,1.6002870128856819,1.6106452896435222,1.6212554095735068,1.6075112248378647,1.6180462275478265],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1,1,1,1,2,2,3,3,4,4,5,6,7,9,10,12,14,17,20,23,28,32,38,45,52,62,72,85,100],"xaxis":"x","y":[1.621621142781644,1.627022514449347,1.6326329746464785,1.6187593477993811,1.6176803163797002,1.626968534070128,1.613112333649198,1.605578502798104,1.6134791998022293,1.6036388856876302,1.5906564718941834,1.5994991326842496,1.595997993199438,1.598657436011277,1.5992071245833466,1.6003710882839333,1.604125221454851,1.609509404592178,1.6137187896517662,1.6316584584156302,1.6254662748232425,1.6370631766123267,1.6285143037163368,1.6359607732375585,1.632734100625866,1.641032119527475,1.6355991588485355,1.6455383324624733,1.6523638522830522,1.6410154418216751],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle AB pour le paramètre<br>adaboostregressor__loss=exponential<br>en fonction de l'hyperparamètre n estimators"},"xaxis":{"title":{"text":"n estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE AdaBoostRegressor\n","# pour le meilleur paramètre loss\n","FigRMSEGRidAB_log = visuRMSEGrid(AdaBoostRegressor(), 'AB', n_estimatorsAB_log,\n","                             'n estimators', GridAB_log, BestParametresAB_log,\n","                             'adaboostregressor__loss')\n","FigRMSEGRidAB_log.show()\n","if write_data is True:\n","    FigRMSEGRidAB_log.write_image('./Figures/EmissionsGraphRMSEAB_log.pdf')\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lasso()</th>\n","      <th>Ridge()</th>\n","      <th>ElasticNet()</th>\n","      <th>KNeighborsRegressor()</th>\n","      <th>RandomForestRegressor()</th>\n","      <th>AdaBoostRegressor()</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>R²</th>\n","      <td>0.392858</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>399.012393</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>104.180260</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>0.339807</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>416.080075</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>94.693823</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.345465</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>414.293389</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>95.009466</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.476201</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>370.615649</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>87.382326</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.842628</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>203.144550</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>63.700648</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.735593</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>263.316554</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>98.513848</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Lasso()     Ridge()  ElasticNet()  KNeighborsRegressor()  \\\n","R²      0.392858         NaN           NaN                    NaN   \n","RMSE  399.012393         NaN           NaN                    NaN   \n","MAE   104.180260         NaN           NaN                    NaN   \n","R²           NaN    0.339807           NaN                    NaN   \n","RMSE         NaN  416.080075           NaN                    NaN   \n","MAE          NaN   94.693823           NaN                    NaN   \n","R²           NaN         NaN      0.345465                    NaN   \n","RMSE         NaN         NaN    414.293389                    NaN   \n","MAE          NaN         NaN     95.009466                    NaN   \n","R²           NaN         NaN           NaN               0.476201   \n","RMSE         NaN         NaN           NaN             370.615649   \n","MAE          NaN         NaN           NaN              87.382326   \n","R²           NaN         NaN           NaN                    NaN   \n","RMSE         NaN         NaN           NaN                    NaN   \n","MAE          NaN         NaN           NaN                    NaN   \n","R²           NaN         NaN           NaN                    NaN   \n","RMSE         NaN         NaN           NaN                    NaN   \n","MAE          NaN         NaN           NaN                    NaN   \n","\n","      RandomForestRegressor()  AdaBoostRegressor()  \n","R²                        NaN                  NaN  \n","RMSE                      NaN                  NaN  \n","MAE                       NaN                  NaN  \n","R²                        NaN                  NaN  \n","RMSE                      NaN                  NaN  \n","MAE                       NaN                  NaN  \n","R²                        NaN                  NaN  \n","RMSE                      NaN                  NaN  \n","MAE                       NaN                  NaN  \n","R²                        NaN                  NaN  \n","RMSE                      NaN                  NaN  \n","MAE                       NaN                  NaN  \n","R²                   0.842628                  NaN  \n","RMSE               203.144550                  NaN  \n","MAE                 63.700648                  NaN  \n","R²                        NaN             0.735593  \n","RMSE                      NaN           263.316554  \n","MAE                       NaN            98.513848  "]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["Scores = ScoresLasso.append(\n","    [ScoresRidge, ScoresEN, ScoreskNN, ScoresRF, ScoresAB])\n","Scores\n"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lasso()</th>\n","      <th>Ridge()</th>\n","      <th>ElasticNet()</th>\n","      <th>KNeighborsRegressor()</th>\n","      <th>RandomForestRegressor()</th>\n","      <th>AdaBoostRegressor()</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>0.121578</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>1.952310</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>1.629347</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>0.186454</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>1.878833</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>1.573171</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.171240</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.896320</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.585481</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.438742</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.560553</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.260167</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.652515</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.227907</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.900115</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.438320</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.561140</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.280275</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Lasso()   Ridge()  ElasticNet()  KNeighborsRegressor()  \\\n","R²_log    0.121578       NaN           NaN                    NaN   \n","RMSE_log  1.952310       NaN           NaN                    NaN   \n","MAE_log   1.629347       NaN           NaN                    NaN   \n","R²_log         NaN  0.186454           NaN                    NaN   \n","RMSE_log       NaN  1.878833           NaN                    NaN   \n","MAE_log        NaN  1.573171           NaN                    NaN   \n","R²_log         NaN       NaN      0.171240                    NaN   \n","RMSE_log       NaN       NaN      1.896320                    NaN   \n","MAE_log        NaN       NaN      1.585481                    NaN   \n","R²_log         NaN       NaN           NaN               0.438742   \n","RMSE_log       NaN       NaN           NaN               1.560553   \n","MAE_log        NaN       NaN           NaN               1.260167   \n","R²_log         NaN       NaN           NaN                    NaN   \n","RMSE_log       NaN       NaN           NaN                    NaN   \n","MAE_log        NaN       NaN           NaN                    NaN   \n","R²_log         NaN       NaN           NaN                    NaN   \n","RMSE_log       NaN       NaN           NaN                    NaN   \n","MAE_log        NaN       NaN           NaN                    NaN   \n","\n","          RandomForestRegressor()  AdaBoostRegressor()  \n","R²_log                        NaN                  NaN  \n","RMSE_log                      NaN                  NaN  \n","MAE_log                       NaN                  NaN  \n","R²_log                        NaN                  NaN  \n","RMSE_log                      NaN                  NaN  \n","MAE_log                       NaN                  NaN  \n","R²_log                        NaN                  NaN  \n","RMSE_log                      NaN                  NaN  \n","MAE_log                       NaN                  NaN  \n","R²_log                        NaN                  NaN  \n","RMSE_log                      NaN                  NaN  \n","MAE_log                       NaN                  NaN  \n","R²_log                   0.652515                  NaN  \n","RMSE_log                 1.227907                  NaN  \n","MAE_log                  0.900115                  NaN  \n","R²_log                        NaN             0.438320  \n","RMSE_log                      NaN             1.561140  \n","MAE_log                       NaN             1.280275  "]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["ScoresLog = ScoresLasso_log.append(\n","    [ScoresRidge_log, ScoresEN_log, ScoreskNN_log, ScoresRF_log,\n","     ScoresAB_log]).rename('{}_log'.format)\n","ScoresLog\n"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lasso()</th>\n","      <th>Ridge()</th>\n","      <th>ElasticNet()</th>\n","      <th>KNeighborsRegressor()</th>\n","      <th>RandomForestRegressor()</th>\n","      <th>AdaBoostRegressor()</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>R²</th>\n","      <td>0.392858</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>399.012393</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>104.180260</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>0.339807</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>416.080075</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>94.693823</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.345465</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>414.293389</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>95.009466</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.476201</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>370.615649</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>87.382326</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.842628</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>203.144550</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>63.700648</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.735593</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>263.316554</td>\n","    </tr>\n","    <tr>\n","      <th>MAE</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>98.513848</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>0.121578</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>1.952310</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>1.629347</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>0.186454</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>1.878833</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>1.573171</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.171240</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.896320</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.585481</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.438742</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.560553</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.260167</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.652515</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.227907</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.900115</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>R²_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.438320</td>\n","    </tr>\n","    <tr>\n","      <th>RMSE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.561140</td>\n","    </tr>\n","    <tr>\n","      <th>MAE_log</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.280275</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Lasso()     Ridge()  ElasticNet()  KNeighborsRegressor()  \\\n","R²          0.392858         NaN           NaN                    NaN   \n","RMSE      399.012393         NaN           NaN                    NaN   \n","MAE       104.180260         NaN           NaN                    NaN   \n","R²               NaN    0.339807           NaN                    NaN   \n","RMSE             NaN  416.080075           NaN                    NaN   \n","MAE              NaN   94.693823           NaN                    NaN   \n","R²               NaN         NaN      0.345465                    NaN   \n","RMSE             NaN         NaN    414.293389                    NaN   \n","MAE              NaN         NaN     95.009466                    NaN   \n","R²               NaN         NaN           NaN               0.476201   \n","RMSE             NaN         NaN           NaN             370.615649   \n","MAE              NaN         NaN           NaN              87.382326   \n","R²               NaN         NaN           NaN                    NaN   \n","RMSE             NaN         NaN           NaN                    NaN   \n","MAE              NaN         NaN           NaN                    NaN   \n","R²               NaN         NaN           NaN                    NaN   \n","RMSE             NaN         NaN           NaN                    NaN   \n","MAE              NaN         NaN           NaN                    NaN   \n","R²_log      0.121578         NaN           NaN                    NaN   \n","RMSE_log    1.952310         NaN           NaN                    NaN   \n","MAE_log     1.629347         NaN           NaN                    NaN   \n","R²_log           NaN    0.186454           NaN                    NaN   \n","RMSE_log         NaN    1.878833           NaN                    NaN   \n","MAE_log          NaN    1.573171           NaN                    NaN   \n","R²_log           NaN         NaN      0.171240                    NaN   \n","RMSE_log         NaN         NaN      1.896320                    NaN   \n","MAE_log          NaN         NaN      1.585481                    NaN   \n","R²_log           NaN         NaN           NaN               0.438742   \n","RMSE_log         NaN         NaN           NaN               1.560553   \n","MAE_log          NaN         NaN           NaN               1.260167   \n","R²_log           NaN         NaN           NaN                    NaN   \n","RMSE_log         NaN         NaN           NaN                    NaN   \n","MAE_log          NaN         NaN           NaN                    NaN   \n","R²_log           NaN         NaN           NaN                    NaN   \n","RMSE_log         NaN         NaN           NaN                    NaN   \n","MAE_log          NaN         NaN           NaN                    NaN   \n","\n","          RandomForestRegressor()  AdaBoostRegressor()  \n","R²                            NaN                  NaN  \n","RMSE                          NaN                  NaN  \n","MAE                           NaN                  NaN  \n","R²                            NaN                  NaN  \n","RMSE                          NaN                  NaN  \n","MAE                           NaN                  NaN  \n","R²                            NaN                  NaN  \n","RMSE                          NaN                  NaN  \n","MAE                           NaN                  NaN  \n","R²                            NaN                  NaN  \n","RMSE                          NaN                  NaN  \n","MAE                           NaN                  NaN  \n","R²                       0.842628                  NaN  \n","RMSE                   203.144550                  NaN  \n","MAE                     63.700648                  NaN  \n","R²                            NaN             0.735593  \n","RMSE                          NaN           263.316554  \n","MAE                           NaN            98.513848  \n","R²_log                        NaN                  NaN  \n","RMSE_log                      NaN                  NaN  \n","MAE_log                       NaN                  NaN  \n","R²_log                        NaN                  NaN  \n","RMSE_log                      NaN                  NaN  \n","MAE_log                       NaN                  NaN  \n","R²_log                        NaN                  NaN  \n","RMSE_log                      NaN                  NaN  \n","MAE_log                       NaN                  NaN  \n","R²_log                        NaN                  NaN  \n","RMSE_log                      NaN                  NaN  \n","MAE_log                       NaN                  NaN  \n","R²_log                   0.652515                  NaN  \n","RMSE_log                 1.227907                  NaN  \n","MAE_log                  0.900115                  NaN  \n","R²_log                        NaN             0.438320  \n","RMSE_log                      NaN             1.561140  \n","MAE_log                       NaN             1.280275  "]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["CompareScores = Scores.append(ScoresLog)\n","if write_data is True:\n","    CompareScores.to_latex('./Tableaux/EmmisionsScoresModèles.tex')\n","CompareScores\n"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"'R²'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m~/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32m~/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'R²'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_32096/3170751436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mrow_titles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'R²'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MAE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     shared_xaxes=True)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mScores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'R²'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mScores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mScores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'R²'"]}],"source":["fig = make_subplots(3,\n","                    2,\n","                    column_titles=(\"Consommation brute\", \"Consommation log2\"),\n","                    row_titles=('R²', 'RMSE', 'MAE'),\n","                    shared_xaxes=True)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['R²']), row=1, col=1)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['RMSE']), row=2, col=1)\n","fig.add_trace(go.Bar(x=Scores.index, y=Scores['MAE']), row=3, col=1)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['R²']), row=1, col=2)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['RMSE']), row=2, col=2)\n","fig.add_trace(go.Bar(x=ScoresLog.index, y=ScoresLog['MAE']), row=3, col=2)\n","fig.update_layout(\n","    title_text=\"Comparaison des scores des modèles d'émissions\",\n","    showlegend=False)\n","fig.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Modèle de prédiction sur la consommation énergétique\n","(SiteEnergyUse) avec les données catégorielles"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["BEBCat = pd.read_csv('BEBCat.csv')"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
